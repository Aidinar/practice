
\renewcommand{\figurename}{\protect\bf Figure}
\renewcommand{\tablename}{\protect\bf Table}
\renewcommand{\bibname}{\protect\rmfamily References}

\def\stat{temnov}

\def\tit{AN APPROACH TO ACTUARIAL MODELING\\
WITH QUASI-MONTE CARLO: SIMULATION OF~RANDOM SUMS DEPENDING
ON~STOCHASTIC FACTORS$^*$}
\def\titkol{An approach to actuarial modeling with Quasi-Monte Carlo:
simulation of random sums depending on stochastic factors}

\def\autkol{G.~Temnov and S.~Kucherenko}
\def\aut{G.~Temnov$^1$ and S.~Kucherenko$^2$}

\titel{\tit}{\aut}{\autkol}{\titkol}

{\renewcommand{\thefootnote}{\fnsymbol{footnote}}\footnotetext[1]
{Part of the work was performed within ``PRisMa Lab'' in Vienna University of Technology.}}

\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Edgeworth Centre for Financial Mathematics, University
College Cork, Ireland, g.temnov@ucc.ie}
\footnotetext[2]{CPSE, Imperial College, London, UK, s.kucherenko@ic.ac.uk}


\Abste{The problem of estimating the characteristics of a random
sum, when the number of summands is also random, is addressed. The
considered case includes an additional stochastic factor: although
the summed random variables come from a distribution of a known
form, the parameters of this distribution are stochastic and can
themselves be viewed as random variables (with known distributions).
The Quasi-Monte-Carlo (QMC) techniques are used to handle this
problem and to analyze its efficiency relative to the regular
Monte-Carlo (MC) simulation methods. The typical area of the application
of the investigations is actuarial practice which often deals with
random sums of financial losses. Besides actuarial applications, the
proposed method may be useful in application to certain problems in
informatics, related to the aggregation of heavy-tailed data. }

\KWE{actuarial modeling; quasi-Monte-Carlo simulation; random sums}

      \vskip 24pt plus 9pt minus 6pt

      \thispagestyle{headings}

      \begin{multicols}{2}

      \label{st\stat}

\section{Introduction}


\subsection{Setting up the problem}

\noindent
Summation of random number of random variables is a well known
problem that has many applications. One of them is the so-called
{\it loss aggregation} problem in insurance. Usually, one needs to
compute with sufficient precision the cumulative distribution of the
random variables (r.v.'s), having a sense of, for example, financial losses aggregated
for some fixed period (normally, one year).


Specifically, we are interested in the distribution of the r.v.
\begin{equation}
\label{firstSum}
S_{N}=\sum\limits_{k=1}^{N}X_k
\end{equation}
where~$N$ is the number of events within a selected period,
generated by a process~$N(t)$ of occurrences, usually called {\it
counting process}. The crucial point is that the summed
r.v.'s~$X_k$ are assumed to be mutually independent and also
independent of the counting process~$N(t)$.

Another important point is that usually in practical applications
r.v.'s $X_k$ having in insurance the sense of single losses can be
viewed as {\bf identically distributed} r.v.'s having a
distribution $\mathbf{P}(X_k<x)=:F_X(x)$. This assumption allows to
apply some particular {\it deterministic techniques}, making the
loss aggregation a relatively simple computational task.

In the current work, the problem of loss aggregation
supposing that the assumption of identical distribution of single
losses~$X_k$ may be violated is addressed. As will be remarked below,
deviations from the assumption of identical distribution of losses
is quite a natural situation in actuarial modeling, making the
application of analytical techniques impossible.

\subsection{Aggregation with stochastic parameters}

\noindent
In practical applications, compound distributions should often be
modeled with respect to the uncertainty of the parameters of
initial distribution ($F_X$ in our terms). This problem can be
handled with the help of Bayesian inference. The basic idea of
Bayesian modeling for taking into account parameters' uncertainty
(see, e.g.,~\cite{Shev4}) is to consider the vector of the
parameters (of initial distribution) as a random vector. Using
some \textit{a~priori} knowledge about the distribution of this random
vector, one can form its {\it prior distribution} probability dension function (pdf)~$\pi(\theta)$.
If an additional information comes into play in the
form of observations~$\mathbf{X}$, the {\it posterior
distribution} pdf with respect to this information can be
calculated:
\begin{equation}
\label{baye}
\pi_{\Theta\,|\,\mathbf{X}}(\theta\,|\,\mathbf{x})\propto f_{\mathbf{X}\,|\,\Theta}
(\mathbf{x}\,|\,\theta)\pi(\theta)\,.
\end{equation}
In the absence of a relevant prior information about the prior distribution,
 $\pi(\theta)$ can be chosen to be a uniform distribution (the case of
 so called {\it noninformative priors}).

Often, the expression~(\ref{baye}) cannot be used for the direct computation
of the posterior distribution and the stochastic modeling has to be used
to produce a pseudo-random sample from $\pi_{\Theta\,|\,\mathbf{X}}(\theta\,|\,\mathbf{x})$.


The sample from posterior distribution can therefore be used to
get the sample from the corresponding compound distribution.
Specifically, if the $g(z\,|\,\theta)$ is the pdf of the compond
distribution (r.v.~$S$ in our terms) given a value of the
parameter $\theta$, then the corresponding {\it full predictive
distribution} is defined as $h(z\,|\,\mathbf{X})=\int
g(z\,|\,\theta)\pi(\theta\,|\,\mathbf{X})d\theta$ (which
is the weighted average with the respect to the distribution of
$\theta$ as an~r.v.).

Obviously, this task is not compatible with {\it deterministic
techniques} (i.e., the ones based on analytical representations of
distribution functions), as each time the realization of the
predictive distribution is modeled, different values of the
parameters $\boldsymbol\theta$ and $\lambda$ should be used. Thus,
one needs to use MC simulation methods. The common
scheme for modeling of the predictive distribution using MC method
could be as follows:
\begin{enumerate}[1.]
\item
Simulate the realization of the severity parameters' vector
$\boldsymbol\theta$ and frequency $\lambda$ from their joint
distribution~$\pi(\boldsymbol\gamma)$ where
$\boldsymbol\gamma=(\theta,\lambda)$.
\item Given $\boldsymbol\theta$ and $\lambda$ generate yearly losses, i.e.,
($i$)~generate the number $N$ of yearly losses
$N\sim\mbox{Pois}(\lambda)$ and ($ii$)~generate the sample
$\{X_j\}=(X_1,\dots,X_N)$.
\item Given $N$ and $\{X_j\}$ calculate the annual loss $S=\sum\limits_{i=1}^N X_i$.
\item Repeat Steps 1--3 $K$ times to get: $\{S\}_{j=1}^K$.
\item Estimate $\alpha$-quanile, $\widehat{Q}_B$, of annual loss
($Q_{\alpha}$\linebreak $\sim\; \mbox{sort}(S_j)[\alpha]$).
\end{enumerate}
However, there are at least two aspects in this context that make
the use of regular MC simulation techniques very
time-demanding. These aspects are ($i$)~actuarial losses are
often {\it heavy tailed} and ($ii$)~one usually needs to estimate a
sufficiently high quantile of the aggregate loss distribution. In
the case of operational risk measurement, the rules prescript to
estimate the $\alpha$-quantile of aggregate loss called {\it
Value-at-Risk}, i.e.,
$VaR_{\alpha}:=\sup\limits_x(x:\,\,F(x)<\alpha)$, at the level
$\alpha=0.999$. Thus, the precision of the modeled predictive
distribution should be high enough to obtain a reliable estimate of
the upper quantile. In practical applications dealing with
operational risk modeling, it is rather usual that not less than
$K=10^6$ repetitions should be made to ensure necessary precision
for $0.999$-quantile estimate (see, e.g.,~\cite{Shev4, TW}).


That motivated authors' search for techniques that would reduce the
number of claimed repetitions in the modeling scheme above and lead
to QMC methods.


\section{Methods}

\subsection{Deterministic techniques}

\noindent
A short comment on deterministic approaches should be made. Some of the
deterministic techniques are based on a passage from probability
distributions to characteristic functions or probability generating
functions (pgf). This approach is applicable to the task~(\ref{firstSum})
in the case of $iid$ summands which will be summarized below.

For a random variable $N$ taking only nonnegative integer values,
consider the pgf $P_N(z) =
\mathbf{E}[z^N]$\linebreak
$=\;\sum\limits_{n=0}^\infty \mathbf{P}[N=n] z^n$ which is
defined and analytic at least for $|z|\le1$. Considering the power
series expansion of this function $P_N(z) = \sum\limits_{n=0}^\infty p_n
z^n$, one is able to retrieve the distribution $\mathbf{P}[N = n] =
p_n$ for $n \ge 0$ by calculating the coefficients of~$P_N(z)$.
Denote the pgf of a compound sum of the form~(\ref{firstSum}) by
$P_S(z)$ ({\it considering integer valued loss sizes}) and using
the independence assumption and find
\begin{equation*}
%\label{WLLW2}
P_S(z)=\mathbf{E}[z^S]\sum_{k=1}^{\infty} \mathbf{P}[N=k]
P_X(z)^k\,.
\end{equation*}
One has the well known representation
\begin{equation}
\label{eq:pgfCompoundSum}
    P_S(z) = P_N(P_X(z))
\end{equation}
where $P_N(z)$ is the pgf of the distribution of loss occurrences
and~$P_X(z)$ corresponds to loss sizes.  For Poisson distributed occurrences,
\begin{equation*}
%\label{eq:pgfPoisson}
        P_S(z) = \exp(\lambda(P_X(z)-1))\,.
\end{equation*}
Exactly the same
representation is valid in terms of characteristic function (chf).

Concerning the calculation of compound distributions {\bf with
fixed parameters}, deterministic methods are well developed and
include, besides the techniques based of pgf and chf, also, e.g.,
recursive techniques related to Panjer recursion. For detailed
discussion of deterministic techniques in actuarial modeling, see,
e.g.,~\cite{KPW} and~\cite{Panjer}. Deterministic methods are
usually more effective than the ones based on MC
modeling from the point of precision and speed of calculations.
For comparison of the effectiveness of different techniques, see,
e.\,g.,~\cite{TW}.

However, when parameters of the distribution of random summands are
also random coming from posterior distribution~(\ref{baye}) as in
the case we deal with, deterministic techniques cannot be used.
Indeed, in this case, the pgf~$P_X(z)$ in right-hand side
of~(\ref{eq:pgfCompoundSum}) will have no closed form, as it will
depend on the distribution of random parameters, which cannot be
included into pgf explicitly. Therefore, to handle this problem, one
has to turn to MC simulation methods.

\subsection{Monte Carlo modeling in~heavy~tailed~cases}

\noindent
Handling heavy tailed distributions, which implies simulation of
rare and severe events, has been a challenging task in applied
statistics (see, e.\,g.,~\cite{Asmus-t} or~\cite{Huang} for detailed
overviews).

Methods usually proposed for solving this problem allow to reduce
the computational effort within using the standard MC modeling.
According to~\cite{Asmus-t}, algorithms involving order statistics and
methods using importance sampling are among the most effective
techniques for handling random sums.
Moreover, various {\it variance reduction} techniques can be used to
increase the efficiency of MC simulation (see, e.\,g.,~\cite{Glasserman, Robert}).
However, in certain cases when the resulting
distribution depends on random factors, standard variance reduction
methods cannot be used, as these techniques rely on explicit
representations of distributions used for the modeling. That is
indeed the case of our problem, as we need to calculate the random
sum of varying volume, and, in addition, the distribution of each
variable includes random parameters.

\subsection{Quasi-Monte Carlo in risk management}

\noindent
From the previous subsections, motivation for QMC becomes
apparent: Clearly, simulation techniques remain a basic tool for
modeling compound sums of the form~(\ref{firstSum}) when the
distributions involved have stochastic parameters, but dealing
with heavy-tailed distributions one needs to handle the variance
of simulations in one or another way, and QMC is one of the
effective and stable methods to reduce the variance of
simulations.

However, QMC methods are not widely used in modeling random sums,
as there are certain natural restrictions for using QMC for that
particular task. Application of QMC in risk management was studied~\cite{Prakash},
including of problem of the summation of random
variables. In~\cite{Prakash}, the high-dimensional Sobol' sequences
were apply to the problem of risk aggregation for a portfolio of
individual losses, when the dimension of the portfolio is fixed.
Thus, the problem reduces to the summation of a specified (fixed)
number of random variables. It relates the methodology described in~\cite{Prakash}
to this work, but there are two specific aspects:
\begin{enumerate}[(1)]
\item in our case, the number of random variables to be summed up
is a (discrete) random variable itself; and
\item we are interested particularly in the summation of heavy tailed
r.v.'s.
\end{enumerate}
These aspects motivated the authors to make a separate study in order to find
out the efficiency in the QMC scheme in the frame of this problem.

\section{Using Quasi-Monte Carlo
for~the~Random~Loss~Aggregation}

\noindent
In the present section, it will be discussed how Sobol' sequence can be used
for the summation of random number of random variables.

\medskip

\noindent
\textbf{Constructing Sobol' sequence.}
The Sobol' sequence is one of the standard quasi-random sequences
and is widely used in QMC applications. The construction of Sobol' sequence
will not be described here, referring to~\cite{Sob1, Sob2}
and related works for technical details.

\subsection{Role of independence}

\noindent
Obviously, the advantage of MC techniques in application to a
statistical problem is that it allows to model {\it independent}
random variables. As already mentioned, the assumption of
independence plays a major role in the modeling of compound
distributions.

Recall that the cumulative probability distribution (cdf) of the compound sum is
\begin{multline}
\label{WLLW}
F_S(x)=\mathbf{P}(S\leq x)\\
{}=
\sum_{k=1}^{\infty}\mathbf{P}[N=k]\mathbf{P}\left(
{X_1 + \dots + X_k\leq x}\right)\\
{}=\sum_{k=1}^{\infty} p_k F_X^{*k}(x)
\end{multline}
where $F_X^{*k}(x)$ is the $k$-fold convolution of the pdf with itself, i.e.,
\begin{equation}
\label{convolut}
F_X^{*k}(x)=\int\limits_0^{x}F_X^{*k}(x-u)\,dF_X(u)\,,
\end{equation}
and $F_X^{0}(x)\equiv 1\,\,(x>0)$.

Once the assumption of independence is dropped, the representation of the compound sum~(\ref{WLLW})
does not reduce to the sum of convolutions~(\ref{convolut}) any longer.
In case of the summation of dependent sequences, in order
to calculate the cdf $F_S(s)=\mathbf{P}\left(X_1+\dots+X_d \leq s\right)$ we would have to
deal with the
integrals of the form $\int\limits_{\Omega_s:=\left[u_1+\dots+u_d\leq s\right]}dF(u_1,\dots,u_d)$
instead of multiple convolutions.

\medskip

\noindent
\textbf{Independence and multidimensional Sobol' sequences.}
Clearly, it is already the scheme of the construction of the
low-discrepancy sequences that claims the QMC sequences to be
dependent. However, if one uses the elements of the sequences from
different dimensions, their relation would ``imitate'' relation
between independent random variables.

The issues related to the independence of different dimensions of
Sobol' sequences were discussed in~\cite{Prakash} where the tests
based on rank correlations were used. For brevity,
the results of independence tests are not indicated here, but refer to~\cite{Prakash}
and related literature stating that spatial
distribution of multidimensional Sobol' sequences relates to the
distribution of independent random variables.

Note that the notions of ``randomness'' is understood in the studied case
certainly not in its usual way. The observed ``distributions'' of
low-discrepancy sequences would not be empirical
probability distributions in its general sense. Nevertheless,
observing the spatial structure formed by the sequencies, one is
able to judge how good is the resulting ``imitation'' of the
independence between values in simulated sequences, due to
independence between different dimensions in~QMC.
{\looseness=1

}

Summarizing the paragraph, note that the right approach for
modeling the sums of independent random variables would be
to use different (sequential) dimensions for the generation of each
of the~r.v.'s



\subsection{Quasi-Monte Carlo: the modeling set}\label{Makin}

\noindent
In operational risk framework, before modeling of the compound sums,
one should use historical data to estimate (single-loss) severity
and frequency distributions. The historical data of operational risk
losses is classified by business lines (BL), and this division is
important for the calculation of regulatory capital for OpRisk.
Particularly, according to the Basel II recommendations, the
Value-at-Risk  (VaR) estimators, defining the regulatory capital, should be
done for each BL, $L_1,\dots,L_K$, separately.

In order to estimate the severity and frequency distributions using
historical data $\{X^{L_j}_i\}$ in an i.i.d.-case, standard methods
such as maximum likelihood estimate (MLE) can be used to estimate the parameters of the
distribution $F(x)=\mathbf{P}\left(X^{L_j}_1<x\right)$. In the case
of uncertain parameters, the distribution parameters are themselves
random variables; hence, the ``parameters of the parameters'' should be
estimated. As soon as the estimates are found, the scheme outlined
here can be applied to each BL, using either the
regular MC, or QMC techniques.

The following notation for Sobol' sequences is used: {\bf Sobol(i, n,
d)=:$S^d_n(i)$} in a unit hypercube $[0,1]^d$ (here, $i$ is the
initial index, $n$ is the length of the sequence, and $d$ is the
number of dimensions). Then, if the inverse pdf of an r.v.~$X$,
$F^{-1}$, is known analytically, the realizations of $X$ can be
generated via $F^{-1}\left(S^d_n(i)\right)$.

Then, the whole algorithm can be represented in the following way.
Suppose for simplicity that there are only one parameter of the
severity distribution and one frequency parameter (which is often
the case in applications).
\begin{enumerate}[I.]
\item Simulate an $N$-length sequence of the severity parameter
$\theta$ by
$I_\theta=F_{\theta}^{-1}\left(S^{d_{\theta}}_n(i_1)\right)$ with
{\bf fixed} $d_j$ (i.\,e., using a one-dimensional QMC sequence), and
a chosen initial index $i_1$; an $N$-length sequence of the
frequency parameter $\lambda$ by
$I_\lambda=F_{\lambda}^{-1}\left(S^{d_{\lambda}}_n(i_1)\right)$
likewise.
\item Simulate $N$ yearly frequencies using $\lambda_j$ from $I_\lambda$,
the quantile function of the Poisson distribution and a new
QMC-sequence $\{\Lambda_j\}$\linebreak $\sim
q\mathrm{Pois}_{\lambda_j}^{-1}\left(S^{d_{\Lambda}}_n(i_\Lambda)\right)$
with fixed $d_{\Lambda}$ and a chosen index $i_\Lambda$. \par

The choice of $i_\Lambda$ can be arbitrary in the initial simulation
set, but one repeats the simulation cycle to obtain a different
estimate of the quantile, then the initial index of next simulation
should be chosen such that the QMC sequence used in the previous
cycle is not used again.
\item Simulate $N$ sequences of yearly losses $\{X\}_1,\dots,\{X\}_N$ by
\par 1.
$\{X^{(1)}_j\}_{i\leq \Lambda_1} =
F_{X}^{-1}\left(S^{d_{i}}_1\right)$\par 2. $\{X^{(2)}_j\}_{i\leq
\Lambda_2} = F_{X}^{-1}\left(S^{d_{i}}_2\right)$
\par $\dots$\par
$N$. $\{X^{(N)}_j\}_{i\leq \Lambda_N} =
F_{X}^{-1}\left(S^{d_{i}}_N\right)$\par

Here, $d_i$ changes sequentially in each case (i.\,e., from~$1$ to
$\Lambda_i$ for each $i$).
\item Loss aggregation.

Consequently,
one gets $n$ sums $S_j = \sum\limits_{i=1}^{N_{j}}X^{(j)}_i$\,
$(j=1,\dots,N)$ which are the realizations of the aggregate loss
that we are interested in. Find the quantile:
\begin{itemize}
\item
put the obtained sample in increasing order to get the order
statistics $L_{1:n}
 \le \cdots \le L_{n:n}$ where $L_{1:n}$ denotes the smallest of the $n$
 simulations and $L_{n:n}$ the biggest simulated loss; and
\item
the element at position $[\alpha n+1]$ of the ordered sample,
where $[\cdot]$ denotes rounding downwards, is the estimator of
the quantile (i.e., of VaR) to the level $\alpha$ (e.g., choose {\bf
$\alpha=0.999$}).
\end{itemize}
\end{enumerate}

Note that in the above construction, two fundamental
properties of the multidimensional QMC sequence have been used:
(1)~the projection
of multidimensional QMC into lower dimensions is again a
low-discrepancy sequence; and (2)~in each multidimensional sequence, the
elements corresponding to different dimensions are independent. That
is, taking the elements of Sobol' sequence sequentally from
different dimensions, one keeps the properties of a low-discrepancy
sequence, still advancing in having the properties of the QMC
sequence.

As a result of the above simulation cycle, a single estimate of
the $0.999$-quantile of the aggregate loss distribution is
obtained. To obtain a different realization of $0.999$-quantile,
one may repeat the whole cycle, correspondingly changing the
initial indices $i_1$, $i_\lambda$, $i_\theta$, etc.

\subsection{Results and rates of convergence}

\noindent
Next, overview the obtained results. The {\it SobolSeq} generator
(see {\sf www.broda.co.uk}, 2009, for the full reference) was used for the
QMC simulation, while the general modeling was made in SPLus. We
were interested in estimating the $0.999$-quantile of the aggregate
loss distribution.

To analyze the rate of convergence to the true quantile,
the performance of the algorithm was traced for a range of the number of
simulation~$N$ from~$10^5$ to $2\cdot10^6$, for both QMC and
pseudo-random realizations.

\medskip

\noindent
\textbf{Remark 3.1.}\
The benchmark for the true quantile we are referring
to can be estimated using, e.\,g., MC Markov Chain (MCMC)
modeling, which is often used as an efficient tool for taking
parameter uncertainty into account (see, e.\,g.,~\cite{Shev4} or~\cite{Shev5}).
Using MCMC, it is possible to model the so-called full
predictive distribution of aggregate losses with random parameters
(though modeling with MCMC is as time demanding as regular MC). The
true quantile that can be used as a benchmark to estimate the
absolute error of MC and QMC modeling is the quantile of the full
predictive distribution.

\medskip


Note that $10^6$ simulations required 525~s of the CPU on
a Pentium 2.0~GHz, 1~GB memory, for the whole scheme described
above including generation of Sobol' sequence and loss aggregation.
The same scheme with the MC requires roughly the same time
for the same number of simulations.

The model used for the modeling set was GPD-Poisson, i.\,e., single
losses are supposed to have Generalized Pareto distribution (GPD)
$$
G(x)= 1-\left(1+\xi\fr{x-\mu}{\sigma}\right)^{-1/\xi}
$$
while the number of yearly losses follows a Poisson process with
intensity~$\lambda$.

Among GPD parameters, the location parameter~$\mu$ is usually fixed
(it plays a role of the threshold), while shape and scale parameters
$(\xi,\sigma)$ are the ones to be estimated. As Maximum likelihood
estimation is often used, it is natural to assume that the
parameters' vector $(\xi,\sigma)$ has the bivariate normal
distribution and also assume normal distribution for the intensity~$\lambda$.
Furthermore, the following parameters for the
distributions of the model parameters were used (the choice of the parameters'
values reflects typical values in operational risk framework):
\begin{itemize}
\item
 the threshold $\mu=7000$;
\item
 vector of mean values for the parameters $(\xi,\sigma)=(1,12\,000)$, vector of
 their variance values (0.18,\,1645.0), and the covariance value~0.64; and
\item
mean and variance of the Poisson intensity $\lambda$ distribution
was taken as~(12,\,1.7).
\end{itemize}

The plots summarizing the results of the 0.999-quautile modeling
illustrating the convergence to the true quantile are presented in
Figs.~\ref{plot1} and~\ref{plot2}. Figure~\ref{plot1} gives
the picture on the log-log scale for the whole range of the
simulation numbers, while Fig.~\ref{plot2} concentrates on the
segment of the larger values of $N$ on a regular scale where local
smooth trend lines show the rate of convergence.




Interpretation of the results indicated on the plots should be made
with respect to specific properties of multidimensional Sobol'
sequences. Analyzing empirical errors of numerical integration via
QMC,~\cite{Morok} shows that actual rates of convergence may differ
sufficiently for different types of QMC sequences, depending on the
dimension. Main theoretical result on integration errors with QMC
known as the {\it Koksma--Hlawka inequality} states
$$
\left|\int\limits_{I^s}f(x)dx - \fr{1}{N}\sum_{i=1}^N f(x_i)
\right|\leq V(f)D_N
$$
where $D_N$ is the discrepancy and $s$ is the dimension of the
integration domain~$I^s$. The bound on the discrepancy of a random
sequence indicates~$N^{-1/2}$, suggesting that a sequence with
smaller discrepancy could give smaller errors. For low-discrepancy
sequences, $D_N\sim N^{-\alpha}$ with~$\alpha$ depending on a
particular type of sequence. Empirically studying the power~$\alpha$
of the rate $N^{-\alpha}$,~\cite{Morok} states quite wide range from~1 to~0.45
depending on the dimension of the function.

In the studied case, the dimensions are ``floating,'' as each of the random
sums consists of random number of variables and is therefore
generated via a QMC sequence of different (random) dimensions.
That fact can at least partially explain the changing rate of
convergence to the true quantile observed in Fig.~\ref{plot1}.
At the same time, as seen from Fig.~\ref{plot2}, the rate of
convergence for QMC sequences is more stable than the one via
pseudo-random numbers simulation, for higher values~$N$
corresponding\linebreak\vspace*{-12pt}
\pagebreak

\end{multicols}

\begin{figure} %fig1
\begin{center}
\vspace*{1pt}
\mbox{%
\epsfxsize=77.546mm
\epsfbox{tem-2.eps}
}
\end{center}
\vspace*{-6pt}
\Caption{Comparison of the performance of MC (points) with
the one obtained by QMC (solid line) for the aggregate--loss
distribution of the generalized Pareto
 \label{plot1}}
%\end{figure*}
\vspace*{24pt}
%\begin{figure*}
\vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=78.546mm
\epsfbox{tem-2.eps}
}
\end{center}
\vspace*{-6pt}
 \Caption{Comparison of the MC precision (\textit{1}) with
the precision of QMC (\textit{2}) in terms of the mean square error
for the aggregate--loss distribution of the generalized Pareto
\label{plot2}}
\vspace*{6pt}
\end{figure}

\begin{multicols}{2}

\noindent
 to higher precision. As estimated by ordinary linear
regression,
 for the range of higher~$N$ starting from
$8\cdot10^5$, the rate of convergence via QMC sequences is~$N^{-0.8}$.
%\pagebreak

Note that not only the rate of convergence can play a role, but also
the absolute value of the error. According to~\cite{Morok}, for
integration problems with discontinuous in high dimensions, the
following result is valid
\begin{equation}
\label{rate}
\,|\,\mbox{error}\,|\, = C_sN^{-s/(2s-1)}
\end{equation}
where $C_s$ changes depending of the particular type of the low
discrepancy sequence and the number of dimensions. Thus, the rate
of convergence much better than that of a random sequence cannot
be expected; however, the precision still can be improved regarding
the constant~$C_s$.

In our case, as illustrated by both Figs.~\ref{plot1} and~\ref{plot2},
the coefficient~$C_s$ is significantly lower than the
one associated with the random sequence, allowing to require much
lower number of simulations for obtaining the same precision of
0.999-quantile, regarding Sobol' sequence.

\renewcommand{\bibname}{\protect\rmfamily References}
{\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{References}
\begin{thebibliography}{99}

\bibitem{Shev4} %1
\Au{Shevchenko,~P.}
2008. Estimation of operational risk capital charge
under parameter uncertainty. \textit{J. Operational Risk} 3(1):51--63.

\bibitem{TW} %2
\Au{Temnov,~G., and R.~Warnung}.
2008. A comparison of loss aggregation
methods for operational risk. \textit{J. Operational Risk} 3(1):3--23.

\bibitem{KPW} %3
\Au{Klugman,~S., H.~Panjer, and G.~Willmot}.
2004. \textit{Loss models.
From data to  decisions}. Hoboken, NJ, USA: Wiley--Interscience. 2nd ed.

\bibitem{Panjer} %4
\Au{Panjer,~H.}
2006. \textit{Operational risk. Modelling analytics}.
Wiley Series in Probability and Statistics. Hoboken, NJ: Wiley-Interscience. 2nd ed.

\bibitem{Asmus-t} %5
\Au{Asmussen, S., K.~Binswanger,  and B.~Hojgaard.}
2000. Rare event
simulation for heavy tailed distributions. \textit{Bernoulli}
6(2):303--22.



\bibitem{Huang} %6
\Au{Huang,~Z., and P.~Shahabuddin}.
2004. A unified approach for
finite-dimensional, rare-event Monte Carlo simulation.
\textit{2004 Winter Simulation Conference Proceedings.}

\bibitem{Glasserman} %7
\Au{Glasserman,~P.}
2003. \textit{Monte Carlo methods in financial engineering}.
Springer.




\bibitem{Robert} %8
\Au{Robert,~C., and G.~Casella}
2004. \textit{Monte Carlo statistical
methods}.  Springer Texts in Statistics, New York. 2nd~ed.

\bibitem{Prakash} %9
\Au{Prakash,~S.}
2005. \textit{On the use of high dimensional quasi-random
sequences for risk measurement}. Master Thesis, ETH Zurich.

\bibitem{Sob1} %10
\Au{Sobol',~I.\,M.}
1976. Uniformly distributed sequences with additional
uniformity properties. \textit{USSR J. Comput. Math.
Phys.} 16(5):236--42.

\bibitem{Sob2} %11
\Au{Sobol',~I.\,M.}
1998. On quasi-Monte Carlo integrations. \textit{Math. Computers Simulation} 47:103--12.


\bibitem{Shev5} %12
\Au{Shevchenko,~P., and G.~Temnov}.
2009 (in press.). Modelling operational risk data
reported above a time varying threshold.
\textit{J. Operational Risk}.

\bibitem{Morok} %13
\Au{Morokoff,~W., and R.~Caflisch}.
1995. Quasi-Monte Carlo integration.
\textit{J. Comput. Physics} 122(2):218--30.

%\bibitem{Berman}
%\Au{Berman,~L.}
%1998. Accelerating Monte Carlo: Quasi-random sequences
%and variance reduction. \textit{J. Comput. Finance}
%1(2):79--95.


%\bibitem{Kucherenko1}
%\Au{Kucherenko,~S., and T.~Shah}.
%2007. The importance of being global.
%Application of global sensitivity analysis in Monte Carlo option
%pricing. \textit{Wilmott Magazine} 4.

%\bibitem{Milton}
%\Au{Milton,~J., and J.~Arnold}.
%1990. \textit{Introduction to probability and statistics: Principles and applications for engeneering and the
%computing sciences}. McGraw-Hill.



%\label{end\stat}


\end{thebibliography}
}
}
\end{multicols}
\renewcommand{\figurename}{\protect\bf Рис.}
\renewcommand{\tablename}{\protect\bf Таблица}
\renewcommand{\bibname}{\protect\rmfamily Литература}

\hrule

\smallskip

\def\tit{ПОДХОД К АКТУАРНОМУ МОДЕЛИРОВАНИЮ НА ОСНОВЕ ПРИМЕНЕНИЯ МЕТОДА КВАЗИ-МОНТЕ-КАРЛО
ДЛЯ СЛУЧАЙНЫХ СУММ, ЗАВИСЯЩИХ~ОТ~СТОХАСТИЧЕСКИХ~ФАКТОРОВ}
\def\aut{Г.~Темнов$^1$, С.~Кучеренко$^2$}

\titelr{\tit}{\aut}

\vspace*{12pt}

\noindent
$^1$Корский университет, Ирландия, g.temnov@ucc.ie\\
\noindent
$^2$Империал Колледж, Лондон, Великобритания, s.kucherenko@ic.ac.uk\\
%\vspace*{10mm}

\medskip

\Abst{Рассматривается задача оценивания характеристик
случайной суммы, в которой число слагаемых также случайно.
Рассматриваемый случай включает дополнительный случайный фактор:
хотя тип распределения слагаемых известен, параметры этого распределения
рассматриваются как случайные величины с известным распределением.
Рассматриваемая задача решается с помощью метода квази-Монте-Карло.
Анализируется эффективность данного подхода по сравнению с обычным
методом Монте-Карло. Рассматриваемые методы имеют применение в актуарной
практике, а также при решении некоторых задач информатики, связанных с
агрегированием данных с тяжелыми хвостами.}


\KW{актуарное моделирование, метод квази-Монте-Карло, случайные суммы}

\label{end\stat}

\renewcommand{\figurename}{\protect\bf Рис.}
\renewcommand{\tablename}{\protect\bf Таблица}
\renewcommand{\bibname}{\protect\rmfamily Литература}
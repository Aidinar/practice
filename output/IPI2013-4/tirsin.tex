\def\stat{tirsin}

\def\tit{ИССЛЕДОВАНИЕ ДИНАМИКИ МНОГОМЕРНЫХ СТОХАСТИЧЕСКИХ 
СИСТЕМ НА~ОСНОВЕ ЭНТРОПИЙНОГО~МОДЕЛИРОВАНИЯ$^*$}

\def\titkol{Исследование динамики многомерных стохастических 
систем на~основе энтропийного моделирования}

\def\autkol{А.\,Н.~Тырсин, О.\,В.~Ворфоломеева}

\def\aut{А.\,Н.~Тырсин$^1$, О.\,В.~Ворфоломеева$^2$}

\titel{\tit}{\aut}{\autkol}{\titkol}

{\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1] {Работа выполнена при поддержке 
проекта 12-М-127-2049 фундаментальных исследований УрО РАН.}}

\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Научно-инженерный центр <<Надежность и ресурс больших систем и машин>> УрО РАН, 
г.~Екатеринбург, at2001@yandex.ru}
\footnotetext[2]{Челябинский государственный университет, ya.olga.work@yandex.ru}

     
  
  \Abst{Описан энтропийный подход к моделированию динамики 
стохастических систем. В его основе лежит представление системы в виде 
многомерного случайного вектора. Показано, что изменение энтропии 
многомерной стохастической сис\-те\-мы может быть выражено через дисперсии и 
условные корреляции компонент случайного вектора. Это позволяет 
обнаружить причину изменения энтропии сис\-те\-мы и оценить этот случайный вектор
количественно. Получено, что энтропия стохастической системы складывается 
из двух компонент, которые характеризуют ее свойства. Первая компонента 
определяет предельную энтропию, соответствующую полной независимости 
элементов системы, и характеризует рассмотрение целостного объекта как 
состоящего из частей (аддитивность). Вторая компонента отражает степень 
взаимосвязей между элементами сис\-те\-мы, характеризуя свойства системы как 
целого (целостность). Описанный подход делает возможным использование 
энтропийной модели в задачах диагностики и контроля состояния 
стохастических сис\-тем, а также эффективного управления ими. 
К~достоинствам предложенного подхода следует отнести простоту реализации 
и интерпретации математической модели, универсальность и применимость к 
стохастическим сис\-те\-мам различной природы, возможность ее использования 
на малых выборках данных. Приведен пример практического 
применения математической модели.}
  
  \KW{многомерная случайная величина; энтропия; динамика; стохастическая 
система; дисперсия; корреляция}

\DOI{10.14357/19922264130401}

\vskip 14pt plus 9pt minus 6pt

      \thispagestyle{headings}

      \begin{multicols}{2}

            \label{st\stat}
  
  \section{Введение}
  
  Энтропия является фундаментальным свойством любых систем с 
неоднозначным, или вероятностным, поведением~[1]. В~настоящее время 
достаточно распространено использование энтропии для описания поведения 
открытых стохастических систем в различных областях~[2--6]. Однако, 
несмотря на частое использование этого термина, использование энтропии для 
моделирования открытых систем, в отличие от термодинамики, недостаточно 
формализовано и носит в основном качественный характер. Отсутствуют 
достаточно простые и адекватные математические модели, позволяющие 
связать энтропию с фактическими характеристиками состояний стохастических 
систем.
  
  Известно~[7], что энтропия непрерывной случайной величины~$X$ 
(дифференциальная энтропия) определяется по формуле:
  \begin{equation}
  H(X) =-\int\limits_{-\infty}^{+\infty} f(x)\ln f(x)\,dx\,,
  \label{e1-t}
  \end{equation}
  где $f(x)$~--- плотность распределения случайной величины~$X$. 
Полученная по формуле~(1) энтропия называется энтропией закона 
распределения или дифференциальной энтропией.
  
  Представим стохастическую систему~$S$ в виде многомерной случайной 
величины $\mathbf{Y}\hm=(Y_1, Y_2,\ldots , Y_m)$. Будем считать, что данное 
представление является адекватной математической\linebreak моделью системы~$S$. 
Каждый элемент~$Y_i$ вектора~\textbf{Y} является одномерной случайной 
величиной, которая характеризует функционирование соответствующего 
элемента исследуемой системы. Элементы могут быть как взаимозависимыми, 
так и не зависеть друг от друга. Совместную дифференциальную энтропию 
многомерной случайной величины~$\mathbf{Y}$ будем определять по 
формуле~[7]:
  \begin{multline}
  H(\mathbf{Y})=-\int\limits_{-\infty}^{+\infty} \cdots \int\limits_{-
\infty}^{+\infty} f_{\mathbf{Y}}(x_1,x_2,\ldots ,x_m)\times{}\\
{}\times \ln  f_{\mathbf{Y}}(x_1,x_2,\ldots ,x_m)\,dx_1dx_2\ldots dx_m\,.
  \label{e2-t}
  \end{multline}
  где $ f_{\mathbf{Y}}(x_1,x_2,\ldots ,x_m)$~--- совместная плотность 
распределения случайных величин $Y_1, Y_2, \ldots , Y_m$.
  
  Аналитическое нахождение энтропии $H(\mathbf{Y})$ в настоящее время 
получено лишь для совместного нормального распределения~\cite{8-t}. 
Рассмотрение других распределений затруднено отсутствием меры нелинейной 
корреляционной взаимосвязи случайных величин с иными распределениями, 
аналогичной определителю корреляционной матрицы для совместного 
нормального распределения. В~\cite{9-t} предпринята попытка оценить 
взаимосвязь случайных величин через совместную энтропию. Но при этом 
требуется вычислить саму энтропию многомерной случайной величины по 
ограниченной выборке, что весьма затруднительно, особенно если законы 
распределения не известны. А~необходимо, наоборот, выразить совместную 
энтропию многомерной случайной величины через характеристики ее 
компонент.
  
  Таким образом, актуальна задача разработки и формального обоснования 
энтропийного подхода к моделированию открытых стохастических систем. 
Поэтому рассмотрим более общий случай, когда случайный вектор~\textbf{Y} 
не имеет многомерного нормального распределения.
  
  \section{Энтропия многомерной непрерывной случайной 
величины}
  
  \noindent
  \textbf{Теорема~1.} \textit{Пусть $X_{1}$, $X_2$~--- две непрерывные 
случайные величины, определенные на всей числовой оси и описываемые 
однотипными законами распределения с плотностями $f_1(x) \hm= 
f(x;\mu_1,\lambda_1)$, $f_2(x)\hm= f(x; \mu_2,\lambda_2)$ соответственно, где 
$\mu_1$, $\mu_2$ и $\lambda_1$, $\lambda_2$~--- параметры положения и 
масштаба случайных \mbox{величин}~$X_1$ и~$X_2$. Тогда разность 
дифференциальных энтропий случайных величин~$X_1$ и~$X_2$ равна}:
  \begin{equation}
  H(X_2) -H(X_1) =\ln \fr{\lambda_2}{\lambda_1}\,.
  \label{e3-t}
  \end{equation}
  
  \noindent
  Д\,о\,к\,а\,з\,а\,т\,е\,л\,ь\,с\,т\,в\,о\,.\ \  Выразим плотность вероятности 
случайной величины~$X_2$ через плотность вероятности случайной 
величины~$X_1$:
  $$
  f(x;\mu_2,\lambda_2) =\fr{\lambda_1}{\lambda_2}\,f\left( 
\fr{\lambda_1}{\lambda_2}\left( x+\mu_2-\mu_1\right);\mu_1,\lambda_1\right)\,.
  $$
  
  С учетом последнего соотношения разность дифференциальных энтропий 
случайных величин~$X_1$ и~$X_2$ равна:

\noindent
  \begin{multline*}
  H(X_2) -H(X_1) = {}\\
  {}= -\int\limits_{-\infty}^{+\infty} f(x;\mu_2,\lambda_2)\ln 
f(x;\mu_2,\lambda_2)\,dx
  +{}\\
  {}+ \int\limits_{-\infty}^{+\infty} f(x;\mu_1,\lambda_1)\ln 
f(x;\mu_1,\lambda_1)\,dx={}\\
  {}= -\fr{\lambda_1}{\lambda_2} \int\limits_{-\infty}^{+\infty} f\left( 
\fr{\lambda_1}{\lambda_2}\left( x+\mu_2-\mu_1\right);\mu_1,\lambda_1\right) \times{}\\
{}\times\ln 
\left[ \fr{\lambda_1}{\lambda_2}f \left( \fr{\lambda_1}{\lambda_2}\left( x+\mu_2-\mu_1\right);\mu_1,\lambda_1 
\right)\right]\,dx+{}\\
  {}+ \int\limits_{-\infty}^{+\infty} f(x;\mu_1,\lambda_1)\ln 
f(x;\mu_1,\lambda_1)\,dx= {}\\
{}=-\ln \fr{\lambda_1}{\lambda_2} \int\limits_{-
\infty}^{+\infty} f(x;\mu_2,\lambda_2)\,dx-{}\\
  {}- \int\limits_{-\infty}^{+\infty} f(t;\mu_1,\lambda_1)\ln 
f(t;,\mu_1,\lambda_1)\,dt +{}\\
{}+
  \int\limits_{-\infty}^{+\infty} f(x;\mu_1,\lambda_1)\ln 
f(x;\mu_1,\lambda_1)\,dx\,,
  \end{multline*}
  где 
  $$
  t=\fr{\lambda_1}{\lambda_2}\left( x+\mu_2-\mu_1\right)\,.
  $$ 
  Отсюда имеем:
  $$
  H(X_2)-H(X_1) =-\ln \fr{\lambda_1}{\lambda_2} + H(X_1) -H(X_1)=\ln 
\fr{\lambda_2}{\lambda_1}\,.
  $$
  
  \noindent
  \textbf{Следствие~1.} \textit{Пусть в условиях теоремы~$1$ $X_1$ 
и~$X_2$~--- две непрерывные случайные величины, имеющие конечные 
дисперсии. Поскольку среднее квадратическое отклонение непрерывной 
случайной величины, если оно существует, пропорционально параметру 
масштаба, то формулу~$(3)$ можно записать в виде}:
  \begin{equation}
  H(X_2)-H(X_1) =\ln \fr{\sigma_2}{\sigma_1}=\ln \sigma_2 -\ln \sigma_1\,,
  \label{e4-t}
  \end{equation}
  \textit{где $\sigma_1$ и $\sigma_2$~--- средние квадратические отклонения 
случайных величин~$X_1$ и~$X_2$}.
  
  \medskip
  
  \noindent
  \textbf{Следствие~2.} \textit{Дифференциальная энтропия непрерывной 
случайной величины~$X$, имеющей конечную дисперсию~$\sigma^2_X$, равна}:
   $$
   H(X)=\ln \sigma_X+C=\ln e^C\sigma_X\,,
   $$
  \textit{где $C\hm=H(\overset{\circ}{X})\hm= H(X/\sigma_X)$~--- энтропия 
случайной величины~$\overset{\circ}{X}$ с единичной дис\-пер\-си\-ей и тем же 
распределением, что и у случайной величины~$X$.}
  
  \medskip
  
  Действительно, из~(\ref{e4-t}) получим 
  $$
  H(X)- H(\overset{\circ}{X})   =\ln \sigma_X\,,
  $$ откуда
  $$
  H(X) =\ln \sigma_X +H(\overset{\circ}{X})=\ln \sigma_X +C =\ln e^C 
\sigma_X\,.
  $$
  
  Отметим, что константа~$C$ характеризует энтропию закона распределения. 
Она может быть выражена через введенный в~\cite{10-t} энтропийный 
коэффициент~$k$ закона распределения как $C\hm= \ln 2k$.
  
  
  \medskip
  
  \noindent
  \textbf{Теорема~2.} \textit{Пусть имеется две системы непрерывных 
случайных величин $\mathbf{Y}^{(1)} \hm= \left(Y_1^{(1)}, Y_2^{(1)},\ldots , 
Y_m^{(1)}\right)$ и $\mathbf{Y}^{(2)} \hm= \left(Y_1^{(2)}, Y_2^{(2)},\ldots , Y_m^{(2)}\right)$, 
каждые со\-от\-вет\-ст\-ву\-ющие компоненты которых $Y_i^{(1)}, Y_i^{(2)}$ 
$(i\hm=1,2,\ldots , m)$ определены на всей числовой оси, имеют конечные 
дисперсии и описываются однотипными законами распределения с 
некоторыми параметрами положения и масштаба. \mbox{Тогда} разность 
совместных энтропий сис\-тем случайных величин $\mathbf{Y}^{(2)} \hm= 
\left(Y_1^{(2)}, Y_2^{(2)},\ldots , Y_m^{(2)}\right)$ и 
$\mathbf{Y}^{(1)} \hm= \left(Y_1^{(1)}, 
Y_2^{(1)},\ldots , Y_m^{(1)}\right)$ равна}:
  \begin{multline}
  \Delta H\left(\mathbf{Y}\right) =H\left(\mathbf{Y}^{(2)}\right) -H\left(\mathbf{Y}^{(1)} \right)={}\\
  {}=\sum\limits_{k=1}^m \ln \fr{\sigma_{Y_k^{(2)}}}{\sigma_{Y_k^{(1)}}}+
\fr{1}{2}  \sum\limits_{k=2}^m \ln \fr{1-R^2_{Y_k^{(2)}/Y_1^{(2)}\cdots Y_{k-1}^{(2)}}} {1-
R^2_{Y_k^{(1)}/Y_1^{(1)}\cdots Y_{k-1}^{(1)}}}\,,
  \label{e5-t}
  \end{multline}
  \textit{где}
  $$
  \sigma_{Y_k^{(j)}/Y_1^{(j)}\cdots Y_{k-1}^{(j)}} =\sigma_{Y_k^{(j)}}\sqrt{1-
R^2_{Y_k^{(j)}/Y_1^{(j)}\cdots Y_{k-1}^{(j)}}}\,;
  $$
  $R^2_{Y_k^{(j)}/Y_1^{(j)}\ldots Y_{k-1}^{(j)}}$~--- \textit{коэффициенты 
детерминации соответствующих регрессионных зависимостей}, $k\hm= 2, 3, 
\ldots , m$, $j\hm=1, 2$.
  
  \medskip
  
  \noindent
  Д\,о\,к\,а\,з\,а\,т\,е\,л\,ь\,с\,т\,в\,о\,.\ \ Совместная энтропия $H(\mathbf{Y})$ 
системы случайных величин~\textbf{Y} согласно свойству иерархической 
аддитивности~\cite{11-t} равна
  \begin{multline}
  H(\mathbf{Y}) =H(Y_1)+H\left( Y_2/Y_1\right) +H\left( 
Y_3/Y_1Y_2\right)+\cdots\\
\cdots  + H\left(Y_m/Y_1\cdots Y_{m-1}\right)\,.
  \label{e6-t}
  \end{multline}
  
  Рассмотрим две системы непрерывных случайных 
величин~$\mathbf{Y}^{(1)}$ и~$\mathbf{Y}^{(2)}$, каждые соответствующие 
компоненты $Y_i^{(1)}$, $Y_i^{(2)}$ ($i\hm=1, 2,\ldots , m$) которых 
определены на всей числовой оси, имеют конечные дисперсии и описываются 
однотипными законами распределения с некоторыми параметрами положения и 
масштаба. Тогда изменение энтропии с учетом~(\ref{e6-t}) равно:
  \begin{multline*}
  \Delta H(\mathbf{Y}) =H\left(\mathbf{Y}^{(2)}\right) -H\left(\mathbf{Y}^{(1)}\right) 
={}\\
{}=
H\left(Y_1^{(2)}\right) -H\left(Y_1^{(1)}\right) +H\left(Y_2^{(2)}/Y_1^{(2)}\right)-{}\\
  {}- H\left( Y_2^{(1)}/Y_1^{(1)}\right) +\cdots + 
H\left(Y_m^{(2)}/Y_1^{(2)}\cdots Y_{m-1}^{(2)}\right) -{}\\
{}- H\left( Y_m^{(1)}/Y_1^{(1)}\cdots Y_{m-
1}^{(1)}\right)\,.
  \end{multline*}
  
  Условное математическое ожидание ${\sf E}\left[ Y_2/Y_1\hm=x\right]$ является 
регрессией~$Y_2$ на~$Y_1$ с коэффициентом 
детерминации~$R^2_{Y_2/Y_1}$. Поэтому дисперсия случайной величины 
$Y_2/Y_1$ равна дисперсии $\sigma^2_{Y_2/Y_1} \hm= \sigma^2_{Y_2}
  (1-R^2_{Y_2/Y_1})$ остаточной случайной компоненты регрессии ${\sf E}\left[ 
Y_2/Y_1=x\right]$~\cite{12-t}. Аналогично величина ${\sf E}\left[ Y_k/Y_1Y_2\cdots 
Y_{k-1}=\mathbf{x}\right]$ является регрессией~$Y_k$ на случайные величины 
$Y_1, Y_2,\ldots , Y_{k-1}$. Дисперсия случайной величины $Y_k/Y_1Y_2\cdots 
Y_{k-1}$ равна дисперсии остаточной случайной компоненты регрессии 
${\sf E}\left[ Y_k/Y_1Y_2\cdots Y_{k-1}=\mathbf{x}\right]$~\cite{12-t}. Поэтому
  \begin{equation}
  \sigma^2_{Y_k/Y_1Y_2\ldots Y_{k-1}}=\sigma^2_{Y_k}(1-
R^2_{Y_k/Y_1Y_2\ldots Y_{k-1}})\,,
  \label{e7-t}
  \end{equation}
  где $ R^2_{Y_k/Y_1Y_2\cdots Y_{k-1}}$~--- коэффициенты детерминации 
соответствующих регрессионных зависимостей, $k\hm=2, 3, \ldots , m$.
  
  Отсюда с учетом~(\ref{e4-t}) и~(\ref{e7-t}) получим
  \begin{multline*}
  \Delta H(\mathbf{Y}) =\ln \fr{\sigma_{Y_1^{(2)}}}{\sigma_{Y_1^{(1)}}}+
  \sum\limits_{k=2}^m \ln \fr{\sigma_{Y_k^{(2)}/Y_1^{(2)}\cdots Y_{k-
1}^{(2)}}}{\sigma_{Y_k^{(1)}/Y_1^{(1)}\cdots Y_{k-1}^{(1)}}} = {}\\
{}=
  \ln \fr{\sigma_{Y_1^{(2)}}}{\sigma_{Y_1^{(1)}}}+\sum\limits_{k=2}^m 
  \ln \fr{\sigma_{Y_k^{(2)}}\sqrt{1-R^2_{Y_k^{(2)}/Y_1^{(2)}\cdots 
  Y_{k-1}^{(2)}}}}{\sigma_{Y_k^{(1)}}\sqrt{1-R^2_{Y_k^{(1)}/Y_1^{(1)}\cdots 
Y_{k-1}^{(1)}}}}={}\\
  {}=\sum\limits_{k=1}^m \ln 
\fr{\sigma_{Y_k^{(2)}}}{\sigma_{Y_k^{(1)}}}+\fr{1}{2}\sum\limits_{k=2}^m 
  \ln \fr{1-R^2_{Y_k^{(2)}/Y_1^{(2)}\cdots Y_{k-1}^{(2)}}}{1-
R^2_{Y_k^{(1)}/Y_1^{(1)}\cdots Y_{k-1}^{(1)}}}\,.
  \end{multline*}
  
  
  Обозначив 
  \begin{align*}
  \Delta H(\mathbf{Y})_{\boldsymbol{\Sigma}} &= \sum\limits_{k=1}^m \ln 
\fr{\sigma_{Y_k^{(2)}}}{\sigma_{Y_k^{(1)}}}\,;\\
  \Delta H(\mathbf{Y})_{\mathbf{R}} &= \fr{1}{2}\sum\limits_{k=2}^m \ln \fr{1-
R^2_{Y_k^{(2)}/Y_1^{(2)}\cdots Y_{k-1}^{(2)}}}{1-
R^2_{Y_k^{(1)}/Y_1^{(1)}\cdots Y_{k-1}^{(1)}}}\,,
  \end{align*}  
  представим формулу~(\ref{e5-t}) как
  \begin{equation}
  \Delta H(\mathbf{Y}) =\Delta H(\mathbf{Y})_{\boldsymbol{\Sigma}}+\Delta 
H(\mathbf{Y})_{\mathbf{R}}\,,
  \label{e8-t}
  \end{equation}
  где $\Delta H(\mathbf{Y})_{\boldsymbol{\Sigma}}$ и $\Delta H(\mathbf{Y})_{\mathbf{R}}$~--- 
приращения энтропии за счет изменения дисперсий и корреляций случайных 
величин $Y_1,Y_2,\ldots , Y_m$.
  
  \medskip
  
  \noindent
  \textbf{Следствие~1.} \textit{Если случайный вектор~$\mathbf{Y}$ является 
гауссовским, то получим рассмотренный в}~\cite{13-t} \textit{частный случай}
  \begin{equation}
  \Delta H(\mathbf{Y}) =\sum\limits_{k=1}^m \ln 
\fr{\sigma_{Y_k^{(2)}}}{\sigma_{Y_k^{(1)}}} +\fr{1}{2}\ln \fr{\vert 
\mathbf{R}_{\mathbf{Y}^{(2)}}\vert}{\vert \mathbf{R}_{\mathbf{Y}^{(1)}}\vert}\,,
  \label{e9-t}
  \end{equation}
  \textit{где $\vert \mathbf{R}_{\mathbf{Y}^{(j)}}\vert$~--- определитель 
корреляционной матрицы $\mathbf{R}_{\mathbf{Y}^{(j)}}$ случайного вектора} 
$\mathbf{Y}^{(j)}$, $j\hm=1, 2$.
  
  \medskip
  
  \noindent
  \textbf{Следствие~2.} \textit{Совместная дифференциальная энтропия 
$H(\mathbf{Y})$ системы непрерывных случайных величин $\mathbf{Y}\hm= 
(Y_1,Y_2,\ldots , Y_m)$ равна}
  \begin{multline}
  H(\mathbf{Y}) ={}\\
  {}=\sum\limits_{k=1}^m H(Y_k) +\fr{1}{2} \sum\limits_{k=2}^m \ln 
\left (1-R^2_{Y_k/Y_1Y_2\cdots Y_{k-1}}\right)\,.
  \label{e10-t}
  \end{multline}
  
  \medskip
  
  Действительно, подставив в~(\ref{e5-t}) вместо $\mathbf{Y}^{(1)}$ 
и~$\mathbf{Y}^{(2)}$ соответственно две системы непрерывных случайных 
величин $\tilde{\mathbf{Y}} \hm= (\tilde{Y}_1,\tilde{Y}_2, \ldots , \tilde{Y}_m)$ 
и $\mathbf{Y}\hm= (Y_1,Y_2,\ldots , Y_m)$, каждые соответствующие 
компоненты~$\tilde{Y}_i$, $Y_i$ ($i\hm=1,2,\ldots ,m$) которых определены на 
всей числовой оси, имеют конечные дисперсии и описываются одинаковыми 
законами распределения, причем $\tilde{Y}_i$ ($i\hm=1,2,\ldots ,m$) являются 
взаимно независимыми, получим:
  \begin{multline*}
  H(\mathbf{Y}) -H(\tilde{\mathbf{Y}})= {}\\
  {}=\sum\limits_{k=1}^m \ln 
\fr{\sigma_{Y_k}}{\sigma_{\tilde{Y}_k}}+\fr{1}{2} \sum\limits_{k=2}^m \ln 
  \fr{1-R^2_{Y_k/Y_1\cdots Y_{k-1}}}{1-R^2_{\tilde{Y}_k/\tilde{Y}_1\cdots 
\tilde{Y}_{k-1}}}\,.
  \end{multline*}
  
  Поскольку $\forall k\ H(\tilde{Y}_k) \hm= H(Y_k)$, $\sigma_{Y_k}\hm= 
\sigma_{\tilde{Y}_k}$, $R^2_{\tilde{Y}_k/\tilde{Y}_1\tilde{Y}_2\cdots 
\tilde{Y}_{k-1}}\hm=0$, то
\begin{multline*}
  H(\mathbf{Y}) =H(\tilde{Y}) +\fr{1}{2}\sum\limits_{k=2}^m \ln \left( 1-
R^2_{Y_k/Y_1Y_2\cdots Y_{k-1}}\right) = {}\\
{}=\sum\limits_{k=1}^m H(Y_k) +\fr{1}{2} 
\sum\limits_{k=2}^m \ln \left (1-R^2_{Y_k/Y_1Y_2\cdots Y_{k-1}}\right)\,.
  \end{multline*}
  
  Формула~(\ref{e10-t}) является обобщением приведенного в~\cite{13-t} 
соотношения для энтропии многомерного нормального распределения
  \begin{equation}
  H(\mathbf{Y}) =\sum\limits_{k=1}^m H(Y_k) +\fr{1}{2} \ln \vert 
\mathbf{R}_{\mathbf{Y}} \vert\,,
  \label{e11-t}
  \end{equation}
  где $\vert \mathbf{R}_{\mathbf{Y}}\vert$~--- определитель корреляционной 
матрицы~$\mathbf{R}_{\mathbf{Y}}$.
  %
  Поэтому, так же как и для~(\ref{e11-t}), и в общем случае 
  согласно~(\ref{e10-t}) энтропия многомерной случайной величины 
складывается из двух со\-став\-ля\-ющих:
  \begin{equation}
      H(\mathbf{Y}) =H(\mathbf{Y})_{\boldsymbol{\Sigma}} +H(\mathbf{Y})_{\mathbf{R}}\,,
  \label{e12-t}
  \end{equation}
  где 
  \begin{align*}
  H(\mathbf{Y})_{\boldsymbol{\Sigma}} & = \sum\limits_{k=1}^m H(Y_k)\,;
\\
  H(\mathbf{Y})_{\mathbf{R}} &= (1/2)\sum\limits_{k=1}^m  \ln \left( 1-
R^2_{Y_k/Y_1Y_2\ldots Y_{k-1}}\right)\,.
\end{align*}
  
  \section{Исследование изменения состояния стохастической 
системы на~основе энтропийной модели}
  
  Аддитивные представления~(\ref{e12-t}) и~(\ref{e8-t}) в виде двух компонент 
как самой энтропии, так и ее изменения показывают ее дуализм. Компонента 
$H(\mathbf{Y})_{\boldsymbol{\Sigma}}$ определяет предельную энтропию, со\-от\-вет\-ст\-ву\-ющую 
полной независимости элементов сис\-те\-мы. Поэтому условно назовем ее 
энтропией хао\-тич\-ности. Величина $H(\mathbf{Y})_{\mathbf{R}}$ равна 
энтропии за счет совместной корреляционной взаимосвязи между элементами 
сис\-те\-мы, ее условно можно назвать энтропией самоорганизации. Следует 
заметить, что дуализм энтропии в том или ином виде отмечался в ряде 
публикаций~\cite{1-t, 2-t, 7-t, 9-t, 13-t, 14-t}.
  
  Таким образом, изменение энтропии происходит аддитивным образом: с 
одной стороны~--- за счет изменения дисперсий, а с другой стороны~--- из-за 
изменения коррелированности случайных величин $Y_1,Y_2,\ldots , Y_m$. 
Следовательно, причины рос\-та и уменьшения энтропии сис\-те\-мы могут быть 
различными. Например, энтропию системы можно увеличить (уменьшить) 
посредством увеличения (уменьшения) дисперсий~$\sigma^2_{Y_k}$ или 
уменьшения (\mbox{увеличения}) коэффициентов детерминации 
$R^2_{Y_k/Y_1Y_2\cdots Y_{k-1}}$ компонент вектора~$\mathbf{Y}$.
  
  Выражение~(\ref{e5-t}) позволяет обнаружить причину изменения энтропии 
системы и оценить его количественно. Это делает возможным использование 
энтропийной модели в задачах контроля и диагностики состояния 
стохастических сист\-ем. Пусть стохастическая система представима в виде 
случайного вектора~$\mathbf{Y}$. Тогда на основе модели~(\ref{e5-t}) можно 
осуществлять мониторинг состояния стохастической системы путем анализа 
изменения ее энтропии. Это можно сделать следующим образом. Будем 
считать, что две системы непрерывных случайных величин~$\mathbf{Y}^{(1)}$ 
и~$\mathbf{Y}^{(2)}$ соответствуют предыдущему и текущему периодам 
функционирования системы. Тогда, отслеживая изменение $\Delta 
H(\mathbf{Y})$ энтропии в целом и ее компонент $\Delta 
H(\mathbf{Y})_{\boldsymbol{\Sigma}}$, $\Delta H(\mathbf{Y})_{\mathbf{R}}$, можно сделать 
выводы о состоянии системы. Анализ изменения каждой из случайных 
величин~$Y_k$
  \begin{align*}
  \Delta H(\mathbf{Y})_{\boldsymbol{\Sigma},k} &=\ln 
\fr{\sigma_{Y_k^{(2)}}}{\sigma_{Y_k^{(1)}}}\,;\\
  \Delta H(\mathbf{Y})_{\mathbf{R},k} &=\fr{1}{2}\ln 
\fr{1-R^2_{Y_k^{(2)}/Y_1^{(2)}\cdots Y_{k-1}^{(2)}}} {1-
R^2_{Y_k^{(1)}/Y_1^{(1)}\cdots Y_{l-1}^{(1)}}}
  \end{align*}
позволит выявить те элементы системы (компоненты системы~$\mathbf{Y}$), 
которые оказали наибольшее влияние на изменение энтропии всей сис\-темы.

  Поскольку $R^2_{Y_k/Y_1Y_2\ldots Y_{k-1}}\hm\geq 
R^2_{Y_k/Y_1Y_2\ldots Y_{k-2}} \hm\geq R^2_{Y_k/Y_1Y_2\cdots Y_{k-3}}\geq 
\cdots \geq R^2_{Y_k/Y_1}$~\cite{12-t}, то оценивать вклад произвольного 
  $l$-го элемента в изменение энтропии самоорганизации целесообразно через 
их предельные значения:
  \begin{multline*}
  \Delta H(\mathbf{Y})^*_{\mathbf{R},l} = \fr{1}{2}\ln 
  \fr{1- R^2_{Y_l^{(2)}/Y_1^{(2)}\cdots Y_{l-1}^{(2)}Y_{l+1}^{(2)}\cdots Y_m^{(2)}}} 
{1-R^2_{Y_l^{(1)}/Y_1^{(1)}\cdots Y_{l-1}^{(1)}Y_{l+1}^{(1)}\cdots Y_m^{(1)}}}\,,\\ 
l=1,2,\ldots ,m\,.
  \end{multline*}
  
  Выше был рассмотрен случай, когда пары случайных величин $Y_i^{(1)}$, 
$Y_i^{(2)}$ ($i\hm=1,2,\ldots ,m$) имели однотипные распределения. Если 
данное предположение не выполняется, то тогда изменение энтропии 
хаотичности согласно~(\ref{e10-t}) равно:
  \begin{multline*}
  \Delta H(\mathbf{Y})_{\boldsymbol{\Sigma}}= H\left(\mathbf{Y}^{(2)}\right)_{\boldsymbol{\Sigma}} - 
H\left(\mathbf{Y}^{(1)}\right)_{\boldsymbol{\Sigma}} ={}\\
{}=\sum\limits_{k=1}^m \Delta 
H(\mathbf{Y})_{\boldsymbol{\Sigma},k} = \sum\limits_{k=1}^m \left[ H\left(Y_k^{(2)}\right)-H 
\left(Y_k^{(1)}\right)\right]\,,
  \end{multline*}
т.\,е.\ потребуется определять энтропии одномерных случайных величин по 
выборочным данным. Отметим, что в настоящее время предложен ряд 
алгоритмов для решения данной задачи~[15--18].
  
  \section{Пример мониторинга состояния многомерных 
стохастических систем}
  
  Рассмотрим задачу мониторинга состояния стохастической системы на 
модельном примере. Пусть некоторая стохастическая система~$S$ 
моделируется в виде случайного вектора $\mathbf{Y}\hm= (Y_1,Y_2,Y_3)$. Для 
упрощения будем считать его нормальным. Рассмотрим динамику 
функционирования системы на основе модели~(\ref{e5-t}). Пусть в 
предыдущем и текущем периодах функционирования системы имеем 
случайные векторы~$\mathbf{Y}^{(1)}$, $\mathbf{Y}^{(2)}$ с 
ковариационными матрицами, равными
  \begin{align*}
\boldsymbol{\Sigma}^{(1)} &=\begin{pmatrix}
  1{,}114 &\ 1{,}131 &\ 0{,}494\\
  1{,}131 &\ 3{,}310&\ 3{,}205\\
  0{,}494 &\ 3{,}205 &\ 5{,}348
  \end{pmatrix}\,;\\[6pt]
\boldsymbol{\Sigma}^{(2)} &= \begin{pmatrix}
  1{,}796 &\ 1{,}713 &\ 0{,}381\\
  1{,}713 &\ 3{,}589&\ 3{,}199\\
  0{,}381&\ 3{,}199&\ 4{,}841 \end{pmatrix}\,.
  \end{align*}
  
  Согласно~(\ref{e5-t}), (\ref{e8-t}) имеем:
  $$
  \fr{\sigma_{Y_1^{(2)}}}{\sigma_{Y_1^{(1)}}} =1{,}270\,;\enskip 
  \ln \fr{\sigma_{Y_1^{(2)}}}{\sigma_{Y_1^{(1)}}}=0{,}239\,;
  $$
  $$
    \fr{\sigma_{Y_2^{(2)}}}{\sigma_{Y_2^{(1)}}} =1{,}041\,; \enskip
  \ln \fr{\sigma_{Y_2^{(2)}}}{\sigma_{Y_2^{(1)}}}=0{,}040\,;
  $$
  $$
    \fr{\sigma_{Y_3^{(2)}}}{\sigma_{Y_3^{(1)}}} =0{,}951\,;\enskip
  \ln \fr{\sigma_{Y_3^{(2)}}}{\sigma_{Y_3^{(1)}}}=-0{,}050\,;
  $$
  $$
  \Delta H(\mathbf{Y})_{\boldsymbol{\Sigma}} =\sum\limits_{k=1}^3 \ln 
\fr{\sigma_{Y_k^{(2)}}}{\sigma_{Y_k^{(1)}}} =0{,}229\,;
  $$
  $$
  \fr{1-R^2_{Y_2^{(2)}/Y_1^{(2)}}}{R^2_{Y_2^{(1)}/Y_1^{(1)}}} =\fr{1-
0{,}455}{1-0{,}347}=0{,}835\,;
  $$
  $$
  \fr{1-R^2_{Y_3^{(2)}/Y_1^{(2)}Y_2^{(2)}}}{1-R^2_{Y_3^{(1)}/Y_1^{(1)}Y_2^{(1)}}} 
=\fr{1-0{,}866}{1-0{,}673}=0{,}409\,;
  $$
  $$
  \fr{1}{2}\ln \fr{1-R^2_{Y_2^{(2)}/Y_1^{(2)}}}{1-
R^2_{Y_2^{(1)}/Y_1^{(1)}}} =-0{,}090\,;
  $$
  $$
  \fr{1}{2}\ln \fr{1-R^2_{Y_3^{(2)}/Y_1^{(2)}Y_2^{(2)}}} 
  {1-R^2_{Y_3^{(1)}/Y_1^{(1)}Y_2^{(1)}}} =-0{,}447\,;
  $$
  $$
  \Delta H(\mathbf{Y})_{\mathbf{R}} =\fr{1}{2}\sum\limits_{k=2}^3 \ln 
  \fr{1-R^2_{Y_k^{(2)}/Y_1^{(2)}\cdots 
  Y_{k-2}^{(2)}}}{1-R^2_{Y_k^{(1)}/Y_1^{(1)}\cdots Y_{k-1}^{(1)}}} =
  -0{,}537\,;
  $$
 
 \vspace*{-12pt}
 
 \noindent
 \begin{multline*}
  \Delta H(\mathbf{Y}) =  \Delta H(\mathbf{Y})_{\boldsymbol{\Sigma}} +\Delta 
H(\mathbf{Y})_{\mathbf{R}} ={}\\
{}=0{,}229-0{,}537=-0{,}308\,.
  \end{multline*}
  
  Таким образом, энтропия системы в текущем периоде уменьшилась на~0,308, 
причем энтропия хаотичности выросла на~0,229 ($\Delta 
H(\mathbf{Y})_{\boldsymbol{\Sigma}}\hm=0{,}229$), а энтропия самоорганизации 
сократилась на~0,537 ($\Delta H(\mathbf{Y})_{\mathbf{R}}\hm= -0{,}537$). 
Это означает, что в текущем периоде в системе преобладала тенденция 
снижения энтропии самоорганизации.
  
  Анализ изменения каждой из компонент $\Delta H(\mathbf{Y})_{\Sigma,k}$ и 
$\Delta H(\mathbf{Y})_{\mathbf{R},k}$ показывает, что на рост энтропии 
хаотичности повлиял первый элемент сис\-те\-мы, а на снижение энтропии 
самоорганизации~--- третий элемент.
  
  \section{Заключение}
  
  \noindent
  \begin{enumerate}[1.]
  \item  Предложено энтропийное моделирование динамики многомерных 
стохастических систем. В~его основе лежит представление системы в виде 
случайного вектора, каждая из компонент которого представляет собой 
непрерывную случайную величину.
\item Получены аналитические выражения для энтропии многомерной 
случайной величины и ее динамики.
\end{enumerate}

Основные достоинства предложенного подхода:
  \begin{itemize}
  \item простота реализации и интерпретации математической модели;
  \item энтропийная модель применима при решении задач диагностики и 
контроля состояния стохастических сис\-тем, а также эффективного управ\-ле\-ния 
ими;
  \item универсальность и применимость к стохастическим системам 
различной природы;
  \item возможность использования на малых выборках данных.
  \end{itemize}
  
{\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{99}
  \bibitem{1-t}
  \Au{Климонтович Ю.\,Л.} Введение в физику открытых сис\-тем.~--- М.: 
Янус-К, 2002. 284~с.
  \bibitem{2-t}
  \Au{Вильсон А.\,Дж.} Энтропийные методы моделирования слож\-ных 
  сис\-тем~/ Пер. с англ.~--- М.: Наука, 1978.  248~с. (\Au{Wilson~A.\,G.}
  Entropy in urban and regional modeling.~--- London: Pion, 1970. 166~p.)
  \bibitem{3-t}
  \Au{Трубецков Д.\,И., Мчедлова Е.\,С., Красичков~Л.\,В.} Введение в 
  тео\-рию самоорганизации открытых сис\-тем.~--- М.: Физматлит, 2002. 200~с.
  \bibitem{4-t}
  \Au{Романовский Ю.\,М., Степанова~Н.\,В., Чернавский~Д.\,С.} 
Математическое моделирование в биофизике.~--- Москва--Ижевск: Институт 
компьютерных исследований, 2003. 402~с.


  \bibitem{6-t} %5
  \Au{Прангишвили И.\,В.} Энтропийные и другие сис\-тем\-ные 
закономерности: Вопросы управ\-ле\-ния сложными сис\-те\-ма\-ми.~--- М.: 
Наука, 2003. 428~с.
  \bibitem{5-t} %6
  \Au{Скоробогатов С.\,М.} Катастрофы и живучесть железобетонных 
сооружений (классификация и элементы теории).~--- Екатеринбург: УрГУПС, 
2009. 512~с.

  \bibitem{7-t}
  \Au{Шеннон К.} Работы по теории информации и кибернетике~/ Пер с англ.
  С.~Карпова.~--- М.:    ИИЛ, 1963. 830~с.
  
  \bibitem{8-t}
  \Au{Cover T.\,M., Thomas~J.\,A.} Elements of information theory.~--- N.Y.: 
Wiley, 1991. 563~p.
  \bibitem{9-t}
  \Au{Pena D., Van der Linde~A.} Dimensionless measures of variability and 
dependence for multivariate continuous distributions~// Commun. 
Stat.: Theor. M., 2007. Vol.~36. Issue~10. P.~1845--1854.
  \bibitem{10-t}
  \Au{Новицкий П.\,В.} Основы информационной теории измерительных 
устройств.~--- Ленинград: Энергия, 1968. 248~с.
  \bibitem{11-t}
  \Au{Стратонович Р.\,Л.} Теория информации.~--- М.: Советское радио, 
1975. 424~с.
  \bibitem{12-t}
  \Au{Greene W.\,H.} Econometric analysis.~--- 7th ed.~--- Prentice Hall, 2011. 1230~p.
  \bibitem{13-t}
  \Au{Тырсин А.\,Н., Соколова~И.\,С.} Энтро\-пий\-но-ве\-ро\-ят\-но\-ст\-ное 
моделирование гауссовских стохастических сис\-тем~// Математическое 
моделирование, 2012. Т.~24. №\,1. С.~88--102.
  \bibitem{14-t}
  \Au{Николис Г., Пригожин~И.} Самоорганизация в неравновесных 
  сис\-те\-мах: от диссипативных структур к упорядоченности через 
флуктуации~/ Пер с англ.~--- М.: Мир, 1979. 512~с. (\Au{Nikolis~G., Prigogine~I.}
Self-organozation in nonequilibrium systems: From dissipative structures to order through fluctuations.~---
N.Y.: John Wile\,\&\,Sons, 1977. 512~p.
  \bibitem{15-t}
  \Au{Beirlant J., Dudewicz~E.\,J., Gyorfi~L., van der Meulen~E.\,C.} 
Nonparametric entropy estimation: an overview~// Int. J.~Math. Stat. Sci., 1997. Vol.~6. Issue~1. P.~17--39.
  \bibitem{16-t}
  \Au{Stowell D., Plumbley~M.\,D.} Fast multidimensional entropy estimation by 
  $k$--$d$ partitioning~// IEEE Signal Proc. Lett., 2009. Vol.~16. Issue~6. 
P.~537--540.
  \bibitem{17-t}
  \Au{Тырсин А.\,Н., Клявин~И.\,А.} Повышение точности оценки энтропии 
случайных экспериментальных данных~// Сис\-те\-мы управ\-ле\-ния и 
информационные технологии, 2010. №\,1(39). С.~87--90.
  \bibitem{18-t}
  \Au{Noughabi H.\,A., Arghami~N.\,R.} A~new estimator of entropy~// J.~Iran. 
Stat. Soc., 2010. Vol.~9. Issue~1. P.~53--64.
  
 
  \end{thebibliography} } }
  



\end{multicols}

  \hfill{\small
\textit{Поступила в редакцию 13.05.13}}




%\vspace*{12pt}

%\hrule

%\vspace*{2pt}

%\hrule

\newpage

\def\tit{STUDY OF THE DYNAMICS OF~MULTIDIMENSIONAL STOCHASTIC 
SYSTEMS BASED ON~ENTROPY MODELING}

\def\aut{A.\,N.~Tyrsin$^1$ and O.\,V.~Vorfolomeeva$^2$}

\def\autkol{A.\,N.~Tyrsin and O.\,V.~Vorfolomeeva}
\def\titkol{Study of the dynamics of~multidimensional stochastic 
systems based on~entropy modeling}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-9pt}


\noindent
$^1$Science and Engineering Center ``Reliability and Resource of Large Systems and Machines,'' 
Ural Branch,\linebreak
$\hphantom{^1}$Russian Academy of Sciences, Yekaterinburg 620049, Russian Federation\\
\noindent
$^2$Chelyabinsk State University, Chelyabinsk 454001, Russian Federation

\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND APPLICATIONS\ \ \ 2013\ \ \ volume~7\ \ \ issue\ 4}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND APPLICATIONS\ \ \ 2013\ \ \ volume~7\ \ \ issue\ 4
\hfill \textbf{\thepage}}}  

\vspace*{15pt}
  
  \Abste{A new entropy approach of modeling of dynamics of stochastic systems 
is described. It is based on the representation of the system in the form of a 
multidimensional stochastic vector. It is shown that the change in entropy of  a
multivariate stochastic system can be expressed in terms of dispersions and conditional 
correlations of a component of a random vector.  This allows to reveal the cause of the 
change in the entropy of the system and to evaluate it quantitatively. It was found that 
the entropy of a stochastic system consists of two components that characterize its 
properties. The first component determines the limit entropy corresponding to the full 
independence of the elements of the system and defines the consideration of the 
integral object as consisting of components (additivity). The second component 
reflects the extent of interrelation between the elements of the system, defining the 
properties of the system as a whole (integrity).
This approach makes it possible to use 
entropy models in the diagnostics and control of stochastic systems as well as efficient
management. The advantages of the proposed approach include the simplicity 
of implementation and interpretation of the mathematical model, the universality and 
adaptability for stochastic systems of different nature, the possibility of its use on 
small samples of data. The article contains an example of the practical application of  a
mathematical model.}
  
  \KWE{multidimensional random variable; entropy; dynamics; stochastic system; 
dispersion; correlation}

\DOI{10.14357/19922264130401}

\Ack 
 The work was supported by the project 12-M-127-2049 of Basic 
Researches of the Ural Branch of the Russian Academy of Sciences. 

\vspace*{9pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{References}
\begin{thebibliography}{99}
  
 \bibitem{1-t-1}
\Aue{Klimontovich, Ju.\,L.} 2002. \textit{Vvedenie v fiziku otkrytykh sistem}
[\textit{Introduction to the physics of open systems}].
Moscow: Janus-K Publ. 284~p.
\bibitem{2-t-1}
\Aue{Wilson A.\,G.} 1970. \textit{Entropy in urban and regional modeling}.  
L.: Pion. 166~p.
\bibitem{3-t-1}
\Aue{Trubeckov, D.\,I., E.\,S.~Mchedlova, and L.\,V.~Krasichkov}. 2002. 
\textit{Vvedenie v teoriyu samoorganizatsii otkrytykh sistem}
[\textit{Introduction to the theory of self-organization of open systems}]. Moscow: 
Publishing House of Physical-Mathematical Literature. 200~p.
\bibitem{4-t-1}
\Aue{Romanovskij, Ju.\,M., N.\,V.~Stepanova, and D.\,S.~Chernavskij}.  2003. 
\textit{Matematicheskoe modelirovanie v biofizike}
[\textit{Mathematical modeling in biophysics}]. Moscow--Izhevsk: Computer Research Institute.
402~p.
\bibitem{6-t-1}
\Aue{Prangishvili, I.\,V.} 2003. \textit{Entropiynye i drugie sistemnye 
zakonomernosti: Voprosy upravleniya slozhnymi sistemami}
[\textit{Entropic and other system regularities: Questions of management of complex
systems}]. Moscow: Nauka. 428~p.
\bibitem{5-t-1}
\Aue{Skorobogatov, S.\,M.} 2009. \textit{Katastrofy i zhivuchest' 
zhelezobetonnykh sooruzheniy (klassifikatsiya i elementy teorii)}
[\textit{Catastrophes and serviceability of reinforced concrete buildings (classification and
elements of theory)}]. Ekaterinburg: 
Ural State University of Railway Transport. 512~p.

\bibitem{7-t-1}
\Aue{Shannon C.} 1948. A mathematical theory of communication.
\textit{Bell Syst. Tech.~J.} 27(3):379--423; 4:623--656.
\bibitem{8-t-1}
\Aue{Cover, T.\,M., and J.\,A.~Thomas}. 1991. \textit{Elements of information 
theory}. N.Y.: Wiley. 563~p.
\bibitem{9-t-1}
\Aue{Pena, D., A.~Van der Linde}. 2007. Dimensionless measures of variability 
and dependence for multivariate continuous distributions. 
\textit{Commun. Stat. Theor. M.} 36(10):1845--1854.
\bibitem{10-t-1}
\Aue{Novickij, P.\,V.} 1968.  \textit{Osnovy informatsionnoy teorii 
izmeritel'nykh ustroystv}
[\textit{Fundamentals of information theory measuring devices}]. Leningrad: Energy Publs. 248~p.
\bibitem{11-t-1}
\Aue{Stratonovich, R.\,L.} 1975.  \textit{Teoriya informatsii}
[\textit{Information theory}]. Moscow: Soviet Radio Publs. 424~p.
\bibitem{12-t-1}
\Aue{Greene, W.\,H.} 2011. \textit{Econometric analysis}. 7th ed. Prentice 
Hall. 1230~p.
\bibitem{13-t-1}
\Aue{Tyrsin, A.\,N., and I.\,S.~Sokolova}. 2012. Entropiyno-veroyatnostnoe 
modelirovanie gaussovskikh sto\-khas\-ti\-che\-skikh sistem 
[Entropy-propabilistic modeling of Gaussian stochastic systems].
\textit{Matematicheskoe 
Modelirovanie} [\textit{Mathematical Modeling}] 24(1):88--102.

\vspace*{3pt}

\bibitem{14-t-1}
\Aue{Nikolis, G., and I.~Prigogine}. 1977. 
\textit{Self-oraganization in nonequilibrium systems:
From dissipative structures to order through fluctuations}.
N.Y.: John Wiley\,\&\,Sons. 512~p.

\vspace*{3pt}

\bibitem{15-t-1}
\Aue{Beirlant, J., E.\,J.~Dudewicz, L.~Gyorfi, and E.\,C.~van der Meulen}. 
1997.  Nonparametric entropy estimation: an overview. \textit{Int. 
J.~Math. Stat. Sci.} 
6(1):17--39.

\columnbreak

\bibitem{16-t-1}
\Aue{Stowell, D., and M.\,D.~Plumbley}. 2009. Fast multidimensional entropy 
estimation by $k$--$d$ partitioning. \textit{IEEE Signal Proc. Lett.} 
16(6):537--540.
\bibitem{17-t-1}
\Aue{Tyrsin, A.\,N., and I.\,A.~Kljavin}. 2010. Povyshenie toch\-nosti otsenki 
entropii sluchaynykh eksperimental'nykh dannykh
[Increase in the accuracy of the estimate of the entropy of a random experimental
data]. \textit{Sistemy Upravleniya i 
Informatsionnye Tekhnologii} [\textit{Control Systems and Information Technologies}]
1(39):87--90.


\bibitem{18-t-1}
\Aue{Noughabi, H.\,A., and N.\,R.~Arghami}. 2010. A~new estimator of entropy. 
\textit{J.~Iran. Stat. Soc.}  9(1):53--64.
  
\end{thebibliography}
} }



\end{multicols}

\hfill{\small \textit{Received May 13, 2013}}

%{%\hrule\par
%\raggedleft\Large \bf%\baselineskip=3.2ex
%C\,O\,N\,T\,R\,I\,B\,U\,T\,O\,R\,S \vskip 17pt
%    \hrule
%    \par
%\vskip 21pt plus 8pt minus 3pt }

\Contr

\noindent
\textbf{Tyrsin Alexander N.} (b.\ 1961)~--- Doctor of Science in technology, leading 
researcher, Science and Engineering Center ``Reliability and Resource of Large 
Systems and Machines,'' Ural Branch of the Russian Academy of Sciences, 
Yekaterinburg 620049, Russian Federation; at2001@yandex.ru

\vspace*{3pt}
 \label{end\stat}

\noindent
\textbf{Vorfolomeeva Olga V.} (b.\ 1987)~--- PhD student, Mathematical Faculty, Chelyabinsk 
State University, Chelyabinsk 454001, Russian Federation; ya.olga.work@yandex.ru

\renewcommand{\bibname}{\protect\rm Литература}
   
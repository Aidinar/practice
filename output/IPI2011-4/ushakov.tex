\def\stat{ushakov}

\def\tit{ДЕКОМПОЗИЦИЯ ПРИ ЧАСТИЧНО ИЗВЕСТНОМ РАСПРЕДЕЛЕНИИ ОШИБКИ$^*$}

\def\titkol{Декомпозиция при частично известном распределении ошибки}

\def\autkol{В.\,Г.~Ушаков, Н.\,Г.~Ушаков}
\def\aut{В.\,Г.~Ушаков$^1$, Н.\,Г.~Ушаков$^2$}

\titel{\tit}{\aut}{\autkol}{\titkol}

{\renewcommand{\thefootnote}{\fnsymbol{footnote}}\footnotetext[1]
{Работа поддержана Российским фондом фундаментальных исследований
(проекты 11-01-00515а и 11-07-00112а), а также Министерством
образования и науки РФ в рамках ФЦП <<Научные и
на\-уч\-но-пе\-да\-го\-ги\-че\-ские кадры инновационной России на 2009--2013~годы>>.}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Факультет вычислительной математики и кибернетики Московского государственного университета
им.\ М.\,В.~Ломоносова; Институт проблем информатики Российской
академии наук, vgushakov@mail.ru}
\footnotetext[2]{Институт
проблем технологии микроэлектроники и особочистых материалов
Российской академии наук, ushakov@math.ntnu.no}


\Abst{Рассмотрена проблема
непараметрического оценивания распределения генеральной совокупности
в случае, когда выборочные значения наблюдаются с некоторой случайной
ошибкой. Предполагается, что распределение ошибки известно лишь
частично. Исследуются проблемы идентифицируемости и состоятельного
оценивания.}

\KW{непараметрическое оценивание; декомпозиция}

 \vskip 14pt plus 9pt minus 6pt

      \thispagestyle{headings}

      \begin{multicols}{2}
      
            \label{st\stat}

\section{Введение}

Модель декомпозиции возникает в ситуациях, когда случайная величина~$X$, 
распределение которой оценивается, не наблюдается
непосредственно, а с некоторой аддитивной ошибкой измерения, т.\,е.\
наблюдаются независимые реализации случайной величины
\begin{equation}
Y=X+\varepsilon\,,\label{e1-us}
\end{equation}
где $\varepsilon$~--- ошибка измерения. В~классической по-\linebreak становке
задачи декомпозиции предполагается,\linebreak что распределение ошибки
известно. Однако в большинстве случаев такое предположение
нереалистично. Один из путей преодоления данной труд\-ности состоит в
создании дополнительной выборки, элементы которой являются
независимыми реализациями ошибки~$\varepsilon$, и оценивании
распределения ошибки по этой выборке. Но и этот путь не всегда
возможен. В~работах~[1--3] исследовались ситуации, когда
распределение~$X$ может быть оценено по выборке из распределения~$Y$
при наличии лишь некоторой информации о распределениях~$X$ и~$\varepsilon$.

Первая проблема, которая возникает при данной постановке,~--- условия
идентифицируемости, т.\,е.\ условия, при которых распределение~$X$
может быть однозначно определено по распределению~$Y$. Вторая
проблема~--- построение состоятельной оценки распределения~$X$. 
В~работах~\cite{1-us, 2-us} исследовался случай, когда распределение ошибки
нормально. Были даны условия иден\-ти\-фи\-ци\-ру\-емости и доказана
состоятельность некоторых типов оценок. Более общие распределения
ошибки~$\varepsilon$ рассматривались в~\cite{3-us}, где, в част\-ности, были
получены некоторые условия идентифицируемости и состоятельности в
случае устойчивого распределения ошибки.

В разд.~2, касающемся иден\-ти\-фи\-ци\-ру\-емости,
также предполагается, что распределение ошибки устойчиво. Будет
получен простой критерий идентифицируемости, который в сочетании с
широким набором известных свойств устойчивых распределений и свойств
операции свертки позволяет указать ряд условий идентифицируемости. 
В~разд.~3, ка\-са\-ющем\-ся построения оценок и их со\-сто\-ятель\-ности,
рас\-смат\-ри\-ва\-ет\-ся общий случай про\-из\-вольных распределений, для которых
идентифицируемость имеет мес\-то, т.\,е.\ для которых распределение~$X$
однозначно определяется из распределения~$Y$.


\section{Идентифицируемость}

В этом разделе будет получено необходимое и достаточное условие
идентифицируемости распределения~$X$ в модели~(1) в случае, когда
распределение ошибки является симметричным устойчивым распределением.
Симметричное устойчивое распределение с показателем~$\alpha$ ($0\hm<\alpha\hm\le2$)
и параметром масштаба~$\gamma$ ($\gamma\hm>0$) будем обозначать
$U_{\gamma,\alpha}$. Таким образом, $U_{\gamma,\alpha}$~--- распределение
с характеристической функцией $\exp(-\gamma|t|^\alpha)$. Множество
всех симметричных устойчивых распределений с показателем~$\alpha$
обозначим ${\cal U}_\alpha$.

Обозначим через ${\cal P}$ множество всех вероятностных распределений на
действительной прямой.
Пусть $P,P_1\in{\cal P}$. Распределение~$P_1$ называется компонентой
распределения~$P$, если существует распределение~$P_2$ такое, что
$P=P_1\ast P_2$ ($\ast$ обозначает свертку). Обозначим через 
${\cal P}_\alpha$ множество всех распределений,
содержащих в качестве компоненты ка\-кое-ли\-бо симметричное устойчивое
распределение с показателем~$\alpha$:
$$
{\cal P}_\alpha=\{P\in{\cal P}|\exists \gamma>0,\exists P_1\in{\cal P}:
P=P_1\ast U_{\gamma,\alpha}\}\,.
$$
Дополнение множества ${\cal P}_\alpha$ будем обозначать, как обычно,
${\cal P}_\alpha^c$.

\smallskip

\noindent
\textbf{Теорема 1.} \textit{Для того чтобы распределение $P_X$ случайной величины~$X$ 
в модели}~(1), \textit{где~$\varepsilon$ имеет сим\-мет\-рич\-ное устойчивое распределение
$U_{\gamma,\alpha}$, было идентифицируемым, необходимо и достаточно, чтобы
$P_X\in{\cal P}_\alpha^c$. Другими словами,}
\begin{itemize}
\item[(\textit{а})] \textit{если
\begin{equation}
P_1,P_2\in{\cal P}_\alpha^c\,,\label{e2-us}
\end{equation}
то из равенства
\begin{equation}
P_1\ast U_{\gamma_1,\alpha}=P_2\ast U_{\gamma_2,\alpha}\label{e3-us}
\end{equation}
следует, что $P_1=P_2$ и $\gamma_1=\gamma_2$;}
\item[(\textit{б})] \textit{если $P_1\not\in{\cal P}_\alpha^c$ (т.\,е.\ $P_1\hm\in{\cal
P}_\alpha$), то существуют $P_2\hm\in{\cal P}$ и $\gamma_1,\gamma_2\hm>0$
такие, что $P_1\not=P_2$, $\gamma_1\not=\gamma_2$ и $P_1\ast
U_{\gamma_1,\alpha}\hm=P_2\ast U_{\gamma_2,\alpha}$.}
\end{itemize}

%\smallskip

\noindent
Д\,о\,к\,а\,з\,а\,т\,е\,л\,ь\,с\,т\,в\,о\,.\ Достаточность. Предположим, что~(\ref{e3-us}) имеет
место, но $\gamma_1\not=\gamma_2$. Пусть для определенности
$\gamma_1\hm<\gamma_2$. Тогда
$$
P_1\ast U_{\gamma_1,\alpha}= P_2\ast U_{\gamma_2-\gamma_1,\alpha}\ast U_{\gamma_1}
$$
и, следовательно,
$$
P_1=P_2\ast U_{\gamma_2-\gamma_1,\alpha}
$$
(поскольку характеристические функции устойчивых законов распределения не
обращаются в нуль),
что противоречит~(\ref{e2-us}). Противоречие доказывает, что
\begin{equation}
\gamma_1=\gamma_2\,.\label{e4-us}
\end{equation}
Равенство $P_1\hm=P_2$ немедленно следует из~(\ref{e3-us}) и~(\ref{e4-us}).

\smallskip

Необходимость. Поскольку
$P_1\in{\cal P}_\alpha$, существуют $P_0\hm\in{\cal P}$ и $\gamma_0\hm>0$
такие, что $P_1\hm=P_0\ast U_{\sigma_0,\alpha}$.
Пусть $\gamma'_0$ и $\gamma$ удовлетворяют условиям $\gamma'_0\hm>0$,
$\gamma\hm>0$, $\gamma'_0\hm<\gamma_0$. Положим $\gamma_1\hm=\gamma\hm+\gamma_0$,
$\gamma_2\hm=\gamma\hm+\gamma'_0$ и $P_2\hm=P_0\ast U_{\gamma_0-\gamma'_0}$.
Тогда $\gamma_1\not=\gamma_2$, $P_1\not=P_2$ и
\begin{multline*}
P_1\ast U_{\gamma_1,\alpha}=P_0\ast U_{\gamma+\gamma_0,\alpha}=
P_0\ast U_{\gamma_0-\gamma'_0,\alpha}\ast U_{\gamma'_0+\gamma,\alpha}={}\\
{}=
P_2\ast U_{\gamma_2,\alpha}\,.
\end{multline*}

Опираясь на полученный критерий, укажем несколько достаточных условий
идентифициру\-емости.

\smallskip

Условие~A. Носитель распределения~$X$ не совпадает со всей действительной
прямой (в случае нормального распределения ошибки это условие получено в~\cite{2-us}).

\smallskip

Условие~Б. Плотность распределения случайной величины~$X$ не является
бесконечно дифференциируемой.

\smallskip

Условие~В. Характеристическая функция $\varphi(t)$ случайной величины~$X$ 
удовлетворяет условию
$$
\lim\limits_{|t|\to\infty} e^{\gamma|t|^\alpha}|\varphi(t)|=\infty
$$
для любого $\gamma>0$ (частные случаи этого условия рассмотрены в~\cite{3-us}).

\section{Оценка. Состоятельность}

Пусть ${\cal P}_0$~--- некоторое множество распределений и
распределение $P^\varepsilon$ ошибки~$\varepsilon$ принадлежит этому
множеству: $P^\varepsilon\in{\cal P}_0$. Обозначим через ${\cal D}_0$ 
множество распределений (необязательно всех) случайной
величины~$X$, для которых модель~(1) идентифицируема. Это значит,
что если $P_1,P_2\hm\in{\cal D}_0$, $Q_1,Q_2\hm\in{\cal P}_0$, и $P_1\ast
Q_1\hm=P_2\ast Q_2$, то $P_1\hm=P_2$ и $Q_1\hm=Q_2$. Если, например,
распределение~$\varepsilon$ сим\-мет\-рич\-ное устойчивое с показателем~$\alpha$, 
т.\,е.\ ${\cal P}_0\hm={\cal U}_\alpha$, то можно взять ${\cal D}_0\hm={\cal P}_\alpha^c$.

Пусть $\rho(\cdot,\cdot)$~--- метрика в пространстве~${\cal P}$,
удовлетворяющая следующим условиям.
\begin{description}
\item[У1.] В этой метрике любое выборочное распределение является
сильно (с вероятностью~1) состоятельной оценкой соответствующего
теоретического распределения. Другими словами,
$$
{\rm P}\left(\lim_{n\to\infty}\rho(\hat P_n,P)=0\right)=1\,,
$$
где $P\in{\cal P}$ и $\hat P_n$~--- выборочное распределение,
построенное по выборке объема~$n$ из генеральной совокупности
с распределением~$P$.
\item[У2.] Если $\rho(P_n,P)\to0$ и $\rho(Q_n,Q)\to0$, то
$\rho(P_n\ast Q_n,P\ast Q)\to0$.
\end{description}

Примеры метрик, удовлетворяющих условиям~У1 и~У2, даны в следующем
разделе. В частности, метрика, используемая в~\cite{2-us}, удовлетворяет им.

Пусть $Y_1,\ldots ,Y_n$~--- случайная выборка в модели~(1), т.\,е.\ это
независимые одинаково распределенные случайные величины, имеющие
одинаковое с~$Y$ распределение. Обозначим через $\hat P^Y_n$
выборочное распределение, построенное по этой выборке. Оценку
определим следующим образом. Пару $(P^X,P^\varepsilon)$ оценим таким
элементом множества ${\cal D}_0\times{\cal P}_0$, который
минимизирует расстояние $\rho(P'\ast P'',\hat P_n^Y)$, и обозначим
его $(\hat P^X_n,\hat P^\varepsilon_n)$. Другими словами,
\begin{equation*}
\rho\left(\hat P_n^X\ast\hat P_n^\varepsilon,\hat P_n^Y\right)
\le\rho\left(P'\ast P'',\hat P_n^Y\right) %\label{e5-us}
\end{equation*}
для любых $P'\in{\cal D}_0$, $P''\in{\cal P}_0$. Заметим, что
если такая оценка существует для каждого~$n$, то
\begin{equation}
{\rm P}\left(\lim_{n\to\infty}
\rho\left(\hat P_n^X\ast\hat P_n^\varepsilon,P^Y\right)=0\right)=1\,.
\label{e6-us}
\end{equation}
Действительно,
\begin{multline*}
\rho\left(\hat P_n^X\ast\hat P_n^\varepsilon,P^Y\right)\le
\rho\left(\hat P_n^X\ast\hat P_n^\varepsilon,\hat P_n^Y\right)+{}\\
{}+
\rho\left(\hat P_n^Y,P^Y\right)\le
\rho\left(P^X\ast P^\varepsilon,\hat P_n^Y\right)+
\rho\left(\hat P_n^Y,P^Y\right)={}\\
{}=2\rho\left(\hat P_n^Y,P^Y\right)\,,
\end{multline*}
откуда~(\ref{e6-us}) следует в силу условия~У1. Кстати, попутно получено
полезное неравенство
$$
\rho\left(\hat P_n^X\ast\hat P_n^\varepsilon,P^Y\right)\le
2\rho\left(\hat P_n^Y,P^Y\right)\,.
$$

\smallskip

\noindent
\textbf{Теорема~2.} \textit{Если множества ${\cal D}_0$ и ${\cal P}_0$ не
пусты и компактны (в метрике $\rho$), то $\hat P_n^X$ и $\hat
P_n^\varepsilon$ существуют и являются сильно (почти наверное)
состоятельными оценками $P^X$ и~$P^\varepsilon$, т.\,е.\
\begin{align*}
{\rm P}\left(\lim_{n\to\infty}\rho\left(\hat P_n^X,P^X\right)=0\right)&=1\,;\\
{\rm P}\left(\lim_{n\to\infty}\rho\left(\hat P_n^\varepsilon,P^\varepsilon\right)=0\right)&=1\,.
\end{align*}
}


\smallskip

\noindent
Д\,о\,к\,а\,з\,а\,т\,е\,л\,ь\,с\,т\,в\,о\,.\ Существование очевидно в силу компактности
множеств ${\cal D}_0$ и~${\cal P}_0$.
Пусть $\omega$~--- произвольное элементарное
событие в исходном вероятностном пространстве, такое что
\begin{equation}
\lim_{n\to\infty}\rho\left(\hat P_n^X\ast\hat P_n^\varepsilon,P^Y\right)=0\,.
\label{e7-us}
\end{equation}
Вероятность события, состоящего из всех таких элементарных
событий, равна~1. Зафиксируем это~$\omega$, и далее в доказательстве
все случайные величины и функции будем рассматривать на нем.

Рассмотрим последовательности $\hat P_n^X\hm=\hat P_n^X(\omega)$ и
$\hat P_n^\varepsilon\hm=\hat P_n^\varepsilon(\omega)$. Предположим,
что либо $\rho(\hat P_n^X,P^X)$, либо
$\rho(\hat P_n^\varepsilon,P^\varepsilon)$ (либо оба)
не сходятся к нулю. Тогда существует подпоследовательность
$\{n_k^{(1)}\}\subseteq\{n\}$
такая, что
$$
\rho\left(\hat P^X_{n_k^{(1)}},P^X\right)\ge\delta\,,
$$
или подпоследовательность
$\{n_k^{(2)}\}\subseteq\{n\}$ такая, что
$$
\rho\left(\hat P^\varepsilon_{n_k^{(2)}},P^\varepsilon\right)\ge\delta
$$
для некоторого $\delta\hm>0$.
Не ограничивая общности (и ради упрощения обозначений), можно
считать, что подпоследовательности $\{n_k^{(1)}\}$ и $\{n_k^{(2)}\}$
совпадают с самой
последовательностью~$\{n\}$, т.\,е.\
\begin{equation}
\rho\left(\hat P^X_n,P^X\right)\ge\delta\label{e8-us}
\end{equation}
или
\begin{equation}
\rho\left(\hat P^\varepsilon_n,P^\varepsilon\right)\ge\delta\,.
\label{e9-us}
\end{equation}
Поскольку последовательность $\{\hat P_n^X\}$ принадлежит компактному
множеству~${\cal D}_0$, существуют подпоследовательность
$\{n_k\}\subseteq\{n\}$ и распределение $P'\in{\cal D}_0$
такие, что
\begin{equation}
\lim\limits_{k\to\infty}\rho\left(\hat P^X_{n_k},P'\right)=0\,.\label{e10-us}
\end{equation}
В свою очередь, поскольку подпоследовательность
$\{P^\varepsilon_{n_k}\}$
содержится в компактном множестве ${\cal P}_0$, существуют ее
подпоследовательность $\{n'_k\}\subseteq\{n_k\}$ и
распределение $P''\in{\cal P}_0$ такие, что
\begin{equation}
\lim\limits_{k\to\infty}\rho\left(\hat P^\varepsilon_{n'_k},P''\right)=0\,.\label{e11-us}
\end{equation}
Из~(\ref{e10-us}) и~(\ref{e11-us}) в силу свойства У2 мет\-ри\-ки~$\rho$ следует, что
$$
\lim\limits_{k\to\infty}
\rho\left(\hat P^X_{n'_k}\ast\hat P^\varepsilon_{n'_k},P'\ast P''\right)=0\,.
$$
С другой стороны, из~(\ref{e7-us}) имеем:
$$
\lim\limits_{k\to\infty}
\rho\left(\hat P^X_{n'_k}\ast\hat P^\varepsilon_{n'_k},P^X\ast P^\varepsilon\right)=0\,.
$$
Таким образом,
\begin{equation}
P^X\ast P^\varepsilon=P'\ast P''\,,\label{e12-us}
\end{equation}
причем
\begin{equation}
P^X,P'\in{\cal D}_0,\ \
P^\varepsilon,P''\in{\cal P}_0\label{e13-us}
\end{equation}
и как минимум одно из неравенств~(\ref{e8-us}), (\ref{e9-us}) имеет место. Если
выполнено~(\ref{e8-us}), то в силу~(\ref{e10-us})
\begin{equation}
P^X\not=P'\,,\label{e14-us}
\end{equation}
а~(\ref{e12-us}) и~(\ref{e14-us}) противоречат~(\ref{e13-us}). 
Если имеет место~(\ref{e9-us}), то в силу~(\ref{e11-us})
\begin{equation}
P^\varepsilon\not=P''\label{e15-us}
\end{equation}
и теперь~(\ref{e12-us}) и~(\ref{e15-us}) противоречат~(\ref{e13-us}). 
Полученные противоречия доказывают теорему.

\medskip

\noindent
\textbf{Следствие.} \textit{Пусть ошибка~$\varepsilon$ имеет симметричное
устойчивое распределение с показателем~$\alpha$ и па\-ра\-мет\-ром масштаба~$\gamma$, 
а случайная величина~$X$ ограничена. Если известны
постоянные $a$ и~$b$ такие, что $|X|\hm\le a$ и $|\gamma|\hm\le b$,
то оценка $(\hat P_n^X,\hat P_n^\varepsilon)$ является состоятельной
для $(P^X,P^\varepsilon)$.}


Другие подобные следствия могут быть легко получены с использованием
условий~A, Б и~В. Предо\-став\-ля\-ем это читателю.

\section{Выбор метрики}

В этом разделе рассмотрим несколько метрик, которые могут
использоваться в качестве метрики $\rho(\cdot,\cdot)$, т.\,е.\
удовлетворяют условиям~У1 и~У2. Пусть $P$ и~$Q$ (с индексом или без
индекса)~--- вероятностные распределения. Обозначим соответ\-ст\-ву\-ющие
функции распределения и характеристические функции как $F(x)$,
$G(x)$ и $\varphi(t)$, $\psi(t)$.

\smallskip
\textbf{1.}\ \textbf{Метрика Колмогорова (равномерная метрика)}

Метрика определяется как равномерное расстояние между функциями
распределения:
$$
\rho(P,Q)=\sup_{-\infty<x<\infty}|F(x)-G(x)|\,.
$$
Условие У1 выполнено в силу теоремы Гли\-вен\-ко--Кан\-тел\-ли. Условие~У2
следует из очевидного неравенства
\begin{multline*}
\sup\limits_{-\infty<x<\infty}|F_n\ast G_n(x)-F\ast G(x)|\le{}\\
{}\le\!\!\!
\sup\limits_{-\infty<x<\infty}|F_n(x)-F(x)|+\!\!\!\sup\limits_{-\infty<x<\infty}|G_n(x)-G(x)|\,.
\end{multline*}

\smallskip
\textbf{2.}\ \textbf{Взвешенная равномерная метрика, основанная на характеристических
функциях}

Хорошо известно, что выборочная характеристическая функция
почти наверное сходится к соответствующей теоретической характеристической
функции равномерно на каждом ограниченном интервале. Более того,
если $\varphi(t)$
и $\varphi_n(t)$~--- соответственно теоретическая и выборочная
характеристические функции ($n$~--- объем выборки), то почти наверное
\begin{equation}
\lim\limits_{n\to\infty}\sup_{|t|\le T_n}|\varphi_n(t)-\varphi(t)|=0\label{e14-1-us}
\end{equation}
для любой последовательности положительных чисел $\{T_n\}$, удовлетворяющей
условию:
$$
\lim\limits_{n\to\infty}\fr{\log T_n}{n}=0\,.
$$
Таким образом, можно использовать следующую метрику:
$$
\rho(P,Q)=\sup_{-\infty<t<\infty}|\varphi(t)-\psi(t)|w(t)\,,
$$
где $w(t)$~--- положительная четная функция, удовлетворяющая условию
$\lim\limits_{t\to\infty}w(t)=0$. Условие~У1 следует из~(\ref{e14-1-us}) (и даже просто из
равномерной сходимости на каждом ограниченном интервале).
Условие~У2 следует из неравенства
\begin{multline}
|\varphi_n(t)\psi_n(t)-\varphi(t)\psi(t)|\le{}\\
{}\le
|\varphi_n(t)-\varphi(t)|+|\psi_n(t)-\psi(t)|\,.\label{e15-1-us}
\end{multline}

\smallskip
\textbf{3.} \textbf{Интегральная метрика, основанная на характеристических
функциях}


$$
\rho(P,Q)=\int\limits_{-\infty}^\infty|\varphi(t)-\psi(t)|^pw(t)\,dt\,,
$$
где $p>0$, а $w(t)$~--- плотность распределения вероятностей. Как и в
предыдущем случае, (\ref{e14-1-us}) гарантирует выполнение условия~У1. Условие~У2 
следует из~(\ref{e15-1-us}). Если $p\hm=1$, эта метрика совпадает с метрикой,
использованной в~\cite{2-us}.

\smallskip
\textbf{4.} \textbf{Метрика Леви}


\begin{multline*}
\rho(P,Q)=L(F,G)=
\inf\{h\ge0:F(x-h)-h\le{}\\
{}\le G(x)\le F(x+h)+h,-\infty<x<\infty\}\,.
\end{multline*}
Сходимость в метрике Леви слабее, чем сходимость в метрике
Колмогорова, поэтому условие~У1 выполнено в силу теоремы
Гли\-вен\-ко--Кан\-тел\-ли. Условие~У2 следует из неравенства
$$
L(F_n\ast G_n,F\ast G)\le L(F_n,F)+L(G_n,G)\,,
$$
которое можно найти в любой книге по вероятностным метрикам.

{\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{9}

\bibitem{1-us}
\Au{Matias C.} Semiparametric deconvolution with unknown noise
variance~// ESIAM Probab. Statist., 2002. Vol.~6. P.~271--292.

\bibitem{3-us}
\Au{Butucea C., Matias C.} Minimax estimation of the noise level
and of the deconvolution density in a semiparametric convolution
model~// Bernoulli, 2005. Vol.~11. No.\,2. P.~309--340.

\bibitem{2-us}
\Au{Schwarz M., Van Bellegem~S.} Consistent density
deconvolution under partially known error distribution~// Stat.
Probab. Lett., 2010. Vol.~80. No.\,3--4. P.~236--241.


\label{end\stat}

%\bibitem{4-us}
%\Au{Cs$\ddot{\mbox{o}}$rg\!\!\!{\ptb{\H{o}}}~S., Totik~V.} On how long interval is the
%empirical characteristic function uniformly consistent?~// Acta Sci.
%Math., 1983. Vol.~45. P.~141--149.
 \end{thebibliography}
}
}


\end{multicols}       
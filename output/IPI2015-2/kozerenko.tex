\def\stat{kozerenko}

\def\tit{АССОЦИАТИВНЫЕ ПОРТРЕТЫ ПРЕДМЕТНОЙ ОБЛАСТИ~--- ИНСТРУМЕНТ
АВТОМАТИЗИРОВАННОГО ПОСТРОЕНИЯ СИСТЕМ BIG DATA ДЛЯ ИЗВЛЕЧЕНИЯ
ЗНАНИЙ: ТЕОРИЯ, МЕТОДИКА, ВИЗУАЛИЗАЦИЯ, ВОЗМОЖНОЕ
ПРИМЕНЕНИЕ$^*$}

\def\titkol{Ассоциативные портреты предметной области~--- инструмент
автоматизированного построения систем big data} % для извлечения
%знаний: теория, методика, визуализация, возможное применение$^*$}

\def\aut{И.\,В.~Галина$^1$, Е.\,Б.~Козеренко$^2$, Ю.\,И.~Морозова$^3$,
Н.\,В.~Сомин$^4$, М.\,М.~Шарнин$^5$}

\def\autkol{И.\,В.~Галина, Е.\,Б.~Козеренко, Ю.\,И.~Морозова и~др.}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Галина И.\,В.}
\index{Козеренко Е.\,Б.}
\index{Морозова Ю.\,И.}
\index{Сомин Н.\,В.}
\index{Шарнин М.\,М.}

{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
{Работа выполнена при поддержке РФФИ (проект 13-07-00272).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Институт проблем информатики Федерального исследовательского
центра <<Информатика и~управление>> Российской академии наук,
irn\_gl@mail.ru}
\footnotetext[2]{Институт проблем информатики Федерального исследовательского
центра <<Информатика и~управление>> Российской академии наук, kozerenko@mail.ru}
\footnotetext[3]{Институт проблем информатики Федерального исследовательского
центра <<Информатика и~управление>> Российской академии наук, yulia-ipi@yandex.ru}
\footnotetext[4]{Институт проблем информатики Федерального исследовательского
центра <<Информатика и~управление>> Российской академии наук, somin@post.ru}
\footnotetext[5]{Институт проблем информатики Федерального исследовательского
центра <<Информатика и~управление>> Российской академии наук, mc@keywen.com}


\Abst{Представлена методика создания систем извлечения
знаний, основанная на подходе, главным инструментом которого является
автоматизированное формирование ассоциативного портрета предметной
области (АППО) и~построение семантического контекстного пространства
(СКП). Идеология АППО базируется на дистрибутивной гипотезе,
утверждающей, что семантически близкие (или связанные) лексемы имеют
похожий контекст и,~наоборот, при похожем контексте лексемы
семантически близки. В применяемой модели используется расширенная
гипотеза, включающая исследование сходства и~различия в~контекстах не
только отдельных лексем, но и~произвольных многолексемных фрагментов~---
значимых словосочетаний (ЗС). Приведены примеры реализованных проектов
для различных предметных областей (ПО).}

     \KW{семантическое моделирование; ассоциативные связи;
математическая статистика; дистрибутивная семантика; big data;
автоматизированные системы извлечения знаний; электронные корпуса
     ЕЯ-текс\-тов; семантический поиск; интеллектуальные
     ин\-тер\-нет-тех\-но\-логии}

\vspace*{-10pt}

     \DOI{10.14357/19922264150211}


\vskip 14pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}


    \section{Введение. Постановка проблемы}

     Создание лингвистической и~предметной базы и~разработка
эффективной методики для систем извлечения и~обработки знаний на
сверхбольших объемах текста (big data), свободно распространяемых в~Интернете, относятся к~фундаментальным задачам компьютерной
лингвистики. При этом актуально существенное снижение трудозатрат,
увеличение скорости автоматизированной обработки с~минимальным
участием человека, повышение точности семантического поиска, качества
результатов работы системы. Один из традиционных методов~---
составление вручную универсальных хорошо выверенных тезаурусов,
который требует колоссальных трудовых и~временн$\acute{\mbox{ы}}$х ресурсов, поэтому в~последние годы ведется поиск новых путей семантического моделирования
для создания лингвистической предметно-ориентированной базы знаний.

Одним из таких направлений является инструмент, названный авторами
<<ассоциативный портрет предметной области>>, на основе
которого можно с~достаточной надежностью решать большинство
важнейших проблем, связанных с~информационным поиском, извлечением
структур знаний и~построением классификаций~[1--3]. Разработка данного
инструмента основана на гипотезе о~привлечении ассоциативных связей для
определения значения, полный смысл которого выявляется с~помощью
контекстных окружений, что дает возможность автоматизации процесса
разграничения значений и~извлечения знаний из текстов.

     Ассоциативный портрет~--- это совокупность наиболее характерных
предметных и~лингвистических знаний, свойственных определенной
ПО. Под предметными знаниями понимаются
присущие ПО термины, понятия, связанные различного рода
ассоциативными связями. Лингвистические знания~--- это варианты
словарного (лексического) выражения понятий. Суть предлагаемой методики
заключается в~анализе текстов естественного языка (ЕЯ), относящихся к~определенной ПО, с~выяв\-лением ассоциативных связей и~построении на этой основе ассоциативных портретов, характери\-зу\-ющих
данную область. В~качестве таких портретов предлагается
СКП, служащее для представления как
лингвистических, так и~предметных \mbox{знаний}.

     Создаваемая методика извлечения знаний на основе АППО строится на
сочетании методов статистики, корпусной лингвистики, дистрибутивной
семантики и~машинного обучения и~реализуется в~комплексной технологии,
которая предполагает разработку лингвистической компоненты~--- СКП,
представляющего собой словарь ЗС ПО, элементы которого соединены ассоциативными связями. Таким
образом, инструмент выделения АППО из текстов лег в~основу технологии,
позволяющей автоматически выявлять ЗС и~ассоциативные связи из
неструктурированных ес\-те\-ст\-вен\-но-язы\-ко\-вых текстов (ЕЯ-текс\-тов),
проходя через этапы предварительного обучения на текстах, в~том числе
взятых из различных ин\-тер\-нет-ре\-сур\-сов. Обработка больших массивов
текстов (big data), постоянно по\-пол\-ня\-емых в~ин\-тер\-нет-сре\-де, позволяет
собирать необходимые статистические данные для формирования достаточно
полной картины о~ПО, представленной в~виде СКП.
Возможность проводить машинное обучение на большом количестве
примеров придает системе определенную гибкость и~улучшает результаты.

     Отметим, что традиционное для дистрибутивной семантики
использование диагностических смыслоразличительных контекстов для
разграничения дистрибутивных признаков ис\-сле\-ду\-емых\linebreak лексем в~предлагаемой модели дополняется расширенным пониманием
дистрибутивной гипотезы, при котором семантически близкими (или
связанными) при сходстве контекстов признаются не~только отдельные
лексемы (термины), но и~произвольные многолексемные фрагменты текста
(ЗС). Предлагаемый подход~[1--7] предусматривает
исследование различных типов и~источников контекста, а~также различных
методов выделения контекста и~оценки силы ассоциативной связи по
контекстным векторам; при этом исследуется как простая совместная
встречаемость ЗС в~тексте, так и~совместная встречаемость ЗС в~текстах в~рамках заданных лек\-си\-ко-син\-так\-си\-че\-ских шаблонов. Кро-\linebreak ме того,
учитывается порядок слов в~лексических последовательностях, что позволяет
добиться более качественного выполнения вышеперечисленных задач, в~том
числе выявления ассоциативных связей словосочетаний, объектов и~именованных сущностей. Объективные критерии статистической\linebreak оценки,
включенные в~методику наряду с~ло\-ги\-ко-линг\-ви\-сти\-че\-ски\-ми методами и~правилами дистрибутивной семантики, применяются для разрешения
семантической неоднозначности лексем (терми\-нов) и~ЗС на основе
вероятностных механизмов, что повышает скорость и~эффективность
семантического поиска, а~также обеспечивает автоматизированное отнесение
обрабатываемого текста к~той или иной ПО. Некоторым
побочным ответвлением указанной методики стала разработка средств
визуализации АППО, которые позволяют точно отобразить координаты
выявленных объектов в~двумерном
СКП и~использовать эвристические методы для установления
степени близости объектов~[8]. Исследования по выявлению АППО
проводятся на материале ЕЯ-текс\-тов на русском (в~основном) и~английском (в меньшем объеме) языках.

     Разработка и~реализация на практике методики АППО и~СКП
предусматривает следующие этапы:
     \begin{enumerate}[1.]
     \item  Создание и~постоянное пополнение кор\-пу\-сов
ЕЯ-текс\-тов на основе обработки большого
потока данных из Интернета (круглосуточным\linebreak мониторингом на двух
серверах).
     \item Создание методов автоматизированного отнесения текстов к~различным ПО.
     \item Применение методов статистики, дистрибутивной семантики,
математической лингвистики и~машинного обучения на постоянно
об\-нов\-ля\-емой выборке текстов для совершенствования технологии и~создания
автоматизированной сис\-те\-мы выявления ЗС и~их ассоциативных связей.
     \item Обработка полученных данных и~построение СКП различных
ПО с~их визуализацией.
     \item Проектирование общей архитектуры системы, основанной на
предлагаемой технологии.
     \end{enumerate}

     Конечным итогом развития инструмента \mbox{АППО} будет создание
комплекса программного обеспечения, отлаженного на обучающих выборках
и решающего следующие задачи:
     \begin{itemize}
\item поиск в~интернет-сре\-де корпусов текстов, связанных с~заданной
ПО;
\item выделение из полученного электронного корпуса текстов
терминологического материала и~оценка значимости каждого термина на
основе статистических критериев;
\item выделение семантических связей (ассоциаций) из корпуса текстов с~использованием методов машинного обучения на основе АППО, а~также
стандартных тезаурусов в~качестве обучающей выборки;
\item выделение терминологических словосочетаний и~именованных
сущностей из корпуса текстов на основе методов машинного обучения и~клас\-тер\-но\-го анализа в~пространстве контекстных векторов (СКП);
\item демонстрация возможностей методики на примере визуализации
ассоциативных портретов ЗС определенной ПО.
\end{itemize}

    \section{Современное состояние исследований}

     В настоящее время известно значительное чис\-ло работ по
автоматическому извлечению семантических связей из больших массивов
текстов на ЕЯ~[1--33].

Наиболее успешные подходы
используют метод дистрибутивной семантики~[1--3, 5--7, 10, 12--15, 18, 19, 25] и~модели семантических векторных
пространств (СВП). Дистрибутивная семантика\linebreak занимает\-ся вычислением
степени семантической близости между лингвистическими %\linebreak
 единицами на\linebreak
основании их контекстных окружений, или дистрибутивных
признаков~\cite{17-koz, 18-koz, 19-koz}. Основными сферами применения
дистрибутивных моделей являются: разрешение лексической
неоднознач\-ности,\linebreak
 информационный поиск, клас\-те\-ри\-за\-ция документов,
автоматическое фор\-ми\-рование словарей (словарей семантических
\mbox{отношений}, двуязычных словарей), создание семантических (визуальных)
карт, определение тематики документа и~др. Теоретические основы
исследования дистрибуций восходят еще к~методике З.~Харриса. Похожие
идеи выдвигали и~основоположники структурной лингвистики Ф.~де Соссюр
и~Л.~Витгенштейн. Дистрибутивная семантика, как и~все опирающиеся на
нее подходы, базируется на гипотезе о~том, что лингвистические элементы со
схожей дистрибуцией имеют близкие значения~\cite{23-koz, 24-koz}.
В~основе всех современных вариантов дистрибутивного подхода лежат
количественные оценки, которые характеризуют совместную встречаемость
языковых единиц текста в~контекстах определенной величины.

     Для оценки степени связанности этих единиц вводится коэффициент
<<силы связи>>, который рассчитывается по некоторой формуле. Обычно в~качестве вычислительного инструмента и~способа представления моделей
используется линейная алгебра. Информация о~дистрибуции
лингвистических единиц представляется в~виде многоразрядных векторов, а~семантическая близость между лингвистическими единицами вычисляется
как расстояние между векторами. Многоразрядные векторы образуют
матрицу, где каждый вектор соответствует лингвистической единице (слову
или словосочетанию), а~каждое измерение вектора соответствует контексту
(документу, параграфу, предложению, словосочетанию, слову). Для
вычисления меры близости\linebreak между векторами могут использоваться
различные формулы: расстояние Минковского, расстояние Манхеттена,
евклидово расстояние, расстояние\linebreak Чебыше\-ва, скалярное произведение,
косинусная мера. Наиболее популярной в~настоящее время является
косинусная мера. Вне за\-ви\-си\-мости от выбранной формулы расчета в~ней
обычно используются характеристики совместной встречаемости пар слов и~одиночной встречаемости каждого из слов. Величина контекста, в~рамках
которого осуществляются подсчеты коэффициентов <<силы связи>>, как
показывают результаты исследований, позволяет наиболее вероятно
устанавливать:
     \begin{itemize}
     \item при малых размерах контекста, ограниченного одним или
двумя соседними словами,~--- контактные синтагматические связи
словосочетаний;
     \item при размере 5--10~слов~--- дистантные синтагматические
связи и~парадигматические отношения;
     \item дальнейшее увеличение ширины контекста до~50--100~слов
(размер предложения, сверхфразового единства, абзаца)~--- тематические
связи между словами.
     \end{itemize}

     Тематические связи могут оказаться доминирующими, если принять
размер контекста величиной с~сам текст. Модели векторных пространств
находят все более широкое применение в~исследованиях, связанных с~семантическими моделями ЕЯ, и~имеют разнообразный спектр
потенциальных и~действующих приложений~\cite{14-koz, 25-koz}. Данная
область в~настоящее время является одной из наиболее актуальных. Следует
отметить модели Word-Space Model~\cite{23-koz} и~Semantic Space
Model~\cite{26-koz}. В~основе этих работ M.~Sahlgren лежит пространство
лексем и~термов. Такое пространство базируется на распределении слов в~корпусе текстов с~целью пред\-став\-ле\-ния их семантической связанности путем
оценки пространственной близости. В~настоящее время существуют
разнообразные варианты подобных методов исследования и~построенных
моделей. Так, Себастьян Падо из университета Саарланд (Германия) и~Мирелла Лапата из Шеффилдского университета (Великобритания)
рассматривают вопрос по\-стро\-ения семантических пространств на основе
традиционных векторных моделей с~учетом синтаксических отношений.
Семантические свойства слов представляются в~виде частотной матрицы,
каж\-дый ряд которой соответствует уникальному целевому слову, а~каждая
колонка~--- лингвистическому контексту. Семантическая информация
извлекается из текстов большого объема на основе анализа окружения слова.
Слово рассматривается как точка в~многомерном семантическом
пространстве. На основе близости между точками семантического
пространства вычисляется семантическое сходство между словами с~использованием метрик. Анализ семантического сходства выполняется на
основе статистических методов с~расчетом частотности появления в~тексте
близких точек семантического пространства. В процессе анализа
рассматриваются метрики Евклида, Жаккара, Куль\-ба\-ка--Лейб\-ле\-ра и~др. По результатам исследования делается вывод о~том, что контекстное
окружение играет важную роль в~распознавании лексических отношений
между словами.

     Существует множество разновидностей моделей дистрибутивной
семантики, которые различаются по следующим параметрам:
     \begin{itemize}
     \item  тип контекста (размер контекста, правый или левый контекст,
ранжирование);
     \item  количественная оценка частоты встречаемости слова в~данном
контексте (абсолютная частота, энтропия, совместная информация и~пр.);
     \item  метод вычисления расстояния между векторами (косинусная
мера, скалярное произведение, расстояние Минковского и~пр.);
     \item  метод уменьшения размерности матрицы (случайная проекция,
сингулярное разложение и~пр.).
     \end{itemize}

     Наиболее известными современными моделями дистрибутивной
семантики являются латентный семантический анализ, разработанный для
решения проблемы синонимии при информационном поиске~\cite{20-koz}, и~модель языка как гиперпространства, разработанная в~виде модели
семантической памяти человека~\cite{21-koz}.

     Концепция СВП впервые была реализована
     в~ин\-фор\-ма\-ци\-он\-но-поиско\-вой системе SMART~\cite{28-koz}.
Идея СВП состоит в~представлении каждого документа из коллекции текстов
в виде точки в~многомерном семантическом пространстве, которой
соответствует вектор в~векторном пространстве ЗС.
Точки, расположенные ближе друг к~другу в~этом пространстве, считаются
более близкими по смыс\-лу. Пользовательский запрос рассматривается как
псевдодокумент и~тоже представляется как точка в~этом же пространстве.
Документы сортируются в~порядке возрастания расстояния, т.\,е.\ в~порядке
уменьшения семантической близости от запроса, и~в~таком виде
предоставляются пользователю.

     В настоящее время многие поисковики используют СВП для измерения
степени близости запроса и~найденных документов~\cite{22-koz}. Baroni и~Lenci~\cite{9-koz} предложили обобщенную модель, названную
<<дистрибутив\-ная память>>, которая является обобщением ранее известных
моделей векторных пространств (vector spaces), семантических пространств
(semantic spaces), пространств слов (word spaces), семантических моделей
корпусной статистики (corpus-based semantic models) и~дистрибутивных
семантических моделей (distributional semantic models). Rapp в~своих
исследованиях~\cite{7-koz} использовал контекстное векторное
пространство для оценки семантической близости слов. Его сис\-те\-ма достигла
результата 92,5\% на тесте по выбору наиболее подходящего синонима из
стандартного теста английского языка TOEFL (Test of English as Foreign Language), в~то время как средний
результат людей был 64,5\%.

     В разрабатываемом проекте используется более широкое понятие,
выраженное термином <<семантическое контекстное пространство>>, где точки пространства соответствуют контекстным векторам не
отдельных слов (терминов), а~ЗС~\cite{1-koz, 6-koz}. Под ЗС
понимаются лексические последовательности, имеющие тенденцию к~совместной встречаемости. В~лингвистике для обозначения
ЗС используется также термин <<коллокация>>, впервые
введенный в~<<Словаре лингвистических терминов>> О.\,С.~Ахмановой еще
в~1966~г. Исследованиям коллокаций русского языка посвящен большой
объем литературы (см., например,~\cite{29-koz, 30-koz}). В~теоретической
лингвисти\-ке под коллокациями понимают словосочетания из двух или более
слов, которые обуслов\-ли\-ва\-ют друг друга семантически и~грамматически~\cite{31-koz}. В~корпусной лингвистике коллокациями
называют статистически устойчивые словосочетания, причем они могут быть
как фразеологизмами, так и~свободными. Для выделения ЗС в~компью-\linebreak терной
лингвистике используются различные статистические меры (названные
мерами ассоциации\linebreak или ассоциативной связанности~---
 association
measures (\textit{англ}.)), вы\-чис\-ля\-ющие силу связи между\linebreak элементами в~составе
коллокации. В~литературе упоминаются десятки способов расчета мер
ассоциативной связанности; например, MI, $t$-score
     и~log-likelihood методы вычисления.

     Мера MI (mutual information), введенная в~работе~\cite{15-koz}, сравнивает зависимые кон\-текст\-но-свя\-зан\-ные
частоты с~независимыми частотами слов в~тексте. Если значение MI
превосходит определенное пороговое значение, то словосочетание считают
статистически значимым. Мера MI вы\-чис\-ля\-ет\-ся по следующей формуле:
     $$
    \mathrm{MI}= \mathrm{log}_2 \fr{f(n,c) N}{f(n) f(c)}\,,
     $$
где $n$~--- первое слово словосочетания; $c$~--- второе слово
словосочетания; $f(n, c)$~--- частота совместной встречаемости двух слов;
$f(n)$ и~$f(c)$~--- абсолютные частоты встречаемости каждого слова по
отдельности; $N$~--- общее число словоупотреблений в~корпусе.

     Мера $t$-score также используется при ответе на вопрос,
насколько неслучайным является сочетание двух или более слов в~тексте.
Для вычисления $t$-score используется следующая формула:
     $$
     t\mbox{-}\mathrm{score} = \fr{f(n,c)-f(n) f(c)/N}{\sqrt{f(n,c)}}\,.
     $$

     Также достаточно часто применяется мера, известная под названием
log-likelihood, или логарифмическая функция правдоподобия, введенная в~\mbox{статье}~\cite{16-koz}. Для вычисления log-likelihood
применяется формула:
     $$
     \mathrm{log}\mbox{-}\mathrm{likehood}\,= 2\sum f(n,c)  \mathrm{log}_2
\fr{f)n,c) N}{f(n) f(c)}\,.
     $$

     Новым и~чрезвычайно важным в~предлагаемом авторами подходе~---
по сравнению с~существу\-ющи\-ми методиками~--- является то, что изначально
СКП представляет собой абстрактную модель, не связанную с~конкретной
ПО. Приложение разрабатываемой модели связано с~автоматизированным выбором той или иной ПО и~ее
компонент: ЗС, ассоциативных связей и,~соответственно, контекстов для их выделения. В~результате строится
система множественных ассоциативных связей, а~затем формируется
АППО. Разработчики СВП
отмечают, что основная проблема этого подхода заключается в~трудностях
учета порядка слов, составляющих контексты. В~рамках же данного проекта
(СКП) эта проблема решается путем перехода от контекста слов к~контексту
ЗС. Отметим, что первоначально технология СВП
развивалась для английского языка. Настоящий проект с~использованием
модели СКП уже на первом этапе его реализации работает как с~русскими
(основное внимание будет уделено все же именно русскому языку), так
и~с~английскими ЕЯ-текс\-та\-ми. Также в~дальнейшем возможно
включение других языков. Таким образом, модель СКП является
расширением моделей СВП.

    \section{Материал исследования}

     Работы над проектом по созданию системы для извлечения знаний,
основанной на подходе, главным инструментом которого является
автоматизированное формирование АППО и~построение СКП, ведутся
коллективом авторов на протяжении последних
нескольких лет~[1--3, 5--7, 16, 21, 28, 29, 33].

     Данный проект реализуется на сверхбольших объемах данных (big
data), свободно представленных в~ин\-тер\-нет-сре\-де, что существенно
повышает качество формируемых АППО. Материалом исследований служат
свободно распространяемые\linebreak сетевые русскоязычные ЕЯ-текс\-ты, а~также~---
для некоторых ПО~--- тексты на английском языке.
В~дальней\-шем возможно расширение проекта на другие языки.

     За данный период были разработаны методы итеративного
формирования корпусов по определенной ПО, на основе
которых созданы постоянно пополняемые из Интернета электронные корпуса
текстов. Для решения этой задачи научным коллективом были арендованы
2~сервера, на которых запущено непрерывное скачивание больших массивов
сетевых текстов (big data) и~разбиение этого массива на ПО.



     К настоящему времени обработаны следу\-ющие~ПО:
     \begin{itemize}
     \item на русском
языке~--- <<Дистанционное зондирование сельскохозяйственных земель>>,
<<Психология делового общения>>;\\[-13.5pt]
\item <<Биз\-нес-про\-цес\-сы
предприятий>>;\\[-13.5pt]
\item <<Со\-ци\-аль\-но-политический портрет регионов России>>
(в качестве обучающей выборки для этой тематики взято сужение темы до
одного федерального региона~--- <<Со\-ци\-аль\-но-по\-ли\-тический
портрет Татарстана>>, источниками сообщений для этой темы являются:
официальные сайты государственных учреждений, партий; общероссийские
и~региональные средства массовой информации (СМИ); социальная сеть Вконтакте, микроблоги Твиттера
и~т.\,п.);\\[-13.5pt]
\item <<Мониторинг общественного мнения в~со\-ци\-аль\-но-политической
сфере>>, суженная до обучающей выборки <<Протестная активность>>;\\[-13.5pt]
 \item  АНПА (<<Автономные необитаемые подводные аппараты>>) на
русском и~английском языке;\\[-13.5pt]
\item <<Computer Science>> (на английском языке)
и~др.
\end{itemize}

     По данным темам найдено и~обработано от десятка тысяч до полутора
миллионов документов на русском и~английском языках, общим объемом
около 160~ГБ. Так, по теме <<Со\-ци\-аль\-но-по\-ли\-ти\-че\-ский портрет
Татарстана>> сформирован дайджест текстов объемом в~20~ГБ, а~по
тематике <<Протестная активность>>~--- в~28~ГБ.

\vspace*{-6pt}

    \section{Методика построения ассоциативного портрета предметной области}

    \vspace*{-1pt}

     \subsection{Основа методики}

     \vspace*{-2pt}

      Методика построения АППО опирается на современный уровень
развития науки и~на име\-ющий\-ся у~коллектива авторов большой задел в~об\-ласти теоретических исследований и~создания действующих систем
различных лингвистических процессоров и~обработки текстовых знаний.
Элементы предлагаемого подхода уже были частично апробированы при
создании энциклопедии ключевых понятий KEYWEN (содержит
260\,000~статей и~5\,000\,000~ключевых фраз, число которых постоянно
растет) и~в ряде работ других участников проекта.

     Система KEYWEN представляет собой средство построения больших
энциклопедий по материалам Интернета и~на их основе составления
рефератов и~аналитических статей. Имеется опыт построения корпуса
английских текстов из Интернета размером более 1~ТБ, проведены
эксперименты по по\-стро\-ению русских корпусов текстов для ряда
ПО.

     Авторский коллектив много лет ведет исследования в~области
аналитической обработки текстов и~извлечения информационных объектов
(именованных сущностей) и~их связей. Разработан ряд прикладных систем,
основанных на глубинном анализе текстов:
\begin{description}
\item[\,]
     <<Криминал>> и~<<Аналитик>>~--- системы, извле\-ка\-ющие
информацию из милицейских сводок для ло\-ги\-ко-ана\-ли\-ти\-че\-ских
решений, которые служат основой для проведения следственных действий;
\item[\,]
     <<Semantix>>~--- многоязычный лингвистический процессор,
обеспечивающий выделение объектов (сущностей) и~их связей из текстов на
английском и~русском языке;
\item[\,]
     <<Антитеррор>>~--- система, извлекающая из текстов информацию о~террористах и~террористических акциях с~автоматическим формированием
базы знаний;
\item[\,]
     <<Резюме>>~--- система семантической обработки резюме и~их
отображения в~формат сайта крупной рекрутинговой компании;
\item[\,]
     <<Памятники>>~--- обеспечивает извлечение информации из текстов
описания памятников культуры и~формирования ролевых функций
относящихся к~ним лиц;
\item[\,]
     <<ДИЕС>>~--- диалоговая среда, обеспечивающая построение
конкретных систем языкового взаимодействия с~пользователем;

     <<ДЕКЛАР>>~--- программная среда, включающая в~себя систему
представления знаний и~язык программирования задач, связанных с~обработкой знаний. В~рамках этой среды авторами разработаны системы
лек\-си\-ко-мор\-фо\-ло\-ги\-че\-ско\-го
     и~син\-так\-ти\-ко-се\-ман\-ти\-че\-ско\-го анализа русских и~английских текстов, которые используются при извлечении знаний.
    \end{description}
     Таким образом, разработка инструмента \mbox{АППО} как\linebreak главного
механизма построения системы извлечения знаний ведется с~привлечением некоторых теоре\-тических и~практических достижений,
уна\-сле\-до\-ван\-ных авторским коллективом от прошлых разработок, а~такжн путем
создания новых алгоритмов методики и~программных средств реализации.

\vspace*{-3pt}

     \subsection{Развитие методов на~основе статистической парадигмы}

     \vspace*{-1pt}

     Подход, создаваемый на основе АППО, носит комплексный характер,
так как использует методы корпусной лингвистики, статистические
механизмы, сочетающие приемы дистрибутивной семантики и~математической статистики (контекстные векторы, веса, вероятности и~др.),
а~также некоторые ло\-ги\-ко-линг\-ви\-сти\-че\-ские методы и~способы
машинного обучения для построения АППО (базового понятия, на которое
опирается СКП). Наиболее
близки по духу к~этому проекту методы известного британского
исследователя Karen Sp$\ddot{\mbox{a}}$rk Jones~\cite{17-koz, 18-koz, 19-koz}.

     Особенность предлагаемого подхода состоит в~возможности создавать
и использовать \mbox{АППО} автоматически. В~связи с~этим трудоемкость
разработки \mbox{АППО} на несколько порядков ниже создания традиционных
тезаурусов, что позволяет \mbox{АППО} стать реальным дополнением тезаурусов в~новом поколении интеллектуальных ин\-тер\-нет-тех\-но\-ло\-гий. При таком
подходе достигается большая полнота охвата терминологии лишь при
незначительном снижении точности. Тем самым преодолева\-ется главная
проб\-ле\-ма~--- трудоемкости ручного труда лингвистов, значительно
увеличивается ско\-рость.

     Данный проект создается в~несколько этапов. На первом этапе был
разработан макетный вариант алгоритма формирования АППО на материале
различных ПО. Создан и~успешно развивается основной
компонент комплексной методики~--- модуль статистического анализа для
алгоритма формирования АППО. Тематический поиск ЕЯ-текс\-тов, скачиваемых из Интернета,
формирование больших корпусов этих текстов, разбиение их на
ПО, выделение ЗС и~создание словарей ЗС
осуществляются в~автоматизированном режиме и~соединены в~единый
итеративный алгоритм, который по сути и~является методикой получения
АППО для любой ПО. Отдельные ее этапы будут
уточняться и~совершенствоваться в~процессе дальнейшей работы.

     На втором этапе существования проекта методика автоматического
формирования АППО получила дальнейшее развитие в~виде уточненных
механизмов семантического поиска в~пространстве ЕЯ-текс\-тов из
Интернета и~детального улучшения алгоритмов методики, разработанных в~первый год существования проекта,~--- за счет обучения модуля
статистического анализа на тестовых выборках с~последующей его
апробацией на реальных текстовых потоках big data из различных ПО.
Семантический поиск~--- первое звено в~неразрывной цепочке методики
формирования АППО, который осуществляется в~виде постоянного
интернет-серфинга, ведущегося как с~помощью специально разработанных
для данного проекта программных средств семантического поиска (частью
которых стали механизмы поиска, функционирующие в~собственной
разработке научного коллектива~--- энциклопедии ключевых понятий
KEYWEN~\cite{11-koz, 10-koz, 12-koz}), так и~с~привлечением крупнейших
поисковиков, каталогов библиотек, электронных магазинов и~т.\,п.
Круглосуточный мониторинг сети ведется на двух\linebreak рабочих станциях
(серверах), что обеспечивает обработ\-ку большого потока сетевых данных, из
которых формируются и~постоянно пополняются корпуса ЕЯ-текс\-тов.
Механизмы поиска были усовершенствованы на обучающих и~тестовых
выборках и~апробированы при непосредственном ин\-тер\-нет-сер\-фин\-ге
для поиска и~извлечения ЕЯ-текс\-тов определенных ПО
(подробнее об алгоритмах поиска см.\ ниже).

     Коллективом авторов были проведены эксперименты на тестовых
выборках и~составлен план дальнейших экспериментов на весь срок
выполнения проекта; на данном этапе продолжается разработка и~улучшение
модулей и~алгоритмов программного обеспечения.

     В настоящее время методика позволяет автоматически выделять
ключевые слова (отдельные сло\-ва-лек\-се\-мы, термины и~ЗС) из
     ЕЯ-текс\-тов, выявлять ассоциативные связи между ними, задавать
иерархию терминов и~ЗС методом кластеризации и~строить словари ЗС
различных ПО, а~также~--- в~качестве сопутствующего
продукта~--- создавать визуальные карты ПО. На выходе алгоритма получается
достаточно информативное <<облако>> ассоциативных связей для
автоматически выделенных лексем (терминов) и~ЗС, из которых и~формируются АППО.
Собственно, полученный <<пучок>> ассоциативных связей определяет
значение термина (ЗС) и~его место в~терминосистеме исследуемой
ПО, и~наоборот: выявленные ассоциативные связи
позволяют отнести рассматриваемый термин и/или ЗС к~той или иной
тематике.

     На втором этапе работа была сосредоточена на детализации алгоритма
формирования АППО и~завершилась созданием успешно
функционирующего варианта. Ядро алгоритма~--- модуль статистического
анализа, основанный на вычислительных методах дистрибутивной
семантики; в~частности, используется косинусная мера вычисления силы
ассоциативной связи (ассоциативная мера) между контекстными векторами
(компонентами вектора ЗС являются частоты совместной встре\-ча\-емости
данного ЗС с~другими ЗС в~одном и~том же контексте). Применив формулу
вычисления косинусной меры между контекстными векторами, получаем
коэффициенты семантической близости между рассматриваемыми~ЗС.

     Идеология АППО базируется на дистрибутивной гипотезе,
утверждающей, что семантически близкие (или связанные) лексемы имеют
похожий контекст и, наоборот, при похожем контексте лексемы
семантически близки. В~применяемой модели используется расширенная
гипотеза, включающая исследование сходства и~различия в~контекстах не
только отдельных лексем, но и~произвольных многолексемных
     фрагментов~--- ЗС.

     В качестве теоретической модели АППО в~настоящем проекте принято
СКП. Формально СКП
определяется как граф $G\hm=(V,\,E)$ с~узлами~$v$ из~$V$,
представляющими значимые термины/словосочетания ПО, и~дугами графа
$(v_i, v_j, \mathrm{Link}, w_{ij})$ из~$E$, описывающими отношения/связи между
словосочетаниями, где $w_{ij}$~--- это вес, выражающий силу связи,
а~Link~--- тип связи, определяемый типом
     лек\-си\-ко-син\-так\-си\-че\-ско\-го шаблона/конструкции,
связывающей словосочетания. Содержательно СКП является словарем
ЗС, элементы которого связаны ассоциативными
связями. Любая АППО является конкретной реализацией СКП~\cite{1-koz}.

\vspace*{-6pt}

     \subsection{Процесс расширения семантической модели}

     \vspace*{-2pt}

     Суть предлагаемого метода формирования \mbox{АППО} состоит в~итерационном расширении первоначального словаря ЗС до полноценного
АППО. Метод можно изложить в~виде приведенного ниже алгоритма.
     \begin{enumerate}[1.]
     \item  Выбор ключевых терминов, задающих ПО,
осуществляется пользователем:
\begin{itemize}
\item вручную (на первом этапе обучающей
выборки или для исследования новых тем);
\item извлекается системой
автоматически на основе частотных характеристик (<<весов>> терминов);
\item  из предыдущего АППО автоматически берется множество
начальных ЗС (ключевых слов).
\end{itemize}
     \item Семантический поиск по заданным ключевым словам и~накопление базы релевантных ин\-тер\-нет-текс\-тов, из которых
формируются корпуса ЕЯ-текс\-тов различных ПО (big data). При отладке и~совершенствовании поиска используются методы машинного обучения.
     \item Автоматическое выделение терминов из текстов и~составление
частотного словаря ПО (методами статистики).
     \item Формирование списка наиболее значимых понятий ПО из
ключевых терминов и~ЗС.
     \item Разбиение текстов на сегменты или предложения.
     \item Построение контекста каждого термина в~виде набора
содержащих его сегментов/предложений.
     \item Построение контекстных векторов, т.\,е.\ для каж\-до\-го значимого
термина (или ЗС) ведется подсчет других значимых терминов/ЗС в~его
контексте и~формируется вектор с~полученной статисти\-кой. Эти контекстные
векторы определяют меру близости и~ассоциативные связи между
ключевыми словами.
     \item Расчет косинусной меры близости контекстных векторов
(компонентами вектора ЗС~--- ключевых слов данной ПО~--- являются
частоты совместной встречаемости данного ЗС с~другими ЗС в~одном и~том
же контексте) и~выбор наиболее сильных ассоциативных связей. Для
вы\-чис\-ле\-ния косинусной меры применяется сле\-ду\-ющая формула:

\noindent
     $$
     \fr{x y}{\vert x\vert \cdot\vert y \vert} = \fr{\sum\nolimits_{i=1}^n x_i
y_i}{ \sqrt{\sum\nolimits_{i=1}^n x_i^2} \sqrt{\sum\nolimits_{i=1}^n y_i^2}}\,,
     $$
где $x$ и~$y$~--- векторы в~пространстве признаков, которыми являются
сами ЗС из данного СКП, а~$i$~--- это индекс, пробегающий число признаков.
     \item Выбор кандидатов в~состав ключевых терминов (для расширения
первоначального частотного словаря ЗС) из числа полученных наиболее
сильных ассоциаций у ключевых терминов, осуществляемый следующим
образом:
     \begin{itemize}
\item проводится поиск кластера тесно связанных ключевых слов;
\item выстраивается иерархическая структура на множестве ЗС методом
иерархической клас\-те\-ри\-зации;
\item в~векторном пространстве рассчитывается центр кластера заданных
ключевых слов (ЗС), относящихся к~ПО;
\item определяются наиболее сильные тер\-ми\-ны-кан\-ди\-да\-ты для включения в~состав ЗС из числа новых терминов, находящиеся ближе к~ центру кластера;
\item наиболее отдаленные от центра кластера ключевые слова (ЗС)
становятся кандидатами на исключение из состава ЗС;
\item принимается итоговое решение по изменению состава ЗС путем
добавления или удаления полученных тер\-ми\-нов-кан\-ди\-датов.
\end{itemize}
     \item Пополнение первоначального состава ключевых терминов
наиболее сильными кандидатами, автоматически выбранными системой, и~переход к~ шагу~1.
     \end{enumerate}

     Таков макетный алгоритм, на базе которого разработан статистический
модуль, решающий задачу создания АППО. Отдельные этапы методики,
включая механизмы семантического ин\-тер\-нет-сер\-фин\-га, будут
развиваться и~совершенствоваться на следующем этапе проекта.


     Итак, применив формулу вычисления косинусной меры между
контекстными векторами, получаем коэффициенты семантической близости
\mbox{между} рассматриваемыми ЗС. В~реальных приложениях матрицы,
содержащие оценочные коэффициенты, содержат порядка миллиона
столбцов (строк). Но~благодаря тому, что подавляющее большинство
элементов заполняется нулями, реально хранимая информация вполне
обозрима. Для нахождения ассоциативных связей, которые могут войти в~ассоциативный портрет, необходимо выбирать из этой матрицы пары
терминов или ЗС с~самыми большими коэффициентами семантической
близости.

     Приведем пример на основе следующего текстового фрагмента
(выборка получена из текстов, свободно существующих в~Интернете) из
ПО \mbox{АНПА}~\cite{32-koz}.

     Для анализа берется обучающий фрагмент из дайджеста текстов.

           \smallskip

      \textbf{Фрагмент дайджеста текстов предметной области <<Автономные необитаемые подводные аппараты>>}

      \smallskip


    {\sf Прошел испытания и~поставлен на вооружение новый подводный аппарат
отечественной разработки <<Об\-зор-600>>, который поможет флоту в~разведке и~исследовании морского дна}.


   {\sf Необитаемый подводный аппарат (АНПА) Knifefish, предназначен для борьбы с~морскими минами различных типов.}

   {\sf Гидролокатор может быть размещен на ТНПА или АНПА и~работать в~качестве
впередсмотрящей навигационной системы, а~также функционировать в~режиме
системы 3D-визуализации, на рабочих глубинах до 2500~м.}

    {\sf Двухлучевой эхолот JJ-Connect Fisherman~600 обеспечивает точность определения
глубины и~структуры дна.}


    \smallskip

     Далее строятся контекстные векторы для ЗС <<АНПА>>, <<Обзор-600>>, <<Knifefish>>, <<Гидролокатор>>,
<<эхолот JJ-Connect Fisherman>> и~слов (ЗС), встречающихся в~текстовом
фрагменте более одного раза. В~клетки таб\-ли\-цы записывается число
предложений с~соответствующей парой~ЗС.


%\addtocounter{figure}{1}


%\begin{table*}
{\small %fig1
     \begin{center}
     {\tabcolsep=4pt
     \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|}
\multicolumn{12}{c}{Матрица оценочных коэффициентов}\\
\multicolumn{12}{c}{\ }\\[-4pt]
     \hline
     &$c_1$ & $c_2$ & $c_3$ & $c_4$ &  $c_5$ & $c_6$ & $c_7$ & $c_8$ & $c_9$ &
$c_{10}$ & $c_{11}$\\
     \hline
     $c_1$ & 0&0&1&1&0&1&1&1&1&0&1\\
     $c_2$ & 0&0&0&0&0&1&1&1&1&1&0\\
     $c_3$ & 1&0&0&0&0&1&1&1&1&0&0\\
     $c_4$ & 1&0&0&0&0&0&0&0&0&0&1\\
     $c_5$ & 0&0&0&0&0&0&0&0&0&1&1\\
     $c_6$ & 1&1&1&0&0&0&2&2&2&1&0\\
     $c_7$ & 1&1&1&0&0&2&0&2&2&1&0\\
     $c_8$ & 1&1&1&0&0&2&2&0&2&1&0\\
     $c_9$ & 1&1&1&0&0&1&1&1&1&1&0\\
     $c_{10}$& 0&1&0&0&1&1&1&1&1&0&1\\
     $c_{11}$& 1&0&0&1&1&0&0&0&0&1&0\\
     \hline
     \end{tabular}}
     \end{center}}
         %\end{table*}

\vspace*{12pt}


     В таблице используются следующие сокращенные обозначения:

     $c_1$\;=\;АНПА;

     $c_2$\;=\;Обзор-600;

     $c_3$\;=\;Knifefish;

     $c_4$\;=\;гидролокатор;

     $c_5$\;=\;эхолот JJ-Connect;

     $c_6$\;=\;подводный аппарат;

     $c_7$\;=\;подводный;

     $c_8$\;=\;аппарат;

     $c_9$\;=\;морского дна, морскими;

     $c_{10}$\;=\;Fisherman;

     $c_{11}$\;=\;глубинах, глубины.

     Применив формулу вычисления косинусной меры между
контекстными векторами, получим следующие коэффициенты
семантической бли\-зости между рассматриваемыми ЗС в~порядке убывания:
\begin{description}
\item[\,]
     <<Гидролокатор>> и~<<эхолот JJ-Connect Fisherman>>~--- 0,50;
\item[\,]
     <<Обзор-600>> и~<<Knifefish>>~--- 0,40;
\item[\,]
     <<АНПА>> и~<<Обзор-600>>~--- 0,34;
\item[\,]
     <<АНПА>> и~<<Knifefish>>~--- 0,34;
\item[\,]
     <<Knifefish>> и~<<гидролокатор>>~--- 0,32;
\item[\,]
     <<АНПА>> и~<<гидролокатор>>~--- 0,26;
\item[\,]
     <<АНПА>> и~<<эхолот JJ-Connect Fisherman>>~--- 0,26;
\item[\,]
     <<Обзор-600>> и~<<эхолот JJ-Connect Fisherman>>~--- 0,17.
\end{description}

     Для нахождения ассоциативных связей, которые могут войти в~АППО, следует выбрать из этой матрицы пары
терминов с~самыми большими коэффициентами семантической близости
(косинусной меры). В~данном случае это будут следующие пары:
\begin{description}
\item[\,]     (<<Гидролокатор>>, <<эхолот JJ-Connect Fisherman>>);
\item[\,]
     (<<Обзор-600>>, <<Knifefish>>);
\item[\,]
     (<<АНПА>>, <<Обзор-600>>);
     \item[\,]
     (<<АНПА>>, <<Knifefish>>).
     \end{description}

     \subsection{Использование машинного обучения}

     Обработка больших массивов текстов из Интернета позволяет
собирать необходимые статистические данные для формирования достаточно
полной картины о~ПО, представленной в~виде СКП.
Возможность проводить машинное обучение на большом количестве
примеров существенно улучшает результаты.

     Машинное обучение при выявлении ключевых слов ПО и~ассоциативных связей между ними строится по следующей схеме:
     \begin{enumerate}[(1)]
     \item создание обучающей и~тестовой выборок, в~которых для каждого
текста (документа) указан его класс;\\[-14pt]
     \item подготовка текстов к~использованию их для обучения: разбиение
текстов на элементы~--- слова, ЗС, знаки препинания и~т.\,д.; выявление
<<шума>> в~тексте и~его отсечение; для некоторых обучающих выборок
делается лемматизация (приведение слов к~нормальной форме);\\[-13.5pt]
     \item преобразование каждого документа в~вектор признаков;\\[-13.5pt]
     \item создание словаря ЗС (классификатора) и~дальнейшее обучение
классификатора на полученном обучающем множестве;\\[-13.5pt]
     \item проверка точности работы классификатора на тестовом массиве
текстов.
     \end{enumerate}

     Совершенствование алгоритма формирования АППО осуществляется,
в частности, и~методами машинного обучения, включая проведение
экспериментов по построению \mbox{АППО} в~удобной для разработ\-ки онтологий
форме. В~качестве обучающей темы для решения указанной задачи была
выбрана ПО <<Со\-ци\-аль\-но-по\-ли\-ти\-че\-ский портрет
регионов России>>, а~в~качестве ее сужения до обуча\-ющей выборки~---
ограничение регионом Татарстана. При построении онтологии определенной
ПО происходит выделение наиболее значимых
именованных сущностей, формирующих классы и~подклассы. Примеры
работы алгоритма на конкретных ПО представлены в~разд.~5.

     Использование онтологии в~информационной системе позволяет
решать несколько различных задач:
\begin{itemize}
\item эталонные сущности служат
образцами для сравнения и~идентификации сущностей, извлеченных из
публикаций о~ПО, происходящих из различных информационных
источников;
\item сущности могут быть представлены пользователю в~качестве
предопределенных объектов для формирования запросов к~интегрированному хранилищу данных;
\item связи эталонных сущностей,
представленные в~онтологии, позволяют дополнять и~обобщать информацию,
содержащуюся в~текстах. Например, если в~тексте найдено упоминание
человека, то можно отнести его имя к~организации, где он работает, а~затем
обобщить до уровня власти и~сферы деятельности.
\end{itemize}

     Достоинствами автоматизированного способа создания онтологий
являются низкие трудозатраты и~--- для рассматриваемой темы~--- учет
<<медиаактивности>> персон и~организаций, т.\,е.\ числа упоминаний в~СМИ. К~недостаткам можно отнести отсутствие полноты полученной
онтологии и~порою случайный характер выделяемых объектов. Полностью
автоматизированный способ получения подобных онтологий пока
невозможен, он еще требует доработки. При составлении первоначального
набора ключевых слов, анализе материала, создании онтологий для разных
обучающих выборок потребовалась некоторая ручная доводка (несколько
итераций по результатам машинного обучения).

     Для повышения полноты и~точности результатов эта работа
осуществлялась методом автоматического ин\-тер\-нет-сер\-фин\-га в~результате круглосуточного функционирования двух серверов и~методом
краудсорсинга как силами основного авторского коллектива, так и~с
привлечением магистрантов и~студентов Московского технического университета связи и информатики~\cite{5-koz}. В~итоге
произошло существенное расширение сформированных на начальном этапе
тематических корпусов ЕЯ-текс\-тов. Данная работа проводится по
следующей схеме:
     \begin{itemize}
\item система производит в~Интернете мониторинг информации, имеющей
отношение к~ка\-кой-ли\-бо ПО;
\item пользователь задает поисковый запрос в~виде набора ключевых
терминов (слов и~словосочетаний);
\item аналитическая система находит на ин\-тер\-нет-сай\-тах предложения с~заданными ключевыми терминами и~фиксирует ссылки (URL~---
    uniform resource locator) на
соответствующие документы;
\item из найденных предложений автоматически выделяются новые ЗС, что
позволяет расширить исходный поисковый запрос и~обеспечивает б$\acute{\mbox{о}}$льшую
полноту извлеченной информации.
\end{itemize}

     \subsection{Развитие механизмов серфинга}

     Отметим, что совершенствование механизмов серфинга~---
необходимое условие для успешного извлечения знаний из
ЕЯ-текс\-тов сверхбольших объемов (big
data); улучшение этих механизмов~--- отдельная подзадача~\cite{33-koz}.
Используемый алгоритм поиска частично унаследован от электронной
сис\-те\-мы~--- энциклопедии ключевых слов KEYWEN; этот алгоритм порождает
в~качестве промежуточного результата дайджесты текс\-тов конкретной
ПО и~выглядит так. Поисковый запрос формируется на
основе множества ЗС. Осуществляется перебор всех
комбинаций ключевых терминов (единичные термины, пары терминов,
тройки и~т.\,д.) так, чтобы длина запроса не превышала определенной
границы~$n$. В~результате поиска должно быть найдено <<среднее>>
количество (порядка нескольких сотен) релевантных документов. Алгоритмы
объединения имеют эмпирический характер~--- значение границы~$n$
определяется опытным путем. Затем
 с~по\-мощью известных поисковых
систем (Google, Яндекс и~пр.)\ поисковые запросы обрабатываются, и~в
результате получается множество~$M_1$ текс\-то\-вых документов. Это
множество просматривается на предмет выделения URL-ссы\-лок (в~этот
момент включаются в~работу средства отсечения дорвеев,
     сгенерированных шумов и~т.\,п.), по выбранным ссылкам формируется
расширенное множество документов~$M_2$. Множество~$M_2$, в~свою
очередь, тоже содержит URL-ссыл\-ки, потому процесс расширения
множества документов можно продолжать и~далее, несколькими итерациями.
При этом необходимо проверять вновь найденные документы на наличие в~них первоначальных ключевых терминов. В~этом случае и~последующие
итерации вносят существенный вклад в~пополнение множества искомых
документов и~списка ключевых слов. Далее документы из множества~$M_2$
делятся на предложения (или на фрагменты, близкие по длине к~обычным
предложениям). В~результате составляется база данных с~записями вида:
$\langle\mbox{текст}$ фрагмента$\rangle\mbox{--}\langle\mathrm{URL},\
\mbox{указывающий }$ на этот\linebreak $\mbox{фрагмент}\rangle$.

     Такая база данных имеет значительный объем (до нескольких~терабайт). Для
ее хранения и~осуществления поиска в~ней недостаточно традиционных
средств работы с~базами данных, необходимо применять специальные
     про\-граммно-ап\-па\-рат\-ные технологии big data. По полученной базе
данных производятся статистические подсчеты: сколько различных URL
ссылается на одно и~то же предложение. Отметим, что при таком подсчете
сравнение URL обычно имеет смысл производить с~точ\-ностью до доменов
первого или второго уровня. На данном этапе приходится решать задачу
отсечения сай\-тов-двой\-ни\-ков, дорвеев и~т.\,д., с~тем чтобы в~выборку
попали только так называемые <<независимые>> сайты. Таким образом,
каждый фрагмент получает определенный рейтинг, равный числу его
вхождений в~независимые сайты. Из топа отсортированных по рейтингу
фрагментов (путем определенной процедуры редактирования) составляется
своеобразный дайджест текстов, содержащий рейтинговую информацию по
данной ПО (уже без URL). Примеры дайджестов из обучающих выборок по
Татарстану и~по другой ПО~--- <<Протестная активность>>
представлены в~разд.~5.

     Разрабатываемый авторским коллективом инструмент АППО, опираясь
на обнаруженные ассоциативные связи, позволяет соотносить лексемы
(термины) и~ЗС с~элементами более высокого уровня~--- подкатегориями,
категориями (или <<рубриками>>).


     Это соотнесение происходит в~соответствии с~выявленными
первичными и/или вторичными признаками; рассчитываются вероятности
вхождения словосочетаний в~документы, связанные с~руб\-ри\-ка\-ми. По этим
вероятностям при помощи специальной формулы рассчитываются веса
признаков, связанных с~рубриками. Вес признака, связанного с~категорией,
может быть рассчитан различными методами, например по вероятности
вхождения признака (термина, словосочетания) в~документы, на которые
ссылается категория. Следующий возможный метод~--- это расчет веса через
вероятность нахождения признака в~контексте основных признаков
категории. Под контекстом здесь понимается множество всех документов и~их компонент, содержащих основные признаки рубрики.

     Например, каждому ключевому слову может быть присвоен вес,
который рассчитывается с~учетом частоты встречаемости ключевого слова в~текстах определенной ПО. Выделенные ключевые\linebreak \mbox{слова} служат основой для
построения классификаторов. Категории в~классификаторах представлены в~виде кортежа, где вес определяет силу ассоциативной связи:
     $\langle$Первичное\ ключевое слово, {Категория}, {Вес}$\rangle.$

          \begin{figure*}[b] %fig2
      \vspace*{9pt}
 \begin{center}
 \mbox{%
 \epsfxsize=163.337mm
 \epsfbox{koz-2.eps}
 }
\end{center}
 \vspace*{-9pt}
     \Caption{Семантическая структура ПО <<Со\-ци\-аль\-но-по\-ли\-ти\-че\-ский
портрет Татарстана>>}
      \end{figure*}

     Соотнесение ключевого слова с~категорией осуществляется по
следующей формуле:

\vspace*{-6pt}

\noindent
      \begin{multline*}
      \mbox{вес(ключевое слово)} = \mbox{вес(категория}\mbox{-}i) ={}\\
      {}=
      \mathrm{log}\fr{p(\mbox{ключевое\ слово}\,\&\,\mbox{категория}\mbox{-}i)}{p(\mbox{ключевое\
слово})p(\mbox{категория}\mbox{-}i)}\,.
     \end{multline*}
     Здесь $p(\mbox{ключевое\ слово}\,\&\,\mbox{категория}\mbox{-}i)$ есть вероятность
совместного проявления двух факторов: ключевое слово существует в~документе и~документ принадлежит категории~$i$. Если ключевое слово и~категория статистически независимы, то вероятность их соотнесения
определяется как произведение $p(\mbox{ключевое\ слово})p(\mbox{категория}\mbox{-}i)$.
\mbox{Если} они не независимы, статистика отражает тенден\-цию к~соотнесению
ключевого слова с~опреде\-ленной категорией, в~таком случае вероятность
$p(\mbox{ключевое\ слово}\,\&\,\mbox{категория}\mbox{-}i)$ будет больше, чем $p(\mbox{ключевое\
слово})p(\mbox{категория}\mbox{-}i)$. Соотношение между $p(\mbox{ключевое\
слово}\,\&\,\mbox{категория}\mbox{-}i)$ и~$p(\mbox{ключе-}$\linebreak вое слово)$p(\mbox{категория}\mbox{-}i)$
является мерой степени статистической зависимости между ключевым
словом и~категорией~$i$. Логарифм этого соотношения есть объем
информации о~наличии ключевого слова, полученной при наблюдении
документа категории~$i$. Так как уравнение симметрично, это также объем
информации о~принадлежности документа к~категории~$i$, получаемой при
наблюдении в~нем ключевого слова (принцип вза\-им\-ности).
Приведенная
формула позволяет вычислить вес ключевого слова, связанного с~данной
категорией, если известна вероятность наличия этого ключевого слова в~документах категории.

     Веса признаков, рассчитанных разными методами, должны по
возможности совпадать. Для этого был предложен следующий метод
согласования весов.

     Допустим, с~помощью двух различных методов найдены две группы
признаков $T_1$ и~$T_2$, связанных с~некоторой категорией. Например, $T_1$~--- это
признаки, входящие в~тестовую выборку, на которую ссылается категория
(это основные термины и~их меньше), а~$T_2$~--- это признаки, связанные с~категориями ассоциативными связями (т.\,е.\ это вторичные признаки) и~выделенные по ин\-тер\-нет-текс\-там (их больше).

     Веса рассчитываются независимо для $T_1$ и~$T_2$ двумя различными
алгоритмами. Но так как множество $T_1$ входит в~$T_2$, то веса для~$T_1$ будут
рассчитаны обоими алгоритмами и~необходимо подбирать коэффициент для
весов из~$T_2$, при котором эти веса максимально приближаются к~весам
из~$T_1$. В~дальнейшем этот коэффициент используется для подсчета весов
вторичных признаков.

    \section{Примеры реализации инструмента ассоциативного портрета предметной области}

     Приведем некоторые примеры работы методики на обучающих
выборках из различных ПО.

     \subsection{Предметная область <<Со\-ци\-аль\-но-по\-ли\-ти\-че\-ский
портрет~Татарстана>>}

     При построении онтологии данной ПО наиболее
значимыми являются именованные сущности следующих классов:
     \begin{itemize}
\item географические названия наиболее крупных населенных пунктов
региона;
\item персоны, исполняющие должности в~административном аппарате
региона, и~их должности;
\item наиболее важные организации региона;
\item наиболее важные события в~со\-ци\-аль\-но-по\-ли\-ти\-че\-ской
жизни региона;
\item сферы деятельности в~регионе.
\end{itemize}

     Данные классы важны при описании со\-ци\-аль\-но-по\-ли\-ти\-че\-ской жизни
всех регионов Российской Федерации. В~случае Татарстана большое
значение имеет разработка полезных ископаемых (нефть, уголь, газ), а~также
определенные виды хозяйственной деятельности в~этом регионе, в~связи с~чем в~онтологию добавился класс <<месторождения>>, а~среди
<<организаций>> были выделены подклассы <<нефтехимия>>,
<<машиностроение>>, <<агрокомплекс>>, <<образование>>.

     Основные классы сущностей, их атрибуты и~взаимосвязи представлены
на рис.~1.



     Для создания дайджеста текстов~--- обучающей\linebreak выборки по ПО
<<Социально-политический портрет Татарстана>> был использован
поисковый запрос, состоящий из двух составляющих:
\begin{enumerate}[(1)]
\item территориальные
образования Татарстана;
\item слова, характер\-ные для текстов о~со\-ци\-аль\-но-по\-ли\-ти\-че\-ской жиз\-ни.
    \end{enumerate}

Итогом работы системы семантического серфинга стал
дайджест из предложений, удовлетво\-ря\-ющих поисковому запросу, объемом
около 40\,000~предложений.

В~результате дальнейшей работы системных
модулей (анализа дайд\-жеста) были получены списки должностных лиц и~организаций Татарстана. Так как речь шла о~подготовке обучающей выборки,
впоследствии эти списки подверглись ручному редактированию, чтобы
исключить объекты, не относящиеся к~изуча\-емой ПО.

\vspace*{-4pt}

     \subsection{Предметная область <<Протестная активность>>}

     Приведем пример поискового запроса из выделенных ключевых слов
(ЗС) для формирования дайджеста ПО <<Протестная
активность>> с~по\-мощью автоматической системы семантического сер\-финга.

\smallskip

      \textbf{Поисковый запрос}

      \smallskip

     % {\small
     \noindent
     {\sf (Активисты OR Акции Протеста OR Власт\-ву\-ющая Элита OR
Внутриэлитный Раскол OR Госпереворот OR Государственный Переворот OR Давление
На Власть OR Демонстрации OR Евромайдан OR Захват Власти OR Конфликт Элит OR
Кризис Легитимной Власти OR Майдан OR Массовый Протест OR Митинги OR
Недовольство OR Общественное Мнение OR Оппозиция OR Пикетирование OR Пикеты
OR Политический Протест OR Протестное Движение OR Протестное Поведение OR
Радикализация Протеста OR Смена Власти OR Собрания OR Состояние Тревожности OR
Уровень Организованности OR Ценностные Конфликты OR Шествия OR Эскалация
Конфликта)}

     На основе запроса была получена обучающая текстовая выборка и~построена онтология данной ПО. Фрагмент верхнего уровня
онтологии ПО <<Протестная активность>>:

  \noindent
  {\sf   Объекты: события (акции протеста), люди (активисты, политики),
организации (партии, оппозиция, Евромайдан), источники информации
(агентства, веб-сай\-ты), время, место}.

 \noindent
 {\sf    Акции протеста: митинги, демонстрации, пикетирования, кампании
гражданского неповиновения, за\-бас\-тов\-ки, собрания, шествия}.

\vspace*{-6pt}

    \section{Визуализация ассоциативного портрета предметной области}

     Отображение многомерных векторов на плоскость является удобным
средством визуализации семантических связей, в~результате чего образуются
визуальные карты ЗС. На таких картах расстояние
между ЗС тем меньше, чем больше сила их ассоциативной связи, что
позволяет выделять сильно связанные и~близкие по смыс\-лу~ЗС.

     В ходе работы над проектом были разработаны методы создания
визуальных карт для различных ПО с~применением
технологии <<multidimencional scaling>> (многомерного масштабирования).
Визуальные карты ПО строятся на основе сочетания
методик дистрибутивной семантики и~математической статистики. Все они
базируются на положении дистрибутивной семантики о~том, что
семантическое сходство между двумя сло\-ва\-ми/тер\-ми\-на\-ми/ЗС\linebreak может быть
смоделировано как функция степени перекрытия их языковых контекстов.
Уменьшение\linebreak размеров и~масштаба оригинального семантического
пространства дает возможность представить на обуча\-ющей выборке
расстояния между терминами\linebreak на карте ПО. Семантическое
векторное пространство, измеренное методами статистики, позволяет
визуализировать сверхбольшие объемы данных: расстояния между
терминами зависят от распределения контекстных векторов, компоненты
которых, в~свою очередь, зависят от распределения большого количества не
видимых на карте ЗС. Таким образом, относительно небольшое число
видимых терминов (ЗС), отражающихся на карте, может зависеть от
распределения в~об\-ласти знаний миллионов невидимых словосочетаний.
Подобная карта распределения ЗС в~СКП дает наглядное пред\-став\-ле\-ние о~важнейших ПО, их границах, взаимопроникновении, о~терминах и~ЗС и~ассоциативных связях между ними.
Пример такой визуальной карты представлен на рис.~2.




     Пространственное положение терминов на карте определяется их
семантической связью. Это расстояние постоянно изменяется в~зависимости
от количества обработанной информации. Так, в~данном примере термин
Friends (<<Друзья>>) является внешним термином по отношению к~рассматриваемой карте ПО. Наблюдение за движением
внешнего термина по отношению к~<<внутренним>> ЗС также оказывается
полезным для изучения семантических связей, их изменений (сдвигов)
внутри огибающей кривой.

%\pagebreak

\noindent
\begin{center}  %fig3
\vspace*{9pt}
\mbox{%
 \epsfxsize=75.912mm
 \epsfbox{koz-3.eps}
 }



%\vspace*{3pt}

\noindent
{{\figurename~2}\ \ \small{Визуальная карта СКП}}
\end{center}



%\vspace*{18pt}


\addtocounter{figure}{1}




     Метод итерационного накопления словарей ЗС
дает возможность создать сначала компактную карту ПО;
агрегация данных из сверхбольших массивов ЕЯ-текс\-тов позволяет
постепенно расширить и~уточнить эту карту. Внедрение новой информации в~корпус текстов может существенно изменить расстояния и~зависимости в~целом между различными терминами на карте, что делает видимым влияние
новой информации. Это также позволяет выявить, обработать и~быстро
идентифицировать пропущенные ранее термины. Таким образом, карты
ПО, построенные на основе про\-стран\-ст\-вен\-но-век\-тор\-ной
семантической модели, становятся эффективным инструментом мониторинга
для измерения расстояния и~определения (визуализации) различий данных
(терминов и~ЗС) в~сверхбольших текстовых корпусах.

    \section{Заключение}

     Инструмент автоматизированного формирования АППО~---
перспективное направление для создания систем извлечения знаний. Кроме
того, этот инструмент подходит для решения ряда других задач.

     Комплексный подход к~автоматической обработке ЕЯ-текс\-тов на
основе сочетания методов статисти\-ки, корпусной лингвистики и~дистрибутивной семантики с~целью выявления ЗС и~их ассоциативных связей для формирования
СКП\linebreak реализуется в~виде итеративной методики
формирования АППО, является новаторским и~акту\-альным для исследований
на материале русского языка. Применяемый подход использует объективные
оценочные статистические критерии корпусной лингвистики (контекстные
векторы, веса и~др.), методы дистрибутивной семантики, дополненные
математическими (вероятностными) и~логико-лингвистическими методами.
Проект реализуется на сверхбольших объемах сетевых русскоязычных и~англоязычных ЕЯ-текс\-тах (big data).

     Полученные результаты могут быть использованы для решения задач
автоматизированного семантического поиска на сверхбольших объемах
данных, извлечения знаний из неструктурированных ЕЯ-текс\-тов и~автоматизированного формирования интерактивных
     пред\-мет\-но-ори\-ен\-ти\-ро\-ван\-ных энциклопедий. Особенно
перспективной областью применения АППО является задача построения
онтологии ПО с~последующим построением схем баз
данных и~знаний.

     Новые результаты, полученные в~ходе выполнения проекта, позволяют
успешно решать проблему\linebreak установления семантических соответствий
(ассоциативных связей) между разноструктурными (терминальными и~нетерминальными) языковыми\linebreak объекта\-ми: лексемами, словосочетаниями,
предложениями, сверхфразовыми единствами. Терминальная единица~---
лексема~--- термин определенной ПО; нетерминальные~---
ЗС, предложение,\linebreak абзац, статья.

     Статистические оценочные критерии, включенные в~методику наряду с~ло\-ги\-ко-линг\-ви\-сти\-че\-ски\-ми методами и~правилами дистрибутивной
семантики,\linebreak позволяют разрешать семантическую неоднозначность лексем
(терминов) и~ЗС на основе вероятностных механизмов, что повышает
скорость\linebreak и~эффективность семантического поиска и~обеспечивает
автоматизированное отнесение обрабатываемого текста к~той или иной
ПО.

     Построение АППО и~СКП с~использованием ассоциативных связей
между выделенными терминами (ЗС), а~также
соотнесения терминов с~категориями, рассчитанных по формуле косинусной
меры, привело к~сле\-ду\-ющим результатам:
     \begin{itemize}
     \item разработан комплекс методов классификации текстов,
использующих методы статистики, дистрибутивной семантики, корпусной
линг\-ви\-сти\-ки, аппарат ассоциативных связей и~некото\-рые другие,
позволяющий классифицировать тексты, изначально не содержащие
термины и~ЗС (коллокации) из обучающей выборки;
     \item создано программное обеспечение, реа\-ли\-зу\-ющее предложенные
методы для отбора релевантных текстов по теме ПО;
     \item сформированы объемные электронные корпуса соотнесенных с~определенной ПО ЕЯ-текс\-тов с~помощью
автоматизированного разби\-ения текстов на ПО;
     \item созданы корпуса словарей терминов ЗС и~предметных словарей, накопленных путем обработки больших
объемов данных из Интернета;
     \item разработаны методы создания визуальных карт для различных
ПО с~применением технологии <<multidimencional
scaling>> (многомерного масштабирования);
     \item получил дальнейшее развитие метод семантического поиска
     ЕЯ-текс\-тов с~целью направленного извлечения из сетевых текстов
(big data) энциклопедической информации.
     \end{itemize}

     В настоящее время авторский коллектив продолжает работу по
развитию методики АППО и~созданию программных средств ее реализации.

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{1-koz}
\Aue{Шарнин М.\,М., Сомин Н.\,В., Кузнецов~И.\,П., Морозова~Ю.\,И.,
Галина~И.\,В., Козеренко~Е.\,Б.} Статистические механизмы формирования
ассоциативных портретов предметных областей на основе
ес\-те\-ст\-вен\-но-язы\-ко\-вых текстов больших объемов для систем
извлечения знаний~// Информатика и~её применения, 2013. Т.~7. Вып.~2.
С.~92--99.

\bibitem{3-koz} %2
\Au{Charnine M., Petrov A., Kuznezov~I.} Association-based identification of
Internet User interests~// WORLDCOMP'13, ICAI 2013: 2013 Conference
(International) on Artificial Intelligence Proceedings.~--- Las Vegas: CSREA
Press, 2013. Vol.~I. P.~77--81.

\bibitem{2-koz} %3
\Au{Козеренко Е.\,Б.} Интегральное моделирование языковых структур
в лингвистических процессорах систем обработки знаний и~машинного
перевода~// Информатика и~её применения, 2014. Т.~8. Вып.~1.
С.~89--98.

\bibitem{7-koz} %4
\Au{Rapp R.} Word sense discovery based on sense descriptor dissimilarity~// 9th
MT Summit Proceedings.~--- New Orleans, LA, USA, 2003. P.~315--322.

\bibitem{5-koz} %5
\Au{Charnine M., Protasov V.} Optimal automated method for collaborative
development of universiry curricula~// WORLDCOMP'13, ICAI 2013: 2013
Conference (International) on Artificial Intelligence Proceedings.~--- Las Vegas:
CSREA Press, 2013. Vol.~I. P.~96--100.
\bibitem{6-koz} %6
\Au{Морозова Ю.\,И.} Построение семантических векторных пространств
различных предметных областей~// Информатика и~её применения, 2013.
Т.~7. Вып.~1. С.~90--93.

\bibitem{4-koz} %7
\Au{Charnine M., Somin N., Nikolaev~V.} Conceptual text generation based on
key phrases~// WORLDCOMP'14, ICAI 2014:  2014 Conference (International)
on Artificial Intelligence Proceedings.~--- Las Vegas: CSREA Press, 2014.
Vol.~II. P.~639--643.


\bibitem{8-koz} %8
\Au{Bacon E., Hagel G., Charnine~M., Foggie~R., Kirk~B., Schagaev~I.,
Kravtsov~G.} WEDUCA: Web-enhanced design of university curricula~//  2013
Conference (International) on Frontiers in Education: Computer Science and
Computer Engineering (FECS'13) Proceedings.~--- Las Vegas: CSREA Press,
2013. P.~288--294.

\bibitem{28-koz} %9
The SMART retrieval system: Experiments in automatic document processing~/
Ed. G.\,M.~Salton.~--- Prentice-Hall, 1971. 556~p.

\bibitem{16-koz} %10
\Au{Dunning T.} Accurate methods for the statistics of surprise and coincidence~//
Comput. Linguist., 1993. Vol.~19. No.\,1. P.~61--74.

\bibitem{29-koz} %11
\Au{Борисова Е.\,Г.} Коллокации. Что это такое и~как их изучать.~---  2-е изд.,
стер.~--- М.: Филология, 1995. 49~с.

\bibitem{15-koz} %12
\Au{Church K., Hanks P.} Word association norms, mutual information, and
lexicography~// Comput. Linguist., 1996. Vol.~16. No.\,1. P.~22--29.

\bibitem{21-koz} %13
\Au{Lund K., Burgess C.} Producing high-dimensional semantic spaces from
lexical co-occurrence~// Behav. Res. Meth. Inst. C.,
1996. Vol.~28. No.\,2. P.~203--208.

\bibitem{17-koz} %14
Readings in information retrieval~/
Eds.\ J.\,K.~Sp$\ddot{\mbox{a}}$rck, P.~Willett.~--- San Franscisco, CA, USA:
Morgan Kaufmann, 1997. 594~p.
\bibitem{18-koz} %15
\Au{Sp$\ddot{\mbox{a}}$rck Jones, K.} A~statistical interpretation of term
specificity and its application in retrieval~// J.~Documentation, MCB University
Press, 2004. Vol.~60. No.\,5. P.~493--502.
\bibitem{11-koz} %16
\Au{Charnine M.\,M., Kuznetsov I.\,P., Kozerenko~E.\,B.} Semantic navigator for
Internet search~// MLMTA'05: Conference (International) on Machine Learning
Proceedings.~--- Las Vegas: CSREA Press, 2005. P.~60--68.

\bibitem{26-koz} %17
\Au{Sahlgren M.} Towards pertinent evaluation methodologies for word-space
models~// LREC 2006: 5th Conference (International) on Language Resources and
Evaluation Proceedings.~--- Genoa, Italy, 2006. P.~821--824.

\bibitem{19-koz} %18
\Au{Sp$\ddot{\mbox{a}}$rck Jones, K.} Statistics and retrieval: past and future~//
Conference (International) on Computing: Theory and Applications. Platinum
Jubilee Conference of the Indian Statistical Institute.~--- Kolkata, India: IEEE,
2007.
\bibitem{20-koz} %19
\Au{Landauer Th.\,K., McNamara D.\,S., Dennis~S., Kintsch~W.} Handbook of
latent semantic analysis.~--- Mahwah, NJ, USA: Lawrence Erlbaum, 2007. 544~p.

\bibitem{31-koz} %20
\Au{Иорданская Л.\,Н., Мельчук И.\,А.} Смысл и~со\-че\-та\-емость в~словаре.~---
М.: Языки славянских культур, 2007. 672~с.

\bibitem{10-koz} %21
\Au{Charnine M., Charnine V.} Keywen category structure.~--- Wordclay, USA,
2008. 60~p.

\bibitem{14-koz} %22
\Au{Lenci A.} Distributional semantics in linguistic and cognitive research~//
Rivista di Linguistica, 2008. Vol.~1. Р.~1--30.

\bibitem{22-koz} %23
\Au{Manning C., Raghavan P., Sch$\ddot{\mbox{u}}$tze~H.} Introduction to
information retrieval.~--- Cambridge: Cambridge University Press, 2008. 581~p.
\bibitem{23-koz} %24
\Au{Sahlgren M.} The distributional hypothesis. From context to meaning~//
Distributional Models of the Lexicon in Linguistics and Cognitive Science:
Special issue of the Italian J.~Linguistics: Rivista di Linguistica, 2008. Vol.~20. No.\,1. P.~33--53.

\bibitem{9-koz} %25
\Au{Baroni M., Lenci A.} Distributional memory: A~general framework for
corpus-based semantics // Comput. Linguist., 2010. Vol.~36. No.\,4.
P.~673--721.

\bibitem{24-koz} %26
\Au{Turney P.\,D., Pantel P.} From frequency to meaning: Vector space models of
semantics~// JAIR, 2010. Vol.~37. P.~141--188.

\bibitem{30-koz} %27
\Au{Захаров В.\,П., Хохлова М.\,В.} Анализ эффек\-тив\-ности статистических
методов выявления коллокаций в~текстах на русском языке~// Компьютерная
лингвистика и~интеллектуальные технологии: По мат-лам ежегодной
Междунар. конф. <<Диалог>> (2010).~---  М.: РГГУ, 2010. Вып.~9(16).
С.~147--143.

\bibitem{12-koz} %28
\Au{Kuznetsov I.\,P., Charnine M.\,M., Kozerenko~E.\,B., Somin~N.\,V.,
Nikolayev~V.\,G., Matskevich~A.\,G.} Intelligent tools for the semantic Internet
navigator design~// Электронные библиотеки: перспективные методы и~технологии, электронные коллекции: Тр. XIV~Всеросс. науч. конф.
RCDL'2012.~--- Пе\-ре\-славль-За\-лес\-ский: Университет города
Переславля, 2012. С.~274--283.
\bibitem{13-koz} %29
\Au{Kuznetsov I.\,P., Kozerenko E.\,B., Charnin~M.\,M.} Technological peculiarity
of knowledge extraction for logical analytical systems~// WORLDCOMP'12,
ICAI'12:\linebreak\vspace*{-12pt}

 \pagebreak

 \noindent
 2012 Conference (International) on Artificial Intelligence
Proceedings.~--- Las Vegas: CSREA Press, 2012. Vol.~II. P.~762--768.


\bibitem{27-koz} %30
\Au{Schumann A.} Towards the automated enrichment of multilingual terminology
databases with knowledge-rich contexts~// Компьютерная лингвистика и~интеллектуальные технологии: По мат-лам ежегодной Междунар. конф.
<<Диалог>> (2012).~--- М.: РГГУ, 2012. Вып.~11(18). Т.~1. С.~559--567.

\bibitem{33-koz} %31
\Au{Шарнин М.\,М., Кузнецов И.\,П.} Особенности семантического поиска
информационных объектов на основе технологии баз знаний~// Информатика
и её применения, 2012. Т.~6. Вып.~2. С.~47--56.
\bibitem{32-koz} %32
\Au{Борисов Т.\,Н., Бронецкий А.\,Е., Клименко~С.\,В., Рыков~В.\,В.,
Шарнин~М.\,М.} Автономные необитаемые подводные аппараты:
автоматическое формирование ассоциативного портрета предметной
об\-ласти~// Ситуационные центры
и~ин\-фор\-ма\-ци\-он\-но-ана\-ли\-ти\-че\-ские системы класса 4i для задач
мониторинга и~без\-опас\-ности (SC-IAS4i-VRTerro2013): Тр. II~Междунар.
науч. конф.~--- Протвино: ИФТИ, 2013. С.~38--43.

\bibitem{25-koz} %33
\Au{Zolotarev O., Charnine M., Matskevich~A.} Conceptual business process
structuring by extracting knowledge from natural language texts~//
WORLDCOMP'14, ICAI 2014: 2014 Conference (International) on Artificial
Intelligence Proceedings.~--- Las Vegas: CSREA Press, 2014. Vol.~I. P.~82--87.

 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-4pt}

\hfill{\small\textit{Поступила в~редакцию 21.04.15}}

%\newpage

\vspace*{6pt}

\hrule

\vspace*{2pt}

\hrule

%\vspace*{12pt}

\def\tit{ASSOCIATIVE PORTRAITS OF SUBJECT AREAS AS~A~TOOL FOR~AUTOMATED
CONSTRUCTION OF~BIG DATA SYSTEMS FOR~KNOWLEDGE EXTRACTION: THEORY,
METHODS, VISUALIZATION, AND~APPLICATION}

\def\titkol{Associative portraits of subject areas as a tool for automated
construction of big data systems for knowledge extraction} %: Theory, methods,  %visualization and application}

\def\aut{I.\,V.~Galina, E.\,B.~Kozerenko, Yu.\,I.~Morozova, N.\,V.~Somin,
and~M.\,M.~Charnine}

\def\autkol{I.\,V.~Galina, E.\,B.~Kozerenko, Yu.\,I.~Morozova, et al.}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Galina I.\,V.}
\index{Kozerenko E.\,B.}
\index{Morozova Yu.\,I.}
\index{Somin N.\,V.}
\index{Charnine M.\,M.}

\vspace*{-9pt}


\noindent
Institute of Informatics Problems, Federal Research Center ``Computer
Science and Control'' of the Russian Academy of Sciences, 44-2 Vavilov Str.,
Moscow, 119333, Russian Federation


\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2015\ \ \ volume~9\ \ \ issue\ 2}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2015\ \ \ volume~9\ \ \ issue\ 2
\hfill \textbf{\thepage}}}

\vspace*{3pt}



\Abste{The paper presents the technique of developing systems for extraction of
knowledge which employs the approach of automated association portrait of a
subject area (APSA) formation and building a semantic context space (SCS). The
ideology of the APSA is based on the distributional hypothesis claiming that
semantically equal (or related) lexemes have a similar context and, \textit{vice
versa}, in a similar context, the lexemes are semantically close. The model uses an
extended  hypothesis that consists in the investigation of  similarities and
differences in contexts not only of  individual words, but of arbitrary multilexeme
fragments of meaningful word-combinations. The examples of implemented projects
for different subject domains are given.}

\KWE{semantic modeling; associations; mathematical statistics; distributive
semantics; big data; automated extraction of knowledge; digital natural language
text corpora; semantic search; intelligent Internet technology}

\DOI{10.14357/19922264150211}

\vspace*{-12pt}

\Ack
The research was partially supported by the Russian Foundation for Basic
Research (project 13-07-00272).



%\vspace*{3pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
 \bibitem{1-koz-1}
\Aue{Charnine, M.\,M., N.\,V.~Somin, I.\,P.~Kuznetsov, Yu.\,I.~Morozova,
I.\,V.~Galina, and E.\,B.~Kozerenko}. 2013. Sta\-ti\-sti\-che\-skie mekhanizmy
formirovaniya assotsiativnykh portretov predmetnykh oblastey na osnove
estestvenno-yazykovykh tekstov bols'shikh ob''emov dlya system izvlecheniya
znaniy [Statistical mechanisms of the subject domains associative portraits formation on the basis of big natural language texts for the systems of knowledge extraction].
\textit{Informatika i~ee Primeneniya}~--- \textit{Inform. Appl.} 7(2):92--99.

\bibitem{3-koz-1} %2
\Aue{Charnine, M., A.~Petrov, and I.~Kuznezov}. 2013. Association-based
identification of Internet user interests. \textit{ICAI'13: Conference (International)
on Artificial Intelligence Proceedings}. Las Vegas, USA: CSREA Press. 77--81.

\vspace*{-4pt}

\bibitem{2-koz-1} %3
\Aue{Kozerenko, E.\,B.} 2014. Integral'noe modelirovanie yazykovykh struktur
v~lingvisticheskikh protsessorakh system obrabotki znaniy i~mashinnogo
perevoda [Integrated modeling of language structures for linguistic processors of
knowledge management and machine translation systems]. \textit{Informatika i~ee
Primeneniya}~--- \textit{Inform. Appl.} 8(1):89--98.

\bibitem{7-koz-1} %4
\Aue{Rapp, R.} 2003. Word sense discovery based on sense descriptor
dissimilarity. \textit{Conference (International) 9th MT Summit Proceedings}.
New Orleans, LA. 315--322.
\bibitem{5-koz-1}
\Aue{Charnine, M., and V.~Protasov}. 2013. Optimal automated method for
collaborative development of universiry curricula. \textit{ICAI'13: Conference
(International) on Artificial Intelligence Proceedings}. Las Vegas, USA: CSREA
Press. 96--100.
\bibitem{6-koz-1}
\Aue{Morozova, Yu.\,I.} 2013. Postroenie semanticheskikh vektornykh prostransv
razlichnykh predmetnykh oblastey [Semantic vector spaces for different knowledge
domains]. \textit{Informatika i~ee Primeneniya}~--- \textit{Inform. Appl.}
7(1):90--93. \bibitem{4-koz-1} %7
\Aue{Charnine, M., N.~Somin, and V.~Nikolaev}. 2014. Conceptual text
generation based on key phrases. \textit{ICAI'14: Conference (International) on
Artificial Intelligence Proceedings}. Las Vegas, USA: CSREA Press. 639--643.

\bibitem{8-koz-1}
\Aue{Bacon, E., G.~Hagel, M.~Charnine, R.~Foggie, B.~Kirk, I.~Schagaev, and
G.~Kravtsov}. 2013. WEDUCA: Web-enhanced design of university curricula.
\textit{FECS'13 Conference (International) on Frontiers in Education: Computer
Science and Computer Engineering Proceedings}. Las Vegas, USA: CSREA
Press. 288--294.
\bibitem{28-koz-1} %9
Salton, G.\,M., ed. 1971. \textit{The SMART retrieval system: Experiments in
automatic document processing}. Prentice-Hall. 556~p.
\bibitem{16-koz-1} %10
\Aue{Dunning, T.} 1993. Accurate methods for the statistics of surprise and
coincidence. \textit{Comput. Linguist.} 19(1):61--74.
\bibitem{29-koz-1} %11
\Aue{Borisova, E.\,G.} 1995. \textit{Kollokatsii. Chto eto takoe i~kak ikh izuchat'}
[Collocations. What are they, and how are they to be studied]. 2nd ed. Moscow:
Filologiya. 49~p.
\bibitem{15-koz-1} %12
\Aue{Church, K., and P.~Hanks}. 1996. Word association norms, mutual
information, and lexicography. \textit{Comput. Linguist.} 16(1):22--29.
\bibitem{21-koz-1} %13
\Aue{Lund, K., and C.~Burgess}. 1996. Producing high-dimensional semantic
spaces from lexical co-occurrence. \textit{Behav. Res. Meth.
Ins. C.} 28(2):203--208.
\bibitem{17-koz-1} %14
Sp$\ddot{\mbox{a}}$rck Jones, K., and P.~Willett, eds. 1997. \textit{Readings in
information retrieval}. San Franscisco, CA: Morgan Kaufmann. 594~p.
\bibitem{18-koz-1} %15
\Aue{Sp$\ddot{\mbox{a}}$rck Jones, K.} 2004. A~statistical interpretation of
term specificity and its application in retrieval. \textit{J.~Documentation, MCB
University Press}. 60(5):493--502.

\bibitem{11-koz-1} %16
\Aue{Charnine, M.\,M., I.\,P.~Kuznetsov, and E.\,B.~Kozerenko}. 2005. Semantic
navigator for Internet search. \textit{MLMTA'05 Conference (International) on
Machine Learning Proceedings}. Las Vegas, USA: CSREA Press. 60--68.
\bibitem{26-koz-1} %17
\Aue{Sahlgren, M.} 2006. Towards pertinent evaluation methodologies for
word-space models. \textit{LREC 2006: 5th Conference (International) on
Language Resources and Evaluation Proceedings}. Genoa, Italy. 821--824.

\bibitem{19-koz-1} %18
\Aue{Sp$\ddot{\mbox{a}}$rck Jones, K.} 2007. Statistics and retrieval: Past and
future. \textit{Conference (International) on Computing: Theory and Applications
Proceedings}. \textit{Platinum Jubilee Conference of the Indian Statistical
Institute}. Kolkata, India: IEEE.
\bibitem{20-koz-1} %19
\Aue{Landauer, Th.\,K., D.\,S.~McNamara, S.~Dennis, and W.~Kintsch}. 2007.
\textit{Handbook of latent semantic analysis}. Mahwah, NJ: Lawrence Erlbaum.
544~p.


\bibitem{31-koz-1} %20
\Aue{Iordanskaya, L.\,N., and I.\,A.~Melchuk}. 2007. \textit{Smysl
i~sochetaemost' v~slovare} [Meaning and combinability in the dictionary].
Moscow: Slavonic Cultures Languages. 672~p.
\bibitem{10-koz-1} %21
\Aue{Charnine, M., and V.~Charnine}. 2008. \textit{Keywen category structure}.
Wordclay, USA. 60~p.
\bibitem{14-koz-1} %22
\Aue{Lenci, A.} 2008. Distributional semantics in linguistic and cognitive research.
\textit{Rivista di Linguistica} 2:1--30.

\bibitem{22-koz-1} %23
\Aue{Manning, C., P.~Raghavan, and H.~Sch$\ddot{\mbox{u}}$tze}. 2008.
\textit{Introduction to information retrieval}. Cambridge: Cambridge University
Press. 581~p.
\bibitem{23-koz-1} %24
\Aue{Sahlgren, M.} 2008. The distributional hypothesis. From context to meaning.
\textit{Distributional Models of the Lexicon in Linguistics and Cognitive Science:
Special issue of the Italian J.~Linguistics: Rivista di Linguistica} 20(1):33--53.

\bibitem{9-koz-1} %25
\Aue{Baroni, M., and A.~Lenci}. 2010. Distributional memory: A~general
framework for corpus-based semantics. \textit{Comput. Linguist.}
36(4):673--721.

\bibitem{24-koz-1} %26
\Aue{Turney, P.\,D., and P.~Pantel}. 2010. From frequency to meaning: Vector
space models of semantics. \textit{JAIR}
37:141--188.
\bibitem{30-koz-1} %27
\Aue{Zakharov, V.\,P., and M.\,V.~Khokhlova}. 2010. Analiz effektivnosti
statisticheskikh metodov vyyavleniya kollokatsiy v~tekstakh na Russkom yazyke
[The analysis of statistical methods for the discovery of collocations in the Russian
language texts].  \textit{Computational Linguistics and Intelligent Technologies.
Dialog'10: Conference (International) Proceedings}.  Moscow: Russian State
University for Humanities. 9(16):147--143.

\bibitem{12-koz-1} %28
\Aue{Kuznetsov, I.\,P., M.\,M.~Charnine, E.\,B.~Kozerenko, N.\,V.~Somin,
V.\,G.~Nikolayev, and A.\,G.~Matskevich}. 2012. Intelligent tools for the
semantic Internet navigator design. \textit{RCDL'2012 Conference (International)
on Digital Libraries Proceedings}. Pereslavl-Zalesski, Russia. 274--283.
\bibitem{13-koz-1} %29
\Aue{Kuznetsov, I.\,P., E.\,B.~Kozerenko, and M.\,M.~Charnin}. 2012.
Technological peculiarity of knowledge extraction for logical analytical systems.
\textit{ICAI'12: Conference (International) on Artificial Intelligence
Proceedings}. Las Vegas, USA: CSREA Press. 762--768.

\bibitem{27-koz-1} %30
\Aue{Schumann, A.} 2012. Towards the automated enrichment of multilingual
terminology databases with knowledge-rich contexts. \textit{Computational
Linguistics and Intelligent Technologies. Dialog'12: Conference (International)
Proceedings}.    Moscow: Russian State University for Humanities.
11(18):559--567.

\bibitem{34-koz-1} %31
\Aue{Sharnin, M.\,M., and I.\,P.~Kuznetsov}. 2012. Osobennosti
semanticheskogo poiska informatsionnykh  ob''ektov na osnove tekhnologii baz
znaniy [Semantic search of natural language information on the basis of
knowledge base technology]. \textit{Informatika i~ee Primeneniya}~---
\textit{Inform. Appl.} 6(2):47--56.
\bibitem{32-koz-1} %32
\Aue{Borisov, T.\,N., A.\,E.~Bronetski, S.\,V.~Klimenko, V.\,V.~Rykov, and
M.\,M.~Charnine}. 2013. Avtonomnye neobitaemye podvodnye apparaty:
Avtomaticheskoe formirovanie assotsiativnogo portreta predmetnoy oblasti
[Autonomous uninhabited submarine apparata: Auto-\linebreak\vspace*{-12pt}

 \pagebreak

 \noindent
 matic creation of the subject
area associative portrait].
 \textit{SC-IAS4i-VRTerro2013 Conference
(International) on Situational Centers and Information Analytical Systems of 4i
Class for the Tasks of Monitoring and Security Proceedings}. Protvino: IFTI.
38--43.

\bibitem{25-koz-1} %33
\Aue{Zolotarev, O., M.~Charnine, and A.~Matskevich}. 2014. Conceptual
business process structuring by extracting knowledge from natural language texts.
\textit{ICAI'14: Conference (International) on Artificial Intelligence
Proceedings}. Las Vegas, USA: CSREA Press. 82--87.
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-3pt}

\hfill{\small\textit{Received April 21, 2015}}

%\vspace*{-18pt}



    \Contr

    \noindent
   \textbf{Galina Irina V.} (b.\ 1967)~--- senior scientist, Institute of
Informatics Problems, Federal Research Center ``Computer Science
and Control'' of the Russian Academy of Sciences, 44-2 Vavilov
Str., Moscow, 119333, Russian Federation; irn\_gl@mail.ru

   \vspace*{3pt}

   \noindent
   \textbf{Kozerenko Elena B.} (b.\ 1959)~---
   Candidate of Science (PhD) in linguistics, Head of Laboratory,
Institute of Informatics Problems, Federal Research Center
``Computer Science and Control'' of the Russian Academy of
Sciences, 44-2 Vavilov Str., Moscow, 119333, Russian Federation;
kozerenko@mail.ru

   \vspace*{3pt}

   \noindent
   \textbf{Morozova Yulia I.}  (b.\ 1984)~--- scientist, Institute of Informatics Problems,
Federal Research Center ``Computer Science and Control'' of the Russian Academy of Sciences,
44-2 Vavilov Str., Moscow, 119333, Russian Federation; yulia-ipi@yandex.ru

   \vspace*{3pt}

   \noindent
   \textbf{Somin Nicolay V.} (b.\ 1947)~---
   Candidate of Science (PhD) in physics and mathematics, leading scientist, Institute of
Informatics Problems, Federal Research Center ``Computer Science and Control'' of the Russian
Academy of Sciences, 44-2 Vavilov Str., Moscow, 119333, Russian Federation; somin@post.ru


\vspace*{3pt}

\noindent
\textbf{Charnine Mikhail M.} (b.\ 1959)~---  Candidate of Science (PhD) in
technology, senior scientist, Institute of Informatics Problems, Federal
Research Center ``Computer Science and Control'' of the Russian Academy of
Sciences, 44-2 Vavilov Str., Moscow, 119333, Russian Federation;
mc@keywen.com

\label{end\stat}


\renewcommand{\bibname}{\protect\rm Литература}


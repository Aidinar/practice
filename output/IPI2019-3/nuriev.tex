
 \def\stat{nuriev}

\def\tit{АРХИТЕКТУРА СИСТЕМЫ НЕЙРОННОГО МАШИННОГО ПЕРЕВОДА}

\def\titkol{Архитектура системы нейронного машинного перевода}

\def\aut{В.\,А.~Нуриев$^1$}

\def\autkol{В.\,А.~Нуриев}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Нуриев В.\,А.}
\index{Nuriev V.\,A.}


%{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
%{Работа частично поддержана РФФИ (проект 18-07-00274).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Институт проблем информатики Федерального исследовательского центра 
<<Информатика и~управ\-ле\-ние>> Российской академии наук, 
\mbox{nurieff.v@gmail.com}}

%\vspace*{-2pt}
 
  
  
  
  \Abst{Описывается архитектура системы нейронного машинного перевода. Актуальность 
объекта исследования обусловлена тем, что сейчас доминирующей парадигмой в~области 
автоматизированного перевода стал машинный перевод с~использованием искусственных 
нейронных сетей. Качество нейронного перевода в~значительной мере превосходит 
результаты, обеспечиваемые предыдущим поколением машинных переводчиков, однако 
и~оно все еще далеко отстоит от качества переводов, выполненных  
че\-ло\-ве\-ком-экс\-пер\-том. Для его улучшения необходимо более четкое понимание 
устройства системы нейронного машинного перевода. В~общем случае архитектура системы 
нейронного машинного перевода включает в~себя две рекуррентные нейронные сети
(РНС), одна из 
которых ответственна за обработку входной текстовой последовательности, а~другая~--- за 
формирование выходного текс\-та-пе\-ре\-во\-да. Часто в~сис\-те\-му также встраивается 
механизм внимания, позволяющий оптимизировать работу с~протяженными входными 
последовательностями. Архитектура системы описывается на примере переводного сервиса 
Google ({\sf translate.google.com}, Google's Neural Machine Translation system,
GNMT), так как он стал 
одним из самых востребованных в~мире на данный момент: ежедневно сервис обрабатывает 
около 143~млрд слов более чем на~100~языках. В~заключение формулируются 
некоторые перспективы исследования.}
  
  \KW{нейронный машинный перевод; искусственные нейронные сети; рекуррентные 
нейронные сети; механизм внимания; архитектура системы машинного перевода; Google's 
Neural Machine Translation system}

\DOI{10.14357/19922264190313} 
  
%\vspace*{1pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}
  
  \section{Введение}
  
  В настоящее время доминирующей парадигмой в~области 
автоматизированного перевода стал машинный перевод с~использованием 
искусственных нейронных сетей~[1--4], пришедший на смену статистическим 
методам\footnote[2]{О~системах статистического машинного перевода см., например,~[5, 6].}. 
Нейронные сети инкорпорированы в~переводные системы компаний Google, 
Microsoft, Яндекс, DeepL, PROMT и~др. В~общем случае архитектура такой 
системы включает в~себя две РНС (recurrent neural 
networks), одна из которых ответственна за обработку входной текстовой 
последовательности, а~другая~--- за формирование выходного текс\-та-пе\-ре\-во\-да. 
Часто в~систему также встраивается механизм внимания, позволяющий 
оптимизировать работу с~протяженными входными 
последовательностями~\cite{1-nur, 7-nur}. 

Качество нейронного перевода 
в~значительной мере превосходит результаты, обес\-пе\-чи\-ва\-емые предыду\-щим 
поколением машинных пе\-ре\-вод\-чиков, однако и~оно все еще далеко отстоит от 
качества переводов, выполненных че\-ло\-ве\-ком-экс\-пер\-том. Для его улучшения 
необходимо более четкое понимание устройства системы нейронного 
машинного перевода.
  
  Цель статьи, следовательно,~--- описать архитектуру такой системы. Она 
описывается на примере переводного сервиса Google~--- 
GNMT\footnote[3]{Интернет-версия переводчика.}, так как он стал одним из самых востребованных в~мире на 
данный момент: ежедневно сервис обрабатывает около~143~млрд слов 
более чем на~100~языках~\cite{8-nur}. В~качестве основных источников 
материала используются статьи, авторство которых принадлежит 
разработчикам GNMT~\cite{9-nur, 10-nur}.
  
  \section{GNMT и~недостатки нейронного машинного перевода}
  
  Первые попытки спроектировать нейронный машинный переводчик выявили 
несколько его существенных недостатков. По сравнению с~системами 
статистического машинного перевода скорость его обучения оказалась 
значительно ниже. Из-за большого числа вовлеченных параметров больше 
времени требовалось на формирование выходной информации. Кроме того, он 
оказался неэффективен при работе с~редкими словами, а~в~отдельных случаях 
ему вообще не удавалось перевести все слова входного предложения.

 Для 
решения проб\-ле\-мы перевода редких слов предлагались разные подходы, 
например применение специального механизма внимания, чтобы копировать 
редкие слова и~в~исходной форме воспроизводить их в~выходном 
текс\-те-пе\-ре\-во\-де~\cite{11-nur}. Практическое применение этого подхода не принесло 
ожидаемых результатов: простое копирование редких слов не всегда 
оказывается лучшим способом перевода~--- оно не подходит, когда согласно 
конвенциям переводящего языка требуется транскрибирование.
  
  Разработка системы нейронного машинного перевода GNMT была нацелена 
на учет указанных недостатков. В~этой системе задействованы 
РНС с~долгой краткосрочной памятью  (ДКП, long short-term 
memory)~[12, 13]. Они имеют восьмислойную структуру, где связь между 
слоями осуществляется, в~том числе, за счет остаточных соединений (residual 
connections), обеспечивающих градиентный поток (gradient flow)~[14]. 

Для 
достижения параллелизма в~обработке данных внимание нижнего уровня 
декодирующей сети подключается к~верхнему уровню кодирующей сети. 
Чтобы уменьшить время вывода, используются вычисления с~низкой точностью 
(low-precision arithmetic), а в~целях дополнительного ускорения~--- специальное 
оборудование (Google's Tensor Processing Unit~--- тензорный процессор 
Google). Перевод редких слов осуществляется путем их представления на входе и~выходе в~форме набора составных элементов (частей слова~--- wordpieces) 
(подробнее см.\ в~[15]). 

Разбивка на составные элементы позволяет достичь 
баланса в~выборе минимальной переводной единицы, не спускаясь на уровень 
отдельных букв, что чрезвычайно увеличило бы входные и~выходные 
последовательности, и~между тем не оставаясь на уровне слова, если слово 
оказывается редким и~незнакомым. Кроме того, это помогает отказаться от 
необходимости вводить специальную обработку незнакомых слов. 

Для решения 
проблемы неполного перевода используется лучевой поиск (beam search), 
с~по\-мощью которого во время декодирования соизмеряются возможные 
варианты (hypotheses) разной протяженности, и~штраф за пропуск слова, что 
должно обеспечивать перевод всей полученной на входе информации.
  
  \section{Архитектура системы нейронного машинного перевода 
GNMT}
  
  Спроектированная модель использует распространенную схему обучения 
<<последовательность к~последовательности>> (sequence-to-sequence learning 
framework) с~механизмом внимания (см.\ подробнее~\cite{1-nur, 16-nur}). Она 
включает в~себя три компонента: кодирующую и~декодирующую сети  
(ко\-дер/де\-ко\-дер), а~также сеть внимания. Кодирующая сеть 
трансформирует исходное предложение в~список векторов, причем на каждый 
входной символ приходится один вектор. Обрабатывая этот список векторов, 
декодер производит поочередно по одному символу до тех пор, пока не 
достигнет специального символа, ассоциированного с~окончанием предложения 
(special end-of-sentence symbol). Кодирующий и~декодирующий сегменты 
соединяются модулем внимания (attention module), ответственным за 
способность декодера фокусироваться на различных отрезках исходного 
предложения в~процессе декодирования.
  
  В нотации при описании архитектуры \mbox{GNMT} для обозначения векторов 
$(\mathbf{v}, \mathbf{o}_i)$ используются\linebreak выделенные полужирным строчные 
буквы, для обозначения матриц $(\mathbf{U}, \mathbf{W})$~--- выделенные %\linebreak 
полужирным прописные буквы
для представления последовательностей $(X, Y)$ 
и~отдельных символов в~них\linebreak $(x_1, x_2)$~--- прописные буквы и~строчные 
буквы со\-от\-вет\-ст\-венно.
  
  Пусть $(X, Y)$~--- это пара, состоящая из исходного и~переводного 
предложений; $X\hm= x_1, x_2, x_3,\ldots , x_M$~--- последовательность 
из~$M$~символов в~исходном предложении, а~$Y\hm= y_1, y_2, y_3, \ldots , 
y_N$~--- последовательность из~$N$~символов в~переводном предложении. 
Тогда кодер~--- это функция следующей формы:
  $$
  \mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3,\ldots , 
\mathbf{x}_M=\mathrm{КодерРНС}\left( x_1, x_2, x_3,\ldots , x_M\right).
  $$
  
  В этом уравнении $\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3,\ldots , 
\mathbf{x}_M$~--- список векторов фиксированного размера, причем чис\-ло 
составляющих этот список членов то же, что и~чис\-ло символов в~исходном 
предложении ($M$~в~данном примере). Применяя цепное правило, условную 
вероятность последовательности ${\sf P}\,(Y\vert X)$ можно разложить как
  \begin{multline*}
  {\sf P}\left(Y\vert X\right)={\sf P}\left(Y\vert \mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3,\ldots , 
\mathbf{x}_M\right) ={}\\
  {}= \prod\limits^N_{i=1} {\sf P}\left( y_i\vert y_0, y_1, y_2, \ldots , y_{i-1}; 
\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3,\ldots, \mathbf{x}_M\right),
 \end{multline*}
где $y_0$~--- специальный ассоциированный с~началом предложения символ 
(``beginning of sentence'' symbol), который ставится в~начале каждого 
переводного предложения.

  При выводе вычисляется вероятность последующего символа с~учетом 
кодировки исходного предложения и~членов выходной по\-сле\-до\-ва\-тель\-ности, 
уже прошедших декодирование:
  ${\sf  P}\left( y_i\vert y_0, y_1, y_2, y_3,\ldots , y_{i-1}; \mathbf{x}_1, \mathbf{x}_2, 
\mathbf{x}_3,\ldots , \mathbf{x}_M\right).$
  
  Декодер представляет собой сочетание РНС и~слоя softmax. Рекуррентная нейронная сеть
   создает 
скрытое состояние~$\mathbf{y}_i$ для следующего прогнозируемого символа, 
которое затем проходит через слой softmax, чтобы сгенерировать 
распределение вероятности по выходным сим\-во\-лам-кан\-ди\-да\-там. 
Согласно экспериментальным данным, для улучшения качества перевода 
в~сис\-те\-мах нейронного машинного перевода необходимо обеспечить 
достаточную глубину ко\-ди\-ру\-ющей и~декодирующей РНС; только так РНС 
могут учитывать тонкие нюансы исходного и~переводящего языков. Похожие 
наблюдения были сделаны в~\cite{16-nur}: глубокие РНС с~ДКП 
значительно превосходят по 
производительности неглубокие (о~ДКП см.\ подробнее~\cite{12-nur}). 
Поэтому здесь, как и,~например, в~\cite{17-nur}, для ко\-ди\-ру\-ющей 
и~декодирующей РНС используется глубокая сеть с~ДКП.
  
  Модуль внимания сформирован, как в~\cite{1-nur}. Пусть $\mathbf{y}_{i-1}$ 
будет выходным сигналом декодирующей РНС для предыдущего временн$\acute{\mbox{о}}$го 
шага декодирования (здесь используется выходной сигнал нижнего слоя 
декодера). Контекст внимания~\textbf{a}$_i$ для текущего временн$\acute{\mbox{о}}$го шага 
вычисляется по следующей формуле:
$$
\mbox{\textbf{а}}_i=\sum\limits^M_{t=1} p_t \mathbf{x}_t\,.
$$
Здесь
$$
p_t= \fr{\exp(s_t)}{\sum\nolimits^M_{t=1}  \exp (s_t)}\ \forall t,\ 1\leq t\leq M\,,
$$
где
$$
  s_t=\mbox{Функция\ внимания}\ \left(\mathbf{y}_{i-1}, \mathbf{x}_t\right)\
  \forall t\,, \ 1\leq t\leq M\,,
$$
 а~Функция внимания~--- это сеть прямой связи с~одним скрытым слоем.
  
  Как уже говорилось выше, глубокие РНС с~ДКП часто показывают лучший 
результат, чем неглубокие. Однако простое наложение слоев в~ДКП работает 
только для определенного числа слоев, за его пределами сеть становится 
слишком медленной и~труднообучаемой~--- вероятно, из-за проб\-лем взрывного 
и~исчезающего градиента (о~них см.\ в~\cite{18-nur, 19-nur}). Эксперименты 
показывают, что для решения широкомасштабных задач перевода простое 
наложение слоев в~ДКП хорошие результаты дает при четырех слоях, 
удовлетворительные~--- при шести и~очень плохие, когда их число превышает 
восемь.
  
  Руководствуясь идеей о том, что для увеличения скорости обучения 
нейронной сети ее слои можно представить как остаточные функции со 
ссылкой на входные данные слоя, а не как функции без ссылок (см.\ подробнее 
в~\cite{14-nur, 20-nur, 21-nur}), разработчики GNMT ввели остаточные 
соединения между организованными в~последовательность слоями ДКП. Пусть 
ДКП$_i$ и~ДКП$_{i+1}$ будут $i$-м и~$(i\hm+1)$-м слоями ДКП 
в~последовательности, параметры которых~--- $\mathbf{W}^i$ 
и~$\mathbf{W}^{i+1}$ соответственно. Тогда на временн$\acute{\mbox{о}}$м шаге~$t$ для 
организованных в~последовательность слоев ДКП без остаточных соединений 
имеем:
  \begin{align*}
  \mathbf{c}_t^i, \mathbf{m}_t^i&=\mbox{ДКП}_i\left(\mathbf{c}^i_{t-1} 
\mathbf{m}^i_{t-1}, \mathbf{x}_t^{i-1};\mathbf{W}^i\right)\,;\\
  \mathbf{c}_t^{i+1}, \mathbf{m}_t^{i+1} &=\mbox{ДКП}_{i+1} 
\left(\mathbf{c}_{t-1}^{i+1}, \mathbf{m}_{t-1}^{i+1}, \mathbf{x}_t^i; 
\mathbf{W}^{i+1}\right)\,,
  \end{align*}
где $\mathbf{x}_t^i$~--- данные, поступающие на вход ДКП$_i$ на временн$\acute{\mbox{о}}$м 
шаге~$t$: 
$$
  \mathbf{x}_t^i=\mathbf{m}_t^i\,,
  $$ 
  а~$\mathbf{m}_t^i$ и~$\mathbf{c}_t^i$~--- скрытые состояния 
и~состояния памяти ДКП$_i$ на временн$\acute{\mbox{о}}$м шаге~$t$ соответственно.

  При введении остаточных соединений между ДКП$_i$ и~ДКП$_{i+1}$ эти 
уравнения принимают следующий вид:
  \begin{align*}
  \mathbf{c}_t^i, \mathbf{m}_t^i&=\mbox{ДКП}_i\left(\mathbf{c}^i_{t-1} 
\mathbf{m}^i_{t-1}, \mathbf{x}_t^{i-1};\mathbf{W}^i\right)\,;\\
  \mathbf{c}_t^{i+1}, \mathbf{m}_t^{i+1} &=\mbox{ДКП}_{i+1} 
\left(\mathbf{c}_{t-1}^{i+1}, \mathbf{m}_{t-1}^{i+1}, \mathbf{x}_t^i; 
\mathbf{W}^{i+1}\right)\,,
  \end{align*}
  где
  $$
    \mathbf{x}_t^i=\mathbf{m}_t^i+\mathbf{x}_t^{i-1}\,.
    $$
  
  Остаточные соединения значительно улучшают поток градиента при 
обратном проходе, что позволяет обучать очень глубокие сети кодера 
и~декодера. В~большинстве экспериментов авторами использовалось~8~слоев 
ДКП для кодера и~декодера, между тем применение остаточных соединений 
дает возможность обучать сети гораздо большей глубины.
  
  Информация, необходимая для перевода некоторых слов, может содержаться 
в~любом месте входного текстового фрагмента. Обычно информационная 
развертка происходит слева направо как на входе, так и~на выходе, однако 
в~зависимости от языковой пары информация (контекст) для перевода 
конкретного выходного слова может распределяться или даже разделяться 
между определенными областями входного фрагмента. Учитывая это, чтобы 
получить наилучший возможный контекст в~каждом отдельном случае, для 
кодера целесообразно использовать двунаправленную РНС (см.\ подробнее 
в~\cite{1-nur, 22-nur}). Вместе с~тем, чтобы обеспечить максимально возможное 
распараллеливание во время вычислений, двунаправленные соединения 
в~GNMT используются только для нижнего уровня кодера~--- все его 
остальные уровни являются однонаправленными. Двунаправленный нижний 
слой кодера, в~свою очередь, состоит из двух слоев, один из которых 
обрабатывает исходное предложение слева направо, другой~--- справа налево. 
Затем их выходные данные объединяются и~подаются на следующий 
(однонаправленный) слой.
  
  Для увеличения скорости обучения в~системе GNMT применяется 
параллелизм модели и~параллелизм данных. Параллелизм данных достигается 
обучением~$n$~копий модели одновременно, применяется алгоритм 
распределенного обучения Downdour SGD (stochastic gradient  
descent)~\cite{23-nur}. Все~$n$~копий оперируют одним и~тем же набором 
параметров модели, при этом каждая копия асинхронно обновляет параметры, 
используя комбинацию адаптивного алгоритма Adam (adaptive moment 
estimation)~\cite{24-nur} и~алгоритма SGD. При экспериментальной 
проверке~$n$, как правило, составляло около~10. Каждая модель обрабатывала 
минипакет (mini-batch) из~$m$~пар предложений за единицу времени, 
$m$~при этом в~большинстве случаев равнялось~128.
  
  В дополнение к~параллелизму данных для повышения скорости вычисления 
градиента на каж\-дой копии используется параллелизм модели. Сети кодера 
и~декодера разделены послойно и~размещаются на разных графических 
процессорах, запуская каж\-дый слой на отдельном графическом процессоре. 
Поскольку все слои, кроме первого слоя кодера, однонаправленные, слой 
$i\hm+ 1$ может начать вычисления до того, как слой~$i$ полностью их 
завершит, что ускоряет обучение. Слой softmax также разделен, причем каждый 
раздел отвечает за подмножество символов в~выходном словаре.
  
  Параллелизм модели накладывает определенные ограничения на ее 
архитектуру. Например, нельзя для каждого слоя кодера иметь 
дву\-на\-прав\-лен\-ный слой ДКП: это уменьшит параллелизм между последующими 
слоями, поскольку каж\-дый очередной слой не сможет начать вычисления, пока 
предыдущий их не закончит в~обоих направлениях. В~результате параллельно 
можно будет использовать только два графических процессора~--- один для 
прямого направления и~один для обратного. В~механизме внимания, чтобы 
достичь максимального параллелизма при работе декодера, выходное 
представление его нижнего слоя выровнено с~выходным представлением 
верхнего слоя кодера. Выравнивание верхнего слоя декодера с~верхним слоем 
кодера препятствовало бы распараллеливанию в~сети декодера, и~применение 
более одного графического процессора для декодирования не имело бы \mbox{смысла.}
{\looseness=1

}

  
  \section{Заключительные замечания}
  
В~настоящей статье на примере GNMT рассмотрена только архитектура системы нейронного машинного 
перевода. Для лучшего понимания механизмов ее работы и,~в~частности, того, как она производит 
спецификацию переводных вариантов, также важно иметь представление, например, о принципах ее обучения 
(об этом см.~\cite{9-nur, 10-nur}). Не вызывает сомнений, что дальнейшее изучение систем нейронного 
перевода необходимо, так как, несмотря на их явные преимущества по сравнению со статистическими 
машинными переводчиками, опытные данные показывают недостаточный уровень качества переводов, 
которое они обеспечивают\footnote{На это указывают и~сами создатели GNMT. Например, с~этим связаны 
эксперименты с~обучением трансферного типа (transfer learning experiments), а также попытки отказаться от 
интерлингвального подхода к~переводу (когда сначала производится перевод на язык-посредник типа 
английского и~затем с~него осуществляется перевод на желаемый выходной язык) и~использовать возможности 
<<zero-shot translation>> (переводить с~одного языка на другой, не используя примеры для этой языковой пары в~процессе обучения)~\cite{10-nur}.}.

  
  В заключение следует отметить, что одним из перспективных направлений 
  в~оптимизации систем нейронного машинного перевода представляется 
разработка способов интеграции языковых моделей в~их  
архитектуру~\cite{25-nur}.
  
 {\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
  \bibitem{1-nur}
  \Au{Bahdanau D., Cho~K., Bengio~Y.} Neural machine translation by jointly 
learning to align and translate~// Conference (International) on Learning 
Representations.~--- San Diego, CA, USA, 2015. P.~1--15. {\sf 
https://\linebreak arxiv.org/pdf/1409.0473.pdf}.
  \bibitem{2-nur}
  \Au{Castilho Sh., Moorkens J., Gaspari~F., Calixto~I., Tinsley~J., Way~A.} Is 
neural machine translation the new state of the art?~// Prague Bull. 
Math. Linguistics, 2017. Vol.~108. No.\,1. P.~109--120.
  \bibitem{3-nur}
  \Au{Zhao Y., Wang Y., Zhang~J., Zong~C.} Cost-aware learning rate for neural 
machine translation~// Chinese computational linguistics and natural language 
processing based on naturally annotated big data~/ Eds. M.~Sun, X.~Wang, 
B.~Chang, D.~Xiong.~--- Cham: Springer International Publishing, 2017. P.~85--93.
  \bibitem{4-nur}
  \Au{Zamora-Martinez~F., Castro-Bleda~M.\,J.} Efficient embedded decoding of 
neural network language models in a~machine translation system~// Int. 
J.~Neural Syst., 2018. Vol.~28. Iss.~9. Art.\ No.\,1850007. P.~1--15.
 
  \bibitem{6-nur}
  \Au{Koehn Ph.} Statistical machine translation.~--- New York, NY, USA: 
Cambridge University Press, 2010. 447~p.
 \bibitem{5-nur}
  \Au{Hearne M., Way~A.} Statistical machine translation: A~guide for linguists 
and translators~// Lang. Linguist. Compass, 2011. Vol.~5. No.\,5. P.~205--226.

  \bibitem{7-nur}
\Au{Choi H., Cho K., Bengio~Y.} Fine-grained attention mechanism for neural 
machine translation~// Neurocomputing, 2018. Vol.~284. P.~171--176.
  \bibitem{8-nur}
  \Au{Davenport C.} Google Translate processes 143~billion words every day. 
androidpolice.com, 2018. {\sf  
https://\linebreak www.androidpolice.com/2018/10/09/google-translate-processes-143-billion-words-every-day}.
  \bibitem{9-nur}
  \Au{Wu Y., Schuster~M., Chen~Z., \textit{et al.}} Google's neural 
machine translation system: Bridging the gap between human and machine 
translation. arXiv.org, 2016. {\sf https://arxiv.org/pdf/1609.08144.pdf}.
  \bibitem{10-nur}
  \Au{Johnson M., Schuster~M., Le~Q.\,V., \textit{et al.}} Google's multilingual 
neural machine translation system: Enabling zero-shot translation~// T.~Assoc.
 Computational Linguistics, 2017. Vol.~5. P.~339--351.
  \bibitem{11-nur}
  \Au{S$\acute{\mbox{e}}$bastien J., Kyunghyun~C., Memisevic~R., Bengio~Y.} 
On using very large target vocabulary for neural machine translation~// 53rd Annual 
Meeting of the Association for Computational Linguistics and the 7th Joint 
Conference (International) on Natural Language Processing of the Asian Federation 
of Natural Language Processing Proceedings.~--- Beijing, China: The Association for 
Computer Linguistics, 2015. Vol.~1. P.~1--10.
  \bibitem{12-nur}
  \Au{Hochreiter S., Schmidhuber~J.} Long short-term memory~// Neural 
Comput., 1997. Vol.~9. No.\,8. P.~1735--1780.
  \bibitem{13-nur}
  \Au{Gers F.\,A., Schmidhuber~J., Cummins~F.} Learning to forget: Continual 
prediction with LSTM~// Neural Comput., 2000. Vol.~12. No.\,10.  
P.~2451--2471.
  \bibitem{14-nur}
  \Au{He K., Zhang~X., Ren~S., Sun~J.} Deep residual learning for image 
recognition~// IEEE Conference on Computer Vision and Pattern Recognition.~--- 
Las Vegas, NV, USA: IEEE, 2016. P.~770--778.
  \bibitem{15-nur}
  \Au{Schuster M., Nakajima~K.} Japanese and Korean voice search~// IEEE 
 Conference (International) on Acoustics, Speech and Signal Processing.~--- 
Piscataway, NJ, USA: IEEE, 2012. P.~5149--5152.
  \bibitem{16-nur}
  \Au{Sutskever I., Vinyals~O., Le~Q.\,V.} Sequence to sequence learning with 
neural networks // 27th Conference (International) on Neural Information Processing 
Systems Proceedings.~--- Cambridge, MA, USA: MIT Press, 2014. Vol.~2.  
P.~3104--3112.
  \bibitem{17-nur}
  \Au{Luong M.-T., Sutskever~I., Le~Q.\,V., Vinyals~O., Zaremba~W.} Addressing 
the rare word problem in neural machine translation~// 53rd Annual Meeting of the 
Association for Computational Linguistics and the 7th Joint Conference 
(International) on Natural Language Processing Proceedings.~--- Beijing, China: 
Association for Computational Linguistics, 2015. Vol.~1. P.~11--19.
 
  \bibitem{19-nur}
  \Au{Hochreiter S., Bengio~Y., Frasconi~P., Schmidhuber~J.} Gradient flow in 
recurrent nets: The difficulty of learning long-term dependencies~// A~field guide to 
dynamical recurrent networks~/ Eds. J.\,F.~Kolen, S.~Kremer.~--- Los Alamitos, CA, USA: 
IEEE Press, 2001. P.~237--243.

 \bibitem{18-nur}
  \Au{Pascanu R., Mikolov~T., Bengio~Y.} On the difficulty of training recurrent 
neural networks. arXiv.org, 2013. {\sf https://arxiv.org/pdf/1211.5063v2.pdf}.

  \bibitem{20-nur}
  \Au{Fahlman S.\,E., Lebiere~C.} The cascade-correlation learning architecture~// 
Advances in neural information processing systems~/ Ed. D.\,S.~Touretzky.~--- 
San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1990. Vol.~2. P.~524--532.
  \bibitem{21-nur}
  \Au{Srivastava R.\,K., Greff~K., Schmidhuber~J.} Highway networks. 
arXiv.org, 2015. {\sf https://arxiv.org/pdf/ 1507.06228.pdf}.
  \bibitem{22-nur}
  \Au{Schuster M., Paliwal~K.} Bidirectional recurrent neural networks~// IEEE 
T.~Signal Proces., 1997. Vol.~45. No.\,11. P.~2673--2681.
  \bibitem{23-nur}
  \Au{Dean J., Corrado~G.\,S., Monga~R., \textit{et al.}} Large scale distributed 
deep networks~// Advances in neural information processing systems, 
2012. Vol.~25. {\sf https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf}.
  \bibitem{24-nur}
  \Au{Kingma D.\,P., Ba~J.} Adam: A~method for stochastic optimization. 
arXiv.org, 2014. {\sf https:/arxiv.org/pdf/ 1412.6980.pdf}.
  \bibitem{25-nur}
  \Au{Gulcehre C., Firat~O., Xu~K., Cho~K., Bengio~Y.} On integrating a language 
model into neural machine translation~// Comput. Speech Lang., 2017. Vol.~45. 
P.~137--148.

 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 30.06.19}}

\vspace*{8pt}

%\pagebreak

%\newpage

%\vspace*{-28pt}

\hrule

\vspace*{2pt}

\hrule

%\vspace*{-2pt}

\def\tit{ARCHITECTURE OF~A~MACHINE TRANSLATION SYSTEM}


\def\titkol{Architecture of~a~machine translation system}

\def\aut{V.\,A.~Nuriev}

\def\autkol{V.\,A.~Nuriev}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-11pt}


\noindent
Institute of Informatics Problems, Federal Research Center ``Computer Science and Control'' of the 
Russian Academy of Sciences, 44-2~Vavilov Str., Moscow 119333, Russian Federation 


\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2019\ \ \ volume~13\ \ \ issue\ 3}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2019\ \ \ volume~13\ \ \ issue\ 3
\hfill \textbf{\thepage}}}

\vspace*{3pt} 
  
  
  
  
  \Abste{The paper describes architecture of a Neural Machine Translation (NMT) system. The 
subject is brought up since NMT, i.\,e., translation using artificial neural networks, is now a leading 
Machine Translation paradigm. 
The NMT systems manage to deliver much better quality of output than 
the machine translators of the previous\linebreak\vspace*{-12pt}}

\Abstend{generation (statistical translation systems) do. Yet, the 
translation they produce still may contain various errors and it is relatively inaccurate compared 
with human translations. Therefore, to improve its quality, 
it is important to see more clearly how an 
NMT system is built and works. Commonly, its architecture consists of two 
recurrent neural networks, one to get the input text sequence and the other to generate translated 
output (text sequence). The NMT system often has an attention mechanism helping it cope with long 
input sequences. As an example, Google's NMT system is taken as the Google Translate service is 
one of the most highly demanded today, it processes around~143~billion words in more 
than~100~languages per day. The paper concludes with some perspectives for future research.}
  
  \KWE{neural machine translation; artificial neural networks; recurrent neural networks; 
attention mechanism; architecture of a machine translation system; Google's Neural Machine 
Translation system}
  
\DOI{10.14357/19922264190313} 

%\vspace*{-14pt}

%\Ack
%   \noindent



%\vspace*{-6pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
  \bibitem{1-nur-1}
  \Aue{Bahdanau, D., K.~Cho, and Y.~Bengio.} 2015. Neural machine translation 
by jointly learning to align and translate. \textit{Conference (International ) on 
Learning Representations}. San Diego, CA. 1--15. Available at: {\sf 
https://arxiv. org/pdf/1409.0473.pdf} (accessed June~20, 2019).
  \bibitem{2-nur-1}
  \Aue{Castilho, Sh., J. Moorkens, F.~Gaspari, I.~Calixto, J.~Tinsley, and A.~Way.} 
2017. Is neural machine translation the new state of the art? \textit{Prague 
Bull. Math. Linguistics} 108(1):109--120.
  \bibitem{3-nur-1}
  \Aue{Zhao, Y., Y. Wang, J. Zhang, and C.~Zong.} 2017. Cost-aware learning rate 
for neural machine translation. \textit{Chinese computational linguistics and natural 
language processing based on naturally annotated big data}. Eds. M.~Sun, 
X.~Wang, B.~Chang, and D.~Xiong. Cham: Springer International Publishing.  
85--93.
  \bibitem{4-nur-1}
  \Aue{Zamora-Martinez,~F., and M.\,J.~Castro-Bleda.} 2018. Efficient embedded 
decoding of neural network language models in a machine translation system. 
\textit{Int. J.~Neural Syst.} 28(9):1850007(1--15).
  \bibitem{5-nur-1}
  \Aue{Hearne, M., and A.~Way.} 2011. Statistical machine translation: A~guide 
for linguists and translators. \textit{Lang. Linguist. Compass} 5(5):205--226.
  \bibitem{6-nur-1}
  \Aue{Koehn, Ph.} 2010. \textit{Statistical machine translation}. New York, NY: 
  Cambridge University Press. 447~p.
  \bibitem{7-nur-1}
  \Aue{Choi, H., K.~Cho, and Y.~Bengio.} 2018. Fine-grained attention mechanism 
for neural machine translation. \textit{Neurocomputing} 284:171--176.
  \bibitem{8-nur-1}
  \Aue{Davenport, C.} 2018. Google Translate processes 143~billion words every 
day. Available at:  {\sf  
https://www. androidpolice.com/2018/10/09/google-translate-processes-143-billion-words-every-day} (accessed June~20, 2019).
  \bibitem{9-nur-1}
  \Aue{Wu, Y., M.~Schuster, Z.~Chen, \textit{et al.}}
   2016. Google'as neural machine translation system: Bridging the gap 
between human and machine translation. Available at: {\sf 
https://arxiv.org/pdf/1609.08144.pdf} (accessed June~20, 2019).
  \bibitem{10-nur-1}
  \Aue{Johnson, M., M.~Schuster, Q.\,V.~Le, \textit{et al.}}
   2017. Google's multilingual neural machine translation system: 
Enabling zero-shot translation. \textit{T.~Assoc. Computational Linguistics} 5:339--
351.
  \bibitem{11-nur-1}
  \Aue{S$\acute{\mbox{e}}$bastien,~J., C.~Kyunghyun, R.~Memisevic, and 
Y.~Bengio.} 2015. On using very large target vocabulary for neural machine 
translation. \textit{53rd Annual Meeting of the Association for Computational 
Linguistics and the 7th  Joint Conference (International) on Natural Language 
Processing of the Asian Federation of Natural Language Processing 
Proceedings}. Beijing, China: The Association for Computer Linguistics. 
1:1--10.
  \bibitem{12-nur-1}
  \Aue{Hochreiter, S., and J.~Schmidhuber.} 1997. Long short-term memory. 
\textit{Neural Comput.} 9(8):1735--1780.
  \bibitem{13-nur-1}
  \Aue{Gers, F.\,A., J.~Schmidhuber, and F.~Cummins.} 2000. Learning to forget: 
Continual prediction with LSTM. \textit{Neural Comput.} 12(10):2451--2471.
  \bibitem{14-nur-1}
  \Aue{He, K., X. Zhang, S.~Ren, and J.~Sun.} 2016. Deep residual learning for 
image recognition. \textit{IEEE Conference on Computer Vision and Pattern 
Recognition}. Las Vegas, NV: IEEE. 770--778.
  \bibitem{15-nur-1}
  \Aue{Schuster, M., and K.~Nakajima.} 2012. Japanese and Korean voice search. 
\textit{IEEE Conference (International) on Acoustics, Speech and Signal 
Processing}. Piscataway, NJ: IEEE. 5149--5152.
  \bibitem{16-nur-1}
  \Aue{Sutskever, I., O.~Vinyals, and Q.\,V.~Le.} 2014. Sequence to sequence learning 
with neural networks. \textit{27th Conference (International) on Neural Information 
Processing Systems Proceedings}. Cambridge, MA: MIT Press. 2:3104--3112.
  \bibitem{17-nur-1}
  \Aue{Luong, M.-T., I.~Sutskever, Q.\,V.~Le, O.~Vinyals, and W.~Zaremba.} 
2015. Addressing the rare word problem in neural machine translation. \textit{53rd 
Annual Meeting of the Association for Computational Linguistics and the 7th  Joint 
Conference (International) on Natural Language Processing Proceedings}. Beijing, 
China: Association for Computational Linguistics. 1:11--19.
 
  \bibitem{19-nur-1}
  \Aue{Hochreiter, S., Y.~Bengio, P.~Frasconi, and J.~Schmidhuber.} 2001. 
Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. 
\textit{A~field guide to dynamical recurrent networks.} Eds. J.\,F.~Kolen, and 
S.~Kremer. Los Alamitos, CA: IEEE Press. 237--243.
 \bibitem{18-nur-1}
  \Aue{Pascanu, R., T.~Mikolov, and Y.~Bengio.} 2013. On the difficulty of 
training recurrent neural networks. arxiv.org.
Available at: {\sf 
https://arxiv.org/pdf/1211.5063v2.pdf} (accessed June~20, 2019).

  \bibitem{20-nur-1}
  \Aue{Fahlman, S.\,E., and C.~Lebiere.} 1990. The cascade-correlation learning 
architecture. \textit{Advances in neural information processing systems}. Ed.\ 
D.\,S.~Touretzky. San Francisco, CA: Morgan Kaufmann Publishers Inc. 2:524--532.
  \bibitem{21-nur-1}
  \Aue{Srivastava, R.\,K., K.~Greff, and J.~Schmidhuber.} 2015. Highway 
networks. arxiv.org.
Available at: {\sf https://arxiv. org/pdf/1507.06228.pdf} (accessed June~20, 
2019).
  \bibitem{22-nur-1}
  \Aue{Schuster, M., and K.~Paliwal.} 1997. Bidirectional recurrent neural 
networks. \textit{IEEE T.~Signal Proces.} 45(11):2673--2681.
  \bibitem{23-nur-1}
  \Aue{Dean, J., G.\,S.~Corrado, R.~Monga, \textit{et al.}} 2012. 
Large scale distributed deep networks. \textit{Advances in neural information 
processing systems}. Vol.~25. Available at:  
{\sf https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf} 
(accessed June~20, 2019).
  \bibitem{24-nur-1}
  \Aue{Kingma, D.\,P., and J.\,Ba.} 2014. Adam: A~method for stochastic 
optimization. arxiv.org. Available at: {\sf https://arxiv.org/ pdf/1412.6980.pdf} (accessed 
June~20, 2019).
  \bibitem{25-nur-1}
  \Aue{Gulcehre, C., O.~Firat, K.~Xu, K.~Cho, and Y.~Bengio.} 2017. On 
integrating a~language model into neural machine translation. \textit{Comput. 
Speech Lang.} 45:137--148.
 \end{thebibliography}

 }
 }

\end{multicols}

%\vspace*{-7pt}

\hfill{\small\textit{Received June 30, 2019}}

%\pagebreak

%\vspace*{-22pt}
 
  
  \Contrl
  
  \noindent
  \textbf{Nuriev Vitaly A.} (b.\ 1980)~--- Candidate of Science (PhD) in philology, 
leading scientist, Institute of Informatics Problems, Federal Research Center 
``Computer Science and Control'' of the Russian Academy of Sciences, 44-2~Vavilov 
Str., Moscow 119333, Russian Federation; \mbox{nurieff.v@gmail.com}



\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература}
\def\stat{gor+kuz}

\def\tit{ПРИМЕНЕНИЕ РЕКУРРЕНТНЫХ НЕЙРОННЫХ СЕТЕЙ\\ ДЛЯ ПРОГНОЗИРОВАНИЯ МОМЕНТОВ КОНЕЧНЫХ\\ 
НОРМАЛЬНЫХ СМЕСЕЙ$^*$}

\def\titkol{Применение рекуррентных нейронных сетей для прогнозирования моментов конечных 
нормальных смесей}

\def\aut{А.\,К.~Горшенин$^1$, В.\,Ю.~Кузьмин$^2$}

\def\autkol{А.\,К.~Горшенин, В.\,Ю.~Кузьмин}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Горшенин А.\,К.}
\index{Кузьмин В.\,Ю.}
\index{Gorshenin A.\,K.}
\index{Kuzmin V.\,Yu.}


{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
{Работа выполнена при поддержке РФФИ (проекты 18-29-03100 и~19-07-00352) 
и~Стипендии Президента Российской Федерации молодым ученым и~аспирантам (СП-538.2018.5). Для ускорения обучения был использован гибридный высокопроизводительный вычислительный комплекс ФИЦ 
ИУ РАН: {\sf http://hhpcc.frccsc.ru}.}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Институт проблем информатики Федерального исследовательского
центра <<Информатика и~управление>> Российской академии наук; факультет
вычислительной математики и~кибернетики Московского государственного 
университета им.\ М.\,В.~Ломоносова, \mbox{agorshenin@frccsc.ru}}
\footnotetext[2]{ООО <<Вай2Гео>>, \mbox{shadesilent@yandex.ru}}

\vspace*{-6pt}



\Abst{Проведено сравнение нейронных сетей прямого распространения и~рекуррентных 
модификаций для решения задачи построения прогнозов непрерывных значений для 
математического ожидания, дисперсии, коэффициентов асимметрии и~эксцесса 
конечных нормальных смесей. Рас\-смот\-ре\-ны~$14$~раз\-лич\-ных архитектур
нейронных сетей, включая 
и~{\sf LSTM} (Long-Short Term Memory).
%, отличающиеся числом скрытых слоев и~нейронов в~них. 
Для повышения ско\-рости обучения использованы высокопроизводительные вычислительные 
средства. Продемонстрировано, что на рассматриваемых данных наилучшие результаты 
для всех моментных характеристик в~смысле качества прогнозирования в~стандартных 
метриках (среднеквадратичная ошибка, функция потерь, средняя абсолютная ошибка) 
достигаются с~использованием двух рекуррентных архитектур~--- с~одним скрытым слоем 
из~$100$ нейронов и~тремя слоями по~$50$~нейронов.}

\KW{рекуррентные нейронные сети; прогнозирование; глубокое обучение; 
высокопроизводительные вычисления; CUDA}

\DOI{10.14357/19922264190316} 
  
%\vspace*{1pt}

%\vspace*{-2pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}


\section{Введение}

%\vspace*{2pt}

Одним из наиболее популярных и~востребованных 
методов анализа данных и~по настоящий день остается
EM (expectation--maximization) 
алгоритм. Обычно рассматриваются различные его модификации, однако 
общий принцип наличия E- и~\mbox{M-ша}\-гов остается без изменений. В~частности, он 
при\-меняется для анализа па\-ра\-мет\-ров смешанных вероятностных моделей в~различных 
прикладных задачах. Также можно упомянуть кластерный 
анализ~\cite{Yang2012, Cai2019,Hassen2019} и~работу с~цензурированными данными 
на основе масштабных смесей нормальных законов~\cite{Zeller2019}. 
Остается популярным на\-прав\-ле\-ние исследований, ориентированное на повышение 
скорости работы EM-ал\-го\-рит\-ма с~помощью различных модификаций~--- за счет
 внедрения искусственного шума~\cite{Osoba2013}, блочных реализаций для 
 организации параллельных вычислений~\cite{Lee2018}, приближений на основе 
 методов Мон\-те-Кар\-ло по схеме марковских цепей~\cite{Wu2019}. Актуальными 
 остаются вопросы о числе компонент в~смешанных 
 моделях~\cite{Verbeek2003,Gorshenin2011,Liu2019}, снижении размерности 
 параметрического пространства~\cite{Yu2019}.

В статье~\cite{Gorshenin2019a} для описания эволюции турбулентных процессов 
в~маг\-ни\-то\-ак\-тив\-ной высокотемпературной плазме использован метод 
скользящего разделения конечных нормальных смесей~\cite{Korolev2011}. 
Исследование моментных характеристик аппроксимирующих распределений позволило 
проанализировать нелинейную стадию развития тур\-бу\-лент\-ности, ее насыщения, 
образования вихрей и~их хаотизации.

В статье~\cite{Gorshenin2019b} исследованы вопросы построения прогнозов 
моментов конечных нормальных смесей с~помощью нейронных сетей прямого 
распространения в~смысле классической задачи кластеризации. Непрерывные 
данные разбивались на некоторые промежутки, а нейронная сеть определяла 
попадание в~тот или иной класс. К~достоинствам данного метода стоит отнести 
полученную высокую точность (вплоть до~$99{,}7\%$) прогнозирования. Однако
 точные значения моментных характеристик (т.\,е.\ решение задачи регрессии), 
 которые бы представляли особый интерес для проведения исследований в~физике 
 турбулентной плазмы, в~упомянутой работе не определялись.

В данной статье будут рассмотрены~$14$ различных топологий
нейронных сетей~\cite{Buduma2017}, 
включая рекуррентную модификацию \verb"LSTM"~\cite{Greff2017}.
Они отличаются между собой количеством скрытых слоев и~числом 
нейронов в~них. Все они предназначены для решения задачи построения прогнозов 
непрерывных значений для математического ожидания (\verb"Exp"), 
дисперсии (\verb"Var"), коэффициентов асим\-мет\-рии (\verb"Skew") и~эксцесса 
(\verb"Kurt") конечных нормальных смесей. Кроме того, в~работе обсуждается 
повышение скорости обучения нейросетей с~использованием средств гибридного 
высокопроизводительного вычислительного комплекса (ГВВК).


\section{Базовые архитектуры нейронных сетей для~решения задачи прогнозирования}
%\label{SecArchitecture}

В этом разделе опишем конфигурации нейронных сетей, выбранных 
для прогнозирования различных моментов конечных смесей нормальных законов с~функцией 
распределения следующего вида ($x \hm\in \R$, $a_i(n)\hm\in\R$, $\sigma_i(n)\hm>0$, 
$p_{i}(n)\hm\geqslant 0$, $\sum p_{i}(n)\hm=1$, $i\hm=\overline{1,k(n)}$, 
$n\hm=1,2,\ldots$):
\begin{equation*}
\label{Mixture}
F(x)=\sum\limits_{i=1}^{k(n)}
\fr{p_i(n)}{\sigma_i(n)\sqrt{2\pi}}\int\limits_{-\infty}^{x}
\exp\left\{-\fr{(y-a_i(n))^2}
{2\sigma_i^2(n)}\right\}\,dy\,.
\end{equation*}

Зависимость от параметра~$n$ (номера шага) в~этой формуле соответствует 
принятому в~методе скользящего разделения смесей (СРС-ме\-то\-де) 
изучению эволюции распределения данных во времени в~режиме сдвигающегося окна. 
В~качестве тестовых выборок будут использоваться ряды из статьи~\cite{Gorshenin2019a}.
 Явные выражения для моментных характеристик, включая матричные представления, 
 приведены в~работах~\cite{Gorshenin2016a,Gorshenin2019b}.

Для исследования моментных характеристик были рассмотрены два типа архитектур~--- 
сети прямого распространения (многослойный перцептрон) и~рекуррентные сети с~долгой 
краткосрочной памятью \verb"LSTM". Строится прогноз на~$1$~шаг, при этом в~качестве 
входных данных используются~$50$~предшествующих наблюдений. Были рассмотрены 
сле\-ду\-ющие архитектуры нейронной сети:
\begin{enumerate}[I:] %[label=(\Roman{enumi}), ref=\Roman{enumi}]
\item \label{I} один скрытый слой, $60$ нейронов;
\item \label{II} один скрытый слой, $100$ нейронов;
\item \label{III} два скрытых слоя по $20$ нейронов в~каждом;
\item \label{IV} два скрытых слоя по $50$ нейронов в~каждом;
\item \label{V} два скрытых слоя по $100$ нейронов в~каждом;
\item \label{VI} три скрытых слоя по $20$ нейронов в~каждом;
\item \label{VII} три скрытых слоя по $50$ нейронов в~каждом.
\end{enumerate}

Обучение проводилось на протяжении~$750$~эпох либо останавливалось 
при отсутствии убывания\linebreak\vspace*{-12pt}

  { \begin{center}  %fig1
 \vspace*{-4pt}
   \mbox{%
 \epsfxsize=79mm 
 \epsfbox{gor-1.eps}
 }


\end{center}


\noindent
{{\figurename~1}\ \ \small{Пример изменения величины ошибок в~процессе обучения
 (логарифмическая шкала): \textit{1}~--- RMSE; \textit{2}~--- Loss; \textit{3}~--- MSE;
 \textit{4}~--- MAE}}
}

\vspace*{11pt}

\addtocounter{figure}{1}


\noindent
 функции потерь, основанной на среднеквадратичной ошибке, 
в~течение~$35$~эпох подряд. На рис.~1 показано изменение 
величины различных использованных метрик в~зависимости от 
эпохи обучения для коэффициента эксцесса, анализируемого с~помощью 
рекуррентной модификации архитектуры~\ref{VII}. Очевидно, что в~данном случае 
нет необходимости в~значительном числе шагов, однако подобная ситуация наблюдалась 
не для всех рядов. Кроме того, через некоторое число эпох после выхода на 
локальное <<плато>> обучение может быть продолжено.



Результаты представлены для метода оптимизации \verb"Adam"~\cite{Kingma2014}; 
использование \verb"SGD"~\cite{Buduma2017} и~\verb"AdaDelta"~\cite{Zeiler2012} 
не давало ощутимых преимуществ.
В качестве функции активации сетей прямого распространения выбрана \verb"ReLU" 
(Rectified Linear Unit)~\cite{Glorot2011}. Для повышения точ\-ности 
прогнозов использовано изменение скорости обучения нейронной сети 
при достижении <<плато>> точности (метод \verb"ReduceLROnPlateau" 
библиотеки \verb"Keras"). Для рекуррентных сетей в~качестве функции активации 
применены гиперболический тангенс и~так называемая рациональная 
сигмоида $x/(1\hm + |x|)$, для которой получены лучшие результаты
 на тестовых данных. Эффект переобучения не наблюдался. Применение 
 дроп\-аут-сло\-ев~\cite{Srivastava2014} не привело к~повышению качества 
 результатов, поэтому в~окончательных вариантах архитектур они не использовались.
 
 \begin{figure*}[b] %fig2
\vspace*{9pt}
    \begin{center}  
  \mbox{%
 \epsfxsize=107.205mm 
 \epsfbox{gor-2.eps}
 }
\end{center}
\vspace*{-9pt}
\Caption{Сравнение среднеквадратичной~(\textit{а}) и~средней абсолютной~(\textit{б}) ошибок
для различных архитектур: \textit{1}~--- математическое ожидание; 
\textit{2}~--- дисперсия; \textit{3}~--- коэффициент асимметрии; \textit{4}~---
коэффициент эксцесса} 
\label{FigRMSE_MAE}
\end{figure*}

\section{Применение нейронных сетей для прогнозирования моментов}
%\label{SecAnalysis}

В данном разделе рассмотрим результаты применения описанных выше архитектур 
к~задаче прогнозирования математического ожидания, дис\-пер\-сии, коэффициентов 
асимметрии и~эксцесса\linebreak
 конечных нормальных смесей. Для сравнения результатов 
прогнозирования использованы сред\-не\-квад\-ра\-тич\-ная ошибка \verb"RMSE", 
функция потерь на основе \verb"MSE" и~$L^2$-регуляризации, а также 
средняя абсолютная ошибка \verb"MAE". Данные нормализованы (сдвинуты и~нормированы) 
так, чтобы все наблюдения принадлежали сегменту $[0, 1]$.

На рис.~\ref{FigRMSE_MAE} представлены величины ошибок\linebreak
 \verb"RMSE" и~\verb"MAE" 
моделей, полученных в~результате обучения~$14$~архитектур на основе базовых 
типов~\ref{I}--\ref{VII}. Символ~<<a>> рядом с~римскими цифрами используется 
для обозначения \verb"LSTM".



Из рис.~\ref{FigRMSE_MAE} видно, что использование рекуррентных архитектур 
во всех случаях уменьшает значение ошибки (точные величины для 
\verb"RMSE" приведены в~табл.~1, наименьшие значения в~каж\-дом столбце 
выделены полужирным шрифтом), при этом в~среднем для всех рядов~--- в~$1{,}33$ 
и~$1{,}31$ раза (\verb"RMSE" и~\verb"MAE" соответственно). Для математического 
ожидания и~коэффициента эксцесса на лучших \verb"LSTM"-ар\-хи\-тек\-ту\-рах 
ошибка в~среднем меньше на~$10\%$--$20\%$, а~для дисперсии и~коэффициента 
асимметрии~--- на $30\%$--$60\%$.  Этот эффект более наглядно проявляется 
для функции потерь (рис.~\ref{FigLoss}).



В среднем для всех рядов в~данной метрике разница составляет~$20{,}3$ раза, 
а~в~отдельных случаях (см.\linebreak\vspace*{-12pt}

\pagebreak



\end{multicols}

\begin{table}
{\small %tabl1
\begin{center}
\parbox{390pt}{\Caption{Значения метрики RMSE и~функции потерь Loss (мантиссы, порядок~--- $10^{-3}$) 
для различных архитектур}
}

%\label{TabErr} 
\vspace*{2ex}

\tabcolsep=10pt
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
%\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{
\multicolumn{2}{|c|}{\ }&\multicolumn{8}{c|}{\bf Момент}\\
\cline{3-10}
\multicolumn{2}{|c|}{\raisebox{6pt}[0pt][0pt]{\bf Архитектура}}&\multicolumn{2}{c|}{\bf Exp}&
\multicolumn{2}{c|}{\bf Var}&\multicolumn{2}{c|}{\bf Skew}&
\multicolumn{2}{c|}{\bf Kurt}\\
\hline
{\bf Тип}&\verb"LSTM"&{\bf \verb"RMSE"}&\verb"Loss"&{\bf \verb"RMSE"}&\verb"Loss"&{\bf \verb"RMSE"}&\verb"Loss"&{\bf \verb"RMSE"}&\verb"Loss"\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf I}}}&Нет&$5{,}03$&$3{,}78$&$8{,}43$&$3{,}45$&$11{,}92$&$2{,}42$&$22{,}18$&$1{,}71$\\
%\cline{2-10}
&Да&$5{,}08$&$0{,}23$&$4{,}5$\hphantom{9}&$0{,}11$&$11{,}34$&$0{,}34$&$21{,}94$&$0{,}65$\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf II}}}&Нет&
$4{,}67$&$2{,}85$&$8{,}25$&$3$\hphantom{,99}&$11{,}5$\hphantom{9}&$2{,}97$&$22{,}18$&$2{,}07$\\
%\cline{2-10}
&Да&$\mathbf {4{,}04}$&$0{,}16$&$4{,}54$&$\mathbf{0{,}05}$&\hphantom{9}$8{,}38$&$\mathbf{0{,}14}$&$21{,}44$&$\mathbf{0{,}55}$\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf III}}}&Нет&$5{,}56$&$4{,}14$&$8{,}51$&$2{,}39$&$12{,}02$&$3{,}15$&$22{,}26$&$2{,}53$\\
%\cline{2-10}
&Да&$5{,}53$&$0{,}19$&6{,}9$\hphantom{9}$&$0{,}22$&\hphantom{9}$9{,}56$&$0{,}23$&$21{,}42$&$0{,}64$\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf IV}}}&Нет&$5{,}31$&$3{,}7$\hphantom{9}&$8{,}7$\hphantom{9}&$4{,}53$&$12{,}14$&$3{,}46$&$22{,}22$&$2{,}54$\\
%\cline{2-10}
&Да&$5{,}45$&$0{,}17$&$4{,}86$&$0{,}09$&\hphantom{9}$8{,}59$&$0{,}18$&$21{,}34$&$0{,}57$\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf V}}}&Нет&$5{,}53$&$4{,}68$&$8{,}61$&$3{,}23$&$11{,}93$&$3{,}16$&$22{,}18$&$2{,}19$\\
%\cline{2-10}
&Да&$4{,}94$&$0{,}19$&$7{,}37$&$0{,}2$\hphantom{9}&\hphantom{9}$\mathbf{7{,}72}$&$0{,}18$&$\mathbf{20{,}06}$&$\mathbf{0{,}55}$\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf VI}}}&Нет&$6{,}14$&$2{,}82$&$8{,}78$&$2{,}41$&$11{,}79$&$1{,}81$&$22{,}19$&$1{,}74$\\
%\cline{2-10}
&Да&$5{,}85$&$0{,}08$&$\mathbf{3{,}38}$&$0{,}09$&\hphantom{9}$9{,}23$&$0{,}22$&$21{,}57$&$0{,}59$\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf VII}}}&
Нет&$5{,}85$&$4{,}67$&$8{,}89$&$4{,}24$&$12{,}03$&$3{,}41$&$22{,}15$&$2{,}6$\hphantom{9}\\
%\cline{2-10}
&Да&$4{,}71$&$\mathbf {0{,}07}$&$3{,}71$&$0{,}11$&\hphantom{9}$8{,}01$&$0{,}17$&$20{,}69$&
$\mathbf{0{,}55}$\\
\hline
\end{tabular}
\end{center}}
%\vspace*{4pt}
%\end{table}
%\begin{figure} %fig3
%\renewcommand{\figurename}{\protect\bf }
\renewcommand{\tablename}{\protect\bf Рис.}
\setcounter{table}{2}
\vspace*{1pt}
    \begin{center}  
  \mbox{%
 \epsfxsize=104.157mm 
 \epsfbox{gor-3.eps}
 }
\end{center}
\vspace*{-14pt}
\Caption{Сравнение величины функции потерь для различных архитектур: 
\textit{1}~--- математическое ожидание; \textit{2}~--- 
дисперсия; \textit{3}~--- коэффициент асимметрии; \textit{4}~---
коэффициент эксцесса}
\label{FigLoss}
\vspace*{-1pt}
\end{table}

\renewcommand{\figurename}{\protect\bf Рис.}
\renewcommand{\tablename}{\protect\bf Таблица}
\setcounter{figure}{3}
\setcounter{table}{1}

\begin{multicols}{2}

\noindent
 математическое ожидание для конфигураций~\ref{VII} 
и~\ref{VII}{a}) получается более чем $65$-крат\-ное уменьшение величины 
ошибки. Отдельно можно выделить конфигурации~\ref{II}{a} и~\ref{VII}{a} (т.\,е.\ 
рекуррентные сети), в~которых для всех моментных характеристик сразу в~обеих 
метриках получены либо наименьшие среди всех, либо близкие к~этому значения. 
Таким образом, применение рекуррентных архитектур ведет к~значительному 
повышению качества обучения в~любой из рассматриваемых метрик. 

\begin{table*}[b]\small %tabl2
\vspace*{-8pt}
\begin{center}
\Caption{Время, затраченное на обучение на ГВВК (в секундах)}
\label{TabTime}
\vspace*{2ex}

\tabcolsep=10pt
\begin{tabular}{|c|c|r|r|r|r|}
\hline
\multicolumn{2}{|c|}{\bf Архитектура}&\multicolumn{4}{c|}{\bf Момент}\\
\hline
{\bf Тип}&\verb"LSTM"&\multicolumn{1}{c|}{{\bf Exp}}&
\multicolumn{1}{c|}{{\bf Var}}&
\multicolumn{1}{c|}{{\bf Skew}}&\multicolumn{1}{c|}{{\bf Kurt}}\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf I}}}&Нет&$38{,}08$&$32{,}99$&$303{,}79$&$67{,}72$\\
%\cline{2-6}
&Да&$2081{,}98$&$1932{,}59$&$1872{,}37$&$2034{,}13$\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf II}}}&Нет&$51{,}21$&$31{,}99$&$37{,}93$&$71{,}48$\\
%\cline{2-6}
&Да&$2170{,}9$\hphantom{9}&$1999{,}57$&$1985{,}41$&$1079{,}17$\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf III}}}&Нет&$45{,}61$&$49{,}82$&$56{,}26$&$80{,}56$\\
%\cline{2-6}
&Да&$3114{,}08$&$2813{,}48$&$2815{,}11$&$1362{,}48$\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf IV}}}&Нет&$63{,}83$&$36{,}16$&$57{,}23$&$80{,}04$\\
%\cline{2-6}
&Да&$3108{,}84$&$2839{,}28$&$2870{,}9$\hphantom{9}&$1680{,}07$\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf V}}}&
Нет&$58{,}79$&$40{,}7$\hphantom{9}&$61{,}33$&$79{,}21$\\
%\cline{2-6}
&Да&$3194{,}87$&$2983{,}34$&$2988{,}7$\hphantom{9}&$3159{,}44$\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf VI}}}&Нет&$73{,}9$\hphantom{9}&$42{,}52$&$71{,}38$&$61{,}83$\\
%\cline{2-6}
&Да&$3897{,}8$\hphantom{9}&$3826{,}78$&$3756{,}39$&$2435{,}18$\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{{\bf VII}}}&Нет&$71{,}72$&$275{,}35$&$68{,}82$&$61{,}96$\\
%\cline{2-6}
&Да&$3534{,}68$&$4018{,}36$&$3890{,}21$&$1909{,}27$\\
\hline
\end{tabular}
\end{center}
\end{table*}


Для сравнения архитектур определим индикатор, который представим как 
сумму отношений ошибки в~некоторой метрике на данной архитектуре к~минимальной
 ошибке прогноза для данного ряда (случай нулевой величины на практике является 
 почти недостижимым). Очевидно, что его наименьшее значение~--- $4$. 

 На тестовых данных минимальные значения данного индикатора достигаются на 
 архитектуре~\ref{II}{a} для математического ожидания, \ref{VI}{a} 
 для дисперсии и~\ref{V}{a} для коэффициентов асимметрии и~эксцесса. 
 При этом оптимальными в~общем случае можно признать конфигурации~\ref{II} 
 и~\ref{II}{a}, поскольку для них получается минимальная средневзвешенная
  сумма ошибок для сетей прямого распространения~($4{,}001$) 
  и~второе по величине значение после архитектуры~\ref{VII}{a}, 
  для которой введенный индикатор равен~$4{,}49$, среди всех рекуррентных сетей.
  
    { \begin{center}  %fig4
 \vspace*{-4pt}
   \mbox{%
 \epsfxsize=78.945mm 
 \epsfbox{gor-4.eps}
 }


\end{center}


\noindent
{{\figurename~4}\ \ \small{Коэффициент эксцесса и~прогнозы архитектуры~\ref{VII}{a}:
\textit{1}~--- данные; \textit{2}~--- модель}}
}

\vspace*{15pt}

%\addtocounter{figure}{1}



Для иллюстрации качества приближения данных обученными моделями 
на рис.~4 продемонстрированы значения для коэффициента 
эксцесса (нормализованные данные) и~аппроксимация ряда с~помощью предсказаний, 
сделанных с~применением рекуррентной архитектуры~\ref{VII}{a}.

%\vspace*{-2pt}

\section{Повышение эффективности обучения нейронных сетей за~счет 
использования высокопроизводительных вычислительных средств}
%\label{SecHPC}

В данном разделе будут обсуждаться вопросы, связанные с~временн$\acute{\mbox{ы}}$ми 
затратами на обучение базовых архитектур~\ref{I}--\ref{VII}. 
Результаты разд.~3 означают, что использование рекуррентных сетей позволило 
существенным образом повысить качество аппроксимации исходных данных. 
Однако нельзя не принимать во внимание тот факт, что усложнение конфигурации 
влечет за собой и~дополнительную вычислительную нагрузку при обучении. 
В~табл.~\ref{TabTime} приведены временн$\acute{\mbox{ы}}$е затраты для <<обычной>> 
и~\verb"LSTM"-ар\-хи\-тек\-тур, при этом не учитываются накладные расходы 
на предварительную обработку данных и~инициализацию графических видеокарт. 



В среднем сети прямого распространения обуча\-лись за~$664$~эпохи, в~то время как 
рекуррентные модификации~--- за $687$. Для рассматриваемых рядов было установлено, 
что для \verb"LSTM"-кон\-фи\-гу\-ра\-ций необходимое время обучения в~среднем 
превышает результаты для классических в~$47$~раз (минимальное значение~--- $6$~раз, 
максимальное~--- $90$) в~за\-ви\-си\-мости от архитектуры и~анализируемого ряда. Из  
отмеченных в~разд.~3 конфигураций~\ref{II}{a} и~\ref{VII}{a} 
лучшее время показывает именно~\ref{II}{a}~--- около~$30$~мин в~среднем на 
моментную характеристику, в~то время как для~\ref{VII}{a} получается более~$55$~мин. 
Таким образом, для более быстрой обработки рядов можно порекомендовать именно 
архитектуру~\ref{II}{a}.

Эффект от использования средств ГВВК проявляется прежде всего при обучении нейронных 
сетей прямого распространения. Так, среднее время на одну эпоху для 
сетей прямого распространения составляет~$2$~с для настольного решения 
(процессор \verb"Core i7", видеокарта \verb"NVIDIA GTX970M"), а~для ГВВК~--- 
$0{,}1$~с. Однако и~для рекуррентных конфигураций получено пятикратное ускорение,
 а~именно: с~$25$ до~$5$~с,~--- которое позволяет рассматривать их в~качестве инструмента 
 решения реальных прикладных задач.


\section{Заключение}

В~работе проведено сравнение результатов для~$14$~различных архитектур нейронных 
сетей в~задаче прогнозирования первых четырех моментных характеристик конечных 
смесей нормальных распределений. Выбраны две лучшие конфигурации 
(с~одним скрытым слоем из~$100$~нейронов и~с~тремя слоями по~$50$~нейронов), 
для которых получены наиболее точные значения во всех рас\-смат\-ри\-ва\-емых метриках. 
При этом на обучение конфигурации с~одним слоем затрачивается меньшее время, 
поэтому она может быть использована в~задачах, для которых более критично 
быстродействие, а~не точ\-ность аппроксимации. Использованные подходы в~достаточной 
степени универсальны, поэтому полученные архитектуры могут быть успешно 
применены для анализа временн$\acute{\mbox{ы}}$х рядов различной природы.


{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{Yang2012} \Au{Yang~M.-Sh., Lai~Ch.-Yo, Lin~C.-Y.} 
A~robust EM clustering algorithm for Gaussian mixture models~// 
Pattern Recogn., 2012. Vol.~45. Iss.~11. P.~3950--3961.

\bibitem{Cai2019} \Au{Cai~T.\,T., Ma~J., Zhang~L.} CHIME: Clustering 
of high-dimensional Gaussian mixtures with EM algorithm and its optimality~// 
Ann. Stat., 2019. Vol.~47. Iss.~3 P.~1234--1267.

\bibitem{Hassen2019} \Au{Ben Hassen~H., Masmoudi~K., Masmoudi~A.} 
Model selection in biological networks using a graphical EM algorithm~// 
Neurocomputing, 2019. Vol.~349. P.~271--280.

\bibitem{Zeller2019} \Au{Zeller~C.\,B., Cabral~C.\,R.\,B., Lachos~V.\,H., Benites~L.} 
Finite mixture of regression models for censored data based on scale mixtures of 
normal distributions~// Adv. Data Anal. Classi., 2019. Vol.~13. Iss.~1. P.~89--116.

\bibitem{Osoba2013} \Au{Osoba~O., Mitaim~S., Kosko~B.} 
The noisy Expectation--Maximization algorithm~// Fluct. Noise Lett., 2013. 
Vol.~12. Iss.~3. Art.~No.\,1350012.

\bibitem{Lee2018} \Au{Lee~S.\,X., Leemaqz~K.\,L., McLachlan~G.\,J.} 
A~block EM algorithm for multivariate skew normal and skew t-mixture
models~// IEEE~T. Neur. Net. Lear., 2018. 
Vol.~29. Iss.~11. P.~5581--5591.

\bibitem{Wu2019} \Au{Wu~D., Ma~J.} 
An effective EM algorithm for mixtures of Gaussian processes via the MCMC 
sampling and approximation~// Neurocomputing, 2019. Vol.~331. P.~366--374.

\bibitem{Verbeek2003} \Au{Verbeek~J.\,J., Vlassis~N., Krose~B.} 
Efficient greedy learning of Gaussian mixture models~// 
Neural Comput., 2003. Vol.~15. Iss.~2. P.~469--485.

\bibitem{Gorshenin2011} \Au{Горшенин~А.\,К.} 
Проверка статистических гипотез в~модели расщепления компоненты~// 
Вестник Мос\-ков\-ско\-го университета. Сер.~15: Вычислительная математика и~кибернетика, 
2011. Вып.~4. С.~26--32.

\bibitem{Liu2019}  \Au{Liu~C., Li~H.-C., Fu~K., Zhang~F., Datcu~M., Emery~W.\,J.} 
A~robust EM clustering algorithm for Gaussian mixture models~// 
Pattern Recogn., 2019. Vol.~87. P.~269--284.

\bibitem{Yu2019}  \Au{Yu~L., Yang~T., Chan~A.\,B.} 
Density-preserving hierarchical EM algorithm: Simplifying Gaussian mixture
models for approximate inference~// IEEE~T. Pattern Anal., 
2019. Vol.~41. Iss.~6. P.~1323--1337.

\bibitem{Gorshenin2019a}   \Au{Batanov~G.\,M., Borzosekov~V.\,D., 
Gorshenin~A.\,K., Kharchev~N.\,K., Korolev~V.\,Yu., Sarskyan~K.\,A.}
Evolution of statistical properties of microturbulence 
during transient process under electron cyclotron resonance heating of the 
L-2M stellarator plasma~// Plasma Phys. Contr.~F., 2019. Vol.~61. Iss.~7. 
Art.~No.\,075006.

\bibitem{Korolev2011}  \Au{Королев~В.\,Ю.} 
Ве\-ро\-ят\-но\-ст\-но-ста\-ти\-сти\-че\-ские методы декомпозиции волатильности 
хаотических процессов.~--- М.: Изд-во Моск. ун-та, 2011. 512~с.

\bibitem{Gorshenin2019b}  \Au{Gorshenin~A.\,K., Kuzmin~V.\,Yu.} 
Improved architecture of feedforward neural networks to increase accuracy of  
predictions for moments of finite normal mixtures~// 
Pattern Recogn. Image Anal., 2019. Vol.~29. No.\,1. P.~79--88.

\bibitem{Buduma2017}  \Au{Buduma~N.} Fundamentals of deep learning: 
Designing next-generation machine intelligence algorithms.~--- 
Sebastopol, CA, USA: O'Reilly Media, 2017. 298~p.

\bibitem{Greff2017}  
\Au{Greff~K., Srivastava~R.\,K., Koutnik~J.,
 Steunebrink~B.\,R., Schmidhuber~J.} 
LSTM: A~search space Odyssey~// IEEE~T. Neur. Net. 
Lear., 2017. Vol.~28. Iss.~10. P.~2222--2232.

\bibitem{Gorshenin2016a}  
\Au{Горшенин~А.\,К.} Концепция он\-лайн-ком\-плек\-са 
для стохастического моделирования реальных процессов~// Информатика и~её 
применения, 2016. Т.~10. Вып.~1. C.~72--81. 

\bibitem{Kingma2014}  
\Au{Kingma~D., Ba~J.}  
Adam: A~method for stochastic optimization~// 3rd 
 Conference (International) for Learning Representations~// arXiv:1412.6980, 2015. 13~p.

\bibitem{Zeiler2012}  
 \Au{Zeiler~M.\,D.} 
ADADELTA: An adaptive learning rate method~// arXiv:1212.5701, 2012. 6~p.

\bibitem{Glorot2011}  
\Au{Glorot~X., Bordes~A., Bengio~Y.} 
Deep sparse rectifier neural
networks~// J.~Mach. Learn. Res., 2011. Vol.~15. P.~315--323.

\bibitem{Srivastava2014}   
\Au{Srivastava~N., Hinton~G., Krizhevsky~A., 
Sutskever~I., Salakhutdinov~R.} Dropout: 
A~simple way to prevent neural networks from overfitting~// J.~Mach. Learn. Res., 
2014. Vol.~15. P.~1929--1958.
 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 04.09.19}}

%\vspace*{8pt}

%\pagebreak

\newpage

\vspace*{-28pt}

%\hrule

%\vspace*{2pt}

%\hrule

%\vspace*{-2pt}

\def\tit{APPLICATION OF RECURRENT NEURAL NETWORKS TO~FORECASTING THE~MOMENTS 
OF~FINITE NORMAL MIXTURES}


\def\titkol{Application of recurrent neural networks to~forecasting the moments 
of~finite normal mixtures}

\def\aut{A.\,K.~Gorshenin$^{1,2}$ and~V.\,Yu.~Kuzmin$^3$}

\def\autkol{A.\,K.~Gorshenin and~V.\,Yu.~Kuzmin}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-11pt}


\noindent
$^1$Institute of Informatics Problems, Federal Research Center ``Computer Science and
Control'' of the Russian\linebreak
$\hphantom{^1}$Academy of Sciences, 44-2~Vavilov Str., Moscow 119333, Russian
Federation  

\noindent
$^2$Faculty of Computational Mathematics and Cybernetics, M.\,V.~Lomonosov Moscow
State University, GSP-1,\linebreak
$\hphantom{^1}$Leninskie Gory, Moscow 119991, Russian Federation

\noindent
$^3$ ``Wi2Geo LLC'', 3-1~Mira Ave., Moscow 129090, Russian Federation


\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2019\ \ \ volume~13\ \ \ issue\ 3}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2019\ \ \ volume~13\ \ \ issue\ 3
\hfill \textbf{\thepage}}}

\vspace*{3pt} 



\Abste{The article compares the application of feedforward and recurrent 
neural networks to forecasting continuous values of expectation, variance,
 skewness, and kurtosis of finite normal mixtures. Fourteen various architectures
of neural networks are considered. 
% including the {\sf LSTM} (Long-Short Term Memory)
%  with different numbers of hidden layers and 
% neurons are considered. 
To increase training speed, the high-performance 
 computing cluster is used. It is demonstrated that the best forecasting 
 results based on standard metrics (root-mean-square error, mean absolute errors, 
 and loss function) are achieved on the two {\sf LSTM} 
 (Long-Short Term Memory) networks: with~100 neurons
  in one hidden layer and~50~neurons in each three hidden layers.}

\KWE{recurrent neural networks; forecasting; deep learning; high-performance
 computing; CUDA}


\DOI{10.14357/19922264190316} 

%\vspace*{-14pt}

\Ack
\noindent
The research is partially supported by the Russian Foundation for Basic Research
(projects 18-29-03100 and 19-07-00352) and the RF Presidential 
scholarship program (project No.\,538.2018.5). The calculations were performed 
using Hybrid high-performance computing cluster of FRC CSC RAS 
({\sf http://hhpcc.frccsc.ru}).


%\vspace*{-6pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{1-gk}
\Aue{Yang,~M.-Sh., Ch.-Yo~Lai, and C.-Y.~Lin.} 2012. A~robust EM clustering algorithm for 
Gaussian mixture models. \textit{Pattern Recogn.} 45(11):3950--3961.

\bibitem{2-gk}
\Aue{Cai,~T.\,T., J.~Ma, and L.~Zhang.} 2019. CHIME: Clustering 
of high-dimensional Gaussian mixtures with em algorithm and its optimality. 
\textit{Ann. Stat.} 47(3):1234--1267.

\bibitem{3-gk}
\Aue{Ben Hassen,~H., K.~Masmoudi, and A.~Masmoudi.} 2019. 
Model selection in biological networks using a graphical EM algorithm. 
 \textit{Neurocomputing} 349:271--280.

\bibitem{4-gk}
\Aue{Zeller,~C.\,B., C.\,R.\,B.~Cabral, V.\,H.~Lachos, and L.~Benites.}
 2019. Finite mixture of regression models for censored 
 data based on scale mixtures of normal distributions. 
  \textit{Adv. Data Anal. Classi.} 13(1):89--116.

\bibitem{5-gk}
\Aue{Osoba,~O., S.~Mitaim, and B.~Kosko.} 2013. 
The noisy Expectation-Maximization algorithm. 
 \textit{Fluct. Noise Lett.} 12(3):1350012.

\bibitem{6-gk}
\Aue{Lee,~S.\,X., K.\,L.~Leemaqz, and G.\,J.~McLachlan.} 2018. 
A~block EM algorithm for multivariate skew normal and skew t-mixture models. 
 \textit{IEEE~T. Neur. Net. Lear.} 29(11):5581--5591.

\bibitem{7-gk}
\Aue{Wu,~D., J.~Ma.} 2019. An effective EM algorithm for mixtures 
of Gaussian processes via the MCMC sampling and approximation. 
 \textit{Neurocomputing} 331:366--374.

\bibitem{8-gk}
\Aue{Verbeek,~J.\,J., N.~Vlassis, and B.~Krose.} 
2003. Efficient greedy learning of Gaussian mixture models. 
 \textit{Neural Comput.} 15(2):469--485.

\bibitem{9-gk}
\Aue{Gorshenin,~A.\,K.} 2011. Testing of statistical hypotheses in the 
splitting component model.  \textit{Mosc. Univ. Comput. Math. 
Cybernetics} 35(4):176--183.

\bibitem{10-gk}
\Aue{Liu,~C., H.-C.~Li, K.~Fu, F.~Zhang, M.~Datcu, and W.\,J.~Emery.} 
2019. A~robust EM clustering algorithm for Gaussian mixture models. 
 \textit{Pattern Recogn.} 87:269--284.

\bibitem{11-gk}
\Aue{Yu,~L., T.~Yang, and A.\,B.~Chan.} 2019. 
Density-preserving hierarchical EM algorithm: Simplifying Gaussian 
mixture models for approximate inference. 
 \textit{IEEE~T. Pattern Anal.} 41(6):1323--1337.

\bibitem{12-gk}
\Aue{Batanov,~G.\,M., V.\,D.~Borzosekov, A.\,K.~Gorshenin, N.\,K.~Kharchev, 
V.\,Yu.~Korolev, and K.\,A.~Sarskyan.}  2019. Evolution of statistical properties 
of microturbulence during transient process under electron cyclotron resonance 
heating of the L-2M stellarator plasma. 
 \textit{Plasma Phys. Contr.~F.} 61(7):075006.

\bibitem{13-gk}
\Aue{Korolev,~V.\,Yu.} 2011.  \textit{Veroyatnostno-statisticheskie metody dekompozitsii 
volatil'nosti khaoticheskikh protsessov} 
[Probabilistic and statistical methods of decomposition 
of volatility of chaotic processes]. Moscow: Moscow University Publishing House. 512~p.

\bibitem{14-gk}
\Aue{Gorshenin,~A.\,K., and V.\,Yu.~Kuzmin.}
 2019. Improved architecture of feedforward neural networks to increase accuracy 
 of predictions for moments of finite normal mixtures. 
  \textit{Pattern Recog. Image Anal.} 29(1):68--77.

\bibitem{15-gk}
\Aue{Buduma,~N.} 2017. \textit{Fundamentals of deep learning: 
Designing next-generation machine intelligence algorithms}. 
 Sebastopol, CA: O'Reilly Media. 298~p.

\bibitem{16-gk}
\Aue{Greff,~K., R.\,K.~Srivastava, J.~Koutnik, B.\,R~Steunebrink, 
and J.~Schmidhuber.}  2017. LSTM: A~search space Odyssey. 
 \textit{IEEE~T. Neur. Net. Lear.} 28(10):2222--2232.

\bibitem{17-gk}
\Aue{Gorshenin,~A.\,K.} 2016. Kontseptsiya onlayn-kompleksa dlya stokhasticheskogo 
modelirovaniya real'nykh pro\-tses\-sov [Concept of online service for stochastic modeling of real 
processes].  \textit{Informatika i~ee Primeneniya~--- Inform. Appl.} 10(1):72--81.

\bibitem{18-gk}
\Aue{Kingma,~D., and J.~Ba.} 2015. Adam: A~method for stochastic optimization. 
 \textit{3rd Conference (International) for Learning Representations}. 
 arXiv:1412.6980. 13~p.

\bibitem{19-gk}
\Aue{Zeiler,~M.\,D.} 2012. ADADELTA: An adaptive learning rate method. 
arXiv:1212.5701. 6~p.

\bibitem{20-gk}
\Aue{Glorot,~X., A.~Bordes, and Y.~Bengio.} 
2011. Deep sparse rectifier neural networks.
 \textit{J.~Mach. Learn. Res.} 15:315--323.

\bibitem{21-gk}
\Aue{Srivastava,~N., G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.}
 2014. Dropout: A~simple way to prevent neural networks from overfitting.
 \textit{J.~Mach. Learn. Res.} 15:1929--1958.
 
 \end{thebibliography}

 }
 }

\end{multicols}

%\vspace*{-7pt}

\hfill{\small\textit{Received September 4, 2019}}

%\pagebreak

%\vspace*{-22pt}


\Contr


\noindent
\textbf{Gorshenin Andrey K.} (b.\ 1986)~--- Candidate of Science (PhD) in physics and
mathematics, associate professor, leading scientist, Institute of Informatics Problems,
Federal Research Center ``Computer Science and Control'' of the Russian Academy of
Sciences, 44-2~Vavilov Str., Moscow 119333, Russian Federation;  
leading scientist, Faculty
of Computational Mathematics and Cybernetics, M.\,V.~Lomonosov Moscow State University, GSP-1,
Leninskie Gory, Moscow 119991, Russian Federation; \mbox{agorshenin@frccsc.ru}

\vspace*{3pt}

\noindent
\textbf{Kuzmin Victor Yu.} (b.\ 1986)~--- Head of Development, ``Wi2Geo LLC,'' 
3-1~Mira
Ave., Moscow 129090, Russian Federation; \mbox{shadesilent@yandex.ru}



\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература}  
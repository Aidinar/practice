\def\stat{gor+mart}

\def\tit{ГИБРИДНЫЕ МОДЕЛИ ЭКСТРЕМАЛЬНОГО ГРАДИЕНТНОГО БУСТИНГА 
ДЛЯ~ВОССТАНОВЛЕНИЯ ПРОПУЩЕННЫХ ЗНАЧЕНИЙ В~ДАННЫХ ОБ~ОСАДКАХ$^*$}

\def\titkol{Гибридные модели экстремального градиентного бустинга 
для~восстановления пропущенных значений в~данных} % об~осадках}

\def\aut{А.\,К.~Горшенин$^1$,  О.\,П.~Мартынов$^2$}

\def\autkol{А.\,К.~Горшенин,  О.\,П.~Мартынов}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Горшенин А.\,К.}
\index{Мартынов О.\,П.}
\index{Gorshenin A.\,K.}
\index{Martynov O.\,P.}


{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
{Постановка задачи и~анализ полученных результатов в~данной статье проведены 
А.\,К.~Горшениным, чьи исследования поддержаны РНФ (проект 18-71-00156). 
Разработка и~программная реализация методов анализа пропущенных значений 
выполнены О.\,П.~Мартыновым.}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Институт проблем информатики Федерального исследовательского
центра <<Информатика и~управление>> Российской академии наук; факультет
вычислительной математики и~кибернетики Московского государственного 
университета имени М.\,В.~Ломоносова, \mbox{agorshenin@frccsc.ru}}
\footnotetext[2]{Факультет вычислительной математики и~кибернетики Московского 
государственного университета им.\ М.\,В.~Ломоносова, 
\mbox{martynov.oleg.mipt@gmail.com}}

%\vspace*{-2pt}



\Abst{Проведено сравнение классического метода экстремального градиентного бустинга, 
реализованного во фреймворке {\sf XGBoost}
 ({\sf  eXtreme Gradient Boosting}, экстремальный 
градиентный бустинг) и~категориальной
 модификации {\sf CatBoost} ({\sf Categorical Boosting}, категориальный бустинг), 
 которая достаточно редко встречается в~научных 
 исследованиях. Предложены некоторые гибридные модели 
 клас\-си\-фи\-ка\-ции-ре\-грес\-сии 
 для повышения точности заполнения пропусков в~реальных данных на примере~14~станций 
 в~Германии. Достигнутая точность в~задачах классификации составила до~92\% 
 при весьма умеренных значениях ошибок прогнозов в~метрике {\sf RMSE}
 ({\sf Root Mean-Square Error}, сред\-не\-квад\-ра\-тич\-ная ошибка). 
 Гибридные методы превзошли по качеству предсказания простые модели 
 классификации и~регрессии. Развиваемые подходы могут быть успешно 
 использованы как для непосредственного анализа метеорологических данных 
 методами машинного обучения, так и~для улучшения качества предсказания 
 на основе физических моделей атмосферных процессов.}

\KW{заполнение пропусков; осадки; классификация; регрессия; градиентный бустинг; 
XGBoost; CatBoost}

\DOI{10.14357/19922264190306} 
  
%\vspace*{1pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}


\section{Введение}

Повышение эффективности алгоритмов машинного обучения привело к~росту их 
востребо\-ванности как в~задачах анализа результатов физических моделей 
предсказания погоды с~целью получения более точного прогноза, так и~в~качестве 
самостоятельных инструментов исследования\linebreak про\-стран\-ственно-вре\-мен\-н$\acute{\mbox{ы}}$х 
метеорологических рядов, %\linebreak 
полученных со спутников и~метеостанций. Такие %\linebreak 
наблюдения в~больших объемах поступают с~огромного числа датчиков и~зачастую 
содержат пропуски, которые могут существенным образом повлиять на качество 
обучения методов или изменить решения статистических моделей анализа различных 
метеорологических явлений, например экстремальных осадков~\cite{Gorshenin2018c}. 
Поэтому  весьма важной оказывается задача корректного заполнения пропусков в~данных.

В настоящей статье развивается подход к~обработке пропущенных значений 
для объемов осадков на основе популярного алгоритма машинного\linebreak обучения, 
называемого градиентным бустингом над деревьями решений~\cite{Friedman2001}. 
Это семейство методов, включая и~его наиболее час\-то применяемую модификацию 
\verb"XGBoost"~\cite{Chen2016}, широко используется для решения задач 
классификации и~регрессии в~значительном спектре прикладных областей. 
Например, можно упомянуть работы по предсказанию биологической активности 
лекарств~\cite{Mustapha2016}, кредитному скоррингу~\cite{Xia2017}, 
прогнозированию кризисов на финансовых рынках~\cite{Chatzis2018}, 
обнаружению дефектов в~вет\-ро\-вых турбинах~\cite{Zhang2018}, вет\-ро\-вой 
энергетике и~солнечной радиации~\cite{Aler2017,Torres-Barran2018}.

Предложенная компанией \verb"Яндекс" модификация метода градиентного 
бустинга, получившая название \verb"CatBoost"~\cite{Prokhorenkova2018}, 
несмотря на ее 
применение\linebreak в~реальных продуктах, в~научных статьях используется достаточно редко. 
В~качестве примеров можно упомянуть протеомные исследования~\cite{Ivanov2019} 
и~<<умные>> сети электроснабжения~\cite{Punmiya2019}. 
В~данной статье восполняется этот пробел для метеорологических данных, для 
которых традиционно применяется классический экстремальный градиентный 
бустинг~\cite{Korner2018,Fan2018}. Будет проведено сравнение \verb"XGBoost" 
и~\verb"CatBoost", а~также предложены некоторые гибридные модели для повышения 
точ\-ности заполнения пропусков в~реальных данных на примере~14~станций в~Германии. 
Кроме того, для извлечения признаков будет использована новая библиотека 
\verb"tsfresh"~\cite{Christ2018} для языка программирования \verb"Python".

\section{Постановка задачи}


Исходные данные представляют собой наборы признаков 
$\{\hat{x}_t\,|\,t \hm\in T\}$ и~соответствующие им измерения $\{y_t\,|\,t \hm\in T\}$ 
за некоторый временной интервал~$T$. На первом этапе наблюдения $\{y_t\,|\,t\hm \in T\}$ 
случайным образом разделяются на тренировочную $\{y_t\,|\,t \hm\in T_{\mathrm{train}}\}$ 
и~тестовую $\{y_t\,|\,t \hm\in T_{\mathrm{test}}\}$ части, причем 
\begin{equation*}
T_{\mathrm{train}} \bigcup T_{\mathrm{test}} = T\,, \quad T_{\mathrm{train}} 
\bigcap T_{\mathrm{test}} = \emptyset.
\end{equation*}

На втором этапе без увеличения объема исходных наблюдений с~помощью 
ряда специальных процедур (см.\ статью~\cite{Christ2018}, где 
также приведено описание пакета на языке \verb"Python") 
формируются дополнительные признаки, которые в~дальнейшем 
будут использованы при обучении моделей. Таким образом, 
проводится дополнение исходных признаков~$\{\hat{x}_t\}$ 
наборами функций~$\{f_i(\cdot)\}$ и~$\{g_i(\cdot)\}$ для получения расширенного 
набора~$\{\hat{x}^e_t\}$:
\begin{multline}
\label{eXtend}
\left\{\hat{x}^e_t\right\} =\left\{\hat{x}_t\right\}\bigcup 
\left\{ f_i\left(y_{t-1}, y_{t-2}, \ldots, y_{t-k+1}\right) \right\} \bigcup{}\\
{}\bigcup
\left\{ g_i\left(y_{t+1}, y_{t+2}, \ldots, y_{t+l-1}\right)\right \},
\end{multline}
который и~будет использован для обучения моделей. Отметим, 
что  в~формуле~\eqref{eXtend} величины~$k$ и~$l$ также могут рассматриваться
 в~качестве свободных параметров. Таким образом, для обучения используются 
 расширенные наборы признаков $\{\hat{x}^e_t\,|\, t \hm\in T_{\mathrm{train}}\}$ 
 и~соответствующие им измерения $\{y_t\,|\,t \hm\in T_{\mathrm{train}}\}$, а~в~качестве 
 выходных данных выступают прогнозы $\{y^{\mathrm{pred}}_t\,|\,t \hm\in T_{\mathrm{test}}\}$.

Для оценивания корректности работы методов в~данной статье будут 
использоваться несколько метрик. Для задач классификации используется 
величина \verb"ACC", определяемая выражением:
\begin{equation}
\label{ACC}
\mathrm{ACC= TPR + TNR}\,,
\end{equation}
где \verb"TPR" (\verb"True" \verb"Positive" \verb"Rate")~--- 
доля верно угаданных положительных классов, а \verb"TNR" (\verb"True Negative Rate")~--- 
доля верно угаданных отрицательных классов (подробнее они будут описаны 
в~подразд.~4.1 с~привязкой к~наличию или отсутствию осадков 
в~конкретный день). Также для сравнения бинарных классификаторов используется 
площадь под \verb"ROC"-кри\-вой (\verb"ROC AUC"~---
area under ROC (receiver operating characteristic) curve)~\cite{Huang2005}, которая 
описывает точность решения модели как вероятность принадлежности к~определенному 
классу и~задается выражением:
\begin{equation}
\label{AUC}
\mathrm{ROC\,AUC} = \int\limits_{0}^{1} \mathrm{TPR}\left(\mathrm{FPR}^{-1}(x)\right)\,dx\,,
\end{equation}
где \verb"FPR" (\verb"False Positive Rate")~-- доля неверно угаданных 
положительных классов. Она позволяет корректнее оценивать точность в~случае, 
когда один из классов является доминирующим, что вполне соответствует исходным 
данным об осадках, в~которых достаточно много нулевых значений. Кроме того, 
используется и~стандартная для непрерывных данных метрика \verb"RMSE":
\begin{equation}
\label{RMSE}
\mathrm{RMSE} = \sqrt{|T|^{-1}\sum\limits_{t\in T} \left(y_t - y^{\mathrm{pred}}_t\right)^2}\,.
\end{equation}

\begin{table*}[b]\small %tabl1
\begin{center}
\Caption{\label{Tab1} Сравнение моделей классификации}
\vspace*{2ex}


\begin{tabular}{|l|c|r|l|c|}
\hline
\multicolumn{1}{|c|}{{\bf Город} }& 
\multicolumn{1}{c|}{{\bf Станция}} & 
\multicolumn{1}{c|}{{\bf Лучшая модель}} & 
\multicolumn{1}{c|}{{\bf ACC}}& \multicolumn{1}{c|}{{\bf ROC AUC}} \\
\hline
      Берлин &   \hphantom{9}93850 &   \verb"XGBoost_Logistic" &  $86{,}11\%$ &  $95{,}64\%$ \\
      Берлин &  103810 &  \verb"CatBoost_Logistic" &  $80{,}56\%$ &  $82{,}19\%$ \\
    Доберлуг &   \hphantom{9}94900 &  \verb"CatBoost_Logistic" &  $83{,}33\%$ &  $88{,}1\%$\hphantom{9} \\
    Доберлуг &  104900 &  \verb"CatBoost_Logistic" &  $86{,}11\%$ & $ 84{,}23\%$ \\
    Хольцдорф&  104760 &   \verb"XGBoost_Logistic" &  $61{,}11\%$ &  $72{,}1\%$\hphantom{9} \\
  Линденберг &   \hphantom{9}93930 &  \verb"CatBoost_Logistic" &  $75\%$ &  $76{,}92\%$ \\
  Линденберг &  103930 &  \verb"CatBoost_Logistic" &  $86{,}11\%$ &  $86{,}55\%$ \\
   Нойруппин &   \hphantom{9}92700 &   \verb"XGBoost_Logistic" &  $72{,}22\%$ & $ 75{,}6\%$\hphantom{9} \\
   Нойруппин &  102700 &  \verb"CatBoost_Logistic" &  $69{,}44\%$ &  $66{,}9\%$\hphantom{9} \\
     Потсдам &   \hphantom{9}93790 &   \verb"XGBoost_Logistic" &  $63{,}89\%$ &  $68{,}75\%$ \\
     Потсдам &  103790 &   \verb"XGBoost_Logistic" &  $66{,}7\%$ &  $78{,}41\%$ \\
  Визенбург &  103680 &   \verb"XGBoost_Logistic" & $66{,}7\%$ &  $70{,}63\%$ \\
  Виттенберг &   \hphantom{9}94740 &   \verb"XGBoost_Logistic" &  $77{,}78\%$ &  $74{,}6\%$\hphantom{9} \\
  Виттенберг &  104740 &   \verb"XGBoost_Logistic" &  $69{,}44\%$ &  $73{,}96\%$ \\
\hline
\end{tabular}
\end{center}
\end{table*}

\section{Подготовка данных и~программная реализация методов машинного обучения}

В качестве исходных данных использованы суточные метеорологические наблюдения 
за различные периоды времени, собранные на~14~станциях в~восьми городах  
Германии\footnote{Открытая база {\sf NNDC} (NOAA's National Data Centers) 
{\sf Climate Data} 
({\sf https://www7.ncdc.noaa.gov/CDO/cdo}) 
Национального управления океанических и~атмосферных исследований (NOAA~---
National Oceanic and Atmospheric Administration), США.}. 
Анализ в~настоящей работе проводится для объемов осадков, а такие показатели, 
как средняя температура, точка росы и~средняя скорость ветра, 
включены в~модели в~качестве дополнительных признаков. Предварительно 
проведена простая регуляризация исходных данных~--- отсечение аномальных выбросов, 
связанных с~пропущенными значениями (в~имеющихся в~распоряжении рядах они заполнены 
большими величинами), на уровне~95\% выборочного квантиля. Отметим, что такое 
преобразование не повлекло изменений очевидно корректных значений в~тестовых наборах, 
следовательно, не повлияло на качество работы методов.

Для возможности верификации раз\-ра\-ба\-ты\-ва\-емых методов заполнения пропусков 
использовались интервалы наблюдений, в~которых данные были полны. 
Для искусственного внесения пропусков исходный интервал разбивался 
на непересекающиеся отрезки на некотором расстоянии друг от друга, 
затем в~каждый из них случайным образом помещалось единственное 
пропущенное значение:

\vspace*{-6pt}

\noindent
\begin{multline*}
\ldots
\underbrace{y_{t+1}, \ldots, y_{t+g}}_{\mbox{{\small 0\ пропусков}}},
\underbrace{y_{t+g+1}, \ldots, y_{t+g+s}}_{\mbox{{\small 1\ пропуск}}},\\
\underbrace{y_{t+g+s+1}, \ldots, y_{t+2g+s+1}}_{\mbox{{\small 0\ пропусков}}}
\ldots,
\end{multline*}

\vspace*{-6pt}

\noindent
где $g = \max\{k, l\}$ (величины~$k$ и~$l$ определяются из формулы~\eqref{eXtend}),
а~параметр $s$ задается следующим выражением: 

%\vspace*{-6pt}

\noindent
$$
s = \fr{{\{\mbox{длина\ интервала}\} \hm- g}}{\{\mbox{число\ пропусков}\}}-g\,. 
$$

В~целях повышения качества предсказания рассматриваемых моделей 
из тренировочной и~тес\-то\-вой частей данных были исключены значения, 
для которых нельзя было выбрать в~точности~$k$ предыду\-щих и~$l$ последующих 
значений для корректного извлечения дополнительных признаков (см.\
 разд.~2).
 {\looseness=1
 
 }

Предварительная статистическая обработка, а~так\-же результаты 
статьи~\cite{Gorshenin2018a} демонстрируют, что распределения суточных 
объемов осадков хорошо аппроксимируются с~помощью гам\-ма-рас\-пре\-де\-ле\-ния. 
Этот факт будет использован в~некоторых моделях, в~частности в~\verb"XGBoost_Gamma". 

Программные инструменты анализа данных реализованы на языке \verb"Python" 
с~использованием биб\-лио\-тек \verb"XGBoost" и~\verb"CatBoost" для со\-от\-вет\-ст\-ву\-ющих 
моделей градиентного бустинга, \verb"tsfresh" для извлечения дополнительных 
признаков, а~так\-же классических инструментов \verb"NumPy", \verb"Pandas", 
\verb"SciPy" и~\verb"scikit-learn". Все модели обучались для различных 
конфигураций па\-ра\-мет\-ров~$k$ и~$l$~\eqref{eXtend}, итоговые результаты приведены 
для $k \hm\equiv l \hm \equiv 5$, для которых получена наибольшая точность 
из всех рассмотренных комбинаций. Всего для рассмотренных станций были 
апробированы порядка~8500~различных конфигураций.

\section{Классические модели градиентного бустинга}

В задачах прогнозирования традиционно выделяются два возможных 
направления: определение попадания предсказываемого наблюдения в~некоторую 
группу (классификация) и~установление точной величины неизвестного значения 
(регрессия). В~этом разделе рассмотрим применение алгоритмов \verb"XGBoost" 
и~\verb"CatBoost" для решения каждой из них.

\setcounter{table}{2}
\begin{table*}[b]\small %tabl3
\begin{center}
\Caption{\label{Tab3} Сравнение моделей регрессии}
\vspace*{2ex}

\begin{tabular}{|l|c|l|c|c|}
\hline
\multicolumn{1}{|c|}{{\bf Город}} & 
\multicolumn{1}{c|}{{\bf Станция}} & {\bf Лучшая модель} & 
\multicolumn{1}{c|}{\bf ACC}& {\bf RMSE} \\
\hline
      Берлин &   \hphantom{9}93850 &  \verb"XGBoost_Gamma" &  $83{,}33\%$ &  $0{,}0631$ \\
      Берлин &  103810 &  \verb"XGBoost_Gamma" &  $69{,}44\%$ &  $0{,}1111$ \\
    Доберлуг &   \hphantom{9}94900 &  \verb"XGBoost_Gamma" &  $86{,}11\%$ &  $0{,}075$\hphantom{9} \\
    Доберлуг &  104900 &   \verb"CatBoost_MAE" &  $83{,}33\%$ &  $0{,}0876$ \\
    Хольцдорф&  104760 &  \verb"XGBoost_Gamma" &  $75\%$\hphantom{,99} &  $0{,}1027$ \\
  Линденберг &   \hphantom{9}93930 &  \verb"XGBoost_Gamma" & $ 75\%$\hphantom{,99} &  $0{,}0721$ \\
  Линденберг &  103930 &  \verb"XGBoost_Gamma" &  $80{,}56\%$ &  $0{,}0912$ \\
   Нойруппин &   \hphantom{9}92700 &  \verb"XGBoost_Gamma" &  $66{,}7\%$\hphantom{9} &  $0{,}061$\hphantom{9} \\
   Нойруппин &  102700 &   \verb"CatBoost_MAE" &  $66{,}7\%$\hphantom{9} &  $0{,}0643$ \\
     Потсдам &   \hphantom{9}93790 &  \verb"XGBoost_Gamma" & $66{,}7\%$\hphantom{9} &  $0{,}0910$ \\
     Потсдам &  103790 &  \verb"XGBoost_Gamma" &  $66{,}7\%$\hphantom{9} & $ 0{,}0886$ \\
  Визенбург &  103680 &  \verb"XGBoost_Gamma" & $ 61{,}11\%$ &  $0{,}1767$ \\
  Виттенберг &   \hphantom{9}94740 &   \verb"CatBoost_MAE" &  $72{,}22\%$ &  $0{,}0591$ \\
  Виттенберг &  104740 &   \verb"XGBoost_RMSE" &  $77{,}78\%$ &  $0{,}0734$ \\
\hline
\end{tabular}
\end{center}
\end{table*}

\subsection{Классификация}
%\label{SecClass}

Поставим в~соответствие измерениям осадков~$\{y_t\}$ последовательность 
$\{ c_t \hm=\mathcal {I} (y_t\hm\geqslant \varepsilon)\}$ 
для\linebreak
 некоторого наперед заданного значения~$\varepsilon$ (например,
 $\varepsilon \hm= 0{,}005$), где~$\mathcal {I}(\cdot)$ обозначает индикатор 
 соответствующего множества. Так формируется разби\-ение исходных данных на 
 два класса~--- наличие и~отсутствие осадков. Первый класс $\{ c_t \hm= 1\}$ 
 будем называть положительным, а второй $\{c_t \hm= 0\}$~--- отрицательным. 
 Модель классификации на вход получает значения~$\hat{x}^e_t$, а~в~качестве 
 результата выдает вероятность принадлежности наблюдений к~положительному 
 классу $\{p^{\mathrm{pred}}_t \hm\equiv \mathbb P(c_t \hm= 1)\}$. 
 Результирующий класс определяется как $\{ c^{\mathrm{pred}}_t \hm=\mathcal {I}
  (p^{\mathrm{pred}}_t
 \hm\geqslant 0{,}5)\}$.
 
 
% \begin{table*}
  %tabl2
\noindent
{{\tablename~2}\ \ \small{Усредненные результаты предсказания моделей классификации}}
%\label{Tab2}}
%\vspace*{2ex}

{\small
\begin{center}
\tabcolsep=13pt
\begin{tabular}{|r|c|c|}
\hline
& \multicolumn{2}{c|}{\bf Метрика}\\
\cline{2-3} %\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{
\multicolumn{1}{|c|}{{\raisebox{6pt}[0pt][0pt]{{\bf Модель}}}} &
{\bf ACC}&   {\bf ROC AUC}\\
\hline
\verb"XGBoost_Logistic"  &  $72{,}22\%$ &  $78{,}26\%$ \\
\verb"CatBoost_Logistic" &  $71{,}03\%$ &  $75{,}96\%$ \\
\hline
\end{tabular}
\end{center}
}
%\end{table*}

\addtocounter{table}{1}

\vspace*{12pt}

Для решения задачи классификации были выбраны модели градиентного 
бустинга \verb"XGBoost_Logistic" и~\verb"CatBoost_Logistic" с~целевой функцией 
логистической регрессии~--- для них требуется меньший объем данных для обучения. 
В~табл.~\ref{Tab1} приведены результаты для лучшей из рассмотренных конфигураций 
по каждой станции в~метриках \verb"ACC"~\eqref{ACC} и~\verb"ROC AUC"~\eqref{AUC}. 

В табл.~2 приведены усредненные сразу по всем вариантам 
параметров значения точности для каж\-дой модели.

В данной задаче модель \verb"XGBoost_Logistic" продемонстрировала в~среднем 
несколько более высокие (порядка $1\%\mbox{--}3\%$) показатели точ\-ности по сравнению
 с~\verb"CatBoost_Logistic". Однако, как видно из табл.~\ref{Tab1}, для шести 
 из~$14$  станций более успешным оказалось применение категориального бустинга.

\subsection{Регрессия}

В данном случае в~качестве входных параметров используются величины~$\{\hat{x}^e_t\}$, 
а~на выходе формируется набор~$\{y^{\mathrm{pred}}_t\}$ (см.\ разд.~2). 
При этом результирующий класс имеет следующий вид: $\{ c^{\mathrm{pred}}_t \hm=
\mathcal {I} (y^{\mathrm{pred}}_t\hm\geqslant \varepsilon)\}$. 
В~качестве моделей регрессии используются \verb"XGBoost" и~\verb"CatBoost" 
с~различными целевыми функциями. В~\verb"XGBoost_RMSE" и~\verb"CatBoost_RMSE" 
используется \verb"RMSE"~\eqref{RMSE}, \verb"CatBoost_MAE"~--- 
средняя абсолютная ошибка (\verb"MAE"), а~в~\verb"XGBoost_Gamma"~--- 
гам\-ма-ре\-грес\-сия с~логарифмической связью. 
В~табл.~\ref{Tab3} приведены результаты лучшей модели для каждой станции 
в~метриках \verb"ACC"~\eqref{ACC} и~\verb"RMSE"~\eqref{RMSE}.

\setcounter{table}{3}
\begin{table*}\small %tabl4
\begin{center}
\Caption{\label{Tab4}  Сравнение результатов гибридных моделей}
\vspace*{2ex}

\begin{tabular}{|l|c|l|c|c|}
\hline
\multicolumn{1}{|c|}{{\bf Город}} & 
\multicolumn{1}{c|}{{\bf Станция}} & 
\multicolumn{1}{c|}{{\bf Лучшая модель}} & 
{\bf ACC}& {\bf RMSE} \\
\hline
      Берлин &   \hphantom{9}93850 &  \verb"XGBoost_Logistic+RMSE+Sigmoid" &  $91{,}67\%$ &  $0{,}0588$ \\
      Берлин &  103810 &          \verb"XGBoost_Logistic+RMSE" & $ 75\%$\hphantom{,99} &  $0{,}1012$ \\
   Нойруппин &   \hphantom{9}92700 &          \verb"XGBoost_Logistic+RMSE" &  $72{,}22 \%$& $ 0{,}0556$ \\
   Нойруппин &  102700 &  \verb"XGBoost_Logistic+RMSE+Sigmoid" &  $69{,}44 \%$&  $0{,}0593$ \\
     Потсдам &   \hphantom{9}93790 &  \verb"XGBoost_Logistic+RMSE+Sigmoid" &  $69{,}44\%$ &  $0{,}0697$ \\
     Потсдам &  103790 &  \verb"XGBoost_Logistic+RMSE+Sigmoid" & $69{,}44\%$ &  $0{,}0777$ \\
  Визенбург &  103680 &          \verb"XGBoost_Logistic+RMSE" &  $66{,}7\%$\hphantom{9} &  $0{,}1418$ \\
  Виттенберг &   \hphantom{9}94740 &  \verb"XGBoost_Logistic+RMSE+Sigmoid" &  $80{,}56\%$ &  $0{,}052$\hphantom{9} \\
\hline
\end{tabular}
\end{center}
\end{table*}


\section{Гибридные модели}

Сравнение результатов точности описанных в~предыдущем разделе моделей показывает, 
что модели регрессии гораздо хуже справляются с~задачей классификации. 
В~этом разделе предложены гиб\-рид\-ные модели, которые сочетают преимущества 
обоих подходов.

\begin{description}
\item[XGBoost\_Logistic+Mean.] Простую комбинированную модель можно получить, 
совместив выход классификатора~$\{c^{\mathrm{cls}}_t\}$ и~среднее значение объемов 
осадков $m \hm= |T|^{-1}\sum\nolimits_{t \in T} y_t$. Тогда выход 
комбинированной модели определим как $\{y^{\mathrm{pred}}_t \hm= c^{\mathrm{cls}}_t m\}$.
\item[XGBoost\_Logistic+RMSE.] Здесь вместо среднего значения~$m$ 
будет использоваться выход прос\-то\-го регрессора~$\{y^{\mathrm{reg}}_t\}$. Тогда выход 
новой модели определим как $\{y^{\mathrm{pred}}_t \hm= 
c^{\mathrm{cls}}_t y^{\mathrm{reg}}_t\}$.
\item[XGBoost\_Logistic+RMSE+Sigmoid.] Пусть теперь выход 
классификатора~$\{p^{\mathrm{cls}}_t\}$ 
определяет вероятность принадлежности к~положительному классу. Введем функцию 
связи вида 
$$
s(x) = \left(1 \hm+ e^{-\alpha (x \hm- \beta)}\right)^{-1},
$$ 
где~$\alpha$ и~$\beta$~--- некоторые заданные действительные коэффициенты. 
Она позволяет использовать вместо бинарного решения~$\{c^{\mathrm{cls}}_t\}$ набор 
непрерывных значений, а~кроме того, предоставляет возможность гибкого 
подбора коэффициентов, наиболее подходящих для конкретных данных. Выход 
такой модели определим как $\{y^{\mathrm{pred}}_t \hm= s\left(p^{\mathrm{cls}}_t\right) 
 y^{\mathrm{reg}}_t\}$.  Отметим, что для рассматриваемых в~статье данных 
наилучшие результаты были получены для конфигураций 
с~$\alpha \hm= 10$ и~$\beta\hm = 0{,}45$.
\end{description}

В табл.~\ref{Tab4} приведены станции, для которых были улучшены 
результаты чисто регрессионных моделей (см.\ табл.~\ref{Tab3}). 
Отметим, что этого удалось достичь для~8 из~14~станций, причем увеличение 
точ\-ности классификации составило от~2\% до~8\% в~зависимости от местоположения. 
Таким образом, использование гибридных моделей оказывается вполне оправданным 
с~точки зрения более точной обработки данных и~корректного заполнения пропусков в~них.



В табл.~5 приведены усредненные сразу по всем вариантам 
параметров значения точности как для чисто регрессионных, так и~для гибридных 
моделей в~порядке возрастания величины \verb"ACC"~\eqref{ACC}. 
Кроме того, для сравнения представлена простая модель заполнения 
средним значением, обозначенная как \verb"Mean".



Отметим, что и~величины ошибок \verb"RMSE" для непрерывных 
значений также остаются весьма умеренными.

\vspace*{6pt}

%\begin{table*}
 %tabl5

\noindent
{{\tablename~5}\ \ \small{Усредненные результаты точности моделей 
регрессии, включая гибридные}}
%\vspace*{2ex}

{\small
\begin{center}
%\tabcolsep=5.5pt
\begin{tabular}{|l|c|c|}
\hline
& \multicolumn{2}{c|}{\bf Метрика}\\
\cline{2-3}
\multicolumn{1}{|c|}{{\raisebox{6pt}[0pt][0pt]{{\bf Модель}}}} 
 & {\bf ACC}& {\bf RMSE}\\
\hline
\verb"Mean"  &  $45{,}24\%$ &  $0{,}0801$ \\
\verb"CatBoost_RMSE"                 &  $47{,}02\%$ &  $0{,}0756$ \\
\verb"XGBoost_RMSE"                  &  $58{,}73\%$ &  $0{,}0759$ \\
\verb"CatBoost_MAE"                  &  $60{,}91\%$ &  $0{,}0804$ \\
\verb"XGBoost_Gamma"                 &  $70{,}83\%$ &  $0{,}0877$ \\
\verb"XGBoost_Logistic+Mean"         &  $72{,}22\%$ &  $0{,}0808$ \\
\verb"XGBoost_Logistic+RMSE"         &  $72{,}42\%$ &  $0{,}0787$ \\
\verb"XGBoost_Logistic+RMSE+Sigmoid" &  $74{,}4\%$\hphantom{9} &  $0{,}0763$ \\
\hline
\end{tabular}
\end{center}
}
%\end{table*}

\section{Заключение}

В работе продемонстрирована возможность высокоточного заполнения 
пропусков в~данных с~использованием гибридных моделей градиентного\linebreak бустинга. 
Достигнутая точность в~задачах классификации составила до~92\% 
при весьма умеренных значениях ошибок прогнозов в~метрике \verb"RMSE". 
Гибридные методы превзошли по качеству предсказания как простые модели 
классификации, так и~регрессии.

Было проведено сравнение методов библиотек \verb"XGBoost" и~\verb"CatBoost", 
которое показало, что в~большинстве случаев для работы с~осадками более 
перспективным представляется использование экс\-тремального градиентного бустинга, однако для отдельных рядов результаты могут быть улучшены за счет применения категориальных моделей. Развиваемые подходы могут быть успешно использованы как для непосредственного анализа метеорологических данных методами машинного обучения, так и~для улучшения качества предсказания на основе физических моделей атмосферных процессов.

\bigskip
Авторы выражают особую признательность профессору В.\,Ю.~Королеву 
за полезные обсуждения в~рамках совместных исследований 
метеорологических явлений.


\vspace*{-6pt}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
 
\vspace*{-3pt}
 
\bibitem{Gorshenin2018c} 
\Au{Горшенин~А.\,К., Королев~В.\,Ю.} 
Определение экстремальности объемов осадков на основе модифицированного 
метода превышения порогового значения~// Информатика и~её применения, 2018. Т.~12. 
Вып.~4. C.~16--24.

\bibitem{Friedman2001}
\Au{Friedman~J.\,H.} 
Greedy function approximation: A~gradient boosting machine~// Ann. Stat., 
2001. Vol.~29. Iss.~5. P.~1189--1232.

\bibitem{Chen2016} 
\Au{Chen~T., Guestrin~C.} XGBoost: A~scalable tree boosting system~// 
22nd ACM SIGKDD  Conference (International) on Knowledge Discovery and Data Mining
Proceedings.~--- San Francisco, CA, USA, 2016. P.~785--794.

\bibitem{Mustapha2016}
\Au{Mustapha~I.\,B., Saeed~F.} 
Bioactive molecule prediction using extreme gradient boosting~// 
Molecules, 2016. Vol.~21. Iss.~8. Art.~No.\,983.

\bibitem{Xia2017}
\Au{Xia~Y., Liu~C., Li~Y., Liu~N.} 
A~boosted decision tree approach using Bayesian hyper-parameter optimization 
for credit scoring~// Expert Syst. Appl., 2017. Vol.~78. P.~225--241.

\bibitem{Chatzis2018}
\Au{Chatzis~S.\,P., Siakoulis~V., Petropoulos~A., Stavroulakis~E., Vlachogiannakis~N.}
 Forecasting stock market crisis events using deep and statistical machine 
 learning techniques~// Expert Syst. Appl., 2018. Vol.~112. P.~353--371.

\bibitem{Zhang2018}
\Au{Zhang~D., Qian~L., Mao~B., Huang~C., Huang~B., Si~Y.}
 A~data-driven design for fault detection of wind turbines using random
forests and XGboost~// IEEE Access, 2018. Vol.~6. P.~21020-21031.

\bibitem{Aler2017} 
\Au{Aler~R., Galvan~I.\,M., Ruiz-Arias~J.\,A., Gueymard~C.\,A.} 
Improving the separation of direct and diffuse solar radiation 
components using machine learning by gradient boosting~// Sol. Energy, 2017. 
Vol.~150. P.~558--569.

\bibitem{Torres-Barran2018} 
\Au{Torres-Barran~A., Alonso~A., Dorronsoro~J.\,R.}
 Regression tree ensembles for wind energy and solar radiation prediction~// 
 Neurocomputing, 2018. Vol.~326. P.~151--160.

\bibitem{Prokhorenkova2018}
\Au{Prokhorenkova~L., Gusev~G., Vorobev~A., Dorogush~A.\,V., Gulin~A.} 
CatBoost: Unbiased boosting with categorical features~// 
Adv. Neur. In., 2018. Vol.~31. P.~6638--6648.

\bibitem{Ivanov2019}
\Au{Ivanov~M.\,V., Levitsky~L.\,I., Bubis~J.\,A., Gorshkov~M.\,V.} 
Scavager: A~versatile postsearch validation algorithm for shotgun proteomics 
based on gradient boosting~// Proteomics, 2019. Vol.~19. Iss.~3. Art.~No.\,1800280.

\bibitem{Punmiya2019}
\Au{Punmiya~R., Choe~S.} Energy theft detection using gradient boosting 
theft detector with feature boost
engineering-based preprocessing~// IEEE T.~Smart Grid, 2019. Vol.~10. 
Iss.~2. P.~2326--2329.

\bibitem{Korner2018}
\Au{Korner~P., Kronenberg~R., Genzel~S., Bernhofer~C.} 
Introducing Gradient Boosting as a~universal gap filling tool 
for meteorological time series~// Meteorol.~Z., 2018. Vol.~27. 
Iss.~5. P.~369--376.

\bibitem{Fan2018}
\Au{Fan~J., Wang~X., Wu~L., Zhou~H., Zhang~F., Yu~X., Lu~X., Xiang~Y.} 
Comparison of Support Vector Machine and Extreme Gradient Boosting for 
predicting daily global solar radiation using temperature and precipitation 
in humid subtropical climates: A case study in China~// 
Energ. Convers. Manage., 2018. Vol.~164. P.~102--111.

\bibitem{Christ2018} 
\Au{Christ~M., Braun~N., Neuffer~J., Kempa-Liehr~A.\,W.} 
Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests 
(tsfresh~--- a~Python package)~// Neurocomputing, 2018. Vol.~307. P.~72--77.

\bibitem{Huang2005} 
\Au{Huang~J., Ling~C.\,X.} Using AUC and accuracy in evaluating learning algorithms~// 
IEEE T.~Knowl. Data En., 2005. Vol.~17. Iss.~3. P.~299--310.
  
\bibitem{Gorshenin2018a}
\Au{Gorshenin~A.\,K., Korolev~V.\,Yu.} Scale mixtures of
Frechet distributions as asymptotic approximations of extreme precipitation~// 
J.~Math. Sci., 2018. Vol.~234. Iss.~6. P.~886--903.

 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 08.07.19}}

\vspace*{8pt}

%\pagebreak

%\newpage

%\vspace*{-28pt}

\hrule

\vspace*{2pt}

\hrule

%\vspace*{-2pt}

\def\tit{HYBRID EXTREME GRADIENT BOOSTING MODELS TO~IMPUTE THE~MISSING DATA 
IN~PRECIPITATION RECORDS}


\def\titkol{Hybrid extreme gradient boosting models to~impute the~missing data 
in~precipitation records}

\def\aut{A.\,K.~Gorshenin$^{1,2}$ and~O.\,P.~Martynov$^2$}

\def\autkol{A.\,K.~Gorshenin and~O.\,P.~Martynov}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-11pt}


\noindent
$^1$Institute of Informatics Problems, Federal Research Center ``Computer Science and
Control'' of the Russian\linebreak
$\hphantom{^1}$Academy of Sciences, 44-2~Vavilov Str., Moscow 119333, Russian
Federation

\noindent
$^2$Faculty of Computational Mathematics and Cybernetics, M.\,V.~Lomonosov Moscow
State University, GSP-1,\linebreak
$\hphantom{^1}$Leninskie Gory, Moscow, 119991, Russian Federation


\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2019\ \ \ volume~13\ \ \ issue\ 3}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2019\ \ \ volume~13\ \ \ issue\ 3
\hfill \textbf{\thepage}}}

\vspace*{3pt} 



\Abste{The article compares the classical method of extreme gradient boosting 
implemented in the {\sf XGBoost} (eXtreme Gradient Boosting) framework with the new modification 
{\sf CatBoost} (Categorial Boosting),
 which is rarely involved in scientific researches. Some hybrid 
 classification-regression models are proposed to improve the accuracy 
 of imputation in missing values in real data using~14~meteorological stations 
 in Germany. The achieved accuracy of the classification is up to~92\% 
 and the root-mean-square errors are quite moderate. The hybrid methods outperformed 
 both simple classification and regression models in prediction accuracy. 
 The proposed approaches can be successfully used for meteorological data 
 analysis by machine learning methods as well as for improving the forecasting 
 accuracy in physical models of atmospheric processes.}


\KWE{data imputation; precipitation; classification; regression; gradient boosting; 
XGBoost; CatBoost}



\DOI{10.14357/19922264190306} 

\vspace*{-14pt}

\Ack
\noindent
The problems formulation and analysis of results through the paper were 
performed by A.\,K.~Gorshenin whose research was supported by the 
Russian Science Foundation (project 18-71-00156). 
Machine learning algorithms for imputation of missing values were 
implemented by BSc student O.\,P.~Martynov.


%\vspace*{-6pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{1-gm}
\Aue{Gorshenin,~A.\,K., and V.\,Yu.~Korolev.}
 2018. {Opredelenie ekstremal'nosti ob''emov osadkov na osnove 
 mo\-di\-fi\-tsi\-ro\-van\-no\-go metoda prevysheniya porogovogo znacheniya} 
 [Determining the extremes of precipitation volumes  based on a modified ``Peaks
  over Threshold'']. \textit{Informatika i~ee  Primeneniya~--- Inform. Appl.} 
  12(4):16--24. 

\bibitem{2-gm}
\Aue{Friedman,~J.\,H.} 2001.
 Greedy function approximation: A~gradient boosting machine. 
 \textit{Ann. Stat.} 29(5):1189--1232.

\bibitem{3-gm}
\Aue{Chen,~T., and C.~Guestrin.} 2016. XGBoost: A~scalable tree boosting system. 
\textit{22nd ACM SIGKDD  Conference (International) on Knowledge Discovery 
and Data Mining Proceedings}. San Francisco, CA. 785--794.

\bibitem{4-gm}
\Aue{Mustapha,~I.\,B., and F.~Saeed.} 2016. 
Bioactive molecule prediction using extreme gradient boosting. 
\textit{Molecules} 21(8):983.

\bibitem{5-gm}
\Aue{Xia,~Y., C.~Liu, Y.~Li, and N.~Liu.} 
2017. A~boosted decision tree approach using Bayesian hyper-parameter optimization 
for credit scoring. \textit{Expert Syst. Appl.} 78:225--241.

\bibitem{6-gm}
\Aue{Chatzis,~S.\,P., V.~Siakoulis, A.~Petropoulos, E.~Stav\-rou\-lakis, 
and N.~Vlachogiannakis.} 2018. Forecasting stock market crisis events 
using deep and statistical machine learning techniques. 
\textit{Expert Syst. Appl.} 112:353--371.

\bibitem{7-gm}
\Aue{Zhang,~D., L.~Qian, B.~Mao, C.~Huang, B.~Huang, and Y.~Si.}
 2018. A data-driven design for fault detection of wind turbines 
 using random forests and XGboost. 
 \textit{IEEE Access} 6:21020--21031.

\bibitem{8-gm}
\Aue{Aler,~R., I.\,M.~Galvan, J.\,A.~Ruiz-Arias, and C.\,A.~Gueymard.}
 2017. Improving the separation of direct and diffuse solar radiation 
 components using machine learning by gradient boosting. 
 \textit{Sol. Energy} 150:558--569.

\bibitem{9-gm}
\Aue{Torres-Barran,~A., A.~Alonso, and J.\,R.~Dorronsoro.} 
2018. Regression tree ensembles for wind energy and solar radiation prediction. 
\textit{Neurocomputing} 326:151--160.

\bibitem{10-gm}
\Aue{Prokhorenkova,~L., G.~Gusev, A.~Vorobev, A.\,V.~Dorogush, and A.~Gulin.}
 2018. CatBoost: Unbiased boosting with categorical features. 
 \textit{Adv. Neur. In.} 31:6638--6648.

\bibitem{11-gm}
\Aue{Ivanov,~M.\,V., L.\,I.~Levitsky, J.\,A.~Bubis, and M.\,V.~Gorshkov.}
 2019. Scavager: A~versatile postsearch validation algorithm for shotgun 
 proteomics based on gradient boosting. \textit{Proteomics} 19(3):1800280.

\bibitem{12-gm}
\Aue{Punmiya,~R., and S.~Choe.} 2019. 
Energy theft detection using gradient boosting theft detector with feature 
boost engineering-based preprocessing. 
\textit{IEEE T.~Smart Grid} 10(2):2326--2329.

\bibitem{13-gm}
\Aue{Korner,~P., R.~Kronenberg, S.~Genzel, and C.~Bernhofer.}
 2018. Introducing Gradient Boosting as a~universal gap filling tool 
 for meteorological time series. \textit{Meteorol.~Z.} 27(5):369--376.

\bibitem{14-gm}
\Aue{Fan,~J., X.~Wang, L.~Wu, H.~Zhou, F.~Zhang, X.~Yu, X.~Lu, and Y.~Xiang.}
 2018. Comparison of Support Vector Machine and Extreme Gradient Boosting
  for predicting daily global solar radiation using temperature and 
  precipitation in humid subtropical climates: A~case study in China. 
\textit{Energ. Convers.  Manage.} 164:102--111.

\bibitem{15-gm}
\Aue{Christ,~M., N.~Braun, J.~Neuffer, and A.\,W.~Kempa-Liehr.}
 2018. Time Series FeatuRe Extraction on basis of Scalable Hypothesis 
 tests (tsfresh~--- a~Python package). \textit{Neurocomputing} 307:72--77.
  
\bibitem{16-gm}
\Aue{Huang,~J., and C.\,X.~Ling.} 2005. 
Using AUC and accuracy in evaluating learning algorithms. 
\textit{IEEE T.~Knowl. Data En.} 17(3):299--310.

\bibitem{17-gm}
\Aue{Gorshenin,~A.\,K., and V.\,Yu.~Korolev.}
 2018. Scale mixtures of Frechet distributions as asymptotic approximations 
 of extreme precipitation. \textit{J.~Math. Sci.} 234(6):886--903.
\end{thebibliography}

 }
 }

\end{multicols}

%\vspace*{-7pt}

\hfill{\small\textit{Received July 8, 2019}}

%\pagebreak

%\vspace*{-22pt}

\Contr


\noindent
\textbf{Gorshenin Andrey K.} (b.\ 1986)~--- 
Candidate of Science (PhD) in physics and
mathematics, associate professor, leading scientist, Institute of Informatics Problems,
Federal Research Center ``Computer Science and Control'' of the Russian Academy of
Sciences, 44-2~Vavilova Str., Moscow 119333, Russian Federation;  leading scientist, Faculty
of Computational Mathematics and Cybernetics, Lomonosov Moscow State 
University, GSP-1,
Leninskie Gory, Moscow, 119991, Russian Federation; \mbox{agorshenin@frccsc.ru}

\vspace*{3pt}

\noindent
\textbf{Martynov Oleg P.} (b.\ 1996)~--- BSc student,  Faculty
of Computational Mathematics and Cybernetics, 
M.\,V.~Lomonosov Moscow State University, GSP-1,
Leninskie Gory, Moscow, 119991, Russian Federation; 
\mbox{martynov.oleg.mipt@gmail.com}
\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература}  
\def\stat{kuzn}

\def\tit{ОЦЕНКА СЕМАНТИЧЕСКОЙ АДЕКВАТНОСТИ ТЕКСТОВ ИНФОРМАЦИОННЫМ МЕТОДОМ}

\def\titkol{Оценка семантической адекватности текстов информационным методом}

\def\autkol{Л.\,А.~Кузнецов, В.\,Ф.~Кузнецова}

\def\aut{Л.\,А.~Кузнецов$^1$, В.\,Ф.~Кузнецова$^2$}

\titel{\tit}{\aut}{\autkol}{\titkol}

%{\renewcommand{\thefootnote}{\fnsymbol{footnote}}\footnotetext[1]
%{Работа поддержана
%Российским фондом фундаментальных исследований (проекты
%11-01-00515а, 11-07-00112а, 11-01-12026-офи-м и 12-07-00115а), Министерством
%образования и науки (госконтракт 16.740.11.0133).}}

\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Липецкий государственный технический университет, kuznetsov.leonid48@gmail.com}
\footnotetext[2]{Липецкий государственный технический университет, kuznetsov@stu.lipetsk.ru}


\Abst{Рассматривается проблема автоматизации проверки знаний обучаемых сравнением ответов 
учащихся с эталонными ответами, хранящимися в базе данных. Для оценки степени соответствия 
ответа эталону разрабатывается оригинальная методология, опирающаяся на теорию информации. 
Приведены некоторые результаты, иллюстрирующие применение методологии для 
автоматической оценки близости англоязычных текстов~--- изложений, написанных студентами.}

\KW{ семантическое подобие текстов; вероятностная модель текста; теория информации; 
энтропия; взаимная информация текстов; автоматизация оценки знаний}

\vskip 14pt plus 9pt minus 6pt

      \thispagestyle{headings}

      \begin{multicols}{2}

            \label{st\stat}


\section{Введение}

Актуальной задачей внедрения современных информационных технологий в сферу 
образования является разработка средств автоматизации оценки знаний обучаемых. 
Разработка инструментов автоматической оценки уровня знаний позволила бы 
решить многочисленные и хорошо известные проблемы аттестации обучаемых на 
всех этапах изменения их статуса. 
     
     Базой для разработки таких инструментов являются методы интеллектуальной 
обработки информации, содержащейся в информационных объектах, 
представленных на естественном языке. Эти методы позволяют автоматически 
оценивать семантическую близость информационных объектов. В~настоящее время 
в ин\-фор\-ма\-ци\-он\-но-по\-иско\-вых системах при классификации текстов, при 
проверке текстов на плагиат~[1, 2] применяются статистические подходы на основе 
век\-тор\-но-про\-стран\-ст\-вен\-ной модели текста, предложенной Г.~Солтоном 
с соавторами в 
1975~г.~[3]. В~ней текст представляется вектором частот входящих в него слов, а 
оценка близости текстов равна косинусу угла между векторами текстов. 
     
     Такой подход не позволяет использовать для оценки степени близости 
содержательные морфологические и синтаксические характеристики естественного 
языка, посредством которых и отражается семантика объектов. В~нем сравнение 
базируется на количественной оценке частот слов в текстах и поэтому фактическая 
близость текстов устанавливается последующим субъективным анализом. На его 
основе не представляется возможным автоматизировать процесс оценки степени 
семантического подобия информационных объектов (текстов), в частности он не 
может быть применен при оценке знаний.
     
     В данной работе рассматривается задача оценки семантической близости 
текстов, которая при\linebreak положительном решении могла бы использоваться в качестве 
основы систем автоматизированной проверки уровня знаний, усвоенных 
обучаемыми.\linebreak Сейчас распространена система проверки знаний на основе тестов, что 
объясняется простотой ее автоматизации. Пятьдесят лет назад тестирующие\linebreak 
установки такого типа изготовлялись на релейных схемах, а в настоящее время та 
же примитивная идеология перенесена на компьютеры. Очевидно,\linebreak что 
традиционный письменный экзамен, при котором обучающийся дает полноценный 
ответ, обеспечивает более качественную проверку уровня\linebreak знаний по сравнению с 
тестовой системой, но в настоящее время он не применяется, например\linebreak при 
проведении ЕГЭ, из-за значительных трудо\-затрат квалифицированных 
проверяющих для\linebreak <<ручной>> проверки. 
     
      Автоматизация приема полноценных ответов обучаемых упирается в 
проблему разработки методологии автоматической оценки семантической близости 
информационных объектов, представленных на естественном языке. Настоящая 
статья посвящена оригинальной методологии~[4] автоматической оценки 
семантической близости текстов, опирающейся на представление текста в виде 
     ве\-ро\-ят\-но\-ст\-но-ста\-ти\-сти\-че\-ской модели, в рамках которой на основе 
морфологической и синтаксической детализации текстов для отражения 
содержащейся в них информации вводится система семантических компонентов. 
     
     Ниже иллюстрируется методология формализации принципиальной базовой 
идеи сопоставления семантического содержания информационных объектов, 
представленных на естественном языке. Максимально простая трактовка ее 
содержательного существа состоит в следующем. При субъективной 
(интеллектуальной) оценке подобия содержания текстов не считаются слова и 
вообще не обращается внимание на конкретные слова, а интуитивно сопоставляется: 
о ком (чем) идет речь, что он делает (с ним делается), когда, где, как, при каких 
условиях и~т.\,п. Ответ на любой из подобных вопросов, извлекаемый из текста, 
назовем для определенности семантическим компонентом. При субъективном 
анализе фактически со\-по\-став\-ля\-ют\-ся семантические компоненты текста, которые 
представляются в нем случайными наборами слов. Предлагаемая методология 
автоматической оценки адекватности текстов базируется на формальном 
представлении такой же схемы.
     
     Принципиальным является вероятностный подход к наполнению 
сопоставляемых семантических компонентов текста, отражающих: кто, что, где, 
когда, как и~т.\,д., которые могут быть пред\-став\-ле\-ны случайными наборами 
конструкций языка. В~большинстве структурированных языков могут быть введены 
формальные правила соотнесения\linebreak семантических компонентов текста с 
морфоло\-гическими (существительными, глаголами, при\-ла\-гательными и~т.\,д.) и 
синтаксическими (под\-ле\-жа\-щими, сказуемыми, обстоятельствами и~т.\,д.)\linebreak 
компонентами языка. Правила морфологической и синтаксической структуризации 
текстов, т.\,е.\ отнесения слов к конкретным морфологическим и синтаксическим 
компонентам, представлены в грамматиках языков, которые во многих случаях уже 
имеют автоматизированные версии.
     
     Предлагаемая методология базируется на формировании множества 
семантических компонентов на основании одного из текстов, принимаемого\linebreak 
(условно) за эталонный. При формализации семантические компоненты 
классифицируются случайными событиями. Содержание семантических 
компонентов формируется из отдельных элементов\linebreak исследуемого текста~--- слов 
или составных конструкций, которые трактуются элементарными\linebreak случайными 
событиями (исходами). Система семантических компонентов (множество случайных 
событий) определяет семантическую структуру текс\-та (алгебру вероятностной 
модели). Множество всех слов (исходов) текста, алгебра и вероятности случайных 
событий образуют вероятностную модель информационного объекта, 
представленного на естественном языке. Для конкретного текста модель отражает 
его ве\-ро\-ят\-ност\-но-ста\-ти\-сти\-че\-ский образ. Другие сравниваемые тексты 
(копии) структурируются по введенной системе семантических компонентов 
(алгебре) и определяются их ве\-ро\-ят\-ност\-но-ста\-ти\-сти\-че\-ские образы. 
Количество информации в вероятностных образах объектов оценивается энтропией, 
а количество совместной (совпада\-ющей) информации в образах двух объектов, 
оценивается взаимной информацией. Методология базируется на сопоставлении 
содержания семантических компонентов текстов, которые представляются 
случайными событиями, и поэтому степень их подобия (количество взаимной 
информации) является вероятностной мерой, изме\-ня\-ющей\-ся от нуля при полном 
несовпадении текстов до количества информации в каждом из них при полном 
совпадении. 
     
     Представление сравниваемых текстов в виде 
     ве\-ро\-ят\-ност\-но-ста\-ти\-сти\-че\-ских образов~[4] позволяет по единой 
шкале оценить количество информации в текстах, количество общей информации в 
текстах и количество информации, отличающей тексты, т.\,е.\ содержащейся в 
одном и не содержащейся в другом. Количественная мера информации, 
содержащейся в объектах, и степени ее подобия вводятся с использованием 
аппарата теории информации. 
     
     При использовании этой методологии для оценки знаний абстрактное 
количество взаимной информации требуется проградуировать в баллах по принятой 
$N$-балль\-ной шкале оценки знаний. Техника градуировки кратко поясняется ниже 
на примере 100-балль\-ной шкалы, используемой в ЛГТУ. 
     
     В примерах, иллюстрирующих адекватность меры количества взаимной 
информации для оценки уровня подобия объектов и некоторые возможности ее 
адаптации к особенностям конкретных информационных объектов, используется 
англоязычный материал. В~качестве фактической эмпирической основы для 
исследования используются результаты изложения, написанного студентами на 
английском языке. Изложения были проверены и оценены преподавателем 
английского языка. Тексты изложений и эталон, прочитанный студентам 
преподавателем, были введены в систему автоматической оценки. Методика 
фор\-маль\-но-ма\-те\-ма\-ти\-че\-ской оценки изложений и полученные результаты 
представлены в статье.
     
\section{Формирование вероятностно-статистической модели текста}
     
     В теории вероятностей~[5] вводится вероятностная модель, в рамках которой 
отражается вся имеющаяся информация об объекте и предоставляется возможность 
структуризации этой информации для отражения и исследования его 
содержательных особенностей, представляющих интерес. Наиболее полная 
информация о случайном объекте пред\-став\-ля\-ет\-ся распределением вероятностей, 
т.\,е.\ множеством элементарных исходов $\Omega\hm=\{\omega_1, \omega_2, \ldots , 
\omega_n\}$ случайной величины и их вероятностями $p(\omega_i)$. 
Содержательное существо объекта может быть введено в модель алгеброй, т.\,е.\ 
структуризацией множества реализаций~$\Omega$ в виде системы случайных 
событий $A_j\hm\in \Omega$, отражающих состояние или поведение вероятностного 
объекта в представляющем интерес смысле. 
     
     Вероятностная модель в общем случае пред\-став\-ля\-ет\-ся в виде:
     \begin{equation}
     M_\omega =\left\{ \Omega, \aleph, P(A_j)\right\}\,,
     \label{e1-kuz}
     \end{equation}
     где $\Omega\hm=\{\omega_1, \omega_2, \ldots , \omega_n\}$~--- пространство 
элементарных исходов;
     $\aleph \hm= \left\{ A_1, A_2, \ldots , A_j,\ \varnothing, \Omega\right\}$~--- 
алгебра событий~$A_j$, составленных с помощью операций логического сложения, 
умножения и отрицания из\linebreak элементарных событий~$\omega_i$ и дополненная 
невозможным $\varnothing$ и достоверным $\Omega$ событиями;
     $P(A_j)$~--- вероятности событий, составляющих алгебру,\linebreak которые 
рассчитываются по вероятностям элементарных исходов $p(\omega_i)$, $i\hm=1, 2, 
\ldots , n_\omega$, со\-став\-ля\-ющих это событие: 
     \begin{equation}
     P(A_j) =\sum\limits_{\omega\in A_j} p(\omega_i)\,.
     \label{e2-kuz}
     \end{equation}
     
     Общая вероятностная модель случайного объекта трансформируется в 
     ве\-ро\-ят\-ност\-но-ста\-ти\-стическую модель информационного объекта 
придани\-ем соответствующего содержания ее компонентам. Так, множеством 
элементарных исходов $\Omega\hm=\{\omega_1, \omega_2, \ldots , \omega_n\}$ 
текстового документа являются слова, составляющие текст, выбор которых для 
отражения некоторого содержания достаточно случаен. Важнейшим компонентом 
вероятностной модели является алгебра событий~$\aleph$, которая в контексте 
статьи представляет систему семантических компонентов, по которым 
<<раскладывается>> содержание текста. Система семантических компонентов 
формируется с использованием частей речи, членов предложения, иных 
конструируемых из них структур, которые отражают семантическое содержание 
представляемого текста. В~приводимых ниже примерах используется 
     ве\-ро\-ят\-ност\-но-ста\-ти\-сти\-че\-ская морфологическая модель (ВСММ), в 
которой семантические компоненты (случайные события) $A_j$, определяющие 
структуризацию текста, отождествляются с частями речи~[4]. При этом артикли, 
частицы, междометия и другие служебные части речи игнорировались и 
рассматривался следующий набор случайных событий: 
     \begin{multline}
\aleph = \left(A_1 = \mbox{Существительное}, A_2 = \mbox{Глагол},\right.\\
A_3 = \mbox{Прилагательное}, A_4 = \mbox{Наречие},\\
A_5 = \mbox{Числительное},\\ 
\left.A_6 = \mbox{Неопределенное слово}\right)\,.     
\label{e3-kuz}
\end{multline}

Алгебры образов эталона и ответа имеют одну и ту же систему событий, но их 
вероятности могут быть различными. Вероятности событий $A_j$, со\-став\-ля\-ющих 
алгебру~$\aleph$, вычисляются по вероятностям $p(\omega_i)$ отдельных слов, 
входящих в случайные события, в соответствии с~(\ref{e2-kuz}). В~результате 
получаются ве\-ро\-ят\-ност\-но-ста\-ти\-сти\-че\-ские морфологические образы 
эталона и ответов в виде модели~(\ref{e1-kuz}): 
\begin{equation}
\left.
\begin{array}{rl}
M_{\mathrm{Э}}&=\left\{ \Omega_{\mathrm{Э}},\aleph, 
P(A_j^{\mathrm{Э}})\right\}\,;\\[6pt]
M_{\mathrm{О}} &=\left\{ \Omega_{\mathrm{О}},\aleph, 
P(A_j^{\mathrm{О}})\right\}.
\end{array}
\right\}
\label{e4-kuz}
\end{equation}
     
     Образы эталона $ M_{\mathrm{Э}}$ и ответа $ M_{\mathrm{О}}$ содержат 
всю информацию о сравниваемых текстах как случайных объектах. В~них 
сравниваемые тексты структурируются в виде систем семантических компонентов, 
отражающих то, о чем (или о ком) идет в них речь, что они делают (или с ними 
делается), как, каким образом, при каких условиях и~т.\,д., и~т.\,п. Система 
семантических компонентов для эталона и для ответа одна и та же, она может 
автоматически синтезироваться при анализе эталона. Вес, уровень значимости 
отдельных событий в контексте определяются их вероятностями~(\ref{e2-kuz}). 
     
     Структурированное представление образов~(\ref{e4-kuz}) вероятностной 
модели~(\ref{e1-kuz}) может быть оформлено в виде таблицы, подобной табл.~1 для 
алгебры~(\ref{e3-kuz}). В~ней шапка отражает~$\aleph$~--- систему семантических 
компонентов~(\ref{e3-kuz}), общую для эталона и ответа. Содержание таблицы 
представляет наполнение семантических компонентов (в примере~--- множество 
слов~$\Omega$, составляющих конкретный информационный объект, ИО). Частоты 
отдельных слов $p(\omega_i)$ представлены в таблице, по ним определяются 
показанные в последней строке таблицы вероятности случайных событий $P(A_j)$. 
Таким образом, табл.~1 представляет в полном объеме 
     ве\-ро\-ят\-ност\-но-ста\-ти\-сти\-че\-ский морфологический образ (ВСМО) 
модели~(\ref{e1-kuz}) соответствующего ИО. В~общем случае алгебра может быть 
расширена включением всех существенных для отражения содержания текста 
семантических компонентов. Процесс расширения может выполняться 
автоматически, но это вопрос отдельного рассмотрения. 

\begin{table*}\small
\begin{center}
\Caption{Табличное представление вероятностной модели ИО с алгеброй~(\ref{e3-kuz})}
\vspace*{2ex}

\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{9}{|c|}{Семантические компоненты}\\
\hline
\multicolumn{2}{|c|}{Существительное}&\multicolumn{2}{c|}{Глагол}&\multicolumn{2}{c|}
{Прилагательное}&$\cdots$&\multicolumn{2}{c|}{Неопределенное}\\
\hline
Слово&Частота&Слово&Частота&Слово&Частота&$\cdots$&Слово&Частота\\
\hline
Advice&$p$(advice)&Allow&$p$ (allow)&American&$p$ (American)&$\cdots$&Of&$p$(of)\\
Bang&$p$(bang)&Appear&$p$ (appear)&British&$p$ (British)&$\cdots$&For&$p$(for)\\
%\hline
$\cdots$&$\cdots$&$\cdots$&$\cdots$&$\cdots$&$\cdots$&$\cdots$&$\cdots$&$\cdots$\\
\hline
\multicolumn{2}{|c|}{$P(A_1)$}&\multicolumn{2}{c|}{$P(A_2)$}&\multicolumn{2}{c|}{$P(A_3)$}&
$\cdots$&\multicolumn{2}{c|}{$P(A_6)$}\\
\hline
\end{tabular}
\end{center}
\end{table*}

     При оценке близости текстов используется и совместная энтропия текстов, 
которая определяется по общей модели вероятностного объекта 
     <<эта\-лон--от\-вет>>, имеющей вид:
     \begin{equation}
     M_{\mathrm{ЭО}}=\left\{ \Omega_{\mathrm{ЭО}},\aleph_{\mathrm{}}, 
P(A_j)\right\}\,,
     \label{e5-kuz}
     \end{equation}
где $\Omega_{\mathrm{ЭО}}=\left\{\Omega_{\mathrm{Э}} + 
\Omega_{\mathrm{О}}\right\}$~--- множество элементарных событий: объединение всех 
слов, принадлежащих и эталону, и ответу; $\aleph_{\mathrm{ЭО}}$~--- система 
событий, получающаяся разделением каждого события~$A_j$ из~(\ref{e3-kuz}) на 
события:
$$
\hspace*{-4pt}A_j^{\mathrm{ЭО}} =\left\{ \omega_{ik}\in \Omega_{\mathrm{О}} 
+\Omega_{\mathrm{Э}} \vert \omega_{ik}\in A_j^{\mathrm{Э}}\cap\omega_{ik} \in 
A_j^{\mathrm{О}}\right\}\!;\!
\eqno{(6\mathrm{а})}
$$
$$
\hspace*{-4pt}A_{\mathrm{О}j}^{\mathrm{Э}} =\left\{ \omega_{ik}\in \Omega_{\mathrm{О}} 
+\Omega_{\mathrm{Э}} \vert \omega_{ik}\in A_j^{\mathrm{Э}}\cap\omega_{ik} 
\not\in A_j^{\mathrm{О}}\right\}\!;\!
\eqno{(6\mathrm{б})}
$$
$$
\hspace*{-4pt}A_{\mathrm{Э}j}^{\mathrm{О}} =\left\{ \omega_{ik}\in \Omega_{\mathrm{О}} 
+\Omega_{\mathrm{Э}} \vert \omega_{ik}\in A_j^{\mathrm{Э}}\cap\omega_{ik} \in 
A_j^{\mathrm{О}}\right\}\!,\!
\eqno{(6\mathrm{в})}
$$
где верхним индексом отмечен образ, в который входит рассматриваемое слово, а 
нижним~--- образ, в который слово не входит. 
\addtocounter{equation}{1}

     Событие~(6а) составляется из слов, входящих в событие~$A_j$ эталона и 
ответа, (6б) объединяет слова, входящие в событие~$A_j$ эталона, но 
отсутствующие в ответе, (6в) объединяет слова, не входящие в событие~$A_j$ в 
эталоне, но имеющиеся в ответе. Других вариантов для слов, присутствующих в 
двух сопоставляемых текстах, нет. 
     
     Таким образом, в модели~(\ref{e5-kuz}) используется детализация текста, 
подобная~(\ref{e3-kuz}), но с определением принадлежности слова к каждому 
тексту (эталону и ответу):
     \begin{equation*}
     \aleph_{\mathrm{ЭО}}=\left\{ A_j=\left[ A_j^{\mathrm{ЭО}}, 
A^{\mathrm{Э}}_{\mathrm{О}j}, A^{\mathrm{О}}_{\mathrm{Э}j}\right]\,,\enskip j=1, 2, 
\ldots , J\right\}\,.
%     \label{e7-kuz}
     \end{equation*}
   
   Для получения количественных оценок уровня подобия ответа эталону, тексты 
формализуются в виде $M_{\mathrm{Э}}$, $M_{\mathrm{О}}$ и $M_{\mathrm{ЭО}}$. 
Вся информация, содержащаяся в образах вероятностных моделей текстов, 
представляется в виде таблиц формата, подобного табл.~1. Естественно, алгебра 
моделей определяется адекватно содержанию ИО. Формирование алгебр базируется 
на автоматическом анализе текста и установлении принадлежности каждого слова к 
определенному событию алгебры~(\ref{e3-kuz}). В~настоящее время имеются 
коммерческие и бесплатные программные продукты для автоматического 
морфологического и синтаксического анализа текстов. Для построения ВСММ 
использовался\footnote{Некоторые из используемых ниже количественных данных взяты из 
магистерской диссертации А.\,С.~Кондаурова, выполненной под руководством Л.\,А.~Кузнецова.} 
свободно распространяемый продукт российской компании Cognitive 
   Technologies~--- система синтаксического анализа и машинного перевода 
Cognitive Dwarf~[6], ориентированная на русский и английский языки. Система 
позволяет выполнять морфологический и синтаксический разбор текста со 
скоростью порядка нескольких килобайт в секунду.
   
\section{Мера семантической близости информационных объектов}
     
     Оценка семантической близости информационных объектов~--- эталона и его 
искаженной копии, формально представленных моделями вида~(\ref{e4-kuz}),\linebreak 
может быть выполнена на основе пред\-став\-ле\-ний теории информации. В~тео\-рии 
информации К.~Шенноном~[7] для оценки близости сообщения, полученного 
приемником, сообщению, посланному передатчиком, введена количественная мера 
информации и потерь информации при ее передаче. Задача оценки уровня знаний 
даже в содержательном смысле подобна передаче сообщений: эталон~--- это 
переданное обучаемому сообщение (знание), а его ответ может трактоваться как 
принятое сообщение (усвоенное знание). Поэтому естественно использовать для 
оценки уровня адекватности (семантического подобия) ответа эталону количество 
их совместной (взаимной) информации, которое в теории информации введено для 
оценки близости переданного и принятого сообщений. 
     
     Количество информации, содержащееся в случайном объекте 
$M_\omega$~(\ref{e1-kuz}), оценивается энтропией: 
     \begin{equation}
     H_\omega=-\sum\limits_{A_j\in \aleph} P(A_j)\ln P(A_j)\,,
     \label{e8-kuz}
     \end{equation}
где обозначения совпадают с использованными в~(\ref{e1-kuz}). 

     Из~(\ref{e8-kuz}) следует, что энтропия~--- мера количества информации 
случайной величины~$A$~--- определяется распределением ее вероятностей. 
Вследствие того, что $P(A)\hm\leq1$ и $\ln P(A)\hm\leq0$, для получения 
положительной величины в~(\ref{e8-kuz}) используется минус. 
     
     Образы эталона и ответа~(\ref{e4-kuz}) позволяют оценить количество 
информации в каждом из них значениями энтропии, а именно: 
     \begin{equation}
     \left.
     \begin{array}{rl}
     H_{\mathrm{Э}}&=-\sum\limits_{A_j^{\mathrm{Э}}\in \aleph} 
P(A_j^{\mathrm{Э}}) \ln P(A_j^{\mathrm{Э}})\,;\\[9pt]
     H_{\mathrm{О}}&=-\sum\limits_{A_j^{\mathrm{О}}\in \aleph} 
P(A_j^{\mathrm{О}}) \ln P(A_j^{\mathrm{О}})\,.
\end{array}
\right\}
     \label{e9-kuz}
     \end{equation}

Образ общего объекта <<эта\-лон--от\-вет>> позволяет вычислить совместную 
энтропию двух текстов в виде:
\begin{multline}
H_{\mathrm{ЭО}}=-\sum\limits_{A_j\in\aleph} \left[ 
P(A^{\mathrm{Э}}_{\mathrm{О}j})\ln P(A^{\mathrm{Э}}_{\mathrm{О}j})+{}\right.\\
\left.{}+
P(A^{\mathrm{О}}_{\mathrm{Э}j})\ln P(A^{\mathrm{О}}_{\mathrm{Э}j})-
P(A_j^{\mathrm{ЭО}})\ln P(A_j^{\mathrm{ЭО}})\right]\,.
\label{e10-kuz}
\end{multline}
На основании значений энтропии~(\ref{e9-kuz}), (\ref{e10-kuz}) определяется 
совместная или взаимная информация: 
\begin{equation}
I_{\mathrm{ЭО}}=H_{\mathrm{Э}}+H_{\mathrm{О}}-H_{\mathrm{ЭО}}\,.
\label{e11-kuz}
\end{equation}
     
     В данном контексте совместная информация может трактоваться как 
количество информации из эталона, содержащееся в ответе. Вследствие 
симметрии~(\ref{e11-kuz}) можно говорить и о количестве информации из ответа, 
содержащемся в эталоне. 
     
     Нетрудно проверить, что получаемое в соответствии с~(\ref{e11-kuz}) 
количество совместной информации согласуется с интуитивными представлениями. 
Для этого можно рассмотреть два предельных варианта: (1)~полное совпадение 
ответа и эталона и (2)~полное их несовпадение. 
      
      Если эталон и ответ полностью совпадают, то в~(\ref{e9-kuz}) 
$A_j^{\mathrm{Э}}\hm=A_j^{\mathrm{О}}$ для всех~$j$ и, следовательно, 
$H_{\mathrm{Э}}\hm=H_{\mathrm{О}}$. Далее, из~(6) видно, что в этом случае 
события~$A_j^{\mathrm{ЭО}}$ будут совпадать с событиями 
$A_j^{\mathrm{Э}}\hm=A_j^{\mathrm{О}}$ и $P(A_j^{\mathrm{ЭО}})\hm= 
P(A_j^{\mathrm{Э}})+P(A_j^{\mathrm{О}})$ для всех $A_j^{\mathrm{ЭО}}$, 
следовательно, $H_{\mathrm{ЭО}}\hm = 0$. Поэтому при полном совпадении эталона и 
ответа количество взаимной информации~(\ref{e11-kuz}) будет 
$I_{\mathrm{ЭО}}\hm=H_{\mathrm{Э}}\hm+H_{\mathrm{О}}$, т.\,е.\ оно равно ее 
содержанию в эталоне и ответе, который совпадает с эталоном.
      
      Другой крайний случай получается при полном несовпадении. При этом, 
очевидно, не будет слов, принадлежащих одновременно эталону и ответу. Поэтому 
все события~(6а) будут иметь нулевую вероятность ($P(A_j^{\mathrm{ЭО}})\hm=0$ 
для всех~$j$). В~теории информации принято считать $0\ln 0 \hm=0$, так что 
вычитаемое в правой части~(\ref{e10-kuz}) будет равно нулю. В~этом случае будет 
иметь место совпадение случайных событий 
$A^{\mathrm{О}}_{\mathrm{Э}j}\hm=A_j^{\mathrm{О}}$, 
      $A^{\mathrm{Э}}_{\mathrm{О}j}\hm=A_j^{\mathrm{Э}}$ и будут равны 
вероятности $P(A^{\mathrm{О}}_{\mathrm{Э}j})=P(A_j^{\mathrm{О}})$, 
$P(A^{\mathrm{Э}}_{\mathrm{О}j})=P(A_j^{\mathrm{Э}})$. Поэтому 
      из~(\ref{e10-kuz}) следует 
$H_{\mathrm{ЭО}}\hm=H_{\mathrm{Э}}\hm+H_{\mathrm{О}}$. Подстановка этого 
результата в~(\ref{e11-kuz}) показывает, что количество совместной информации в 
этом случае получается равным нулю. 
     
     Таким образом, количество взаимной информации по~(\ref{e11-kuz}) при 
описанном способе определения совместных событий изменяется от нуля до 
количества информации, содержащегося в эталоне и ответе, что соответствует 
содержательному смыс\-лу оценки уровня адекватности ответа эталону. Это является 
достаточным обоснованием использования количества взаимной информации в 
качестве меры соответствия информационных объектов, представленных на 
естественном языке. 
     
      Зададимся вопросом: а какова будет мера близости эталона и ответа, если 
они отличаются только порядком слов? Ответ на этот вопрос может быть извлечен 
из алгебры событий~(\ref{e3-kuz}) и табличной иллюстрации (см.\ табл.~1) образа 
объекта, представленного с ее использованием. По мнению авторов, широкие 
возможности предлагаемой методики обеспечиваются именно вариацией алгебры, 
которая позволяет для разложения текста использовать фактически любую систему 
морфологических и синтаксических конструкций языка, которые, в данном 
применении вероятностной модели, являются случайными событиями. 
Используемая в статье алгебра~(\ref{e3-kuz})~--- частный случай, ориентированный 
на наглядность представления. 
      
      Алгебра~(\ref{e3-kuz}) не учитывает порядок следования слов. Более того, 
отмечено, что предлоги и частицы для простоты игнорировались. Поэтому по 
текстам, отличающимся только порядком слов, будут синтезированы совпадающие 
образы в виде табл.~1, в которых по морфологической принадлежности будут 
собраны совпадающие столбцы существительных, прилагательных и~т.\,д. 
Вероятности, вычисленные по совпадающим таблицам, также совпадут, так что при 
используемой алгебре, не учитывающей порядок, количество информации от него 
не зависит. 
      
      Однако ничто не мешает в алгебру ввес\-ти со\-бытия вида 
<<существительное\;$\rightarrow$\;глагол>>, 
<<при\-лагатель\-ное\;$\rightarrow$\;су\-щест\-ви\-тель\-ное>>, 
<<пред\-лог\;$\rightarrow$\;су\-щест\-ви\-тель\-ное>> и~т.\,д., включая многословные\linebreak 
конструкции, которые могут оказаться полезными в конкретном варианте 
последующего использования системы. 

\begin{table*}[b]\small
\vspace*{-3pt}
\begin{center}
\Caption{Распределение реализаций (слов) по системе событий алгебры морфологической модели 
текста}
\vspace*{2ex}


\begin{tabular}{|l|l|l|l|l|l|}
\hline
&&&&&\\[-9pt]
\multicolumn{1}{|c|}{Существительное, $A_1$}&\multicolumn{1}{c|}{Глагол$^1$,
$A_2$}&\multicolumn{1}{c|}{Прилагательное$^2$, $A_3$}&\multicolumn{1}{c|}{Наречие,
$A_4$}&\multicolumn{1}{c|}{Предлог,
$A_5$}&\multicolumn{1}{c|}{Союз,
$A_6$}\\
\hline
\tabcolsep=0pt\begin{tabular}{l}примерах\\ \sout{аспекты}\\ количества\\ информации\\ меры \\
\sout{близости}\\ подобия\\ объектов\\ языке\\ \sout{\sout{характеристики}} 
\end{tabular}&\tabcolsep=0pt\begin{tabular}{l}иллюстрируются\\представленных\\
\sout{\sout{являющейся}} \end{tabular}&
\tabcolsep=0pt\begin{tabular}{l}\sout{наглядных}\\ \sout{некоторые}\\ взаимной\\ 
информационные\\
естественном\\ \sout{английском} \\ \sout{\sout{ярких}}\\ \sout{\sout{главные}} \end{tabular}&
\tabcolsep=0pt\begin{tabular}{l}ниже\\ \sout{достаточно}  \end{tabular}&
\tabcolsep=0pt\begin{tabular}{l}на\\ на \end{tabular}&
\tabcolsep=0pt\begin{tabular}{l}\sout{Или}\\ \sout{как}\end{tabular}\\
\hline
\multicolumn{6}{p{160mm}}{\footnotesize \textbf{Примечания:}\newline 
$^1$К~глаголам отнесены причастия, являющиеся атрибутивной глагольной формой.\newline 
$^2$К~прилагательным отнесено местоимение для сокращения таблицы, имеющей 
иллюстративное значение для демонстрации способа вычисления количественной оценки.}
\end{tabular}
\end{center}
\end{table*}

      
      Одной из наиболее важных отличительных особенностей предлагаемой 
методологии является ее фор\-маль\-но-ма\-те\-ма\-ти\-че\-ская основа, которая 
позволяет формализовать сам процесс синтеза автоматизированной системы оценки 
близости текстов. В~контексте сопоставления текстов алгебра представляет собой 
модель текста. 
      
      Известно, что модель объекта не может с ним совпадать, а является лишь 
некоторым его отражением. Модели используются для оптимизации управления 
объектами, и при этом они должны адекватно отражать свойства объекта, учет 
которых необходим для эффективного управления. Чтобы приблизить модель к 
объекту, используется ее идентификация, призванная повышать степень 
адекватности модели объекту в разрезе решаемых с по\-мощью модели задач. 
      
      Формализация текста позволяет видеть возможность структурной 
идентификации модели представления текста, т.\,е.\ алгебры~(\ref{e3-kuz}). 
Формализация в виде структуры, приведенной в табл.~1, позволяет оценить 
вероятности отдельных морфологических и синтаксических компонентов, по 
вероятностям могут быть сконструированы дополнительные составные компоненты, 
которые добавляются в ал\-геб\-ру. Этот процесс может быть организован в виде 
итерационной процедуры последовательных приближений. Критерием 
оптимальности алгебры и целесообразности продолжения процесса идентификации 
алгебры (или, что одно и то же, обучения системы) может быть количество 
взаимной информации между эталоном и образом (ответом). При повышении 
уровня детализации текста эталона будет увеличиваться количество событий 
(столбцов) в табл.~1 и возрастать энтропия. Ответ будет структурироваться в виде 
такой же таблицы, с тем же набором случайных событий. Процесс детализации 
может быть прекращен, когда величина изменения взаимной информации в 
следующих одна за другой итерациях станет меньше заданного порогового 
значения. 
      
      \smallskip
      
      Ниже на достаточно наглядных примерах иллюстрируются некоторые 
аспекты количества взаимной информации как меры близости, или подобия, 
информационных объектов, представленных на естественном, английском, 
языке.
      

\section{Пример содержательной интерпретации формальных 
конструкций}
      
      Формально-математические конструкции, используемые при оценке 
адекватности текстов, возможно несколько непривычные, будут 
проиллюстрированы на очень простом примере. В~качестве эталонного текста 
используется предшествующий данному разделу и выделенный полужирным 
курсивом абзац: <<Ниже на \sout{достаточно наглядных} примерах иллюстрируются 
\sout{некоторые аспекты} количества взаимной информации \sout{как} меры 
\sout{близости}, или подобия, информационных объектов, пред\-став\-лен\-ных на 
естественном, \sout{английском}, языке>>.
      
      Ответом будет следующий, несколько измененный вариант этого же текста: 
<<Ниже на \sout{\sout{ярких}} примерах иллюстрируются \sout{\sout{главные характеристики}} количества 
взаимной информации, \sout{\sout{являющейся}}\linebreak мерой подобия информационных объектов, 
представленных на естественном языке>>. Для на\-гляд\-ности слова, убранные из 
эталона, зачеркнуты одной линией, а вставленные в ответ~--- двойной\linebreak линией. 

      \begin{table*}\small %tabl3
      \begin{center}
      \Caption{Вероятности и энтропии случайных событий~(\ref{e3-kuz}), (6) в текстах 
примера}
       \vspace*{2ex}
       
       \tabcolsep=7pt
       \begin{tabular}{|l|l|c|c|c|c|c|c|}
       \hline  %\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{Объект}}&
\multicolumn{1}{c|}{\raisebox{-6pt}[0pt][0pt]{Характеристика}}&\multicolumn{6}{c|}{Алгеб
ра~--- система случайных событий}\\
\cline{3-8}
&&$A_1$&$A_2$&$A_3$&$A_4$&$A_5$&$A_6$\\
\hline
\raisebox{-6pt}[0pt][0pt]{Эталон}&Вероятность&0,210&0,048&0,143&0,048&0,048&0,048\\
&Энтропия&0,330&0,145&0,278&0,145&0,145&0,145\\
\hline
\raisebox{-6pt}[0pt][0pt]{Ответ}&Вероятность &0,190&0,071&0,119&0,024&0,048&0\\
&Энтропия&0,316&0,188&0,253&0,089&0,145&0\\
\hline
\raisebox{-6pt}[0pt][0pt]{$A^{\mathrm{ЭО}}$}&Вероятность&0,333&0,095&0,143&0,048&0,095&0\\
&Энтропия&0,366&0,224&0,278&0,145&0,224&0\\
\hline
\raisebox{-6pt}[0pt][0pt]{$A_{\mathrm{О}}^{\mathrm{Э}}$}&Вероятность&0,048&0&0,071&0,024&0&0,048\\
&Энтропия&0,145&0&0,188&0,080&0&0,145\\
\hline
\raisebox{-6pt}[0pt][0pt]{$A^{\mathrm{О}}_{\mathrm{Э}}$}&Вероятность&0,028&0,024&0,048&0&0&0\\
&Энтропия&0,089&0,089&0,145&0&0&0\\
\hline
\end{tabular}
\end{center}
\end{table*}
      
      В~примере используется алгебра, содержащая шесть случайных событий 
$A_1$--$A_6$, которые показаны в шапке табл.~2, представляющей конкретизацию 
табл.~1. В~табл.~2 представлены реализации и их распределение по всем 
случайным событиям моделей~(4)--(6): 
\begin{itemize}
\item эталон образуют слова, не зачеркнутые и зачеркнутые одной чертой; 
\item ответ~--- слова, не зачеркнутые и зачеркнутые двойной чертой;
\item незачеркнутые слова входят в оба текста;
\item слова, зачеркнутые одной чертой, входят только в эталонный текст;
\item слова, зачеркнутые двойной чертой, входят только в ответ. 
\end{itemize}


      В табл.~3 приведены количественные значения вероятностей и энтропий, 
вычисленных по данным табл.~2. Вероятности реализаций (слов) вы\-чис\-ля\-лись в 
виде относительных частот: количество вхождений слова\,/\,общее число слов. 
      
      


      Построчным суммированием энтропий в табл.~3 получается: 
      
энтропия эталона~--- 1,188, 

энтропия ответа~--- 0,992, 

энтропия $A^{\mathrm{ЭО}}$~--- 1,237, 

энтропия $A_{\mathrm{О}}^{\mathrm{Э}}$~--- 0,827, 

энтропия $A_{\mathrm{Э}}^{\mathrm{О}}$~--- 0,323. 

      Количество взаимной информации в текстах примера определяется по 
энтропиям в виде: $I\hm = H_{\mathrm{Э}}\hm+H_{\mathrm{О}}\hm- 
H_{\mathrm{ЭО}}\hm = H_{\mathrm{Э}}\hm+H_{\mathrm{О}}\hm- 
H_{\mathrm{Э}}^{\mathrm{О}}\hm-H^{\mathrm{Э}}_{\mathrm{О}}\hm+
H^{\mathrm{ЭО}}\hm= 2{,}266$. Полученный 
результат приближается к варианту совпадения текстов, что согласуется с 
незначительным уровнем искажения ответа по отношению к эталону. 
      
      Завершая пример, можно отметить, что в зависимости от задачи вероятности 
могут исчисляться различными способами, что может привести к некоторым 
масштабным преобразованиям количественных характеристик. Может быть 
использован аппарат условных вероятностей, т.\,е.\ введено отличие отдельных слов 
по их принадлежности к отдельным образцам сопоставляемых текстов. В~этом 
случае также работает предлагаемая методология через условные энтропии и 
информацию. Все многообразие возможных модификаций методологии не 
представляется возможным изложить в пределах одной статьи, но практически 
готовы работы, в которых достаточно наглядно будут продемонстрированы 
возможности адаптации алгебры, т.\,е.\ модели представления текста, и оценки 
семантической близости слов русского языка, которые следуют из предлагаемого 
подхода.
      
\section{Проверка монотонности информационной меры оценки близости 
текстов}

     Важным требованием к мере оценки близости текстов является монотонность 
ее изменения при монотонном изменении уровня адекватности текстов. Параметры 
меры фактического соответствия реальной близости текстов могут варьироваться, 
однако для эффективной практической реализации автоматического сравнения 
текстов монотонность меры должна иметь место.
     
     Для исследования характера изменения совместной информации при 
задаваемом монотонном изменении уровня совпадения текстов выполнен 
эксперимент планированного искажения \mbox{текста}. На основе одного эталонного текста 
на английском языке, состоящего из 174~слов, который обозначается 
$T_{\mathrm{Э}}$, синтезирован массив его искаженных копий~$T_j$, $j\hm=1, 2, 
\ldots , 20$. Копии формировались следующим образом. Эталонный текст был 
разбит на 20~частей. Первая искаженная копия~$T_1$ получена из эталонного 
текста путем замены 1/20\linebreak\vspace*{-12pt}

\

\vspace*{-12pt}

\vspace*{2pt}
\begin{center}  %fig1
\mbox{%
 \epsfxsize=71.659mm
 \epsfbox{kuz-1.eps}
 }
 \end{center}
% \vspace*{6pt}
{{\figurename~1}\ \ \small{Зависимость количества совместной информации от доли слов из эталона, сохраненных в 
копии}}



%\pagebreak

\vspace*{12pt}

\addtocounter{figure}{1}

\noindent
 (по одному слову в каждой части) его слов на слова из 
текста с другой тематикой, вторая копия~$T_2$ получается заменой в эталоне 2/20 
слов и~т.\,д. Последняя искаженная копия не содержит ни одного слова эталонного 
текста (заменено 20/20). Для каждой искаженной копии~$T_j$ определена прямая 
оценка ее соответствия эталону в виде доли слов из эталонного текста, сохраненных 
в копии, по 100-балль\-ной шкале: 
     \begin{equation}
     Q\left(T_{\mathrm{Э}},T_j\right) =\fr{m_j}{n}\cdot 100\,,
     \label{e12-kuz}
     \end{equation}
     где $m_j$~--- число слов из эталонного текста $T_{\mathrm{Э}}$ в копии~$T_j$; 
$n$~--- число слов в эталоне.
     
     Для полученного массива из 20~английских текс\-тов с известными 
оценками~(\ref{e12-kuz}) рассчитаны значения количества совместной информации 
$I_{\mathrm{ЭО}j}$ между копиями~$T_j$ и эталонным текс\-том~$T_{\mathrm{Э}}$ 
по~(\ref{e11-kuz}). На рис.~1 показано соответствие между количеством совместной 
информации, определенным по~(\ref{e11-kuz}), и оценкой~(\ref{e12-kuz}) степени 
искажения эталона при формировании копии.



      На рис.~1 видно, что значение совместной информации монотонно 
изменяется при монотонном изменении реального уровня искажения текстов, 
причем между ними существует линейная связь с коэффициентом 
корреляции~0,992.
      
\section{Сопоставление информационной меры с~реальными оценками 
преподавателя и градуировка информационной меры по~$N$-балльной 
шкале оценок}
     
     Возможность практического использования методологии была оценена 
сопоставлением реальных оценок, выставленных преподавателем, с оценками, 
сформированными автоматически на основании количества взаимной информации. 
Исходными данными для такого исследования являлись результаты контрольной 
проверки знаний студентов по английскому языку. Контроль осуществлялся 
написанием изложения по обычной технологии~--- преподаватель прочитал 
эталонный текст, который был по памяти воспроизведен студентами. Целью 
написания изложения в соответствии с методикой обучения языку была проверка 
знания орфографии и умения передачи содержания. Изложения были проверены и 
оценены преподавателем английского языка стандартным образом на предмет 
орфографии (первая оценка), полноты и правильности передачи содержания (вторая 
оценка) по 100-балль\-ной шкале. В~настоящем исследовании использовалась 
вторая оценка за полноту и правильность изложения эталонного текста, 
отражающая семантическую близость ответа эталону, орфографические ошибки 
игнорировались (автоматически исправлялись). 
     
     Оценка по количеству взаимной информации осуществлялась 
автоматизированной системой, реализующей изложенную выше методологию, 
использующую для классификации слов Cognitive Dwarf~[6]. Эталонный текст и 
22~варианта его изложения студентами были введены в систему. Для текстов 
формировались ве\-ро\-ят\-ност\-но-ста\-ти\-сти\-че\-ские морфологические образы с 
алгеброй событий~(\ref{e3-kuz}). По ним определялись энтропии образов~(\ref{e9-kuz}), 
(\ref{e10-kuz}) и количество взаимной информации~(\ref{e11-kuz}). 
     
     Количество взаимной информации~(\ref{e11-kuz}) является абстрактной 
величиной, поэтому для со\-по\-став\-ле\-ния необходимо произвести его градуировку в 
баллах принятой в образовательном учреждении системы оценок. Фактически 
градуировка сводит-\linebreak ся к установлению однозначного соответствия\linebreak значений 
информационной меры~(\ref{e11-kuz}) оценкам\linebreak по выбранной шкале. 
В~простейшем случае это соответствие может быть получено в виде линейной 
регрессии $y\hm=a \hm+ bx$, где $x$~--- информационная мера соответствия ответа 
эталону, вычисля\-емая автоматически, а $y$~--- оценка по 100-балль\-ной шкале, 
принятой в образовательном учреждении. Для градуировки, т.\,е.\ преобразования 
количества взаимной информации в реальные оценки могут быть использованы 
более сложные модели, поз\-во\-ля\-ющие учесть конкретные со\-дер\-жа\-тель\-но-ме\-то\-ди\-че\-ские 
особенности дисциплин и регламентов. Шкала оценок может быть 
произвольной $N$-балль\-ной. 


     
     Для иллюстрации приведем два варианта модели градуировки автоматически 
формируемой оценки по принятой в университете 100-балль\-ной шка-\linebreak\vspace*{-12pt}

\end{multicols}

\begin{table}\small  %tabl4
\begin{center}
\Caption{Результаты расчета оценок изложений студентов}
\vspace*{2ex}

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline  %\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{\tabcolsep=0pt\begin{tabular}{c}№\\ ответа\\ студента\end{tabular}}}&
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{\tabcolsep=0pt\begin{tabular}{c}Оценка\\ преподавателя\end{tabular}}}&
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{\tabcolsep=0pt\begin{tabular}{c}Количество\\ информации,\\ $I_{\mathrm{ЭО}}$\end{tabular}}}&
\multicolumn{2}{c|}{\tabcolsep=0pt\begin{tabular}{c}Оценка \\ по количеству\\ информации\end{tabular}}&
\multicolumn{2}{c|}{\tabcolsep=0pt\begin{tabular}{c}Векторно-пространственная\\ модель текста 
Г.~Солтона\end{tabular}}\\
\cline{4-7}
&&&\tabcolsep=0pt\begin{tabular}{c}Без\\ взвешивания\end{tabular}&
\tabcolsep=0pt\begin{tabular}{c}С весами\\ частей речи\end{tabular}&
$\cos\left(T_0, T_j\right)$&\tabcolsep=0pt\begin{tabular}{c}Оценка\\ по $\cos(T_0, T_j)$\end{tabular}\\
\hline
\hphantom{9}1&25&0,2413&41,83&35,35&0,444&32,74\\
\hphantom{9}2&30&0,2485&42,28&30,24&0,532&43,01\\
\hphantom{9}3&35&0,3365&47,68&32,31&0,589&49,54\\
\hphantom{9}4&45&0,3348&47,58&47,18&0,638&55,34\\
\hphantom{9}5&45&0,5127&58,50&50,15&0,694&61,82\\
\hphantom{9}6&50&0,3681&49,62&44,24&0,495&38,64\\
\hphantom{9}7&50&0,4157&52,55&60,10&0,730&65,94\\
\hphantom{9}8&53&0,5318&59,67&54,50&0,788&72,77\\
\hphantom{9}9&53&0,2753&43,92&43,05&0,606&51,57\\
10&55&0,4472&54,48&52,92&0,713&63,98\\
11&60&0,4954&57,44&60,40&0,617&52,83\\
12&60&0,4490&54,59&65,80&0,641&55,61\\
13&65&0,5648&61,70&70,57&0,735&66,52\\
14&78&0,8275&77,83&74,87&0,829&77,48\\
15&80&0,8549&79,52&88,53&0,846&79,43\\
\textbf{16}&\textbf{80}&\textbf{0,4352}&\textbf{53,74}&\textbf{65,45}&\textbf{0,601}&
\textbf{51,03}\\
17&85&1,0268&90,07&90,43&0,868&82,09\\
18&85&1,0163&89,42&80,51&0,883&83,73\\
19&88&0,9875&87,66&88,62&0,867&81,94\\
20&88&1,0240&89,90&81,74&0,889&84,48\\
\textbf{21}&\textbf{90}&\textbf{0,5166}&\textbf{58,74}&\textbf{83,32}&\textbf{0,641}&
\textbf{55,69}\\
22&97&1,1603&98,27&96,74&0,944&90,82\\
\hline
\end{tabular}
\vspace*{4pt}
\end{center}
\end{table}

\begin{multicols}{2}

\noindent
ле, результаты 
применения которых представлены в табл.~4. Первый вариант имел вид регрессии 
$y\hm=a \hm+ bx$, для которой параметры~$a$ и~$b$ определялись методом 
наименьших квадратов по рядам значений оценок преподавателя (см.\ столбец~2 в 
табл.~4) и количества взаимной информации (см.\ столбец~3 в табл.~4). По этим данным 
получена линейная регрессия: $y\hm = 27{,}02\hm+61{,}41x_j$, где $x_j 
\hm=I_{\mathrm{ЭО}j}$~--- количество взаимной информации в эталонном тексте и $j$-м 
изложении, а $y$~--- автоматически формируемая по нему оценка. 




     Второй вариант модели пересчета информационной оценки в 100-балль\-ную 
отличался от первого введением учета различной значимости в текстах 
семантических компонентов. При использовании алгебры~(\ref{e3-kuz}) это 
сводится к учету различной значимости частей речи. Для иллюстрации 
возможностей такого варианта уточнения перед слагаемыми~$A_j$, 
соответствующими различным частям речи (см.\ табл.~1), в~(\ref{e9-kuz}), 
     (\ref{e10-kuz}) были введены параметры~$\beta_j$,\linebreak значения которых 
определялись методом наименьших квадратов в процессе получения уравнения 
регрессии. Фактически эти параметры отражают различный <<вклад>> 
семантических компонентов в\linebreak соответствие между эталоном и ответом. 
В~результате было получено следующее уравнение множественной регрессии: $y_1 
\hm= 0{,}8436 \hm- 242{,}23A_1\hm+ 451{,}27A_2 \hm+131{,}60A_3 \hm- 262{,}16A_4\hm- 
110{,}75A_5$, где $y_1$~--- оценка по 100-балль\-ной шкале, а $A_j$, $j\hm=1, 2, 
\ldots , 5$,~--- час\-ти речи (семантические компоненты~--- случайные события 
алгебры~(\ref{e3-kuz})).
     
     Значения оценок $y$ и $y_1$ приведены соответственно в четвертом и пятом 
столбцах табл.~4. Среднеквадратичное отклонение оценок~$y$ от оценок 
преподавателя составляет 11,04 балла, коэффициент корреляции равен~0,847. 
Некоторое усложнение формулы перевода количества информации в 
     100-балль\-ную оценку введением весовых коэффициентов позволило 
существенно снизить уклонение оценок, сформированных автоматически, от 
оценок, выставленных преподавателем. Значения 100-балль\-ных оценок, 
вычисленных с использованием множественной регрессии с весовыми 
коэффициентами частей речи показаны в пятом столбце табл.~4. Их 
среднеквадратичное уклонение от оценок преподавателя составило 6,324~балла, а 
множественный коэффициент корреляции достиг~0,955. Видно, что среднее 
уклонение оценки, фор\-ми\-ру\-емой автоматически по количеству совместной 
информации, от оценки преподавателя уменьшилось\linebreak\vspace*{-12pt}

\pagebreak

\

\vspace*{-12pt}

%\vspace*{2pt}
\begin{center}  %fig1
\mbox{%
 \epsfxsize=80mm
 \epsfbox{kuz-2.eps}
 }
 \end{center}
 \vspace*{-2pt}
{{\figurename~2}\ \ \small{Сопоставление оценок преподавателя и оценок системы:
\textit{1}~--- при градуировке без взвешивания частей речи; \textit{2}~--- при взвешивании 
частей речи}}

%\pagebreak

\vspace*{9pt}

\addtocounter{figure}{1}


\noindent
 почти в два раза. Этот пример 
показывает возможности повышения адекватности автоматически формируемых 
оценок идентификацией формул перевода количества информации в баллы 
реальной шкалы оценок. Кроме того, для повышения адекватности автоматической 
оценки, следуя методике~[4], может быть использован механизм вариации алгебры, 
т.\,е.\ системы семантических компонентов~--- случайных событий, отраженной в 
табл.~1. 

На рис.~2 представлено наглядное сопоставление оценок, выставленных 
преподавателем, и оценок $y$ (без взвешивания) и $y_1$ (с взвешиванием час\-тей 
речи), сформированных автоматически по\linebreak количеству взаимной информации. И на 
рис.~2 видны две точки, которые вносят наибольший вклад в ошибку. В~табл.~4 
они выделены полужирным шрифтом (16-я и 21-я). При детальном анализе 
выяснилось, что оценки, выставленные преподавателем, были не в полной мере 
адекватны содержанию изложений. Интересно отметить, что исключение этих точек 
и расчет по оставшимся 20~точ\-кам принципиально меняет результат: 
среднеквадратичная ошибка оценки~$y$ становится равной 1,593~балла, а оценки 
$y_1$~--- 1, 248~балла. 
     
     
     Выше отмечалось, что в настоящее время многие 
     ин\-фор\-ма\-ци\-он\-но-по\-иско\-вые системы базируются на 
     век\-тор\-но-про\-стран\-ст\-вен\-ной модели текс\-та, предложенной Г.~Солтоном
     с соавторами~[3], в которой мерой 
близости текстов служит косинус угла между векторами, отражающими 
надлежащим образом преобразованные тексты. В~принятых ранее обозначениях это 

\vspace*{2pt}

\noindent
     $$
     \cos\left( T_{\mathrm{Э}},T_j\right) = \fr{\left( T_{\mathrm{Э}},T_j\right) }{\vert 
T_{\mathrm{Э}}\! \parallel T_j\vert }\,.
     $$
     
     Такая мера также была вычислена для всех 22~изложений и эталонного 
текста. Эта мера была сопоставлена с оценками преподавателя и получена 
соответствующая регрессия для перевода ее значений в 100-балль\-ные оценки:
     $$
Q_{100} = - 18{,}97 + 116{,}3\cos\left(T_{\mathrm{Э}}, T_j\right)\,.
$$

\vspace*{-2pt}
     
     Результаты вычислений представлены в 6-м и 7-м столбцах табл.~4. 
Среднеквадратичное отклонение прогноза оценки по этой регрессии от оценки 
преподавателя составило 13,27~балла, а коэффициент корреляции равен~0,769. 
После удаления из массива 16-й и 21-й точек ошибка составила 2,155~балла. 
Уклонение оценки, автоматически полученной по разработанной методике, от 
оценки преподавателя оказалось примерно в два раза меньше. Обычно 
предполагается нормальное распределение для ошибки и величина ее оценивается 
3$\sigma$-ин\-тер\-ва\-лом. В~этом случае возможное максимальное уклонение 
оценки, формируемой автоматически по разработанной методике, от оценки 
преподавателя оценивается в 3,744~балла, в то время как для 
     век\-тор\-но-про\-стран\-ст\-вен\-ной модели оценка ошибки дает 6,466~балла. 
Без удаления проблемных 16-й и 21-й точек соотношение ошибок предсказания 
(примерно в два раза) сохраняется. 
     
     Таким образом, из приведенного примера видно, что автоматизированная 
проверка знаний (оценка семантической близости текстов) не утопия. Реальный путь 
повышения надежности и уровня адекватности сопоставления текстов 
пред\-став\-ля\-ет\-ся в исследовании возможностей детализации системы семантических 
компонентов, адаптации градуировки количества информации в принятой шкале 
оценок, учета синонимов. Но все эти задачи укладываются в предложенную 
методологию, и варианты их решения достаточно хорошо просматриваются. Важно 
отметить, что вероятностная модель текста и информационная мера близости\linebreak 
текстов содержательно соответствуют существу задачи семантического 
сопоставления текстов и имеют практически неограниченные возможности 
детализа\-ции и повышения на этой основе достоверности оценки семантической 
адекватности ответа эталону.

\vspace*{-9pt}
     
\section{Заключение}

\vspace*{-3pt}

     Изложена оригинальная методологическая основа автоматизации процедуры 
оценки уровня семантического подобия текстов, которая может быть использована в 
автоматических системах обработки вербальной информации, в част\-ности в 
сис\-те\-мах оценки уровня знаний учащихся. Системы оценки знаний, разработанные 
на ее основе, смогут воспринимать реальные полноценные ответы учащихся на 
поставленные вопросы и обеспечивать автоматическую оценку уровня адекватности 
ответа содержанию эталонного ответа, хранящегося в базе данных системы. 
Представлены примеры, иллюстрирующие содержательную интерпретацию\linebreak 
     фор\-маль\-но-ма\-те\-ма\-ти\-че\-ско\-го аппарата и возможности настройки 
перевода количества взаимной информа\-ции в оценку по принятой в учебном 
заведении шкале оценки знаний. Разработанная математическая основа может 
адаптироваться к задачам синтеза различных моделей представления текстов, 
позволяющих отразить специфику дисциплин и методических приемов. 

\vspace*{-6pt}

{\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{9}
    
\bibitem{1-kuz}
\Au{Безсуднов И.\,В.} Интернетика. Навигация в сложных сетях: модели и 
алгоритмы.~--- М.: Либроком, 2009. 264~с.
\bibitem{2-kuz}
\Au{Manning~Ch.\,D., Raghavan~P., Sch$\ddot{\mbox{u}}$tz~H.} An introduction to 
information retrieval.~--- Cambridge: University Press, 2009. 569~p.
\bibitem{3-kuz}
\Au{Salton~G., Wong~A., Yang~C.\,S.} A~vector space model for automatic 
indexing~// Communications of the ACM, 1975. Vol.~18. No.\,11. P.~613--620. 
\bibitem{4-kuz}
\Au{Кузнецов Л.\,А.} Теоретические основы автоматизированной оценки 
знаний~// Качество. Инновации. Образование, 2010. № \,11. С.~8--19.
\bibitem{5-kuz}
\Au{Гнеденко Б.\,В.} Курс теории вероятностей: Учебник.~--- 9-е изд., испр.~--- 
М.: ЛКИ, 2007. 448~с.
\bibitem{6-kuz}
Программный пакет синтаксического разбора и машинного перевода Cognitive 
Dwarf. {\sf http://cs.isa.ru: 10000/dwarf/}.

\label{end\stat}

\bibitem{7-kuz}
\Au{Шеннон К.} Работы по теории информации и кибернетике~/ Пер. с англ.~--- 
М.: ИЛ, 1963.
\end{thebibliography}
}
}

\end{multicols}
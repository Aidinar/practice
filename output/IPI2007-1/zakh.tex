\def\stat{zakh}
\def\tit{СРЕДСТВА ОБЕСПЕЧЕНИЯ ОТКАЗОУСТОЙЧИВОСТИ ПРИЛОЖЕНИЙ$^*$}
\def\titkol{Средства обеспечения отказоустойчивости приложений}
\def\autkol{В.\,Н.~Захаров, В.\,А.~Козмидиади}
\def\aut{В.\,Н.~Захаров$^1$, В.\,А.~Козмидиади$^2$}

\titel{\tit}{\aut}{\autkol}{\titkol}

\Abst{Рассмотрены проблемы построения отказоустойчивых серверов,
возникающие в связи с недетерминированностью поведения приложений.
Предложена формальная модель, описывающая поведение приложения,
основными объектами которой являются ресурсы и события. Предложены
алгоритмы протоколирования работы приложения на резервном узле кластера,
а также восстановления и продолжения его работы при отказе основного узла.
При этом для клиентов сбой остается незаметным, за исключением некоторого
увеличения времени обслуживания. }

\KW{сервер приложений; прозрачная
отказоустойчивость; процесс; ресурс;
событие; контрольная точка; детерминированность}



\vskip 24pt plus 9pt minus 6pt

\begin{multicols}{2}

\section{Введение}

{\nwt

Средства вычислительной техники стали использоваться в областях, требующих 
безотказной работы систем в течение многих лет (24~часа в сутки, 365~дней в 
году).

\label{st\stat}

{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]{Работа 
выполнена при поддержке РФФИ, гранты №\,06-07-89188 и №\,06-07-08072.} 
\renewcommand{\thefootnote}{\arabic{footnote}}}
 \footnotetext[1]{Институт 
проблем информатики РАН, vzakharov@ipiran.ru} \footnotetext[2]{Институт проблем 
информатики РАН, kozmidiady\_v@tochka.ru}

К таким областям относятся, например, центры хранения и обработки
данных  в сетях (системы резервирования билетов, биллинговые,  банковские
и~т.д.), массированные распределенные вы\-чис\-ле\-ния (GRID-вычисления)
и~др.

\thispagestyle{headings}

Обычно в подобных системах применяются частные решения, ориентированные, в 
основном, на обеспечение надежного хранения данных (напри\-мер, файловые 
серверы, ис\-поль\-зу\-ющие для хранения RAID-контроллеры) и корректного их 
состояния при отказах (серверы баз данных с транзакционным выполнением 
запросов). Однако большинство приложений не гарантируют, что не произойдет 
потери части данных при отказе системы. Обычно предполагается, что клиентские 
средства должны повторять запросы после восстановления серверов, для того чтобы 
данные не были потеряны, или что можно сделать возврат по времени на некоторое 
время назад и повторить работу с этого места. Однако далеко не все клиентские 
средства и условия применения приложений допускают\linebreak
 это.

Отказоустойчивые системы для критически важных приложений, корректно
решающие проблемы восстановления после сбоев,   пред\-лагаемые ведущими
производителями, как правило, дороги. Кроме того, они включают
специфические серверные и клиентские приложения, не совместимые со
стандартными приложениями, не обеспечивающими отказоустойчивость.
Примером такого подхода к решению проблемы отказоустойчивости  хранения
данных являются системы NetApp FAS компании Network Appliance,
работающие на базе специализированной операционной системы Data
ONTAP~\cite{1zak}.

Построение отказоустойчивых систем, использующих серверы со
стандартными приложениями, в свете вышесказанного, является
 актуальной
проблемой, вызывающей значительный интерес. Рассмотрение методов
до\-сти\-же\-ния прозрачной отказоустойчивости таких сис\-тем и является
предметом статьи.

}

\section{Основные понятия и~подходы}
{\nwt

Под {\bfseries\textit{сервером}} в данной работе понимается вычислительный центр
(отдельный компьютер или кластер) в сети, предоставляющий
 клиентам
(пользователям, клиентским компьютерам) определенные услуги, разделяя
между ними свои ресурсы. Подобные серверы названы {\bfseries\textit{серверами
приложений}}. Широко распространенным примером сервера такого типа
является файловый сервер, обеспечивающий удаленный коллективный доступ к
файловой системе. Часто используются вычислительные серверы,
предоставляющие клиентам возможность выполнять на них свои программы
(например, в центрах коллективного пользования).
{\looseness=1

}

Обычно приложение представляет собой программу или группу программ,
ра\-бо\-та\-ющих в операционной среде, создаваемой операционной сис\-те\-мой (в
другой терминологии~--- один или несколь\-ко взаимодействующих
{\bfseries\textit{процессов}} или потоков (\textit{threads})), которые
 реализуют
функциональность сервера. Для
 построения отказоустойчивых серверов
приложений широко используется кластерная технология. Следуя~\cite{2zak},
{\bfseries\textit{кластером}}, названа разновидность параллельной или распределенной
сис\-те\-мы, которая:
\begin{itemize}
\item состоит из нескольких компьютеров (узлов кластера), связанных как
минимум необходимыми коммуникационными каналами;
\item используется как единый, унифицированный компьютерный ресурс.
\end{itemize}

{\bfseries\textit{Прозрачная отказоустойчивость}} (\textit{Transparent Fault Tolerance},
\textit{TFT}) сервера приложений~--- это такое его поведение при
возникновении аппаратных или программных отказов либо отказов в сети, при
котором:
\begin{itemize}
\item отказ не вызывает потери или искажения данных, находящихся в базе
данных сервера;
\item сервер продолжает нормально функционировать, несмотря на имевшие
место отказы.
\end{itemize}

Клиенты сервера <<не замечают>> произошедших отказов. Единственным
допустимым отклонением сервера от нормального поведения с точки зрения
клиента является некоторое увеличение времени обслуживания (на несколько
секунд или десятков секунд).

Обычно приложения, работающие на серверах приложений, не
ориентированы на прозрачную отказоустойчивость. Они <<заботятся>> лишь о
собственной целостности (например, состоянии файловой системы или базы
данных). Восстановление работоспособности сервера приводит к разрыву
соединений с клиентами и потере их запросов. Это замечают клиенты~---
запросы следует повторять, на что клиентские приложения далеко не всегда
рассчитаны. В данной работе предполагается, что приложения (прикладные
программные средства), выполняемые на сервере, являются стандартными, то
есть не имеют специальных средств, обеспечивающих отказоустойчивость.

Серьезные исследования в области обеспечения отказоустойчивости
серверов были развернуты после создания вычислительных серверов,
предназначенных для решения задач, требующих больших вычислительных
ресурсов. Решение этих задач выполняется на суперкомпьютерах,
обеспечивающих массово-параллельные вычисления и представляющих собой
кластеры из сотен и тысяч узлов (процессоров). Однако даже на этих
<<монстрах>> решение может требовать десятков или сотен часов, и
одиночный сбой, если не предприняты специальные меры, может привести к
необходимости начинать работу сначала. Обычно решение вычислительной
задачи в таких случаях осуществляется в модели относительно
 редко
взаимодействующих между собой процессов, выполняемых на разных узлах кластера. 
Эти взаимодействия нужны для координации работы процессов, в частности, для 
обмена  данными и промежуточными\linebreak результатами. Взаимодействия 
опираются на специальный протокол, называемый \textit{MPI} 
(\textit{Message-Passing Interface}) и представляющий собой стандарт ``de 
facto''~\cite{3zak}.

Для преодоления последствий сбоя до\-ста\-точно давно была разработана и широко 
при\-ме\-ня\-ется технология, опирающаяся на механизм 
{\bfseries\textit{контрольных точек}} (\textit{checkpoints})~[4--6]. По этой 
техно\-ло\-гии система должна иметь {\bfseries\textit{стабильную память}}, 
которая не меняется при отказах. Со\-от\-вет\-ст\-ву\-ющие программные средства 
периодически сохраняют информацию о состоянии процессов приложения в стабильной 
памяти. Все про\-цессы также имеют доступ к {\bfseries\textit{устройству 
стабильной памяти}}.  В случае отказа или сбоя записанная в стабильную память 
информация используется для повторения вы\-чис\-ле\-ния с момента,\linebreak 
когда была записана эта информация, то есть выполняется 
{\bfseries\textit{откат}} назад по времени. Данные, сохранение которых 
позволяет выполнить откат, называются {\bfseries\textit{контрольной точкой}}. В 
качестве устройства стабильной памяти может использоваться дисковый том, 
энергонезависимая оперативная память, память другого узла или узлов 
клас\-те\-ра. В последнем случае узел, которому требуется сохранить информацию, 
пересылает ее
 через быстрый канал связи на другой
узел. Стабильная память после отказа одного из узлов должна быть доступной
узлу, на котором делается повтор.

Однако решение, опирающееся только на контрольные точки, не является
прозрачным, поскольку не скрывает от клиентов факт отказа сис\-те\-мы и требует
от них выполнения определенных действий. Так как при работе процессы
обмениваются сообщениями, возможны два варианта решения проблемы.
Первый~--- все процессы выполняют записи контрольных точек одновременно,
что затруднительно. Второй вариант, при несоблюдении синхронности,~---
возврат в каждом процессе к такому скоординированному набору контрольных
точек, при котором невозможна противоречивая ситуация. Такая ситуация
возникает, когда один процесс вернулся к контрольной точке, после которой он
должен получить сообщение от другого процесса, а этот другой процесс
вернулся к точке, которая следует за выдачей этого сообщения. Однако при
повторе ожидаемое первым процессом сообщение не поступит. В этом случае
возможен эффект домино, в результате процессы оказываются отброшены как
угодно далеко назад.
{ %\looseness=-1

}

В этом состоит \textbf{первая проблема}, которую необходимо преодолеть.

Если нужно, чтобы последствия отказа узла не были видны клиенту,  это
означает, что клиент не должен:
\begin{itemize}
\item
терять и потом восстанавливать соединения с сервером;
\item  повторять свои запросы;
\item  повторно получать сообщения, которые он уже получил.
\end{itemize}

\textbf{Вторая проблема}, которую надо решать, связана с
недетерминированностью поведения сервера приложений. Приведем пример.
Пусть имеется сис\-те\-ма продажи билетов на самолеты. Два клиента
одновременно обратились к системе с запросом билета на один и тот же рейс.
Клиентам безразлично, какие места им зарезервирует сис\-тема. Сис\-те\-ма
выполняет запросы клиентов параллельно, поэтому в какой-то момент между
процессами, обрабатывающими эти запросы, может возникнуть конкуренция за
ресурс~--- в данном случае, скажем, рейс. Один из процессов захватывает
ресурс первым, резервирует место и освобождает ресурс. Потом второй
процесс проделывает то же самое.

Порядок, в котором в этом примере процессы захватили ресурс, зависит от
многих факторов и, в конечном счете, случаен. Однако  это не мешает
правильному функционированию сис\-те\-мы, поскольку клиентам важно одно~---
получить билеты, причем на разные места. Однако отсутствие детерминизма в
поведении  приложения приводит к тому, что при повторном выполнении могут
быть получены другие результаты: например, клиенту уже сообщено, что ему
зарезервировано место~№\,5, а при повторе может получиться, что
зарезервировано место~№\,6. Система должна устранить это несоответствие и
сделать его невидимым для клиента.

Недетерминированность поведения сис\-те\-мы~--- это следствие, по крайней мере,
двух обстоятельств. Во-первых, это присущая сис\-те\-мам с разделением времени
неопределенность в порядке выполнения процессов. Во-вторых, это
конкуренция процессов за общие ресурсы. Перечислим некоторые причины
недетерминированного поведения приложений:
{ %\looseness=1

}

\noindent
\begin{itemize}
\item синхронизация процессов с помощью семафоров или атомарных
операций над операндами в общей памяти процессов;
\item зависимость от порядка получения клиентских запросов;
\item время, затраченное процессом на обработку полученного запроса;
\item генераторы случайных чисел;
\item системное управление процессами и потоками;
\item локальные таймеры;
\item доступ к реальному времени.
\end{itemize}

По различным  причинам время, которое тратится на выполнение
вычислительной задачи с одними и теми же исходными данными, не является
константой, то есть повторное выполнение может дать другое время. Процессы
используют общие ресурсы, обращение к которым требует организации
очередности выполнения (сериализации)~--- первым пришел, первым захватил.
И, наконец,  результат работы процесса может зависеть от состояния ресурса, а
это состояние может изменить другой процесс, ранее захвативший ресурс. Все
это создает значительные трудности при попытках воспроизведения поведения
процессов с сохраненной  контрольной точки.

Прозрачная отказоустойчивость серверов приложений обычно
осуществляется переносом при\-ло\-же\-ния на другой узел кластера, идентичный
первому по конфигурации аппаратных средств и\linebreak операционной среды. Это
делается методом, на\-зы\-ва\-емым \textit{snapshot/restore}. На основном узле
(оригинале)  периодически фиксируется состояние приложения на этом узле
кластера (так называемый {\bfseries\textit{снимок}} или
\textit{snapshot}). После отказа
оригинала на резервном узле (копии) делается {\bfseries\textit{восстановление}}
(\textit{restore}), то есть восстанавливается последнее
 зафиксированное
состояние приложения. Операционная среда при этом приводится в состояние,
которое соответствует моменту изготовления снимка. После этого узел-копия
продолжает работу с зафиксированного места.  Сравнение метода
\textit{snapshot/restore} с другими подходами приведено в~\cite{7zak}.

Ниже рассматриваются информационные  технологии, позволяющие решить
ряд принципиальных вопросов, связанных с реализацией прозрачной
отказоустойчивости серверов приложений. Ими являются:
\begin{itemize}
\item виртуализация операционной среды, в которой работает серверное
приложение;
\item отказоустойчивая реализация протокола управ\-ле\-ния передачей TCP
(\textit{transmission control protocol});
\item создание контрольных точек состояния приложения и файловой
системы, которые делаются внешним по отношению к приложению образом;
\item восстановление серверного приложения на осно\-ва\-нии контрольной
точки.
\end{itemize}

}

\section{Модель описания поведения приложения}

{\nwt

Предлагаемый подход опирается на построение модели вычислений, связанной с 
использованием понятия времени в многопроцессных приложениях. Впервые подобные 
проблемы были изучены в классической работе Л.~Лампорта~\cite{8zak}.

Многопроцессными приложения называются потому, что в них параллельно
работают не\-сколь\-ко процессов. Процесс ведет \mbox{себя} детерминированно, пока в
предписанном кодом порядке выполняет процессорные инст\-рук\-ции. Конечно,
его работа может быть прервана практически в любой момент и процессор
передан другому процессу или ядру. Поэтому абсолютное время, которое
затрачивает процесс на выполнение определенной работы, не  константа, а
случайная  величина. То же  относится к относительному времени, то есть
времени, которое процесс занимал процессор,  поскольку одни и те же
обращения к операционной среде могут вызвать работы разной длительности, а
значит потребовать разное время на свое выполнение.
{ %\looseness=-1

}

Кэшированность инструкций и данных, а также длина хэш-списков влияют
на действительное время пребывания в операционной  среде. Утрачивает смысл
понятие {\bfseries\textit{одно\-вре\-мен\-ность действий}}, поскольку  нельзя установить,
выполнили ли два разных процесса какие-либо действия одновременно или
одно из них предшествовало другому. Таким образом, с процессом можно
связать только его локальное время, которое линейно упорядочивает события,
происходившие в этом процессе.  Глобальное время, линейно
упо\-ря\-до\-чи\-ва\-ющее
 действия во всех процессах, отсутствует. Расстояние (в этом
качестве используется время) между действиями оказывается случайной
величиной.
{ %\looseness=1

}

Эти соображения важны, поскольку процессы в интересующих нас приложениях 
взаимодействуют и используют общие ресурсы. Для взаимодействия они используют 
средства синхронизации, предоставляемые операционной средой~--- например, 
набо\-ры семафоров SVR4 (System~V Release~4), POSIX-се\-ма\-фо\-ры, бинарные 
семафоры и другие примитивы взаимного исключения (POSIX mutual exclusion locks) 
и~т.д. Подобные средства операционной среды, которые позволяют процессам 
синхронизировать свою деятельность друг с другом или сериализовать обращения к 
совместно ис\-поль\-зу\-емым объектам,  будут ниже  называться 
{\bfseries\textit{ресурсами}}.

С каждым ресурсом связано свое локальное время, линейно
упорядочивающее {\bfseries\textit{события}} в жизни ресурса. Например, в случае
двоичных семафоров это создание семафора, а также его захват и освобождение
процессом. Заметим, что событие~--- это не намерение процесса (например,
захватить бинарный семафор), а сам факт захвата семафора процессом (то есть
успешное выполнение намерения). От изъявления намерения до его
осуществления может многое произойти. Например,
 семафор, который хочет
захватить рассматриваемый процесс, принадлежал другому процессу, потом
тот процесс его освободил, но семафор был сначала передан операционной
средой третьему процессу, который также на него претендовал, и~т.д.
Поведение рассматриваемого процесса в это время нас не интересует~--- он
ресурсом еще не овладел, а только его захват определяет его дальнейшее
поведение. По причинам,  изложенным выше, расстояние между двумя
событиями~--- случайная величина. Однако события замечательны тем, что
они одновременно присутствуют и в локальном времени процесса, и в
локальном времени ресурса. Поэтому все, что произошло в истории процесса
или/и ресурса до этого события, предшествует ему. Далее  будет считаться, что
истории и ресурсов, и процессов состоят только из событий, причем между
двумя последовательными событиями в жизни процесса последний ведет себя
детерминированно.
{ %\looseness=1

}

Это означает, что на  поведении процесса сказывается только его предыдущая 
история, то есть состояние ресурсов, с которыми он взаимодействовал. Это 
свойство процессов ниже будет называться {\bfseries\textit{локальной 
детерминированностью}}. Этим свойством не обладают ресурсы, поскольку следующее 
событие в истории ресурса не определяется однозначно по его предыду\-щей 
истории. Утверждение, ка\-са\-ющее\-ся детерминированного поведения процессов, 
неявно опирается на предположение,  что учтены все ресурсы, которые могут 
привести к недетерминированности процессов.

Таким образом, описанное нами очень неформально время в
многопроцессном комплексе представляет собой отношение частичного
порядка, введенное на множестве событий. Зная полное состояние комплекса в
некоторый момент времени,  нельзя однозначно определить, какое событие в
истории ресурса наступит следующим. Можно говорить только о вероятности
наступления того или иного события. Недетерминированность поведения есть
следствие двух обстоятельств. Во-первых, это неопределенность времени,
которое тратит процесс на переход от одного события к другому. Во-вторых,
конкуренция процессов за общие ресурсы.
{ %\looseness=-2

}

Выполнение приложения, на множестве событий которого введена частичная
упорядоченность, можно описать направленным ациклическим графом
выполнения. Вершинами этого графа являются события, с каждым  из которых
связаны две входящие в него дуги. Одна дуга начинается в событии, которое
непосредственно предшествует данному событию в истории процесса,
другая~--- в истории ресурса.

Построение средств обеспечения прозрачной отказоустойчивости приложений опирается
на следующее утверждение:

\emph{Для восстановления работы приложения после отказа достаточно располагать:
\begin{itemize}
\item контрольной точкой, которая отражает на некоторый момент времени состояния
процессов и других ресурсов, образующих приложение;
\item графом выполнения приложения, который описывает работу
приложения, начинающуюся с контрольной точки и заканчивающуюся отказом.
\end{itemize}
}
Данные, которые нужны для построения графа выполнения, далее
называются протоколом.
%
Вся эта информация должна находиться в стабильной памяти, не
разрушающейся при отказе.

Ниже неформально описан алгоритм восстановления работы приложения
после отказа, который опирается на наличие контрольной точки и графа
выполнения. Будем считать, что в распоряжении имеются средства,
позволяющие остановить процесс в тот момент, когда он \textbf{намерен}
совершить некоторую операцию над ресурсом. Заметим, что событие в графе
выполнения соответствует не изъявлению намерения, а его удовлетворению, то
есть завершению выполнения операции.

Предварительно сделаем следующее:
\begin{itemize}
\item используя контрольную точку, приведем приложение в состояние,
соответствующее этой контрольной точке;
\item в графе выполнения пометим все вершины (события) как <<не
наступившие>>. У некоторых вершин графа отсутствуют им непосредственно
предшествующие; соответствующие события наступили сразу же  после
создания контрольной точки. Для каж\-дой такой вершины включим в граф
дополнительную вершину, ей пред\-шес\-т\-ву\-ющую в истории процесса, и отметим
эту дополнительную вершину как <<наступившую>>;
\item разрешим процессам приложения выполняться.
\end{itemize}

Пусть некоторый процесс проявляет намерение выполнить операцию над
каким-либо
 ресурсом. Отыщем для этого процесса в его истории последнее
наступившее событие. Следующее в его истории событие~--- это то, которое
соответствует требуемой операции. По\-смот\-рим, наступило ли событие в
истории ресурса, которое ему пред\-шес\-т\-ву\-ет. Если нет, переведем процесс в
состояние ожидания, отметив в предшествующем событии, что данный процесс
ожидает его наступления. Если да, разрешим процессу выполняться, то есть
выполнить операцию над ресурсом.

Пусть некоторый процесс объявляет, что он выполнил операцию над
каким-либо ресурсом (это\linebreak соответствует моменту протоколирования при
оригинальном выполнении). Отыщем для этого процесса в его истории
последнее наступившее событие и перейдем к следующему событию в его
истории. Это опять то событие, которое мы рас\-смат\-ри\-ва\-ем. Отметим его как
<<наступившее>>. Если наступления этого события ожидал какой-нибудь
процесс, выведем этот процесс из со\-сто\-яния ожидания. Наконец, разрешим
процессу, выполнившему операцию, продолжаться дальше.

Когда выясняется, что наступили все события графа выполнения, повторное
выполнение считается законченным.

Естественным следствием из сказанного является следующее утверждение:

\emph{Для того, чтобы размер протокола не рос неограниченно, нужно периодически 
создавать контрольные точки, очищая при этом протокол.}

}

\section{Формальное описание модели поведения
многопроцессного приложения}

Опишем формально поведение приложения, неформальное описание
которого было приведено выше. Рассматриваются два типа объектов:
\begin{itemize}
\item {\bfseries\textit{ресурсы}} ($r$), например наборы семафоров (POSIX- или
SVR4-семафоры), бинарные семафоры (POSIX-mutex's), таймер
реального времени, сокеты (\textit{sockets}), то есть двусторонние виртуальные
соединения с внешним миром;
\item {\bfseries\textit{процессы}} ($p$), например процессы или потоки
(\textit{threads}) пользователя.
\end{itemize}

Процесс может выполнять над ресурсом {\bfseries\textit{операции}}. Примерами
операций являются: за\-хват свободного бинарного семафора или его
осво\-бож\-де\-ние; изменение значений семафоров в наборе; получение запроса от
клиента или посылка ему сообщения. Предполагается, что два процесса не
могут одновременно производить операции над одним и тем же ресурсом, а
могут это делать только последовательно. Операция процесса $p$ над ресурсом
$r$ называется {\bfseries\textit{событием}} и обозначается как $\{p,\,r,\,m,\,n\}$
(назначение переменных $m$ и $n$ будет объяснено  ниже).
{\looseness=-1

}

С каждым процессом~$p$ связывается его история $H_p$~---
последовательность событий, в которых он участвовал, то есть его операций
над ресурсами. В обозначении события $m$~--- это номер события в истории
$H_p$, $m > 0$.

С каждым ресурсом $r$ связывается его история $H_r$~---
последовательность событий, в которых он участвовал, то есть операций,
которые над ним производили процессы. В обозначении события $n$~--- это
номер события в истории $H_r$, $n > 0$.

В начало истории каждого процесса $p$ включим {\bfseries\textit{фиктивное
событие}} $\{p,\,\varnothing ,\,0,\,0\}$,
а в начало истории каждого ресурса $r$~--- фиктивное
событие $\{\varnothing ,\, r,\,0,\,0\}$ ($\varnothing$~--- отсутствующий объект).

Если один и тот же процесс $p$ выполнил две операции над одним и тем же
ресурсом~$r$ (события  $\{p,\,r,\,m_1,\,n_1\}$ и $\{p,\,r,\,m_2,\,n_2\}$), причем если
$m_1 < m_2$, то $n_1 < n_2$. Верно и обратное, то есть если $n_1 < n_2$, то $m_1 <
m_2$.

Событие $\{p_1,\,r_1,\,m_1,\,n_1\}$ непосредственно предшествует
($\ll_p (\ll_r)$) в истории $H_p$ ($H_r$) событию $\{p_2,\,r_2,\,m_2,\,n_2\}$,
если $p_1 = p_2$ и $m_1 < m_2$ ($r_1 = r_2$ и $n_1 < n_2$), причем в истории $H_p$
($H_r$) нет события $\{p_3,\,r_3,\,m_3,\,n_3\}$ такого, что $p_1 = p_3$ и $m_1 < m_3
< m_2$ ($r_1 = r_3$ и $n_1 < n_3 < n_2$).

Транзитивное замыкание отношения $\ll_p (\ll_r)$ дает
отношение предшествования в истории
 $H_p$ ($H_r$), которое обозначим как $<_p (<_r)$.
Для этих отношений должны выполняться следующие аксиомы, которые их
связывают:
\begin{multline*}
\{ p,\,r,\,m_1,\,n_1 \} <_p \{ p,\,r,\,m_2,\,n_2 \} \equiv\\
\equiv \{ p,\,r,\,m_1,\,n_1 \} <_r  \{ p,\,r,\,m_2,\,n_2\}\,;
\end{multline*}
\vspace*{-18pt}
\begin{align*}
&\{p_1, r_1, m_1, n_1\} <_p \{ p_1,\,r_2,\,m_2,\,n_2 \} <_r\\
&<_r \{p_2, r_2, m_3, n_3 \} <_p  \{ p_2,\,r_1,\,m_4,\,n_4\} \supset\\
&\ \ \ \ \ \ \ \ \ \ \supset \{p_1,\,r_1,\,m_1,\,n_1\} <_r \{p_2,\,r_1,\,m_4,\,n_4\}\,;
\end{align*}
\vspace*{-18pt}
%\noindent
%\vspace*{-18pt}
\begin{align*}
&\{p_1,\,r_1,\,m_1,\,n_1\} <_r \{p_2,\,r_1,\,m_2,\,n_2\} <_p\\
&<_p \{p_2,\,r_2,\,m_3,\,n_3\} <_r
\{p_1,\,r_2,\,m_4,\,n_4\} \supset\\
&\ \ \ \ \ \ \ \ \ \ \supset\{p_1,\,r_1,\,m_1,\,n_1\} <_p \{p_1,\,r_2,\,m_4,\,n_4\}\,.
\end{align*}

Введем отношение {\bfseries\textit{непосредственного пред\-шес\-т\-во\-ва\-ния 
событий}}, $\Rightarrow$, следующим образом (в обозначениях событий переменные 
$m$ и $n$ для краткости опущены): 

\vspace*{-12pt}
\begin{align*}
& \{p_1,\,r_1\} \Rightarrow \{p_2,\,r_2\} \stackrel{\mathrm{def}}{=}\\
 & \stackrel{\mathrm{def}}{=}
((p_1 = p_2) \& (\{p_1,\,r_1\}
\ll_p\{p_2,\,r_2\}))\lor\\
&\ \ \ \ \ \lor ((r_1 = r_2) \& (\{p_1,\,r_1\}
\ll_r\{p_2,\,r_2\}))\,.
\end{align*}

Будем рассматривать события как вершины направленного графа~$G$,
причем от вершины $V_1$ ведет дуга к вершине $V_2$, если и только если
$V_1\Rightarrow V_2$. Транзитивное замыкание отношения непосредственного
предшествования дает {\bfseries\textit{отношение предшествования событий}}, которое
мы обозначим через $\rightarrow$. Вершина $V_1$ предшествует вершине $V_2$,
если в графе существует путь, ведущий от $V_1$ к $V_2$. Граф~$G$ называется
{\bfseries\textit{графом выполнения приложения}}. Добавим теперь две аксиомы, которые
позволяют доказать, что граф~$G$ не может иметь циклов, то есть является
на\-прав\-лен\-ным ациклическим графом:
\begin{align*}
(\{p,\,r_i,\,m_1,\,n_1\} &\rightarrow \{p,\,r_j,\,m_2,\,n_2\}) \supset (m_1 <  m_2)\,,\\
(\{p_i,\,r,\,m_1,\,n_1\}&\rightarrow \{p_j,\,r,\,m_2,\,n_2\}) \supset (n_1 <  n_2)\,.
\end{align*}

Подграф $G^\prime$ графа $G$ называется {\bfseries\textit{правильным}}, если 
вместе с любой вершиной $V$ подграфа~$G^\prime$ он содержит также все 
вершины~$V^\prime$ графа~$G$ такие, что $V^\prime \rightarrow V$ в графе~$G$. 
Отношение $\rightarrow$ описывает час\-тич\-но упорядоченное время в 
многопроцессорном узле, процессы которого ведут себя недетерминировано. Укажем 
на имеющий место {\bfseries\textit{дуализм}} процессов и ресурсов: все аксиомы 
остаются верными, если всюду под процессами иметь в виду ресурсы, и наоборот.

Вершины графа, соответствующие событиям вида $\{\varnothing ,\,r\}$,
называются входными вершинами или описателями ресурсов. Входные
вершины не имеют предшествующих им вершин.

Два выполнения одного и того же приложения называются
{\bfseries\textit{одинаковыми}}, если их графы совпадают.

В отношении приложений предполагается, что они используют только
штатные средства операционной среды, в которой работают. В частности,
считается, что они не требуют внесения изменений в код операционной
системы и не применяют собственные примитивы синхронизации и
блокировки, которые могут быть построены, например, с помощью
ассемблерных программ, применяющих атомарные машинные инструкции.

Для того чтобы имелась возможность восстановить работу приложения после
отказа, необходимо заносить в стабильную память:
\begin{itemize}
\item контрольные точки для процессов, которые образуют
приложение, работающее на основном узле. Создание контрольных точек
нужно делать периодически;
\item информацию о получаемых от клиентов запросах и других
событиях, влияющих на детерминированность поведения приложения.
\end{itemize}

Это занесение, называемое далее протоколированием, нужно делать
динамически, по мере необходимости. Весь набор подобных записей
называется {\bfseries\textit{протоколом}}.

Протокол, находящийся в стабильной памяти, должен быть
достаточным для построения графа выполнения приложения после последней
контрольной точки. Если это условие выполняется, можно утверждать
следующее:
\begin{enumerate}[1.]
\item На основании последней контрольной точки перед отказом и графа
выполнения приложения от этой точки можно так организовать повторение
выполнения приложения, что граф этого повторного выполнения будет
совпадать с исходным графом.
\item Повторное выполнение приложения не может привести к тупиковым
ситуациям, когда выполняемые процессы блокируют друг друга.
\end{enumerate}

Алгоритмы ведения протоколов и повторного выполнения приложения
рассмотрены также в~\cite{9zak}.

%\vspace*{-6pt}

\section{Аппаратная и операционная платформы сервера
приложений}

Практическая реализация технологии прозрачной отказоустойчивости
основана на том, что аппаратная платформа сервера приложений должна
представлять собой многоузловой кластер. В нем все узлы, кроме одного,
являются основными, а один~--- резервным. Оперативная память резервного
узла выступает в роли стабильной памяти. На основных узлах происходит
оригинальное выполнение приложения.

В случае отказа одного из основных узлов на резервном узле происходит
восстановление состояния приложения, выполнявшегося на отказавшем узле,
после чего он берет на себя функции основного узла. Оперативная память
резервного узла может выступать в роли стабильной памяти, поскольку она: %\\[-14pt]
\begin{itemize}
\item сохраняет свое содержимое при отказе основного узла, на
котором происходило оригинальное исполнение; %\\[-16pt]
\item доступна при восстановлении, которое делается на резервном узле.
\end{itemize}
%\vspace*{-6pt}

Реализация предлагаемых методов подразумевает внесение некоторых
изменений в код ядра сис\-те\-мы, поэтому предполагается, что узлы кластера
работают под управлением UNIX-подобной сис\-те\-мы с открытым кодом.

\vspace*{-6pt}
\section{Состав приложения}
\vspace*{-2pt}

В начале работы приложения для него создается исходный процесс, например, с 
по\-мощью функции \emph{execve(\,)}. Далее приложение во время работы может 
создавать (открывать) новые ресурсы (процессы и потоки, файлы, трубы 
(\textit{pipes}), наборы семафоров, бинарные семафоры (\textit{mutex's}) 
и~т.д.) и закрывать (уничтожать) их. В каждый момент работы приложения оно 
состоит из множества открытых ресурсов, которое будет называться 
{\bfseries\textit{составом приложения}} в данный момент. В начале жизни 
приложения в его состав входит один единственный ресурс~--- исходный процесс. 
Свойство ресурса <<принадлежать составу приложения>> является наследуемым, то 
есть каждый новый ресурс, созданный процессами или потоками, уже входящими в 
состав приложения, включается в этот состав, а уничтоженный~--- исключается из 
него. Заметим, что создаваемым ресурсом может быть новый процесс или поток.  
Предполагаем, что приложение не может использовать ресурсы, не входящие в его 
состав.

Для упрощения изложения будем считать, что на сервере приложений
функционирует только одно приложение, для которого должна быть
обеспечена отказоустойчивость. Оно называется {\bfseries\textit{контролируемым}}. Помимо
процессов и потоков контролируемого приложения на кластере могут
выполняться другие процессы, но они не должны влиять на работу
контролируемого приложения.

\section{Функциональные компоненты реализации прозрачной
отказоустойчивости}

В разработанной реализации восстановление состояния приложения после отказа и 
продолжение его работы реализуется компонентами, вы\-пол\-ня\-ющи\-ми следующие 
функции:
\begin{itemize}
\item перехват обращений к ресурсам;
\item определение, входит ли ресурс в состав контролируемого приложения;
\item виртуализация операционной среды (VIRT-компонент);
\item создание контрольных точек состояния состава приложения и восстановление на
основании сохраненной контрольной точки аналогичного состояния на другом экземпляре
операционной среды, включая состояния всех ресурсов, файловой сис\-те\-мы и~т.д.
(SNAPSHOT/RESTORE-ком\-по\-нент);
\item отказоустойчивая реализация протокола TCP (FT-TCP-компонент);
\item протоколирование работы приложения (LOGGING-компонент);
\item повторение работы приложения, начинающееся с последней контрольной точки
(REPLAY-компонент);
\item драйвер стабильной памяти.
\end{itemize}

Перечисленные компоненты можно разделить на две группы:
\begin{enumerate}[1.]
\item Компоненты, реализация которых в той или иной степени известна из
литературы. Это VIRT-компонент~\cite{10zak, 11zak},
SNAPSHOT/ RESTORE-компонент~[4--6], драйвер стабильной
памяти~\cite{12zak}. В ряде случаев к ним предъявляются дополнительные
требования. Во-первых, необходимо минимизировать эти требования, и,
во-вторых, требуется их точно сформулировать.
\item Новые компоненты. Кроме обеспечения их функциональности
основное требование к таким компонентам~--- уменьшение накладных
расходов при оригинальном выполнении приложения. Поэтому рекомендуется
минимальная функциональность, реа\-ли\-зу\-емая на основном узле, даже если это
приводит к непропорциональному увеличению загрузки резервного узла или
увеличению времени восстановления после отказа.
\end{enumerate}

\vspace*{-6pt}
\section{Организация перехвата обращений к~ресурсам
и~определение принадлежности к~приложению}

Перехват обращений к ресурсам необходим, прежде всего, для того чтобы
осуществить протоколирование. Перехват нужен также в случае отказа основного 
узла для повторения работы приложения, начиная с последней контрольной точки.  
Способ перехвата зависит от типа ресурса и может осуществляться как в 
пользовательском режиме, так и в режиме ядра. При выборе режима следует 
учитывать два обстоятельства:
\begin{enumerate}[(1)]
\item системную реализацию обращений к ресурсу. Если при таком обращении всегда
делается вызов модуля ядра, перехват обычно можно делать в любом режиме. Так, например,
обстоит дело с процессами.  Если же обращения при некоторых условиях выполняются без
вызова ядра, перехват необходимо делать в пользовательском режиме;
{ %\looseness=1

}
\item необходимость блокировки занесения записей в протокол. Например,
если один процесс выполняет некоторую операцию над набором семафоров, может 
оказаться, что после этого другой процесс, который находился в ожидании, теперь 
может выполнить свою операцию над набором, которую до этого он выполнить не 
мог. Таким образом, в протоколе должны быть отмечены два события, причем первое 
предшествует второму. Возникают трудности, если этот порядок в протоколе не 
соблюдается. Чтобы этого избежать, требуются специальные средства блокировки, 
однако в ядре они могут присутствовать, хотя и для других целей. Именно так 
обстоит дело с наборами семафоров, поэтому перехват для них делается в режиме 
ядра. 
{%\looseness=-1

}
\end{enumerate}

Для реализации перехватов в ядре необходимо внести изменения в код ядра.
Например, чтобы организовать перехват обращений к наборам семафоров,
нужно внести изменения в модуль ядра, который ответствен за обслуживание
семафоров. Перехват обращений в пользовательском режиме не требует
внесения изменений в библиотечные функции, а делается с помощью
определенной <<надстройки>> над ними. О структуре программ перехвата,
которые содержат функции обращения к LOGGING-компоненту и
REPLAY-компоненту, будет сказано далее. Здесь отметим, что перехват
может выполняться как при обращении к ресурсу для выполнения операции,
так  и при обращении к нему с подтверждением, что намерение удовлетворено,
то есть операция выполнена.

Перехватив обращение к ресурсу, следует определить, входит ли он в состав
приложения, поскольку если нет, протоколировать такое обращение не нужно.
Аналогично и при повторном выполнении. Компонент определения основан на
том, что свойство принадлежности является {\bfseries\textit{на\-сле\-ду\-емым}}.
Относительно исходного процесса приложения его вхождение в состав должно
быть известно. Все ресурсы, созданные процессами, входящими в состав
приложения, считаются также входящими в этот состав. Учитывая, что
перехват обращений к ресурсам всегда выполняется в контексте
обращающегося процесса, при создании новых процессов (потоков) в их
контексте можно отметить, что новый процесс или поток также входит в состав
приложения. Если происходит обращение к ресурсу, отличному от создания
или уничтожения процесса (потока), то достаточно проверить, входит ли
обращающийся процесс (поток) в состав приложения. Если это так, он  может
обращаться только к ресурсам, входящим в этот состав.

\section{Создание контрольных точек}

Контрольная точка (или снимок) пред\-став\-ля\-ет данные, необходимые для
восстановления со\-сто\-яния всех процессов и ресурсов, входящих в состав
приложения, а также состояния операционной среды. Существенно, что
контрольная точка создается в тот момент, когда в истории каждого из
процессов уже наступило некоторое событие, но еще не наступило следующее
событие. Фактически это означает, что контрольная точка~--- это событие в
истории всех
 процессов, то есть такая вершина в графе выполнения, которой
непосредственно предшествуют события в историях всех процессов.

Контрольная точка включает для каждого процесса следующие элементы
(это далеко не полный список):
\begin{itemize}
\item его псевдоним (описание см.\ ниже);
\item его состояние с точки зрения операционной среды;
\item содержимое его адресного пространства;
\item характеристики всех его локальных ресурсов, которые открыты на
момент создания контрольной точки. В частности, это могут быть потоки
процесса, для каждого из которых требуется псевдоним, содержимое его стека,
аппаратных регистров, указателя текущей инструкции, состояние с точки
зрения операционной среды и~т.д.;
\item состояние открытых сокетов.
\end{itemize}

Контрольная точка для всего приложения в целом должна включать
следующие элементы (список также далек от полноты):
\begin{itemize}
\item псевдонимы и содержимое сегментов общей (межпроцессной) памяти;
\item состояние файловых систем целиком и отдельных файлов;
\item состояние наборов семафоров и их псевдонимы;
\item состояние межпроцессных труб.
\end{itemize}

При каждом создании контрольной точки она заносится в стабильную
память, замещая предыду\-щую контрольную точку. При восстановлении
выполняется обратная работа, то есть на основании последней созданной
контрольной точки воссоздаются процессы и их потоки, а также все их ресурсы
в том состоянии, в котором они находились в момент создания этой точки. В
частности, это касается со\-сто\-яния файловых систем и отдельных файлов, для
чего применяется специальная технология клонов.
{ %\looseness=1

}

\section{Протоколирование работы приложения между
контрольными точками}

При восстановлении приложения в другом экземпляре среды неизбежно
повторение работы приложения, начинающееся с последней контрольной
точки, достигнутой при работе приложения в оригинальной среде. Это
приводит к необходимости протоколировать при оригинальном выполнении
приложения факты обращений к некоторым ресурсам, то есть хранить историю
ресурсов, которую после размещения в стабильной памяти можно называть
{\bfseries\textit{журналом}}. Журнал~--- это именованный список последовательных
записей событий. Порядок записей в списке соответствует порядку наступления
событий в истории ресурса. Имена списков находятся во взаимно однозначном
соответствии с именами ресурсов.

Перехват обращения к ресурсу делается непосредственно после выполнения
операции над ресурсом, и в протокол заносятся сведения о факте выполнения
операции. Протоколирование начинается перед началом работы приложения, то
есть перед тем, как уже существующий процесс превращается в исходный
процесс приложения. Такое протоколирование и есть основное назначение
LOGGING-компонента.  Поскольку он работает в контексте процессов
приложения при оригинальном выполнении, эффективность его программной
реализации очень важна. Время выполнения отдельного запроса клиента
линейно зависит от времени работы LOGGING-компонента, связанного с
обслуживанием запроса.

Далее предполагается атомарность занесения записей в протокол, которая
означает выполнение следующих четырех предположений:
%\begin{itemize}

\medskip
\textbf{Предположение~1:} \textit{Занесение происходит \textbf{синхронно} с 
выполнением процесса. Например, это может означать, что драйвер стабильной 
памяти всегда работает в режиме сквозной записи (\textit{write-through});}

\medskip
\textbf{Предположение~2:} \textit{Занесение записи, соответствующей некоторому 
событию, происходит \textbf{своевременно}, то есть до того, как для этого же 
ресурса может наступить следующее событие;}

\medskip
\textbf{Предположение~3:} \textit{Занесение происходит \textbf{неделимо}, то 
есть если в момент занесения происходит отказ, в протоколе запись либо будет 
присутствовать, либо нет, промежуточного состояния протокола с искаженной 
записью быть не может;}

\medskip
\textbf{Предположение~4:} \textit{Занесение записи по некоторому событию 
происходит \textbf{одновременно} с событием, то есть если происходит отказ, то 
возможно лишь одно из двух: событие наступило, и о нем есть запись в протоколе; 
либо событие не наступило, и о нем нет записи в протоколе.}
%\end{itemize}

\medskip
Предположение~1 (о синхронности) может привести к значительному
увеличению времени ответа на запросы. Если в качестве стабильной
используется память резервного \mbox{узла}, синхронность означает, что процесс,
который затребовал занесения записи в протокол, приостанавливается до тех
пор, пока от резервного узла не будет получено подтверждение приема.
Поэтому время реакции системы
 на запрос клиента  увеличивается на
некоторую величину. Эта величина равна промежутку времени от момента
запроса на занесение записи в протокол до момента получения подтверждения.
Известен способ, позволяющий ослабить это предположение.

Предположение~2 (о своевременности). Наступление события,  например,
увеличение значения семафора некоторым процессом, может привести к тому,
что в другом процессе становится возможным наступление другого события
(уменьшение значения семафора). Своевременность гарантирует, что в
протоколе первое событие (увеличение значения) будет пред\-шес\-т\-во\-вать
второму событию (уменьшение значения).

Предположение~3 (о неделимости)~--- это требование к драйверу стабильной
памяти, реализация его не представляет особых трудностей.

Предположение~4 (об одновременности) явля\-ет\-ся очень жестким, однако
отказаться от него нелегко, а решение может зависеть от типа ресурса.

Возможно, что определенный выше протокол избыточен,  и в нем
зафиксированы лишние события. Уменьшение количества фиксируемых
событий  может опираться на знание причинно-следственных отношений
между событиями, а такое знание отсутствует.

Следствием принятых предположений является следующее утверждение.

\textbf{Теорема 1.} \textit{По журналам, находящимся в стабильной памяти,
однозначно вос\-ста\-нав\-ли\-ва\-ет\-ся граф, являющийся правильным подграфом
графа выполнения приложения, начиная с последнего сделанного снимка. }

\section{Повторение работы приложения}

Если на основном узле происходит отказ, на резервном узле выполняется
восстановление состояния приложения на момент создания контрольной точки.
Однако на основном узле после\linebreak со\-зда\-ния контрольной точки приложение еще
некоторое время работало, и эта работа зафиксирована в протоколе. Основное
назначение REPLAY-компонента~--- повторить выполнение приложения,
начиная с момента создания последней контрольной точки и до момента
возникновения отказа. Такое повторение преследует две цели:
\begin{enumerate}[(1)]
\item избежать недетерминированности при повторении работы, так как если поведение
приложения при повторении  отличается от поведения его же при работе на оригинале,
это может привести к различным результатам, доступ\-ным внешнему миру;
\item компенсировать то обстоятельство, что при повторении приложение не
может получить от внешнего мира сообщения,
 которые были получены при
оригинальном выполнении до возникновения отказа. Аналогично, во внешний
мир не могут быть посланы сообщения, которые уже были посланы при
оригинальном выполнении до момента возникновения отказа.
\end{enumerate}

REPLAY-компонент, располагая протоколом выполнения приложения после
создания последней контрольной точки, сначала строит граф выполнения
приложения. Это выполняется в режиме ядра. Далее начинается повторение.
Оно опирается на перехват обращений к ресурсам, причем в отличие от
LOGGING-компонента нужен пере\-хват как непосредственно \textbf{до}, так и
не\-по\-средст\-вен\-но \textbf{после}  выполнения операции над ресур\-сом. Получив
управление до выполнения операции, REPLAY-компонент проверяет,
действительно ли в соответствии с графом выполнения в истории ресурса
должно наступить соответствующее
 событие. Если это не так, процесс
переводится в со\-сто\-яние ожидания и пребывает в нем, пока это условие не
будет выполнено. Если же это так, процессу разрешается выполнить операцию
над ресурсом. Получив управление после операции, REPLAY-компонент
отмечает в графе выполнения, что событие наступило, и проверяет, не ожидает
ли какой-нибудь процесс этого события. Компонент отчасти работает в режиме
ядра, отчасти~--- в пользовательском режиме.

Помимо описанных выше основных функций, которые выполняются  после
отказа, REPLAY-компонент выполняет и некоторые вспомогательные функции.

Более формально алгоритм восстановления работы приложения после отказа
можно описать следующим образом.

Свяжем с журналом каждого ресурса $r$ натуральное число $n(r)$, равное номеру 
первой, еще не обработанной записи в журнале. Пусть $n(r) =0$, если журнал пуст 
или если обработаны все его записи (записи в журнале нумеруются, начиная с~1). 
Пусть $n$ таково, что $0 < n \leq N_r$ ($N_r$~--- число записей в журнале 
ресурса~$r$); определим функцию $p_r(n)$ равной идентификатору процесса из 
$n$-й записи жур\-нала. {\looseness=1

}

Используя снимок (контрольную точку), приведем приложение в состояние,
соответствующее этому снимку, после чего разрешим процессам приложения
выполняться. Когда процесс~$p$ проявляет намерение выполнить операцию над 
ресурсом~$r$, проверяется условие $(n(r)>0) \& (p_r(n(r)) =p)$. Если это 
условие не выполняется, процесс переводится в состояние ожидания. Если 
выполняется, процессу дается возможность выполнить операцию над ресурсом. Для 
выполнения операции могут потребоваться специфические данные (например, 
реальное время, полученное на узле-оригинале). Эти данные берутся из записи 
журнала. После того как процесс~$p$ выполнил операцию над ресурсом~$r$, 
следующим образом изменяется~$n(r)$:
$$
n(r) = (n(r) + 1)(\mathrm{mod}\;N_r  + 1)\,.
$$

Затем просматриваются все процессы, ранее переведенные в состояние
ожидания. Если среди них оказывается процесс, для которого выполняется
условие $(n(r)>0) \& (p_r(n(r)) = p)$, но уже с новым значением $n(r)$,  процесс
выводится из ожидания и ему предоставляется возможность выполнить
желаемую операцию. Если оказывается, что для всех $r$ $n(r)=0$, все
процессы, переведенные в ожидание, выводятся из этого состояния.
Восстановление работоспособного состояния приложения после этого
считается завершенным.

Сформулируем два утверждения, касающиеся правильности описанного
алгоритма.

\textbf{Теорема~2.} \textit{Выполнение приложения на основном узле после
изготовления последнего снимка, описанное журналами, находящимися в стабильной 
памяти, одинаково с выполнением приложения в соответствии с приведенным выше 
алгоритмом.}

Доказательство этого утверждения опирается на Теорему~1 и делается индукцией по 
суммарной длине всех журналов, находящихся в стабильной па\-мяти.

\textbf{Теорема~3.} \textit{Выполнение приложения в соответствии с приведенным 
выше алгоритмом не может привести к тупиковым ситуациям.}

Справедливость этого утверждения есть прямое следствие подразумеваемой
корректности приложения. Это  означает, что независимо от порядка
поступления клиентских запросов и времени, затрачиваемого процессами для
перехода от одного события к другому, приложение не должно зацикливаться
или попадать в тупиковую ситуацию.

\section{Некоторые вопросы программной реализации}

\subsection*{Протоколируемые события}

Набор протоколируемых событий и содержимое записей о каждом событии
должны быть  достаточными:
\begin{itemize}
\item для построения графа выполнения приложения;
\item для идентичного оригинальному повторного выполнения, начинающегося с
последней контрольной точки.
\end{itemize}

\subsection*{Классы учитываемых ресурсов}

Учитываемые ресурсы~--- это те ресурсы, рабо\-та с которыми может повлиять
на де\-тер\-ми\-нирован\-ность поведения приложения, а потому обращения к ним
следует протоколировать. Для многих приложений \textbf{классы ресурсов},
которые нужно учитывать, исчерпываются сле\-ду\-ющим списком:
\begin{itemize}
\item процессы (PROCESS);
\item потоки (POSIX\_THREAD);
\item бинарные семафоры~(POSIX\_MUTEX);
\item блокировки чтение--запись\newline (POSIX\_RWLOCK);
\item межпроцессные и внутрипроцессные семафоры (POSIX\_SEM);
\item наборы семафоров (SVR4\_SEM);
\item сокеты (SOCKET).
\end{itemize}

Кроме перечисленных классов в список включается также класс
{\bfseries\textit{административных}} ресурсов, который используется для служебных
целей.

\subsection*{Типы протоколируемых событий}

Операции над любыми ресурсами можно разделить на четыре группы~---
создание (открытие), чтение, изменение, уничтожение (закрытие). В составе
каждой из групп, в зависимости от класса ресурса, может быть нуль или
несколько операций. Однако для по\-стро\-ения графа выполнения важно деление
операций на три \textbf{типа}:
\begin{enumerate}[(1)]
\item создание (открытие) ресурса~--- CREATE;
\item уничтожение (закрытие) ресурса~--- DESTROY;
\item прочие операции~--- OTHER.
\end{enumerate}

Для повторного выполнения не требуется дальнейшая детализация операций
типа \mbox{OTHER}, однако включение конкретных операций в этот тип (то есть
необходимость их протоколирования) зависит от класса ресурса. Для ресурсов
некоторых классов могут отсутствовать операции типа CREATE и DESTROY.
Примером могут служить бинарные семафоры.

\subsection*{Запись протокола и ее состав}

{\bfseries\textit{Запись протокола}}~--- описание одного события в историях процесса и
ресурса. Это  событие  заключается в том, что процесс выполнил некоторую
операцию над ресурсом. Удобно, чтобы длина записи была кратна~8. Запись
заносится в протокол и содержит:
\begin{enumerate}[(1)]
\item класс ресурса;
\item псевдоним или системный идентификатор конкретного ресурса, над
которым выполнена операция. Выбор зависит от того, в каком режиме
происходит протоколирование;
\item тип операции;
\item псевдоним или системный идентификатор процесса, выполнившего
операцию;
\item системный идентификатор процесса;
\item признак, указывающий на необходимость выполнить синхронизацию
протокола;
\item необязательные дополнительные данные и их длина, например это
могут быть данные, полученные из сокета.
\end{enumerate}

Протоколирование создания процессов и потоков осуществляется в пользовательском 
режиме. Поэтому при создании процесса в по\-ле~4 записи вносится псевдоним 
процесса-родителя, в поле~2~--- псевдоним созданного процесса, а в поле~5~--- 
сис\-тем\-ный идентификатор этого процесса. Во всех остальных случаях (для 
других ресурсов или других событий) поле~5 не используется. При обращениях к 
любым ресурсам, отличающимся от процессов, если протоколирование происходит в 
пользовательском режиме, в поле~4 вносится псевдоним процесса, выполнившего 
операцию, а в поле~2~--- псевдоним ресурса, над которым выполнена операция.  
При протоколировании в режиме ядра в поле~4 вносится сис\-тем\-ный 
идентификатор процесса, а в поле~2~--- системный идентификатор ресурса.

Поле~5 необходимо, поскольку протоколирование обращений к ресурсам
разных классов может происходить в разных режимах. При этом в записях
могут появляться как псевдонимы, так и системные идентификаторы одного и
того же процесса. Поэтому, чтобы построить историю процесса в графе
выполнения, необходимо знать их соответствие.

\subsection*{Протоколирование}

При реализации протоколирования необходимо учитывать следующие
обстоятельства:
\begin{itemize}
\item занесение записей в протокол может выполняться и в пользовательском режиме, и в
режиме ядра;
\item должна быть предусмотрена возможность занесения в протокол записей и
фиксированной, и переменной длины. Для пере\-чис\-лен\-ных классов ресурсов только
обращение к сокетам требует использования записей переменной длины, поскольку,
например, запись о чтении из сокета должна сопровождаться прочитанными данными.
Однако частота протоколирования подобных записей обычно на несколько порядков меньше
частоты записей фиксированной длины;
\item размер записей фиксированной длины достаточно мал ($\sim 32$~байта), поэтому
для лучшего использования канала, свя\-зы\-ва\-юще\-го основной и резервный узлы, необходимо
объединение записей во {\bfseries\textit{фреймы}}, то есть блоки, длина которых близка к стандартной,
которую использует этот канал;
\item должна быть возможность синхронизации протокола. Это средство, позволяющее
приостановить процесс приложения до тех пор, пока от резервного узла не будет получено
уведомление, что указанная запись ему доставлена;
\item необходимо обеспечить минимальные накладные расходы при
протоколировании, поскольку оно выполняется в контексте процесса
приложения и от этих расходов линейно зависит время выполнения запроса
клиента.
\end{itemize}

\section{Заключение}

Рассмотрены некоторые вопросы реализации прозрачной
отказоустойчивости серверов при\-ло\-же\-ний. В отличие от наиболее час\-то
при\-ме\-ня\-емых методов использования специальных програм\-мных систем
сервером и приложениями предлагаемый подход позволяет работать со
стандартными серверными и
 клиентскими приложениями. Пред\-лага\-емая
тех\-нология  связана с внесением некоторых изменений в состав ядра
операционной системы. Поэтому эта технология может быть реализована либо
в кластерах, работающих под UNIX-подобными системами с открытым кодом,
либо самими организациями-разработчиками используемых операционных
систем.

Основная идея предлагаемой реализации прозрачной отказоустойчивости
приложений состоит в совместном использовании технологий виртуализации
операционной среды, протоколирования событий и создания контрольных
точек, восстановления работы серверного приложения после отказа на
основании последней контрольной точки и протокола. Каждая из этих
технологий не является новой, они применялись в различных системах, однако
пока неизвестны прозрачные отказоустойчивые сис\-те\-мы, построенные на
сумме этих технологий и обеспечивающие безотказную работу стандартных
серверных приложений.

При реализации предлагаемых методов важным аспектом является
минимизация накладных расходов,  связанных с дополнительными
вы\-чис\-литель\-ны\-ми работами, необходимыми для ор\-га\-низа\-ции
протоколирования  работы в стабильной памяти. Проведенные исследования
показали причины основных накладных расходов и позволили
конкретизировать направление исследований с \mbox{целью} их снижения.
По-видимому, накладные расходы удастся довести до 30\% или немного меньшей
величины. Такую потерю производительности можно рассматривать как
приемлемую плату за реализацию важной функцио\-нальности по обеспечению
прозрачной отказоустойчивости сервера приложений. Целью продолжающейся
работы является разработка эффективных алгоритмов и их программная
\mbox{реализация}, позволяющая достичь именно таких параметров.

{\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{99}
\bibitem{1zak}
{\sf
http://www.netapp.com/products/filer/index.html} (сайт фирмы Network Appliance Inc.).
\bibitem{2zak}
\Au{Pfister~G.}
In search of clusters: The coming battle in lowly parallel computing. N.Y.:
Prentice Hall, 1995.
\bibitem{3zak}
MPI-2: Extensions to the Message-Passing Interface.
{\sf http://www.mpi-forum.org/docs/mpi-20-html/mpi2-report.html}.

\bibitem{4zak}
\Au{Zhong~H., Nieh~J.}
CRAK: Linux checkpoint/restart as a kernel module. Technical Report
CUCS-014-01. Department of Computer Science, Columbia University,
2001.

\bibitem{5zak}
\Au{Roman~E.}
A survey of checkpoint/restart implementations. Berkeley: Lawrence Berkeley
National Laboratory Technical Report (publication LBNL-54942), 2003.

\bibitem{6zak}
\Au{Sankaran~S., Squyres~J.\,M., Barrett~B., Lumsdaine~A., Duell~J.,
Hargrove~P., Roman~E.}
The LAM/MPI checkpoint/restart framework: System-initiated checkpointing.
LACSI Symposium, Sante Fe, New Mexico, USA, 2003.

\bibitem{7zak}
\Au{Захаров~В.\,Н., Козмидиади~В.\,А.}
Реализация отказоустойчивости серверов приложений~// Системы высокой
доступности, 2006. Т.\,2. №\,2. С.~56--62.

\bibitem{8zak}
\Au{Lamport~L.}
Time, clocks, and the ordering of events in a distributed system~//
Communications of the ACM, 1978. V.~21. №\,7. P.~558--565.

\bibitem{9zak}
\Au{Захаров~В.\,Н., Козмидиади~В.\,А., Кузьмин~А.\,В.}
Описание недетерминированного поведения и проблема отказоустойчивости
приложений~// В кн.: Системы и средства информатики~/ Под. ред. И.А.
Соколова. М.: Наука, 2005.  Вып.~15. С.~338--358.

\bibitem{10zak}
\Au{King~S.\,T., Dunlap~G.\,W., Chen~P.\,M.}
Operating system support for virtual machines.
2003 Annual USENIX Technical Conference Proceedings, 2003.

\bibitem{11zak}
\Au{Захаров~В.\,Н.}
Виртуализация как информационная технология~// В кн.: Системы и
средства информатики: Спец. вып. Научно-методологические проблемы
информатики~/ Под ред. К.\,К.~Колина. М.: ИПИ РАН, 2006. С.~279--298.

\bibitem{12zak}
\Au{Maloy~J.}
Transparent inter process communication (TIPC). {\sf
http://tipc.sourceforge.net/}.
\end{thebibliography}

}
}

\end{multicols}


%\vskip 24pt plus 9pt minus 6pt

%\def\tit{MEANS PROVIDING APPLICATIONS FAULT TOLERANCE}
%\def\aut{V.~Zakharov$^1$ and V.~Kozmidiady$^2$}
%\titel{\tit}{\aut}{\autkol}{\titkol}
%
%\footnotetext{$^1$IPI RAN, vzakharov@ipiran.ru}
%\footnotetext{$^2$IPI RAN, kozmidiady\_v@tochka.ru}
%
%
%\Abst{The problems of fault tolerant servers creation, caused by
%nondeterminate applications behavior, are considered. A formal model based
%on resources and events and describing an application behavior is described.
%The algorithms of an application execution logging on the reserved cluster
%node, of restoring and continuation of running after main node fault are
%proposed. In proposed approach, both fault and recovery are hidden from
%the client who experiences slight service quality degradation at worst.}
%
%\KWN{applications server $\diamond$  transparent fault tolerance $\diamond$
%process $\diamond$  resource $\diamond$ event $\diamond$  check point
%$\diamond$  determinate}
%
%
\label{end\stat}
\renewcommand{\figurename}{\protect\bf Figure}

\def\stat{kalinichenko}


\def\tit{METHODS AND TOOLS FOR HYPOTHESIS-DRIVEN
RESEARCH SUPPORT: A~SURVEY$^*$}

\def\titkol{Methods and tools for hypothesis-driven
research support: A~survey}

\def\autkol{L.~Kalinichenko, D.~Kovalev, D.~Kovaleva,
and~O.~Malkov}

\def\aut{L.~Kalinichenko$^1$, D.~Kovalev$^1$, D.~Kovaleva$^2$,
and~O.~Malkov$^2$}

\titel{\tit}{\aut}{\autkol}{\titkol}

{\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1] {This work has been partially supported by the RFBR grants 13-07-00579
and 14-07-00548.}}

\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Institute of Informatics Problems, Russian Academy of Sciences, 44-2 Vavilov Str., Moscow 119333, Russian
Federation}
\footnotetext[2]{Institute of Astronomy, Russian Academy of Sciences, 48 Pyatnitskaya Str., Moscow 119017, Russian Federation}


%\vspace*{-12pt}

\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND APPLICATIONS\ \ \ 2015\ \ \ volume~9\ \ \ issue\ 1}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND APPLICATIONS\ \ \ 2015\ \ \ volume~9\ \ \ issue\ 1
\hfill \textbf{\thepage}}}


\Abste{Data intensive research (DIR) is being developed in frame of the new
paradigm of research study known as the Fourth paradigm, emphasizing an
increasing role of observational, experimental, and computer simulated data
practically in all research domains. The principal goal of DIR is an extraction
(inference) of knowledge from data.
  The intention of this work is to make an overview of the existing approaches,
methods, and infrastructures of the data analysis in DIR accentuating the role of
hypotheses in such process and efficient support of hypothesis formation,
evaluation, and selection in course of the natural phenomena modeling and
experiments carrying out.  An introduction into various concepts, methods, and
tools intended for effective organization of hypothesis-driven experiments in DIR
is presented.}

\KWE{data intensive research; Fourth paradigm; hypotheses; models; theories;
hypothetico-deductive method; hypothesis testing; hypothesis lattice; Galaxy
model; connectome analysis; automated hypothesis generation}

\DOI{10.14357/19922264150104}

%\vspace*{6pt}


\vskip 12pt plus 9pt minus 6pt

      \thispagestyle{myheadings}

      \begin{multicols}{2}

                  \label{st\stat}


\section{Hypotheses, Theories, Models and~Laws in~Data Intensive Science}

  \noindent
  Data intensive research is being developed in accordance with the Fourth
Paradigm~[1] of research study (following three previous historical paradigms of the
science development (empirical science, theoretical science,
and computational science))
emphasizing that science as a~whole is becoming increasingly dependent on data as
the core source for discovery. Emerging of the Fourth Paradigm is motivated by the huge
amount of data coming from scientific instruments, sensors, simulations, as well as
from people accumulating data in Web or social nets. The basic objective of DIR is to
infer knowledge from the integrated data organized in networked infrastructures
(such as warehouses, grids, clouds). At the same time, ``Big Data'' movement has
emerged as a~recognition of the increased significance of massive data in various
domains. Open access to large volumes of data, therefore, becomes a~key prerequisite
for discoveries in the XXI~century. Data intensive research denotes a~crosscut of
DIR/IT areas aimed at the creation of effective data analysis technologies for DIR
covering scientific and other data intensive domains (including finance, economy,
social environment, business, etc.).
{\looseness=1

}

  Science endeavors to give a~meaningful description of the world of natural
phenomena using that are known as laws, hypotheses, and theories. Hypotheses,
theories,\linebreak
\begin{center}  %fig1
\vspace*{-6pt}
\mbox{%
 \epsfxsize=72.481mm
 \epsfbox{kal-1.eps}
 }

\vspace*{9pt}

\noindent
{{\figurename~1}\ \ \small{Multiple incarnations of hypotheses}}

\end{center}


\vspace*{6pt}


\noindent
 and laws in their essence have the same fundamental character (Fig.~1)~[2].


  \textit{A scientific hypothesis} is a~proposed explanation of a~phenomenon which
still has to be rigorously tested. In contrast, \textit{a~scientific theory} has undergone
extensive testing and is generally accepted to be the accurate explanation behind an
observation. A~\textit{scientific law} is a~proposition, which points out any such
orderliness or regularity in nature, \textit{the prevalence of an invariable association
between a~particular set of conditions and particular phenomena}. In the exact
sciences, laws can often be expressed in the form of mathematical relationships.
Hypotheses explain laws, and well-tested, corroborated hypotheses become theories
(see Fig.~1). At the same time, the laws do not cease to be laws, just because they did not
appear first as hypotheses and pass through the stage of theories.

  Though theories and laws are different kinds of knowledge, actually, they represent
different forms of the same knowledge construct. Laws are generalizations, principles,
or patterns in nature, and theories are the explanations of those generalizations.
However, classification expressed in Fig.~1 is subjective. Article~[3]
provides examples
showing that the differences between laws, hypotheses, and theories consist only in
that they stand at different levels in their claim for acceptance  depending on how
much empirical evidence is amassed. Therefore, there is no essential difference
between constructs used for expressing hypotheses, theories, and laws. Important role
of hypotheses in scientific research can scarcely be
 overestimated. In the edition of
M.~Poincar$\acute{\mbox{e}}$'s book~[4],
 it is stressed that \textit{without
hypotheses, there is no science}. Thus, it is not surprising that so much attention in the
scientific research and the respective publications is devoted to the methods for
hypothesis manipulation in experimenting and modeling of various phenomena
applying the means of informatics. The idea that the new approaches are needed that
can address both \textit{data-} and \textit{hypothesis-driven sciences} runs all through
this paper.  Such symbiosis alongside with the hypothesis-driven tradition of science
(``first hypothesize-then-experiment'') might cause wide application of another one
that is typified by ``first experiment-then-hypothesize'' mode of research. Often, the
``first experiment'' ordering in DIR is motivated by the necessity of analysis of the
existing massive data to generate a~hypothesis.



  In the course of the present study, paying attention to the issue of
  inductive and deductive
reasoning in hypothesis-driven sciences will be emphasized.  In Fig.~2,
such ways of
knowledge production are shown~[2]. Here, ``generalization'' means any subset of
hypotheses, theories, and laws and ``Evidence'' is any subset of all facts accumulated
in a~specific DIR.

  All researchers collect and interpret empirical evidence through the process called
\textit{induction}. This is a~technique by which individual pieces of evidence are
collected and examined until a~law is discovered or a~theory is invented. Frances
Bacon first formalized induction~\cite{5-kl}. The method of
(naive) induction (see Fig.~2), he suggested, is, in part, the
principal way by which humans traditionally have produced generalizations that
permit predictions. The problem with induction is that\linebreak

\begin{center}  %fig2
\vspace*{-3pt}
\mbox{%
 \epsfxsize=65.106mm
 \epsfbox{kal-2.eps}
 }

\vspace*{6pt}

\noindent
{{\figurename~2}\ \ \small{Enhanced knowledge production diagram}}

\end{center}


%\vspace*{9pt}



\addtocounter{figure}{2}


\noindent
 it is impossible to collect
all observations pertaining to a~given situation in all time~--- past, present, and future.

  The formulation of a~new law begins through induction as facts are heaped upon
other relevant facts. Deduction is useful in checking the validity of a~law. Figure~2
shows that a~valid law would permit the accurate prediction  of  facts  not  yet
known.  Also an \textit{abduction}~\cite{6-kl} is the process of validating a~given
hypothesis through reasoning by successive approximation. Under this principle, an
explanation is valid if it is the best possible explanation of a~set of known data.
Abductive validation is common practice in hypothesis formation in science.
Hypothesis related logic reasoning issues are considered in more details in section~3.

  In~\cite{4-kl}, the useful hypotheses of science are considered to be of two kinds:
  \begin{enumerate}[(1)]
\item the hypotheses which are valuable \textit{precisely} because they are either
verifiable or, else, refutable through a~definite appeal to the tests furnished by
experience; and
\item the hypotheses which, despite the fact that experience suggests them, are
valuable \textit{despite}, or even \textit{because}, of the fact that experience can
neither confirm nor refute them.
\end{enumerate}

  Aspects of science which are determined by the use of the hypotheses of the
second kind are considered in~\cite{4-kl}
as ``constituting an essential human way of viewing nature, an interpretation rather
than a~portrayal or a~prediction of the objective facts of nature, an adjustment of our
conceptions of things to the internal needs of our intelligence.'' According to
Poincar$\acute{\mbox{e}}$'s discussion, the central problem of the logic of
science becomes the problem of the relation between the two fundamentally distinct
kinds of hypotheses, i.\,e., between those which cannot be verified or refuted through
experience and those which can be empirically tested.

  The analysis in this paper will be focused mostly on the modeling of hypotheses of
the first kind, leaving issues of analysis of the relations between such two kinds of
hypotheses to further study.

  The rest of the paper is organized as follows.  Section~2 discusses the basic
concepts defining the role of hypotheses in the formation of scientific knowledge and
the respective organization of the scientific experiments. Approaches for hypothesis
formulation, logical reasoning, hypothesis modeling, and testing are briefly
introduced in section~3. In section~4,
 a~general overview of the basic facilities provided by
informatics for the hypothesis-driven experimentation scenarios, including conceptual
modeling, simulations, statistics and machine learning methods is given. In
section~5, several examples of organization of hypothesis-driven scientific
experiments are included. Concluding remarks summarize the discussion.

\section{Role of Hypotheses in~Scientific Experiments: Basic Principles}

  \noindent
  Normally, scientific hypotheses have the form of a~mathematical model.
Sometimes, one can also formulate them as existential statements, stating that some
particular instance of the phenomenon under examination has some characteristic and
causal explanations, which have the general form of universal statements, stating that
every instance of the phenomenon has a~particular characteristic (e.\,g., \textit{for all
x, if x is a~swan, then x is white}). Scientific hypothesis considered as a~declarative
statement identifies the predicted relationship (associative or causal) between two or
more variables (independent and dependent).  In causal relationship, a~change caused
by the independent variable is predicted in the dependent variable. Variables are more
commonly related in noncausal (associative) way~\cite{7-kl}.

  In experimental studies, the researcher manipulates the independent variable. The
dependent variable is often referred to as consequence or the presumed effect that
varies with a~change of the independent variable. The dependent variable is not
manipulated. It is observed and assumed to vary with changes in the independent
variable. Predictions are made from the independent variable to the dependent
variable. It is the dependent variable that the researcher is interested in understanding,
explaining, or predicting~\cite{7-kl}.

  In case when a~possible correlation or similar relation between variables is
investigated (such as, for example, whether a~proposed medication is effective in treating a~disease, that is, at least to some extent and for some patients), a~few cases in which
the tested remedy shows no effect do not falsify the hypothesis. Instead, statistical
tests are used to determine how likely it is that the overall effect would be observed if
no real relation as hypothesized exists. If that likelihood is sufficiently small, the
existence of a~relation may be assumed. In statistical hypothesis testing, two
hypotheses are compared, which are called the \textit{null hypothesis} and the
\textit{alternative hypothesis}. The null hypothesis states that there is no relationship
between the phenomena (variables) whose relation is under investigation or, at least,
not of the form given by the alternative hypothesis. The alternative hypothesis, as the
name suggests, is the alternative to the null hypothesis: it states that there \textit{is}
some kind of relation.

  Alternative hypotheses are generally used more often than null hypotheses because
they are more desirable to state the researcher's expectations. But in any study that
involves statistical analysis, the underlying null hypothesis is usually
assumed~\cite{7-kl}. It is important that the conclusion ``do not reject the null
hypothesis'' does not necessarily mean that the null hypothesis is true. It suggests that
there is not sufficient evidence against the null hypothesis in favor of the alternative
hypothesis.  Rejecting the null hypothesis suggests that the alternative hypothesis
may be true.

  Any useful hypothesis will enable \textit{predictions by reasoning} (including
\textit{deductive reasoning}). It might predict the outcome of an experiment in a~laboratory setting or the observation of a~phenomenon in nature. The prediction may
also invoke statistics assuming that a~hypothesis must be
  \textit{falsifiable}~\cite{8-kl} and that one cannot regard a~proposition or theory
as scientific if it does not admit the possibility of being shown false. The way to
demarcate between hypotheses is to call \textit{scientific} those for which we can
specify (beforehand) one or more potential falsifiers as the respective experiments.
Falsification was supposed to proceed deductively instead of inductively.
{\looseness=1

}

  Other philosophers of science have rejected the criterion of falsifiability or
supplemented it with other criteria, such as verifiability (only statements about the
world that are empirically confirmable or logically necessary are cognitively
meaningful). They claim that science proceeds by ``induction''~--- that is, by finding
confirming instances of a~conjecture. Popper treated confirmation as never
certain~\cite{8-kl}. However, a~falsification can be sudden and definitive. Einstein
said: ``No amount of experimentation can ever prove me right; a~single experiment
can prove me wrong.'' To scientists and philosophers outside the Popperian
belief~\cite{8-kl}, science operates mainly by induction (confirmation), and also and
less often by disconfirmation (falsification). Its language is almost always one of
induction. For this survey both philosophical treatment of hypotheses are acceptable.
Sometimes such way of reasoning is called the \textit{hypothetico-deductive
method}. According to it, scientific inquiry proceeds by formulating a~hypothesis in a~form that could conceivably be falsified by a~test on observable data. A~test that
could and does run contrary to predictions of the hypothesis is taken as a~falsification
of the hypothesis. A~test that could but does not run contrary to the hypothesis
corroborates the theory.
{\looseness=1

}

  A scientific method involves experiment to test the ability of some hypothesis to
adequately answer the question under investigation. A~prediction enabled by
hypothesis suggests a~test (observation or experiment) for the hypothesis thus
becoming testable. If a~hypothesis does not generate any observational tests, there is
nothing that a~scientist can do with it.

  For example, not testable hypothesis: ``Our universe is surrounded by another,
larger universe, with which we can have absolutely no contact;'' not verifiable
(though testable) hypothesis: ``There are other inhabited planets in the universe;''
scientific hypothesis (both testable and verifiable):  ``Any two objects dropped from
the same height above the surface of the earth will hit the ground at the same time as
long as air resistance is not a~factor'' ({\sf
http://www.batesville.k12.in.us/physics/phynet/\linebreak aboutscience/hypotheses.html}).

  A \textit{problem} (\textit{research question}) should be formulated as an issue of
what relation exists between two or more variables. The problem statement should be
such as to imply possibilities of empirical testing; otherwise, this will not be a~scientific problem.
Problems and hypotheses being generalized relational statements
enable to deduce specific empirical manifestations implied by the problem and
hypotheses. In this process, hypotheses can be deduced from theory and from other
hypotheses. A~problem cannot be scientifically solved unless it is reduced to
hypothesis form, because a~problem is not directly testable~\cite{9-kl}.

  Most formal hypotheses connect concepts by specifying the expected relationships
between \textit{propositions}. When a~set of hypotheses are grouped together, they
become a~type of \textit{conceptual framework}. When a~conceptual framework is
complex and incorporates causality or explanation, it is generally referred to as a~\textit{theory}~\cite{10-kl}.  In general, hypotheses have to reflect the multivariate
complexity of the reality. A~scientific theory summarizes a~hypothesis or a~group of
hypotheses that have been supported with repeated testing. A~theory is valid as long
as there is no evidence to dispute it. \textit{Scientific paradigm} explains the working
set of theories under which science operates.

  Elements of hypothesis-driven research and their relationships are shown in
Fig.~3~\cite{12-kl, 11-kl}. The hypothesis triangle relations, \textit{explains},
\textit{formulates}, and \textit{represents}, are functional in the scientist's final decision in
adopting a~particular model $m_1$ to formulate a~hypothesis~$h_1$, which
is meant to explain phenomenon~$p_1$.

  In~\cite{11-kl}, the lattice structure for hypothesis interconnection is proposed as
shown in Fig.~4. A~hypothesis lattice is formed by considering a~set of hypotheses
equipped with \textit{wasDerivedFrom} as a~strict order (from the bottom to
the top). Hypotheses directly derived from exactly one hypothesis are \textit{atomic},
while those directly derived from at least two hypotheses are \textit{complex}.



  The hypothesis lattice is unfolded into model and phenomena isomorphic lattices
according to the hypothesis triangle (see Fig.~3)~\cite{11-kl}. The lattices are
isomorphic if one takes subsets of~$M$ (Model), $H$ (Hypotheses), and~$P$
(Phenomenon) such that \textit{formulates, explains, and represents} are both
  one-to-one and onto mappings (i.\,e., bijections), seen as structure-preserving
mappings (morphisms). Example of the isomorphic lattice is shown in
  Fig.~\ref{f5-kl}~\cite{11-kl}. This particular lattice corresponds to the case in
Computational Hemodynamics considered in~\cite{11-kl}. Here, model~$m_1$
formulates hypothesis~$h_1$, which explains phenomenon~$p_1$.
Similarly,  $m_2$ formulates~$h_2$, which explains~$p_2$, and so on. Prop-\linebreak\vspace*{-12pt}
\begin{center}  %fig3
\vspace*{-3pt}
 \mbox{%
 \epsfxsize=77.487mm
 \epsfbox{kal-3.eps}
 }

\vspace*{6pt}

\noindent
{{\figurename~3}\ \ \small{Elements of hypothesis-driven research}}

\end{center}


\vspace*{12pt}

\begin{center}  %fig4
\vspace*{-3pt}
\mbox{%
 \epsfxsize=74.477mm
 \epsfbox{kal-4.eps}
 }
 \end{center}

%\vspace*{6pt}

\noindent
{{\figurename~4}\ \ \small{A lattice theoretic representation for hypothesis relationship}}


\vspace*{16pt}



\addtocounter{figure}{2}


\noindent
erties of the
hypothesis lattices and operations over them are considered in~\cite{13-kl}.



  \textit{Models} are one of the principal instruments of modern science. Models can
perform two fundamentally different representational functions: a~model can be a~representation of a~selected part of the world, or a~model can represent a~theory in the
sense that it interprets the laws and hypotheses of that theory.

  Here, let consider scientific models to be representations in both senses at the same
time. One of the most perplexing questions in connection with models is how they
relate to theories. In this respect, models can be considered as a~complement to
theories, as preliminary theories, can be used as substitutions of theories when the
latter are too complicated to handle. Learning about the model is done through
experiments, thought experiments, and simulation. Given a~set of parameters, a~model
can generate expectations about how the system will behave in a~particular situation.
A~model and the hypotheses it is based upon are supported when the model
generates expectations that match the behavior of its real-world counterpart.

\pagebreak

\end{multicols}

\begin{figure} %fig5
\vspace*{1pt}
 \begin{center}
 \mbox{%
 \epsfxsize=155.928mm
 \epsfbox{kal-5.eps}
 }
 \end{center}
 \vspace*{-9pt}
\Caption{Hypothesis lattice unfolded into model and phenomenon isomorphic lattice}
\label{f5-kl}
\end{figure}

\begin{multicols}{2}

  A law generalizes a~body of observations. Generally, a~law represents a~group of
related undisputable hypotheses using a~handful of fundamental concepts and
equations to define the rules governing a~set of phenomena. A~law does not attempt
to explain why something happens~--- it simply states that it does.

  Facilities for support of the hypothesis-driven experimentation will be discussed in
the remaining sections.

\section{Hypothesis Manipulation in~Scientific Experiments}

\subsection{Hypothesis generation}

  \noindent
  Researchers that support rationality of scientific discovery presented several
methods for hypothesis generation, including discovery as abduction, induction,
anomaly detection, heuristics programming, and use of analogies~\cite{14-kl}.

  \textit{Discovery as abduction} characterizes reasoning processes that take place
before a~new hypothesis is justified. The abductive model of reasoning that leads to
plausible hypotheses formulation is conceptualized as an inference beginning with
data. According to~\cite{15-kl}, an abduction happens as follows:
\begin{enumerate}[(1)]
\item some
phenomena $p_1, p_2, p_3,\ldots$ are encountered for which there is no or little
explanation;
\item however,  $p_1, p_2, p_3,\ldots$ would not be surprising if a~hypothesis~$H$ were added. They would certainly follow from something like~$H$
and would be explained by it; and
\item therefore, there is a~good reason for elaborating
a~hypothesis~$H$~--- for proposing it as a~possible hypothesis from which the
assumption $p_1, p_2, p_3,\ldots$ might follow.
\end{enumerate}
 The abductive model of reasoning is
primarily a~process of explaining anomalies or surprising phenomena~\cite{16-kl}.
The scientists' reasoning proceeds abductively from an anomaly to an explanatory
hypothesis in light of which the phenomena would no longer be surprising. There can
be several different hypotheses that can serve as the explanations for phenomena; so,
additionally some criteria for choosing among different hypotheses are required.

  One way to implement abductive model of reasoning is the abductive logic
programming~\cite{17-kl}. Hypothesis generation in abduction logical framework is
organized as follows. During the experiment, some new observations are
encountered. Let~$B$ represents the background knowledge and $O$~is the set of facts
that represents observations. Both~$B$ and~$O$ are the logic programs (set of rules in
some rule language). In addition, $\Gamma$ stands for a~set of literals representing
the set of abducibles, which are candidate assumptions to be added to~$B$ for
explaining~$O$. Given~$B$, $O$, and~$\Gamma$, the hypothesis-generation
problem is to find a~set~$H$ of literals (called a~hypothesis) such that:
\begin{enumerate}[(1)]
\item $B$ and~$H$ entail~$O$;
\item $B$ and~$H$ are consistent; and
\item $H$ is some subset
of~$\Gamma$.
\end{enumerate}
  If all conditions are met, then~$H$ is an explanation of~$O$ (with
respect to~$B$ and~$\Gamma$).  Examples of abductive logic programming systems
include ACLP~\cite{18-kl}, A-system~\cite{19-kl}, ABDUAL~\cite{20-kl}, and
ProLogICA~\cite{21-kl}. Abductive logic programming can also be implemented by
means of Answer Set Programming systems, e.\,g., by the DLV system~\cite{22-kl}.

  The example abductive logic program in ProLogICA describes a~simple model of
the lactose metabolism of the bacterium E.Coli~\cite{21-kl}. The background
knowledge~$B$ describes that E.coli can feed on the sugar lactose if it makes two
enzymes permease and galactosidase. Like all enzymes (E), these are made if they are
coded by a~gene (G) that is expressed. These enzymes are coded by two genes (lac(y)
and lac(z)) in cluster of genes (lac(X)) called an operon that is expressed when the
amounts (amt) of glucose are low and lactose are high or when they are both at
medium level. The abducibles, $\Gamma$, declare all ground instances of the
predicates ``amount'' as assumable. This reflects the fact that in the model, it is not
known what are the amounts at any time of the various substances. This is incomplete
information that should be found out in each problem case that is examined. The
integrity constraints state that the amount of a~substance (S) can only take one value.
 {\small \begin{verbatim}
##  Background Knowledge (B)
feed(lactose):- make(permease),
   make(galactosidase).
make(Enzyme):- code(Gene,Enzyme),express(Gene).
express(lac(X)):-amount(glucose,low),
   amount(lactose,hi).
express(lac(X)):-amount(glucose,medium),
   amount(lactose,medium).
code(lac(y),permease).
code(lac(z),galactosidase).
temperature(low):-amount(glucose,low).
false :- amount(S,V1), amount(S,V2), V1 != V2.

##  Abducibles (Г)
abducible_predicate(amount).

## Observation (O)
feed(lactose).

This goal generates two possible hypotheses:
{amount(lactose,hi), amount(glucose,low)}
{amount(lactose,medium),amount(glucose,medium)}
\end{verbatim}
}

Below, just a~couple of another examples of real rule-based systems, where abductive
logic programming is used, are presented.
Robot Scientist (see subsection~4.4) abductively hypothesizes new
facts about the yeast functional biology by inferring what is missing from a~model~\cite{23-kl}. In~\cite{24-kl}, both abduction and induction are used to
formulate hypotheses about inhibition in metabolic pathways. Augmenting
background knowledge is done with abduction; after that, induction is used for
learning general rules.  Authors of~\cite{25-kl} use SOLAR reasoning system to
abductively generate hypotheses about the inhibitory effects of toxins on the rat
metabolisms.

  The process of discovery is deeply connected also with the search of
\textit{anomalies}. There are a~lot of methods and algorithms to discover anomalies.
Anomaly detection is an important research problem in data mining
aimed at search of the
objects that are considerably dissimilar, exceptional, and inconsistent with respect to
the majority data in an input database~\cite{26-kl}.

  \textit{Analogies} play several roles in science. Not only do they contribute to
discovery but they also play a~role in the development and evaluation of scientific
theories (new hypotheses) by analogical reasoning.

\subsection{Hypothesis evaluation}

  \noindent
  Being testable and falsifiable, a~scientific hypothesis provides a~solid basis to its
further modeling and testing. There are several ways to do it, including the use of
statistics, machine learning, and logic reasoning techniques.

\subsubsection{Statistical testing of hypotheses }

  \noindent
  The classical (frequentist) and Bayesian statistic approaches are applicable for
hypothesis testing and selection. Brief summary of the basic differences between
these approaches are as follows~\cite{27-kl}.

  Classical (frequentist) statistics is based on the following beliefs:
  \begin{itemize}
\item probabilities refer to relative frequencies of events. They are objective
properties of the real world;
\item parameters of hypotheses (models) are fixed, unknown constants. Because
they are not fluctuating, probability statements about parameters are meaningless; and
\item statistical procedures should have well-defined long-run frequency
properties.
\end{itemize}

  In contrast, Bayesian approach takes the following assumptions:
  \begin{itemize}
\item probability describes the degree of subjective belief, not the limiting
frequency. Probability statements can be made about things other than data,
including hypotheses (models) themselves as well as their parameters;
and
\item inferences about a~parameter are made by producing its probability
distribution~--- this distribution quantifies the uncertainty of our knowledge about
that parameter. Various point estimates, such as expectation value, may then be
readily extracted from this distribution.
\end{itemize}

  The Bayesian interpretation of probability can be seen as an extension
of propositional logic that enables reasoning with hypotheses, i.e.,
the propositions whose truth or falsity is uncertain.

  Bayesian probability belongs to the category of evidential probabilities; to evaluate
the probability of a~hypothesis, the Bayesian probabilist specifies some prior
probability, which is then updated in the light of new,
relevant data (evidence)~\cite{28-kl}. The Bayesian interpretation provides a~standard set of procedures and formulae to perform this calculation.

\vspace*{-6pt}

  \paragraph*{Hypothesis testing in classical statistic style.} After null and alternative
hypotheses are stated, some statistical assumptions about data samples should be
done, e.\,g., assumptions about statistical independence or distributions of observations.
Failure in providing correct assumptions leads to the invalid test results.

  A common problem in classical statistics is to ask whether a~given sample is
consistent with some hypothesis. For example, one might be interested in whether a~measured value~$x_i$, or the whole set $\{x_i\}$, is consistent with being drawn
from a~Gaussian distribution $N(\mu ,\sigma)$. Here, $N(\mu,\sigma$) is the
\textit{null hypothesis}.

  It is always assumed that we know how to compute the probability of a~given
outcome from the null hypothesis: for example, given the cumulative distribution
function, $0 \leq H_0(x) \leq 1$, the probability that we would get a~value at least as
large as $x_i$ is $p(x > x_i ) = 1 - H_0(x_i)$ and is called the $p$-\textit{value}.
Typically, a~threshold~$p$ value is adopted, called \textit{the significance
level}~$\alpha$, and the null hypothesis is rejected when $p\leq \alpha$ (e.\,g., if
$\alpha = 0.05$ and $p < 0.05$, the null hypothesis is rejected at a~0.05~significance
level). If one fails to reject a~hypothesis, it does not mean that
its correctness is proved
because it may be that the sample is simply not large enough to detect an effect.

  When performing these tests, one can meet with two types of errors, which
statisticians call \textit{Type~I and Type~II errors}. Type~I errors are
the cases when the
null hypothesis is true but incorrectly rejected. In the context of source detection,
these errors represent spurious sources or, more generally, false positives (with
respect to the alternative hypothesis). The false-positive probability when testing a~single datum is limited by the adopted significance level~$\alpha$. Cases when the
null hypothesis is false but it is not rejected are called Type~II errors (missed
sources, or false negatives (again, with respect to the alternative hypothesis)). The
false-negative probability when testing a~single datum is usually called~$\beta$ and
is related to \textit{the power of}~$\alpha$~\textit{test as} $(1 -\beta)$. Hypothesis
testing is intimately related to comparisons of distributions.

  As the significance level~$\alpha$ is decreased (the criterion for rejecting the null
hypothesis becomes more conservative), the number of false positives decreases and
the number of false negatives increases. Therefore, there is a~trade-off to be made to
find an optimal value of~$\alpha$, which depends on the relative importance of false
negatives and positives in a~particular problem. Both the acceptance of false
hypotheses and the rejection of true ones are errors that scientists should try to avoid.
There is discussion as to what states of affairs is less desirable; many people think
that the acceptance of a~false hypothesis is always worse than failure to accept a~true
one and that science should in the first place try to avoid the former kind of error.

  When many instances of hypothesis testing are performed, a~process called
\textit{multiple hypothesis testing}, the fraction of false positives can significantly
exceed the value of~$\alpha$. The fraction of false positives depends not only
on~$\alpha$ and the number of data points, but also on the number of true positives
(the latter is proportional to the number of instances when an alternative hypothesis is
true).

  Depending on data type (discrete vs.\ continuous random variables) and what one
can assume (or not) about the underlying distributions, and the specific question
one asks, different statistical tests can be used. The underlying idea of statistical tests is to
use data to compute an appropriate statistic and then compare the resulting
  data-based value to its expected distribution. The expected distribution is evaluated
by \textit{assuming that the null hypothesis is true}. When this expected distribution
implies that the data-based value is unlikely to have arisen from it by chance (i.\,e.,
the corresponding $p$ value is small), the null hypothesis is rejected with some
threshold probability~$\alpha$, typically 0.05 or 0.01 ($p<\alpha$). Note again
that~$p>\alpha$ does \textit{not} mean that the hypothesis is \textit{proven} to be
correct.

  The number of various statistical tests in the literature is overwhelming and their
applicability is often hard to decide (see~\cite{29-kl, 30-kl} for variety of statistical
methods in SPSS (Statical Package for the Social Sciences)).
When the distributions are not known, tests are called
nonparametric, or distribution-free tests. The most popular nonparametric test is the
Kolmogorov--Smirnov (K-S) test, which compares the cumulative distribution
function, $F (x)$, for two samples, $\{x_{1i}\}$, $i = 1,\ldots  , N_1$, and $\{x_{2i}\}$,
$i = 1,\ldots  , N_2$. The K-S test is not the only option for nonparametric comparison
of distributions. The Cram$\acute{\mbox{e}}$r\,--\,von Mises criterion, the Watson
test, and the Anderson--Darling test are similar in spirit to the K-S test, but consider
somewhat different statistics. The Mann--Whitney--Wilcoxon test (or the Wilcoxon
rank-sum test) is a~nonparametric test for testing whether two data sets are drawn
from distributions with different location parameters (if these distributions are known
to be Gaussian, the standard classical test is called the~$t$~test). A~few standard
statistical tests can be used when it is known, or can be assumed, that both~$h(x)$ and~$f(x)$
are the Gaussian distributions (e.\,g., the Anderson--Darling test, the Shapiro--Wilk
test)~\cite{27-kl}. More on statistical tests can be found
in~\cite{27-kl, 29-kl, 30-kl, 31-kl}.

\vspace*{-6pt}

  \paragraph*{Hypothesis (model) selection and testing in Bayesian style.} The
Bayesian approach can be thought of as formalizing the process of continually
refining our state of knowledge about the world, beginning with no data (as encoded
by the \textit{prior}), then updating that by multiplying in the likelihood once the
data  are observed to obtain the \textit{posterior}. When more data are taken, then the
posterior based on the first data set can be used as the prior for the second analysis.
Indeed, the data sets can be different.

  The question often arises as to which is the `best' model (hypothesis) to use;
`\textit{model selection}' is a~technique that can be used when we wish to
discriminate between competing models (hypotheses) and identify the best model
(hypothesis) in a~set, $\{M_1,\ldots , M_n\}$, given the data.

  Let remind the basic notation. The Bayes theorem can be applied to
calculate the posterior probability $p(M_j\vert d)$ for each model (or hypothesis) $M_j$
representing our state of knowledge about  the truth of the model (hypothesis) in the
light of the data~$d$ as follows:
  $$
  p(M_j\vert d) = p(d\vert M_j) \fr{p(M_j)}{p(d)}
  $$
  where $p(M_j)$ is the prior belief in the model (hypothesis) that represents our
state of knowledge (or ignorance) about the truth of the model (hypothesis) before
the  current data have been analyzed; $p(d\vert M_j)$ is the model
 (hypothesis)
\textit{likelihood} (represents the probability that some data are produced under the
assumption of this model);  and $p(d)$ is the normalization constant given by
  $$
  p(d) = \sum\limits_i p(d\vert M_i) p(M_i)\,.
  $$

  The relative `goodness' of models is given by a~comparison of their posterior
probabilities; so, to compare two models $M_a$ and~$M_b$, let look at the ratio of the
model posterior probabilities:
  $$
  \fr{p(M_a\vert d)}{p(M_b\vert d)} = \fr{ p(d\vert M_a)p(M_a)}{p(d\vert M_b)p(M_b)}\,.
  $$
  The Bayes factor, $B_{ab}$, can be computed as the ratio of the model likelihoods:
  $$
  B_{ab} = \fr{p(d\vert M_a)}{p(d\vert M_b)}\,.
  $$
  Empirical scale for evaluating the strength of evidence from the Bayes
factor~$B_{ij}$ between two models is shown in the table~\cite{32-kl}.

\vspace*{3pt}

%\begin{table*}
{\small
  \begin{center}


\begin{tabular}{ccc}
\multicolumn{3}{c}{Strength of evidence for Bayes factor $B_{ij}$ for two models}\\
&&\\[-6pt]
\hline
  $\vert \ln B_{ij}\vert$&Odds&Strength of evidence\\
  \hline
  $<$1.0\hphantom{$<$}&$< 3 : 1$&Inconclusive\\
  1.0&$\sim 3 : 1$&Weak evidence\\
  2.5&$\sim 12 : 1$\hphantom{9}&Moderate evidence\\
  5.0&$\sim150 : 1$\hphantom{99}&Strong evidence\\
  \hline
  \end{tabular}
  \end{center}}
  %  \end{table*}

  \vspace*{12pt}


\noindent


  The Bayes factor gives a~measure of the `goodness' of a~model regardless of the
prior belief about the model; the higher the Bayes factor, the better the model is. In
many cases, the prior belief in each model in the set of proposed models will be
equal; so, the Bayes factor will be equivalent to the ratio of the posterior probabilities
of the models. The `best' model in the Bayesian sense is the one which gives the best
fit to the data with the smallest parameter space.

  A special case of model (hypothesis) selection is \textit{Bayesian hypothesis
testing}~\cite{27-kl, 33-kl}. Taking $M_1$ to be the ``null'' hypothesis, one can ask
whether the data supports the alternative hypothesis~$M_2$, i.\,e., whether one
can reject the null hypothesis. Taking equal priors $p(M_1) = p(M_2)$, the odds
ratio is
  $$
  B_{21} =\fr{p(d\vert M_1)}{p(d\vert M_2)}\,.
  $$

  The inability to reject $M_1$ in the absence of an alternative hypothesis is very different from the hypothesis testing procedure in classical statistics. The latter procedure rejects the null hypothesis if it does not provide a~good description of the data, that is, when it is very unlikely that the given data could have been generated as
prescribed by the null hypothesis. In contrast, the Bayesian approach is
based on the posterior rather than on the data likelihood
and cannot reject a~hypothesis if there are no alternative
explanations for observed data~\cite{27-kl}.

  Comparing classical and Bayesian approaches~\cite{27-kl}, it is rare for a~mission-critical analysis be done in the ``fully Bayesian'' manner, i.\,e., without the
use of the frequentist tools at the various stages. Philosophy and
beauty aside, the reliability and efficiency of the underlying computations
required by the Bayesian
framework are the main practical issues. A~central technical issue at the
heart of this is that it is much easier to do optimization (reliably
and efficiently) in high dimensions than it is to do integration in high
dimensions.
Thus, the usable machine learning methods, while there are ongoing
efforts to adapt them to Bayesian framework, are almost all rooted in
frequentist methods.

  Most users of Bayesian estimation methods, in practice, are likely to use a~mix of Bayesian and frequentist tools. The reverse is also true~--- frequentist data analysts, even if they stay formally within the frequentist framework, are often influenced by
``Bayesian thinking,'' referring to ``priors'' and ``posteriors.'' The most advisable
position is probably to know both paradigms well, in order to make informed
judgments about which tools to apply in which situations~\cite{27-kl}.  More details on Bayesian style of hypothesis testing can be found
in~\cite{27-kl,  28-kl, 33-kl}.

\vspace*{-8pt}

\subsubsection{Logic-based hypothesis testing}

%\vspace*{-1pt}

\noindent
  According to the hypothetico-deductive approach, the hypotheses are tested by
deducing predictions or other empirical consequences from general theories. If these
predictions are verified by experiments, this supports the hypothesis. It should be noted that not
everything that is logically entailed by a~hypothesis can be confirmed by
a proper test for it. The relation between hypothesis and evidence is often
empirical rather than logical. A~clean deduction of empirical consequences from
a~hypothesis,
as it may sometimes exist in physics, is practically inapplicable in biology.
Thus, entailment of the evidence by hypotheses under test is neither sufficient
nor necessary
for a~good test. Inference to the best explanation is usually construed
as a~form of inductive inference (see abduction in subsection~3.1) where hypothesis' explanatory
credentials are taken to indicate its truth~[34].

  An inductive logic is a~system of evidential support that extends deductive
  logic to less-than-certain inferences.  For valid deductive arguments, the
  premises logically
entail the conclusion where the entailment means that the truth of the premises
provides a~guarantee of the truth of the conclusion. Similarly, in a~good
inductive argument, the premises should provide some degree of support for the conclusion,
where such support means that the truth of the premises indicates with some degree of strength that the conclusion is true. If the logic of good inductive arguments is to be of any real value, the measure of support it articulates should meet the Criterion of
Adequacy (CoA): as evidence accumulates, the degree to which the collection of true evidence statements comes to support a~hypothesis, as measured by the logic, should
tend to indicate that the hypotheses are probably false or probably true.
  In~\cite{35-kl}, the extent to which a~kind of logic based on the Bayes theorem
can estimate how the implications of hypotheses about evidence claims influences the degree to which hypotheses are supported is discussed in detail. In particular, it is
shown how such a~logic may be applied to satisfy the CoA: as evidence accumulates, false hypotheses will very probably come to have evidential support values (as
measured by their posterior probabilities) that approach~0; and as this happens, a~true hypothesis will very probably acquire evidential support values (measured by
their posterior probabilities) that approach~1.

\vspace*{-8pt}

\subsubsection{Parameter estimation }

%\vspace*{-1pt}

  \noindent
  Models (hypotheses) are typically described by parameters~$\theta$  whose
  values are to be estimated from data. The authors describe this process according
  to~\cite{27-kl}.
For a~particular model~$M$ and prior information~$I$, one gets:
  $$
  p(M, \theta\vert d, I) =
  \fr{p(d\vert M, \theta, I) p(M, \theta\vert I)}{p(d\vert I)}\,.
  $$
  The result $p(M, \theta\vert d, I)$ is called the \textit{posterior} probability density
function (pdf) for model~$M$ and parameters~$\theta$, given data~$d$ and other
prior information~$I$. This term is a~$(k + 1)$-dimensional pdf in the space spanned
by $k$~model parameters and the model~$M$. The term $p(d\vert M, \theta, I)$ is
the \textit{likelihood} of data \textit{given} some model~$M$ and some fixed values of parameters~$\theta$
describing it and all other prior information~$I$. The term
$p(M, \theta\vert I)$ is the \textit{a~priori} joint probability for model~$M$ and its
parameters~$\theta$ in the absence of any of the data used to compute likelihood
and is often simply called the \textit{prior}.

  In the Bayesian formalism, $p(M, \theta\vert d, I)$ corresponds to the state of our \textit{knowledge} (i.\,e., belief) about a~model and its parameters, given data~$d$. To simplify the notation, $M(\theta)$ will be substituted by~$M$ whenever the
absence of explicit dependence on~$\theta$ is not confusing.
A~completely Bayesian data analysis has the following conceptual steps.
  \begin{enumerate}[1.]
\item Formulation of the data likelihood $p(d\vert M, I)$.\\[-14pt]
\item Choice of the prior $p(\theta\vert M,I)$, which incorporates all other
knowledge that might exist, but is \textit{not} used when computing the likelihood
(e.\,g., prior measurements of the same type, different measurements, or simply an
uninformative prior). Several methods for constructing ``objective'' priors have
been proposed. One of them is the \textit{principle of maximum entropy} for
assigning uninformative priors by maximizing the entropy over a~suitable set of
pdfs, finding the distribution that is least informative (given the constraints).
Entropy maximization with no testable information takes place under a~single
constraint: the sum of the probabilities must be one. Under this constraint, the
maximum entropy for a~discrete probability distribution is given by the uniform
distribution.\\[-14pt]
\item Determination of the posterior $p(M\vert d, I)$, using Bayes theorem above.
In practice, this step can be computationally intensive for complex
multidimensional problems.\\[-14pt]
\item The search for the best model~$M$ parameters, which maximizes $p(M\vert
d, I)$, yielding the \textit{maximum a~posteriori} (MAP) estimate. This point
estimate is the natural analog to the \textit{maximum likelihood estimate} (MLE)
from classical statistics.\\[-14pt]
\item Quantification of uncertainty in parameter estimates, via \textit{credible
regions}. As in MLE, such an estimate can be obtained analytically by doing
mathematical derivations specific to the chosen model. The same as in MLE, various
numerical techniques can be used to simulate samples from the posterior. This can be viewed as an analogy to the frequentist approach, which can simulate draws of samples from the true underlying distribution of the data. In both cases, various descriptive statistics can then be computed on such samples to examine the
uncertainties surrounding the data and estimators of
model parameters based on that data.\\[-14pt]
\item Hypothesis testing as needed to make other conclusions about the model
(hypothesis) or parameter estimates.
\end{enumerate}

\vspace*{-6pt}

\subsection{Algorithmic generation and evaluation of~hypotheses}

\vspace*{-2pt}

\noindent
  Two cultures of data analysis (\textit{formulaic modeling}\footnote{In~\cite{36-kl},
instead of ``formulaic modeling,'' the term ``data modeling'' is used that looks misleading in the computer science
context.} and \textit{algorithmic modeling}) distinguished here in accordance
with~\cite{36-kl} can be applied to the hypothesis extraction and generation based on data.

  \textit{Formulaic modeling} is a~process for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables,
when the focus is on the formulae $y = f(x)$ that give a~relation specifying a~vector of dependent variables~$y$ in terms of a~vector of independent variables~$x$. In a~statistics experiment
(based on various regression techniques), the dependent variable
defines the event studied and is expected to change whenever the independent
variable (\textit{predictor} variables, extraneous variables) is altered.
Such methods as linear regression, logistic regression, and multiple regression are
the well-known examples of the representatives of this modeling approach.

%\pagebreak

  In the \textit{algorithmic modeling} culture, the approach is to find an algorithm
that operates on~$x$ to predict the responses~$y$. What is observed is a~set of $x$'s that go in and a~subsequent set of $y$'s that come out. Predictive accuracy and properties of the algorithms (such as,
for example, their convergence if they are iterative) are the issues to be investigated. \textit{Machine learning algorithms} focus on prediction, based on known properties learned from the training data. Such machine learning algorithms as decision tree, association rule, neural networks, support vector
machines as well as other techniques of learning in Bayesian and probabilistic
models~\cite{38-kl, 37-kl} are examples of the methods that belong to this second
culture.

  The models that best emulate the nature in terms of predictive accuracy are also the
most complex and inscrutable. Nature forms the outputs~$y$
from the inputs~$x$ by means of a~black box with complex and unknown interior.
Current accurate
prediction methods are also \textit{complex black boxes} (such as neural nets,
forests, support vectors). So, we are facing two black boxes, where ours seem
only slightly less inscrutable than nature's~\cite{36-kl}. In a~choice
between \textit{accuracy} and
\textit{interpretability}, in applications, people sometimes prefer interpretability.

  However, the goal of a~model is not interpretability (a~way of getting information), but getting useful, accurate information about the relation between the response and
predictor variables. It is stated in~\cite{36-kl} that algorithmic models can
give better predictive accuracy than formulaic models, providing also better
information about the underlying mechanism. And actually, this is what the goal of statistical analysis is.
The researchers should be focused on solving the problems instead of asking
what regression model they can create.

  An objection to this idea (expressed by Cox) is that prediction without some understanding of underlying process and linking with other sources of information
becomes more and more tentative. Due to that, it is suggested to construct the
stochastic calculation models that summarize the understanding of the phenomena under study. One of the objectives of such approach might be an understanding and
test of hypotheses about underlying process. Given the relatively small sample size,
following such direction could be productive. But data characteristics are rapidly changing. In many of the most interesting current problems, the idea of starting with
a~formal model is not tenable. The methods used in statistics for small
sample sizes and a~small number of variables are not applicable.
Data analytics need to be more
pragmatic. Given a~statistical problem, find a~good solution, whether
it is a~formulaic model, an algorithmic model, or a~Bayesian model
 or a~completely different approach.
 {\looseness=-1

 }

  In the context of the hypothesis-driven analysis, one should pay attention to the question how far can we go applying the algorithmic modeling for hypothesis
generation and testing. Various approaches to machine learning use related to
hypothesis formation and selection can be found
in~\cite{27-kl, 36-kl, 37-kl}.

  Besides machine learning, an interesting example of algorithmic generation of hypotheses can be found in the IBM Watson project~\cite{39-kl} where the
symbiosis of the  general-purpose reusable natural language processing (NLP) and knowledge representation and reasoning (KRR) technologies (under the name
DeepQA) is exploited for answering arbitrary questions over the existing natural language documents as well as structured data resources. Hypothesis generation takes the results of question analysis and produces candidate answers by searching the available data sources and extracting answer-sized snippets from the search results.
Each candidate answer plugged back into the question is considered a~hypothesis, which the system has to prove correct with some degree of confidence. After
merging, the system must rank the hypotheses and estimate confidence based on their merged scores.
A~machine-learning approach adopted is based on running the system over a~set of training questions with known answers and training a~model based on the scores. An important consideration in dealing with NLP-based scorers is that the
features they produce may be quite sparse, and so, accurate confidence estimation requires the application of confidence-weighted learning techniques~\cite{39-kl}~---
a~new class of online learning methods that maintain a~probabilistic measure of confidence in each parameter. It is important to note that instead of statistics based hypothesis testing, contextual evaluation of a~wide range of loosely coupled
probabilistic question and semantic based content analytics is applied for scoring different questions (hypotheses) and content interpretations. Training different
models on different portions of the data in parallel and combining the learned classifiers into a~single classifier
allow to make the process applicable to the large collections of data. More details on that can be found
in~\cite{39-kl, 40-kl} as well as in other Watson project related publications.
{\looseness=1

}

\subsection{Bayesian motivation for discovery }

\noindent
  One way for discriminating between competing models of some phenomenon is to use Bayesian model selection approach
  (see paragraph~3.2.1), the Bayesian evidences for each of
the proposed models (hypotheses) can be computed and the models can then be
ranked by their Bayesian evidence. This is a~good method for identifying which is the best model in a~given set of models, but it gives no indication of the \textit{absolute
goodness} of the model. Bayesian model selection says nothing about the
\textit{overall quality} of the set of models (hypotheses) as a~whole~---
the best model in the set may merely be the best of in a~set of poor models.
Knowing that the best model in the current set of models is not particularly
good model would provide \textit{motivation to search for a~better model} and,
hence, may lead to model discovery.

  One way of assigning some measure of the absolute goodness of a~model is
  to use the concept of Bayesian doubt first introduced in~\cite{41-kl}. Bayesian doubt
works by comparing all the known models in a~set with an idealized model, which acts as a~benchmark model.

  An application of the Bayesian doubt method for the cosmological model building
  is given in~\cite{32-kl, 42-kl}. One of the most important questions in cosmology is to identify the fundamental model underpinning the vast amount of observations nowadays available. The so-called `cosmological concordance model' is based on the cosmological principle (i.\,e., the Universe is isotropic and homogeneous, at least on large enough scales) and on the hot big bang scenario, complemented by an inflationary epoch. This remarkably simple model is able to explain with only half a~dozen free parameter observations spanning
  a~huge range of time and length-scales.
Since both a~cold dark matter (CDM) and a~cosmological constant ($\Lambda$)
component are required to fit the data, the concordance model is often referred
to as `the $\Lambda$CDM model.'

   Several different types of explanation are possible for the apparent late time
acceleration of the Universe, including different classes of dark energy model such as $\Lambda$CDM, $w$CDM; theories of modified gravity; void models or the back reaction~\cite{32-kl}. The methodology of Bayesian doubt which gives an absolute measure of the degree of goodness of a~model has been applied to the issue of
whether the $\Lambda$CDM model should be doubted.

  The methodology of Bayesian doubt dictates that an unknown idealized
model~$X$ should be introduced against which the other models may be compared.
Following~\cite{41-kl}, `doubt' may be defined as the posterior probability of the unknown model:
  $$
  D \equiv  p(X\vert d) = \fr{p(d\vert X)p(X)}{p(d)}\,.
  $$
  Here, $p(X)$ is the prior doubt, i.\,e., the prior on the unknown model, which represents the degree of belief that the list of known models does not contain the true model. The sum of  all the model
  priors must be unity.

  The methodology of Bayesian doubt requires a~baseline model
  (the best model in the set of known models), for which, in this application,
  the $\Lambda$CDM has been chosen. The average Bayes factor
  between $\Lambda$CDM and each of the known models is given by:
  $$
  \langle B_{i\Lambda}\rangle \equiv \fr{1}{N} \sum\limits_{i=1}^N
B_{i\Lambda}\,.
  $$

  The ratio $R$ between the posterior doubt and prior doubt,
  which is called the relative change in doubt, is:
  $$
  R\equiv \fr{D}{p(X)}\,.
  $$

  For doubt to grow, i.\,e., the posterior doubt to be
  greater than the prior doubt ($R\ll 1$), the Bayes factor between
  the unknown model~$X$ and the baseline model must be much greater
  than the average Bayes factor:
  $$
  \fr{\langle B_{i\Lambda}\rangle}{BX\Lambda} \ll 1\,.
  $$

  To genuinely doubt the baseline model, $\Lambda$CDM, it is not sufficient that
$R > 1$, but additionally, the probability of $\Lambda$CDM must also decrease
such that its posterior probability is greater than its prior probability, i.\,e.,
$p(\Lambda \vert d) < p(\Lambda)$. One can define:
  $$
  R_\Lambda \equiv \fr{p(\Lambda\vert d)}{p(\Lambda)}\,.
  $$

  For $\Lambda$CDM to be doubted, the following two conditions must be fulfilled:
  $$
  R>1\,;\quad R_\lambda <1\,.
  $$
  If these two conditions are fulfilled, then it suggests that the set of
  known models is incomplete, and gives motivation to search for a~better
  model not yet included, which may lead to model discovery.

  In~\cite{41-kl}, a~way of computing an absolute upper bound for $p(d\vert X)$
achievable among the class of known models  has been proposed. Finally, it was
found that current cosmic microwave background (CMB), matter power spectrum
(mpk), and Type~Ia supernovae (SNIa) observations do not require the introduction of
an alternative model to the baseline $\Lambda$CDM model. The upper bound of the Bayesian evidence for a~presently unknown dark energy model against
$\Lambda$CDM gives only weak evidence in favor of the unknown model. Since
this is an absolute upper bound, it was concluded that $\Lambda$CDM remains a~sufficient phenomenological description of currently available observations.

\section{Facilities for~the~Scientific Hypothesis-Driven Experiment Support}
\subsection{Conceptualization of~scientific experiments}

  \noindent
  Data intensive research increasingly becomes dependent on computational resources to aid complex
researches. It becomes paramount to offer scientists mechanisms to manage the
variety of knowledge produced during such investigations. Specific conceptual
modeling facilities~\cite{43-kl} are investigated to allow scientists to represent
scientific hypotheses, models, and associated computational or simulation
interpretations which can be compared against phenomena observations (see Fig.~3). The model allows scientists to record the existing knowledge about an observable
investigated phenomenon, including a~formal mathematical interpretation of it, if any.
Model evolution and model sharing need also to be supported taking either a~mathematical or computational view (e.\,g., expressed by scientific workflows).
Declarative representation of scientific model allows scientists to concentrate on the
scientific issues to be investigated. Hypotheses can be used also to bridge the gap between an ontological description of studied phenomena and the simulations.
Conceptual views on scientific domain entities allow for searching for definitions supporting scientific models sharing among different scientific groups.

  In~\cite{11-kl}, the engineering of hypothesis as linked data is addressed.
A~semantic view on scientific hypotheses shows their existence apart from a~particular statement formulation in some mathematical framework. The mathematical
equation is considered as not enough to identify the hypothesis: first,
because it must be physically interpreted, and second, because there can be many
ways to formulate the  same hypothesis. The link to a~mathematical expression,
however, brings to the  hypothesis concept higher semantic precision.
Another link, in addition, to an explicit  description of the explained
phenomenon (emphasizing its ``physical interpretation'')  can bring forth the
intended meaning. By dealing with that hypothesis as a~conceptual  entity,
the scientists make it possible to change its statement formulation or even
to  assert a~semantic mapping to another incarnation of the hypothesis in
case someone  else reformulates it.

  In~\cite{43-kl}, the following elements related to hypothesis-driven science are
conceptualized: a~phenomenon observed, a~model interpreting this phenomenon, the
metadata defining the related computation together with the simulation definition
(for
simulation, a~declarative logic-based language is proposed).
In this work, specific attention is devoted to hypothesis definition.
The explanation, a~scientific hypothesis  conveys, is a~relationship between the causal phenomena and the simulated one,
namely, that the simulated phenomenon is caused by or produced under the
conditions set by the causal phenomena. By running the simulations defined by the antecedents in the causal relationship, the scientist aims at providing hypothetical  analysis of the studied phenomenon.

  Thus, the scientific hypothesis becomes an element of the scientific model that may replace a~phenomenon. When computing a~simulation based on a~scientific
hypothesis, i.\,e., according to the causal relationship it establishes, the output results
may be compared against phenomenon observations to assess the quality of the
hypothesis. Such interpretation provides for bridging the gap between qualitative
description of the phenomenon domain (scientific hypotheses may be used in
qualitative (i.\,e., ontological) assertions) and the corresponding quantitative
valuation obtained through simulations. According to the approach~\cite{43-kl},
complex scientific models can be expressed as the composition of computation
models similarly to database views.
{\looseness=1

}

\subsection{Hypothesis space browsers}

  \noindent
  In the HyBrow (Hypothesis Space Browser) project~\cite{44-kl}, the hypotheses
for the biology domain are represented as a~set of first-order predicate calculus
sentences. In conjunction with an axiom set specified as rules that model known
biological facts over the same universe and experimental data, the knowledge base
may contradict or validate some of the sentences in hypotheses, leaving the remaining
ones as candidates for new discovery. As more experimental data are obtained and
rules are identified, discoveries become positive facts or are contradicted. In the case of
contradictions, the rules that caused the problems must be identified and eliminated
from the theory formed by the hypotheses. In such model-theoretical approach, the
validation of hypotheses considers the satisfiability of the logical implications
defined in the model with respect to an interpretation. This might be applicable also
for simulation-based research, in which validation is solved based on the
quantitative analysis between the simulation results and the
  observations~\cite{43-kl}. HyBrow is based on an OWL ontology and application-
level rules to contradict or validate hypothetical statements. HyBrow provides for
designing hypotheses and evaluating them for consistency with existing knowledge
and uses an ontology of hypotheses to represent hypotheses in machine understandable
form as relations between objects (agents) and processes~\cite{45-kl}.

  As an upgrade of HyBrow, the HyQue~\cite{46-kl} framework adopts linked data
technologies and employs Bio2RDF linked data to add to HyBrow semantic
interoperability capabilities. HyBrow/HyQue's hypotheses are domain-specific
statements that correlate biological processes (seen as events) in the First-Order
Logic (FOL). Hypotheses are formulated as instances of the HyQue Hypothesis
Ontology and are evaluated through a~set of SPARQL queries against
  biologically-typed OWL and HyBrow data. The query results are scored in terms
of how the set of events correspond to background expectations. A score indicates the
level of support the data lend the hypothesis.  Each event is evaluated independently
in order to quantify the degree of support it provides for the hypothesis posed.
Hypothesis scores are linked as properties to the respective hypothesis.

  OBI (the Ontology for Biomedical Investigations) project
  ({\sf http://obi-ontology.org}) aims to model the design of an investigation: the
protocols, the instrumentation, and the materials used in experiments and the data
generated~\cite{47-kl}. Ontologies such as EXPO and OBI enable the recording of
the whole structure of scientific investigations: how and why an investigation was
executed, what conclusions were made, the basis for these conclusions, etc. As a~result of these generic ontology development efforts, the Minimum Information about
a Genotyping Experiment (MIGen) recommends the use of terms defined in OBI. The
use of a~generic or a~compliant ontology to supply terms will stimulate
  cross-disciplinary data-sharing and reuse. As much detail about an investigation as
possible in order to make the investigation more reproducible and reusable can be
collected~\cite{48-kl}.

  Hypothesis modeling is embedded into the knowledge infrastructures being
developed in various branches of science. One example of such infrastructure is
considered under the name SWAN~--- a~Semantic Web Application in
Neuromedicine~\cite{47-kl}. SWAN is a~project for developing an integrated
knowledge infrastructure for the Alzheimer disease (AD) research community.
SWAN incorporates the full biomedical research knowledge lifecycle in its
ontological model, including support for personal data organization, hypothesis
generation, experimentation, laboratory data organization, and digital prepublication
collaboration. The common ontology is specified in an RDF Schema. SWAN's
content is intended to cover all stages of the ``truth discovery'' process in biomedical
research, from formulation of questions and hypotheses to capture of experimental
data, sharing data with colleagues, and ultimately, the full discovery and publication
process.

  \begin{figure*}[b] %fig6
  \vspace*{1pt}
 \begin{center}
 \mbox{%
 \epsfxsize=160.208mm
 \epsfbox{kal-6.eps}
 }
 \end{center}
 \vspace*{-9pt}
  \Caption{Elements of the scientific hypothesis model}
  \label{f6-kl}
  \end{figure*}

  Several information categories created and managed in SWAN are defined as
subclasses of Assertion. They include Publication, Hypothesis, Claim, Concept,
Manuscript, DataSet, and Annotation. An Assertion may be made upon any other
Assertion, or upon any object specifiable by URL. For example, a~scientist can make
a Comment upon, or classify, the Hypothesis of another scientist. Linking to objects
``outside'' SWAN  by URL allows one to use SWAN as metadata to organize, for
example, all one's PDFs of publications, or the Excel files in which one's
laboratory data are stored, or all the websites of tools  relevant to Neuroscience.
Annotation may be structured or unstructured. Structured annotation means attaching
a Concept (tag or term) to an Assertion. Unstructured annotation means attaching free
text. Concepts are nodes in controlled vocabularies, which may also be hierarchical
(taxonomies).

\subsection{Scientific hypothesis formalization}

  \noindent
  An example showing in Fig.~\ref{f6-kl} the diversity of the components of a~scientific hypothesis model has been borrowed from the applications in Neuroscience
[43, 49] and in a~human cardiovascular system in Computational
Hemodynamics~\cite{11-kl, 50-kl}. The formalization of a~scientific hypothesis was
provided by a~mathematical model, by a~set of differential equations for continuous
processes, quantifying the variations of physical quantities in continuous space--time,
and by the mathematical solver (HEMOLAB) for discrete processes. The
mathematical equations were represented in MathML, enabling models interchange
and reuse.

  In~\cite{51-kl}, the formalism of quantitative process models is presented that
provides for encoding of scientific models formally as a~set of equations and
informally in terms of processes expressing those equations. The model revision
works as follows. For input, it is required an initial model; a~set of constraints
representing acceptable changes to the initial model in terms of processes; a~set of
generic processes that may be added to the initial model;
and observations to which the
revised model should fit. These data provide the approach with a~heuristics that guides
search toward parts of the model space that are consistent with the observations. The
algorithm generates a~set of revised models that are sorted by their distance from the
initial model and presented with their mean squared error on the training data. The
distance between a~revised model and the initial model is defined as the number of
processes that are present in one but not in the other. The abilities of the approach
have been successfully checked in several environmental domains.

  \begin{figure*}[b] %fig7
  \vspace*{1pt}
 \begin{center}
 \mbox{%
 \epsfxsize=150.042mm
 \epsfbox{kal-7.eps}
 }
 \end{center}
 \vspace*{-9pt}
  \Caption{Hypothesis-driven closed-loop learning}
  \label{f7-kl}
  \end{figure*}

  Formalisms for hypothesis formation are mostly monotonic and are considered to
be not quite suitable for knowledge representation, especially in dealing with
incomplete knowledge, which is often the case with respect to biochemical networks.
In~\cite{52-kl}, knowledge-based framework for the general problem of hypothesis
formation is presented. The framework has been implemented by extending
BioSigNet-RR~--- a~ knowledge-based system that supports elaboration tolerant
representation and nonmonotonic reasoning. The main features of the extended
system provide:
\begin{enumerate}[(1)]
\item seamless integration of hypothesis formation with knowledge
representation and reasoning;
\item use of various resources of biological data as well
as human expertise to intelligently generate hypotheses; and
\item support for ranking
hypotheses and for designing experiments to verify hypotheses.
\end{enumerate}
 The extended system
is positioned as a~prototype of an intelligent research assistant of molecular
biologists.

\subsection{Hypothesis-driven robots}

\noindent
  The Robot Scientist~\cite{53-kl} oriented on genomic applications is a~physically
implemented system which is capable of running cycles of scientific experimentation
and discovery in a~fully automatic manner: hypothesis formation, experiment
selection to test these hypotheses, experiment execution using robotic system, results
analysis and interpretation, repeating the cycle  (closed-loop in which the results
obtained are used for learning from them and feeding the resulting knowledge back
into the experimental models). Deduction, induction, and abduction are the types of
logical reasoning used in scientific discovery (see section~3). The full automation of
science requires `closed-loop learning,' where the computer not only analyses the
results, but learns from them and feeds the resulting knowledge back into the next
cycle of the process (Fig.~\ref{f7-kl}).

  In the Robot Scientist, the automated formation of hypotheses is based on the
following key components:
  \begin{enumerate}[(1)]
\item machine-computable representation of the domain knowledge;
\item abductive or inductive inference of novel hypotheses;
\item an algorithm for the selection of hypotheses; and
\item deduction of the experimental consequences of hypotheses.
\end{enumerate}

  Adam, the first Robot Scientist prototype, was designed to carry out microbial
growth experiments to study functional genomics in the yeast \textit{Saccharomyces
cerevisiae}, specifically to identify the genes encoding `locally orphan enzymes.'
Adam uses a~comprehensive logical model of yeast metabolism, coupled with a~bioinformatic database (Kyoto Encyclopaedia of Genes and Genomes~--- KEGG)
and standard bioinformatics homology search techniques (PSI-BLAST and FASTA)
to hypothesize likely candidate genes that may encode the locally orphan enzymes.
This hypothesis generation process is abductive.

  To formalize Adam's functional genomics experiments, the LABORS ontology
(LABoratory Ontology for Robot Scientists) has been developed. LABORS is a~version of the ontology EXPO (as an upper layer ontology) customized for Robot
scientists to describe biological knowledge. LABORS is expressed in OWL-DL.
LABORS defines various structural research units, e.\,g., trial, study, cycle of study
and replicate as well as design strategy, plate layout, expected actual results. The
respective concepts and relations in the functional genomics data and metadata are
also defined. Both LABORS and the corresponding database (used for storing the
instances of the classes) are translated into Datalog in order to use the SWI-Prolog
reasoner for required applications~\cite{48-kl}.

  There were two types of hypotheses generated. The first level links an orphan
enzyme, represented by its enzyme class (E.C.)\ number, to a~gene (ORF) that
potentially encodes it. This relation is expressed as a~two-place predicate where the
first argument is the ORF and the second is the E.C.\ number. An example of
hypothesis at this level is: \textit{encodesORFtoEC(`YBR166C', `1.1.1.25')}.

  The second level of hypothesis involves the association between a~specific strain,
referenced via the name of its missing ORF, and a~chemical compound which should
affect the growth of the strain, if added as a~nutrient to its environment. This level of
hypothesis is derived from the first by logical inference using a~specific model of
yeast metabolism. An example of such a~hypothesis is: \textit{affects
growth(`C00108',`YBR166C')}, where the first argument is the compound (names
according to KEGG) and the second argument is the  strain considered.

  Adam then designs the experimental assays required to test these hypotheses for
execution on the laboratory robotic system. These experiments are based on a~two-
factor design that compares multiple replicates of the strains with and without
metabolites compared against wild type strain controls with and without metabolites.



  Adam follows a~hypothetico-deductive methodology (see section~2). Adam
abductively hypothesizes new facts about yeast functional biology, then it deduces
the experimental consequences of these facts using its model of metabolism, which it
then experimentally tests. To select experiments, Adam takes into account the variable
cost of experiments, and the different probabilities of hypotheses. Adam chooses its
experiments to minimize the expected cost of eliminating all but one hypothesis.
This
is, in general, an~NP complete problem and Adam uses heuristics to find a~solution~\cite{45-kl}.

  It is now likely that the majority of hypotheses in biology are computer-generated.
Computers are increasingly automating the process of hypothesis formation, for
example: machine learning programs (based on induction) are used in chemistry to
help design drugs; and in biology, genome annotation is essentially a~vast process of
(abductive) hypothesis formation. Such computer-generated hypotheses have been
necessarily expressed in a~computationally amenable way, but it is still not common
practice to deposit them into a~public database and make them available for
processing by other applications~\cite{45-kl}.

  The details describing the software and informatics decisions in the Robot Scientist
project can be found in~\cite{45-kl, 53-kl} and online at the website {\sf
http://www.aber.ac. uk/compsci/Research/bio/robotsci/data/informatics/}. The details
for developing the formalization used for Adam's functional genomics investigations
can be found in~\cite{48-kl, 54-kl}. An ontology-based formalization based on graph
theory and logical modeling makes it possible to keep an accurate track of all the
result units used for different goals, while preserving the semantics of all the
experimental entities involved in all the investigations. It is shown how
experimentation and machine learning are used to identify additional knowledge to
improve the metabolic model~\cite{54-kl}.

\subsection{Hypotheses as~data in~probabilistic databases}

\noindent
  Another view of hypotheses encoding and management is presented
  in~\cite{55-kl}. Authors use probabilistic database techniques for hypotheses
systematic construction and management. MayBMS~\cite{56-kl}, a~probabilistic
database management system, is used as a~core for hypothesis management. This
methodology (called $\gamma$-DB) enables researchers to maintain several
hypotheses explaining some phenomena and provides evaluation mechanism based
on Bayesian approach to rank them.

  The construction of $\gamma$-DB database comprises several steps. In the first
step, phenomenon and hypothesis entities are provided as input to the system.
Hypothesis is a~set of mathematical equations expressed as functions in W3C
MathML-based format and is associated with one or more simulation trial dataset,
consisting of tuples with input variables of equation and its corresponding output as
functionally dependent variables (the predictions). Phenomenon is represented
by at least one empirical dataset similar to simulation trials. In the next step, the
system deals with hypotheses  and phenomena in the following way:
\begin{enumerate}[(1)]
\item researcher
has to provide some metadata about hypotheses and phenomena; e.\,g., hypotheses
need to be associated with the respective phenomena and assigned a~prior confidence
distribution (uniform by default according to the principle of maximum
entropy~(see paragraph~3.2.3));
\item functional dependencies (FD) are extracted from equations in
order to obtain database schema to store simulations and experimental data; it should
be mentioned that to precisely identify hypothesis formulation, the special attributes
for phenomena and hypothesis references are introduced into FD;
\item tuples are
synthesized from simulation trials and observational data by uncertain
  pseudotransitive closure and  reasoning; and finally,
  \item  the probabilistic
  $\gamma$-DB database is formed.
  \end{enumerate}

   Once phenomenon and hypothesis (with
empirical datasets and simulation trials) are produced, it becomes possible to
manipulate them with database tools.

  MayBMS provides tools to evaluate competing hypotheses for the explanation of a~single phenomenon. With prior probabilities already provided,
  the system allows to
make one or more (if new observational data appears) Bayesian inference steps. In
each step, the prior probability is updated to posterior according to Bayes' theorem.
As a~result, hypotheses which better explain phenomenon get higher probabilities
enabling researchers to make more confident decisions (see also
paragraph~3.2.1). The
  $\gamma$-DB approach provides a~promising way to analyze hypotheses in
  large-scale DIR as uncertain predictive database in face of empirical data.

  \vspace*{-6pt}

\section{Examples of~Hypothesis-Driven Scientific Research}

\vspace*{-3pt}

\subsection{Hypotheses in~Besan{\!\!\fontsize{14pt}{10pt}\selectfont\ptb{\c{c}}}on Galaxy model}

\vspace*{-1pt}

  \noindent
  Various models in astronomy heavily rely on hypotheses. One of the most
impressive is the Besan{\!\fontsize{12pt}{10pt}\selectfont\ptb{\c{c}}}on galaxy model (BGM)~[57--59]
evolving for many years and representing the population and structure synthesis
model for the Milky Way. It allows astronomers to test hypotheses on the star
formation history, star evolution, and chemical and dynamical evolution of the
Galaxy.  As the result of simulation process, one can get the following:
multidimensional histograms of intrinsic star properties or observable properties, a~catalog
of pseudoobservations, or the integrated luminosity in a~specified photometric
band~\cite{60-kl}. From the beginning, the aim of the BGM was not only to be able to
simulate reasonable star counts but further to test scenarios of Galactic evolution
from assumptions on the rate of star formation (SFR), initial mass function (IMF),
and stellar evolution.

  The model has explicit and implicit hypotheses associated with it. Explicit
hypotheses are usually some sets of equations, taken from the literature studies and
put as the ingredient of the model. Some of explicit hypotheses are passed as the
input of the model, e.\,g., star formation rate, initial mass function, evolutionary
tracks, chemical evolution, atmosphere models, density laws, interstellar extinction
model.

  The model has some implicit hypotheses as well. For example, it is assumed that
no star population comes from the outside of the Galaxy. There are several more
implicit hypotheses about disk formation and dark matter assumptions encoded inside
the model. It is usually much harder to get all the implicit hypotheses, since many of
them are not described in the articles and are difficult to pin from the code.

  BGM has not only the large number of explicit and implicit hypotheses, but also a~complex interrelations between them. So, some of the hypotheses are being
independent, e.\,g., IFM and SFR; so, it is possible to change them independently. On
the other side, some of the hypotheses are connected, e.\,g., the age distribution,
the density laws, and the potential are linked with the age--velocity dispersion via the
Boltzmann equation and need to be consistent. Such kind of dependencies make the
model hard to be tested and to keep it consistent while varying different parameters
during model fitting. Another example of interrelations of hypotheses is competing
hypotheses.

  BGM has changed drastically over the last 30~years. This has happened because of
the appearance of new data surveys, technologies, and methods of observation
development. As an example of such evolution, the model developed in~2014
compared to previous versions handles variations of the SFR, IMF, evolutionary
tracks, and atmosphere models. These hypotheses are passed as input parameters to
the model; so, the user can vary them.

The second improvement of the model is the
implementation of the stellar binarity, being an important change since binaries can
account for about 50\% of the total stellar content of the Milky Way. The authors of
the new version underline the importance of understanding interrelationships between
different hypotheses and need for model evolution tools~\cite{60-kl}: ``In practice, to build a~Galaxy from the fundamental building-blocks, we had to reconstruct the previous
model and apply important changes in the code arrangement. That required to
understand well the underlying relations between all mentioned
components.''

  It is planned further to focus on the renewed BGM~\cite{57-kl}, in which authors
draw their attention to the Galaxy thin disk treatment and use of Tycho-2 as a~testing
dataset. The parameters of BGM (such as IMF, SFR and evolutionary track sets)
explicitly and model ingredients implicitly can be treated as hypotheses. Model
ingredients include the treatment of binarity, the local stellar mass densities of thin
disk, extinction model,  age-metallicity and age-velocity relations, radial scale length,
the age of the Galaxy thin disc, different sets of the star atmosphere models, etc.

  Tycho-2 dataset  and $\chi^2$-type statistics test is used to test various versions of
these hypotheses in order to choose the most appropriate ones and update model to
better fit the provided data. The tests were made by comparing star counts and
  $(B-V)_T$ color distributions between data and simulations. Two different tests
were used to evaluate the adequacy of the stellar densities globally and to test the
shape of the color distribution. Other parameters to be tested are: star counts, radio
velocity, magnitudes, colors, proper motions, parallax, effective temperatures,
gravity, and metallicity. Authors use histograms, 2~goodness of fit (maximum
likelihood and $\chi^2$-test) and for velocity parameter, Kolmogorov--Smirnov
and Henderson--Darling tests.

  Due to the fact that some ingredients of the model are highly correlated (such as
the IMF, SFR, and the local mass density), the authors defined default models as a~combination of a~new set of ingredients that significantly improve the fit to Tycho
data.  So, 11~IMF functions, 2~SFR functions, 2~evolutionary track sets, 3~sets of
atmosphere models, 3~values for the age of the formation of the thin disk,
and 3~sets of
values of the thin disk local stellar volume mass density were tested.
As a~result of
testing, the two most appropriate IMS and SFR hypotheses were chosen.

%\pagebreak

  BGM authors have plans to incorporate other star surveys and test the model
against them. To do simulations directly comparable with data, the selected
magnitudes from the surveys need to be complete in terms of magnitude. Among
these surveys, there are the Geneva-Copenhagen survey, SDSS-II/III,
SEGUE/SEGUE2, APOGEE, RAVE, LAMOST, Gaia, Gaia-ESO, GALAH LSST,
WEAVE, 4MOST, and \mbox{MOONS} surveys~\cite{61-kl}.

\subsection{Hypothesis testing applying connectome data }

\noindent
  In the neuroscience community, the development of common paradigms for
interrogating the myriad functional systems in the brain remains to be the core
challenge. Building on the term ``\textit{connectome},'' coined to describe the
comprehensive map of neural connections in the human brain, the ``functional
connectome'' denotes the collective set of functional connections in the human brain
(its ``wiring diagram'')~\cite{62-kl}. More broadly, a~connectome would include the
mapping of all neural connections within an organism's nervous system. The
production and study of connectomes, known as \textit{connectomics}, may range in
scale from a~detailed map of the full set of neurons and synapses within part or all of
the nervous system of an organism to a~macroscale description~\cite{63-kl} of the
functional and structural connectivity between all cortical areas and subcortical
structures. The ultimate goal of connectomics is to map the human brain. In
functional magnetic resonance imaging (fMRI), associations are thought to represent
functional connectivity in the sense that the two regions of the brain participate
together in the achievement of some higher-order function, often in the context of
performing some task. fMRI has emerged as a~powerful tool used to interrogate a~multitude of functional circuits simultaneously. This has elicited the interest of
statisticians working in that area. At the level of basic measurements, neuroimaging
data can be considered to consist typically of a~set of signals (usually, time series) at
each of a~collection of pixels (in two dimensions) or voxels (in three dimensions).
Building from such data, various forms of higher-level data representations are
employed in neuroimaging. In recent years, a~substantial interest in network-based
representations has emerged in neuroimaging to use \textit{networks} to summarize
relational information in a~set of measurements, typically assumed to be reflective of
either functional or structural relationships between regions of interest in the brain.
With neuroimaging, now, a~standard tool in clinical neuroscience, quickly moving
towards a~time in which we will have available databases composed of large
collections of secondary data in the form of \textit{network-based data objects}, is
predictable.

  One of the most basic tasks of interest in the analysis of such data is the testing of
hypotheses in answer to questions such as ``Is there a~difference between the
networks of these two groups of subjects?'' Networks are not Euclidean objects and,
hence, classical methods of statistics do not directly apply. Network-based analogues
of classical tools for statistical estimation and hypothesis testing are
investigated in~\cite{64-kl, 65-kl}. Such research is motivated by the 1000 Functional
Connectomes Project (FCP) launched in 2010~\cite{62-kl}.
  The 1000 FCP~\cite{66-kl} constitutes the largest data set of its kind similarly to
large data sets in genetics. Other projects (such as the Human Connectome Project
(HCP)) are aimed to build a~network map of the human brain in healthy, living adults.
The total volume of data produced by the HCP will likely be multiple
petabytes~\cite{67-kl}. HCP informatics platform includes data management system
ConnectomeDB that is based on the XNAT (eXtensive Neuroimaging Archive Toolkit)
imaging informatics
  platform~\cite{68-kl}, a~widely used open source system for managing and sharing
imaging and related data.

  Now, HCP has information about more than 500~subjects including structural scans
(T1w and T2w), resting-state fMRI (rfMRI), task fMRI (tfMRI), and high angular
resolution diffusion imaging (dMRI). In addition, some resting-state MEG (rMEG)
and/or task MEG (tMEG) data are available.

  Data come in several formats: ``unprocessed'' raw data, ``minimally
preprocessed,'' and ``analysis'' datasets. Preprocessed datasets have spatial distortions
minimized and data have been aligned across modalities and across subjects using
appropriate volume-based and surface-based registration methods. HCP consortium
recommends to use the preprocessing dataset.

  Visualization, processing, and analysis of high-dimensional data such as images
often require some kind of preprocessing to reduce the dimensionality of the data
and find a~mapping from the original representation to a~low-dimensional vector
space. The assumption is that the original data resides in a~low-dimensional subspace
or manifold~\cite{69-kl}, embedded in the original space. This topic of research is
called dimensionality reduction, nonlinear dimensionality reduction, including
methods for parameterization of data using low-dimensional manifolds as models.
Within the neural information processing community, this has become known as
manifold learning. Methods for manifold learning are able to find nonlinear
manifold parameterizations of datapoints residing in high-dimensional spaces, very
much like Principal Component Analysis (PCA) is able to learn or identify the most
important linear subspace of a~set of data points (projecting data on a~$n$-dimensional
linear subspace which maximizes the variance of the data in the new space).
{\looseness=1

}

  In~\cite{64-kl}, necessary mathematical properties associated with a~certain notion
of a~`space' of networks used to interpret functional neuroimaging
  connectome-oriented data are established. Extension of the classical statistics tools
to network-based datasets, however, appeared to be highly nontrivial. The main
challenge in such an extension is due to the fact that networks are not Euclidean
objects (for which classical methods were developed)~--- rather, they are
combinatorial objects, defined through their sets of vertices and edges.
  In~\cite{64-kl}, it was shown that networks can be associated with certain natural
subsets of Euclidean space and demonstrated that through a~combination of tools
from geometry, probability on manifolds, and high-dimensional statistical analysis, it
is possible to develop a~principled and practical framework in analogy to classical
tools. In particular, an asymptotic framework for one- and two-sample hypothesis
testing has been developed. Key to this approach is the correspondence between an
undirected graph and its Laplacian, where the latter is defined as a~matrix (associating
with a~network). Graph Laplacian appeared to be particularly appropriate to be used
for such matrices. The space of graph Laplacians is used working in certain subsets of
Euclidian space which are some submanifolds of the standard Euclidian space.

  The 1000 FCP describes functional neuroimaging data from 1093~subjects,
located in 24~community-based centers. The mean age of the participants was
29~years, and all subjects were 18~years old or older. It is of interest to compare the
subject-specific networks of males and females in the 1000 FCP data set.
  In~\cite{64-kl},  for the 1000 FCP, database comparing networks with respect to
the sex of the subjects, over different age group, and over various collection sites is
considered. It is shown that it is necessary to compute the means in each subgroup of
networks. This was done by constructing the Euclidean mean of the Laplacians for
each group of subjects in different age groups. Such group-specific mean Laplacians
can then be interpreted as the mean functional connectivity in each group. Such
approach provides for building the hypothesis tests about the average of networks or
groups of networks to investigate the effect of sex differences on entire networks.

  For the 1000 FCP data set, it was tested using the two-sample test for Laplacians
whether sex differences were significant to influence patterns of brain connectivity.
The null hypothesis of no group differences was rejected with high probability.
Similarly for the three different age cohorts, the null hypothesis of no cohort
differences also was rejected with high probability.

  On such examples, it was shown~\cite{64-kl} that the proposed global test has
sufficient power to reject the null hypothesis in cases when mass-univariate approach
(considered to be the gold standard in fMRI research~\cite{70-kl}) fails to detect the
differences at the local level. According to the mass-univariate approach, statistical
analysis is performed iteratively on all voxels to identify brain regions whose fMRI
detected responses display significant statistical effects. Thus,
it was shown that a~framework for network-based statistical testing is more
statistically powerful than a~mass-univariate approach.

  It is expected that in the near future, there will be a~plethora of databases of
network-based objects in neuroscience motivating the development and extension of
various tools from classical statistics to global network data.


  In paper~\cite{71-kl} discussing the relationship between neuroimaging and
Big Data areas, it is analyzed how modern neuroimaging research represents a~multifactorial and broad ranging data challenge, involving the growing size of the
data being acquired; sociological and logistical sharing issues; infrastructural
challenges for multisite, multidatatype archiving; and the means by which to
explore and mine these data. As neuroimaging advances further, e.\,g., aging,
genetics, and age-related disease, new vision is needed to manage and process this
information while marshalling of these resources into novel results.
It is predicted
that on this way, ``big data'' can become ``big'' brain science.

  In~\cite{72-kl}, authors formulate a~hypothesis about the brain connectivity and
evaluate it against HCP data. They use the task fMRI data, there is specific data about
a well-validated task used to probe animate motion detection. The audience was
shown short videoclips (20~s) of objects (squares, circles, and triangles) either
interacting in some way (animate motion) or moving mechanically (inanimate
motion). Participants rated the video by selecting if there was any social interaction,
no interaction, or not sure for interaction. There were 2~sessions comprised
of~5~videoblocks.

  Hypothesis states that some regions of the brain (V5 and pSTS) are effectively
connected and impacted by animate motion.

  To test it, general linear models were used. The time series were modeled with
regressors All motion\,--\,No motion, Animate--Inanimate motion. Together with
regressors about head, tongue, and finger movement, these regressors were used to
build general linear model. A~group level ANOVA was performed to identify
significant regional effects for the All Motion contrast and a~contrast for
Animate--Inanimate motion. For effective connectivity discovery,
 Dynamic Causal Modeling
(DCM) technique was used. DCM tells about self-, forwards, and
backward connections between active brain regions during an experiment, enabling
to infer the way of brain regions impact each other mostly. As the result of DCM
modeling, 16~models were created and passed as the input to Bayesian Model
Selection procedure, which chose the winning model among them.
{\looseness=1

}

  The results show that there is a~connectivity between V5 and the pSTS brain
regions in both hemispheres, which was independent of the type of motion. Animate
motion stimulates the forward and backward connection between V5 and the pSTS in
both hemispheres.

\subsection{Climate in~Australia}

\noindent
  Another view on hypothesis representation and evaluation is presented
  in~\cite{73-kl}. Authors argue that as long as in DIR data relevant to some
hypotheses get continuously aggregated as time passes, hypotheses should be
represented as programs that are executed repeatedly, as new relevant amounts of
data get aggregated. Their method and techniques are illustrated by examining
hypotheses about temperature trends in Australia during the 20th century. The
hypothesis being tested comes from~\cite{74-kl}, stated that the temperature series is
not stationary and is integrated of order~1 (I(1)). Nonstationarity means that the
level of the time series is not stable in time and can show increasing and decreasing
trends; I(1) means that by differentiating the stochastic process,
a~stationary process
(main statistical properties of the series remain unchanged) is obtained. Phillips--Perron test and the Kwiatkowski--Phillips--Schmidt--Shin (KPSS) test are used and
both of them are executed in~R. Several data sources are crawled: ($i$)~The National
Oceanographic and Atmospheric Administration marine and weather information; and
($ii$)~Australian Bureau of Meteorology dataset. The framework consists
of~R~interpreter and R~\textit{SPARQL}, \textit{tseries} packages. Authors also
used \mbox{agINFRA} for computation and rich semantics to support traditional scientific
workflows for natural sciences. Authors received further evidence on different
independent dataset that time series is integrated of order~1.

\subsection{Financial market}

\noindent
  Efficient-market hypothesis (EMH) is one of the most prominent in finance and
``\textit{asserts that financial markets are ``informationally efficient}.''
  In~\cite{75-kl}, authors test the weak form of EMH, stating that prices on traded
assets (e.\,g., stocks, bonds, or property) already reflect all past publicly available
information. The null hypothesis states that successive prices changes are
independent (random walk). The alternative hypothesis states that they are dependent.
To check if the successive closing prices are dependent of each other, the following
statistical tests were used: a~serial correlation test, a~runs test, an augmented
Dickey--Fuller test, and the multiple variance ratio test. Tests were performed on daily closing
prices from the six European stock markets (France, Germany, U.K., Greece,
Portugal, and Spain) during the period between~1993 and~2007. The result of each
test states whether successive closing prices are dependent of each other.

     Test provides evidence that for monthly prices and returns, the null hypothesis
should not be rejected for all six markets. If daily prices are concerned, the null
hypothesis is not rejected for France, Germany, U.K., and Spain, but this hypothesis is
rejected for Greece and Portugal. However, on the 2003--2007 dataset, the null
hypothesis for these two countries is not rejected as well.

  In \cite{76-kl}, Bollen \textit{et al.}\ use different approach to test EMH. Authors
investigate whether public sentiment, as expressed in large-scale collections of daily
Twitter posts, can be used to predict the stock market. They build public mood time
series by sentiment analysis of tweets from February~28 to December~19,
2008 and try to show that it can predict Dow Jones Index corresponding values. The
null hypothesis states that the mood time series do not predict DJIA
(Dow Jones Industrial Average) values. Granger
causality analysis in which Dow Jones values and mood time series are correlated is
used to test the null hypothesis. Granger causality analysis is used to determine if one
time series can predict another time series. Its results reject the null hypothesis and
claim that public opinion is predictive of changes in DJIA closing values.

\subsection{Publication-based automated hypothesis generation in~life sciences}

  \noindent
  Researchers and scientists from leading academic, pharmaceutical, and other
research centers have begun deploying  IBM's  Watson Discovery Advisor to rapidly
analyze and test hypotheses using data in millions of scientific papers available in
public databases. A~new scientific research paper is published nearly every
30~s, which equals more than a~million annually. According to the National
Institutes of Health, a~typical researcher reads about 23~scientific papers per month,
which translates to nearly~300~per~year, making it humanly impossible to keep up
with the evergrowing body of scientific material available. Building on Watson's
ability to understand nuances in natural language, Watson Discovery Advisor can
understand the language of science, such as how chemical compounds interact,
making it a~uniquely powerful tool for researchers in life sciences and other research
and industrial domains.  Specifically, the Watson Discovery Advisor for Life Sciences
is armed with expertise and understands field-specific lexicon in areas such as
clinical trial data, genomics, drugs, and human anatomy.

  Recently, scientists of Baylor College of Medicine and IBM using the Baylor
Knowledge Integration Toolkit (KnIT), based on Watson technology, identified new
enzymes (called kinases) that can modify p53, an important protein related to many
cancers~\cite{77-kl}. There are over 240,000~papers that mention one or more of
500+ known human kinases in their Medline abstract. There are over 70,000~papers
published on p53 to make their analysis manually is completely unrealistic task.
Watson analyzed the scientific articles related to p53 to predict proteins that turn on
or off p53's activity. This automated analysis led the Baylor cancer researchers to
identify six potential p53 kinases to target for new research. These results are notable,
considering that over the last 30~years, scientists averaged one p53 kinase discovery
per year. Knowing which proteins are modified by each kinase and, therefore, which
kinases would make good drug targets is a~difficult and unsolved problem. There are
over~500 known human kinases and tens of thousands of possible proteins they can
target.

  KnIT collects the abstracts to be mined applying queries. A~specific kinase name
and its synonyms are used in this process. Entity resolution process looks as follows.
The words and phrases that make up the document feature space are determined by
counting the number of documents in which each word appears and identifying the
words with the highest counts.  A~phrase is considered to be a~sequence of two
words. Only the~$N$~most frequent words and phrases are selected. This becomes the
feature space.

  Once a~feature space is received, a~representation of each kinase by averaging the
feature vectors of all documents that contain the kinase is created. This is the kinase
centroid. Next, a~distance matrix is calculated that measures the distance between each
kinase and every other kinase in the space.

  Finally, a~meaningful picture of kinase--kinase relationships is obtained.  Thus, it is
possible to identify a~set of kinases that may modify p53. However, some sort of
principled ranking scheme is needed in order to prioritize the kinases for further
experimentation. To provide such a~scheme, the graph diffusion method~\cite{78-kl}
was used. Graph diffusion is a~semisupervised learning approach for classification
based on labeled and unlabeled data. It takes known information (initial labels) and
then constrains the new labels to be smooth in respect to a~defined structure (e.\,g.,
a~network). In the case considered, it is known which kinases can modify p53
(initial labels); one
would like to know which other proteins can modify p53 (final labels). The distance
matrix based on the literature gives the structure of the kinase network. The initial
labels are extracted from current knowledge found in review articles.

  To test the algorithm, it was first applied in a~retrospective analysis to show
whether recent annotations of new p53 kinases occurring after a~certain date (2003)
could be predicted from a~model that only took into account papers written before
that date, at a~time when these discoveries of p53 kinases were still unknown. Next,
it was asked whether some variations in the algorithm could improve p53 kinase
prediction as  its performance  was compared to the common approach used most
typically to identify functionally similar proteins in biology. Finally, the analysis was
expanded to a~larger set of proteins to test scalability.

  This research represents the first stage in the IBM--Baylor collaborative effort
and as such, it proves the principle that mining past literature is a~viable strategy for
predicting previously unknown biological events. It was shown that p53 kinases
predicted with the text mining methods are supported by laboratory findings. In the
future, it should be possible to make many other kinds of predictions on a~much
larger scale as the infrastructure and capabilities will be increased. In the future, it is
planned to focus on a~wider area of proteins and functions, building up
comprehensive networks of interactions and predicting where new connections ought
to exist based on everything else that is known. It is expected that  this will ultimately
accelerate the pace of cancer discoveries by an order of magnitude and allow
scientists to come to a~much more complete understanding of the mechanisms behind
this disease.

  Expanding KnIT to other areas of biology or the physical sciences is not
straightforward. For example, to generalize to more proteins and genes is a~big
problem. In subjects like physics, results tend to be presented using equations and
graphs rather than words. However, data-mining groups are working to retrieve
information from these, too.

\vspace*{-6pt}

\section{Concluding Remarks}



\noindent
  The objective of this study is to analyze, collect, and systematize information on the
role of hypotheses in the DIR process as well as on support of
hypothesis formation, evaluation, selection, and refinement in course of the natural
phenomena modeling and scientific experiments. The discussion is started with the
basic concepts defining the role of hypotheses in the formation of scientific
knowledge and organization of the scientific experiments. Based on such concepts,
the basic approaches for hypothesis formulation applying logical reasoning, various
methods for hypothesis modeling and testing (including classical statistics, Bayesian
hypothesis, and parameter estimation methods, hypothetico-deductive approaches)
are briefly introduced. Special attention is given to discussion of the data mining and
machine learning methods role in process of generation, selection, and evaluation of
hypotheses as well as the methods for motivation of new hypothesis formulation.
Facilities of informatics for support of hypothesis-driven experiments, considered in
the paper, are aimed at the conceptualization of scientific experiments, hypothesis
formulation, and browsing in various domains (including biology, biomedical
investigations, neuromedicine, and astronomy), automatic organization of
hypothesis-driven   experiments. Examples of scientific researches applying hypotheses
considered in the paper include modeling of population and structure synthesis of the
Galaxy, connectome-related hypothesis testing,  studying of temperature trends in
Australia, analysis of stock markets applying the EMH,
as well as algorithmic generation of hypotheses in the collaborative project based on
IBM Watson--Baylor Knowledge Integration Toolkit applying the NLP and
knowledge representation and reasoning technologies. An introduction into the state
of the art of the hypothesis-driven research presented in the paper opens a~way for
investigation of the generalized approaches for efficient organization of
hypothesis-driven experiments applicable for various branches of DIR.

\renewcommand{\bibname}{\protect\rmfamily References}

\vspace*{-6pt}

{\small\frenchspacing
{%\baselineskip=10.8pt
\begin{thebibliography}{99}



\bibitem{1-kl}
Hey, T., S. Tansley, and K. Tolle, eds. 2009. \textit{The Fourth paradigm:
Data-intensive scientific discovery}. Redmond, Microsoft Research. 252~p.
\bibitem{2-kl}
\Aue{McComas, W.\,F.} 1998. The principal elements of the nature of science:
Dispelling the myths of science. \textit{Nature of science in science education:
Rationales and strategies}. Ed.\ W.\,F.~McComas.
Kluwer Academic Publs. 53--70.
\bibitem{3-kl}
\Aue{Lakshmana Rao, J.\,R.} 1998. Scientific `Laws,' `Hypotheses' and `Theories'.
\textit{Meanings Distinctions Reson.} 3:69--74.
\bibitem{4-kl}
\Aue{Poincar$\acute{\mbox{e}}$, H.} 2012. The foundations of science: Science and
hypothesis, the value of science, science and method. \textit{The Project
Gutenberg EBook}.  No.\,39713. 554~p. Available at:
{\sf http://www.gutenberg.org/files/39713/39713-8.txt}
(accessed February~10, 2015).
\bibitem{5-kl}
\Aue{Bacon, F.}
1952. {The new organon}. \textit{Great
books of the Western World. Vol.~30. The works of Francis Bacon}.
Ed. R.\,M.~Hutchins. Chicago: Encyclopedia
Britannica, Inc. 107--195.
\bibitem{6-kl}
\Aue{Menzies, T.} 1996. Applications of abduction: Knowledge-level modeling.
\textit{Int. J.~Hum.-Comput. St.} 45(3):305--335.
\bibitem{7-kl}
\Aue{Haber, J.} 2010. Research questions, hypotheses, and clinical questions.
\textit{Evolve resources for nursing research}. 7th ed. Elsevier. 27--55.
\bibitem{8-kl}
\Aue{Popper, K.} 2005. \textit{The logic of scientific discovery}.
  London\,--\,New York: Routledge,  Taylor \& Francis. 545~p.
  Available at: {\sf http://strangebeautiful.com/other-texts/popper-logic-scientific-discovery.pdf}
  (accessed February~10, 2015).
\bibitem{9-kl}
\Aue{Kerlinger, F.\,N., and H.\,B.~Lee}. 1964. \textit{Foundations of behavioral
research: Educational and psychological inquiry}. New York: Holt, Rinehart and
Winston. 739~p.
\bibitem{10-kl}
\Aue{Hempel, C.\,G.} 1952. Fundamentals of concept formation in empirical science.
\textit{Int. Encyclopedia Unified Sci.} 2(7). Available at: {\sf
http://www.iep.utm.edu/hempel/} (accessed February~10, 2015).

\bibitem{12-kl} %11
\Aue{Porto, F., and S.~Spaccapietra}. 2011. Data model for scientific models and
hypotheses. \textit{Evolution Conceptual Modeling} 6520:285--305.

\bibitem{11-kl} %12
\Aue{Gon{\!\,\fontsize{10pt}{10pt}\selectfont\ptb{\!\c{c}}}alves, B., and F.~Porto}. 2013. A~lattice-theoretic approach
for representing and managing hypothesis-driven research. \textit{25th Conference
(International) on Scientific and Statistical Database Management (ACM)
Proceedings}. Baltimore. 41.


\bibitem{13-kl}
\Aue{Gon{\!\fontsize{10pt}{10pt}\selectfont\ptb{\!\c{c}}}alves, B., F.~Porto, and A.\,M.\,C.~Moura}. 2012. On the
semantic engineering of scientific hypotheses as linked data. \textit{2nd Workshop
(International) on Linked Science Proceedings}. Boston.
\bibitem{14-kl}
\Aue{Woodward, J.} 2011. Scientific explanation. \textit{The Stanford Encyclopedia
of Philosophy}. Available at:  {\sf
http://plato. stanford.edu/archives/win2011/entries/scientific-explanation/} (accessed
February~10, 2015).
\bibitem{15-kl}
Nickles, T., ed. 1980. \textit{Scientific discovery: Case studies}. Taylor \& Francis.
501~p.
\bibitem{16-kl}
\Aue{Schickore, J.} 2014. Scientific discovery. \textit{The Stanford
Encyclopedia of Philosophy}. Available at: {\sf
http://plato. stanford.edu/archives/spr2014/entries/scientific-discovery/}
(accessed February~10, 2015).
\bibitem{17-kl}
\Aue{Kakas, A.\,C., R.\,A. Kowalski, and F.~Toni}. 1993. Abductive logic
programming. \textit{J.~Logic Comput.} 2(6):719--770.
\bibitem{18-kl}
\Aue{Kakas, A.\,C., A. Michael, and C.~Mourlas}. 2000. ACLP: Abductive constraint
logic programming. \textit{J.~Logic Program.} 44(1):129--177.
\bibitem{19-kl}
\Aue{Van Nuffelen, B., and A.~Kakas}. 2001. A-system: Declarative programming
with abduction. \textit{Logic programming and nonmotonic reasoning}.
Eds.\ T.~Eiter, W.~Faber, and M.~Truszczy$\acute{\mbox{n}}$ski.
Lecture notes in computer science ser. Berlin--Heidelberg:
Springer. 2173:393--397.
\bibitem{20-kl}
\Aue{Alferes, J.\,J., L.\,M. Pereira, and T.~Swift}. 2004. Abduction in well-founded
semantics and generalized stable models via tabled dual programs. \textit{Theor.
Pract. Log. Progr.} 4(4):383--428.
\bibitem{21-kl}
\Aue{Ray, O., and A.~Kakas.} 2006. ProLogICA: A~practical system for Abductive
Logic Programming. \textit{11th Workshop (International) on Non-Monotonic
Reasoning Proceedings}. 304--312.
\bibitem{22-kl}
\Aue{Citrigno, S., T. Eiter, W.~Faber, G.~Gottlob, C.~Koch, N.~Leone, and
F.~Scarcello}. 1997. The dlv system: Model generator and application frontends.
\textit{12th Workshop on Logic Programming Proceedings}.
128--137.
\bibitem{23-kl}
\Aue{King, R.\,D., M. Liakata, C.~Lu, S.\,G.~Oliver, and L.\,N.~Soldatova}. 2011.
On the formalization and reuse of scientific research. \textit{J.~Roy. Soc.
Interface} 8(63):1440--1448.
\bibitem{24-kl}
\Aue{Tamaddoni-Nezhad, A., R. Chaleil, A.~Kakas, and S.\,H.~Muggleton}. 2006.
Application of abductive ILP to learning metabolic network inhibition from temporal
data. \textit{Mach. Learn.} 64:209--230.
\bibitem{25-kl}
\Aue{Inoue K., T. Sato, M.~Ishihata, Y.~Kameya, and H.~Nabeshima}. 2009.
Evaluating abductive hypotheses using and EM algorithm on BDDs. \textit{21st Joint
Conference (International) on Artificial Intelligence (IJCAI09) Proceedings}.
Pasadena. 810--815.
\bibitem{26-kl}
\Aue{Bartha, P.} 2013. Analogy and analogical reasoning. \textit{The Stanford
Encyclopedia of Philosophy}. Available at:    {\sf
http://plato.stanford.edu/archives/fall2013/entries/ reasoning-analogy/} (accessed
February~10, 2015).
\bibitem{27-kl}
\Aue{Ivezi$\acute{\mbox{c}}$, {\ptb{\v{Z}}}., A.\,J.~Connolly, J.\,T.~VanderPlas,
and A.~Gray}. 2014. \textit{Statistics, data mining, and machine learning in
astronomy: A~practical Python guide for the analysis of survey data}. Princeton
University Press. 552~p.
\bibitem{28-kl}
\Aue{Sivia, D.\,S., and J.~Skilling}. 2006. \textit{Data analysis. A~Bayesian tutorial}.
New York: Oxford University Press Inc. 264~p.
\bibitem{29-kl}
\Aue{Field, A.} 2013. \textit{Discovering statistics using IBM SPSS statistics}.
4th ed. Sage.  915~p.
\bibitem{30-kl}
IBM SPSS Statistics for Windows, Version 22.0. 2013. Armonk, N.Y.: IBM Corp. IBM
SPSS Statistics base.  Available at: {\sf
https://www.uio.no/tjenester/it/forskning/\linebreak
statistikk/hjelp/programveilednigner/ibm\_spss\_\linebreak statistics\_brief\_guide-2.pdf}
(accessed February~10, 2015).
\bibitem{31-kl}
\Aue{Ihaka, R., and R.~Gentleman}. 1996. R: A~language for data analysis and
graphics. \textit{J.~Comput. Graph. Stat.} 5(3):299--314.
\bibitem{32-kl}
\Aue{March, M.\,C., G.\,D.~Starkman, R.~Trotta, and P.\,M.~Vaudrevange}. 2011.
Should we doubt the cosmological constant? \textit{Mon. Not. Roy.
Astron. Soc.} 410(4):2488--2496.
\bibitem{33-kl}
\Aue{Rouder, J.\,N., P.\,L.~Speckman, D.~Sun, R.\,D.~Morey, and G.~Iverson}.
2009. Bayesian t tests for accepting and rejecting the null hypothesis.
\textit{Psychon. Bull. Rev.} 16(2):225--237.
\bibitem{34-kl}
\Aue{Weber, M.} 2014. Experiment in biology. \textit{The Stanford Encyclopedia of
Philosophy}. Available at: {\sf
http://plato. stanford.edu/archives/fall2014/entries/biology-experiment/} (accessed
February~10, 2015).
\bibitem{35-kl}
\Aue{Hawthorne, J.} 2014. Inductive logic. \textit{The Stanford Encyclopedia of
Philosophy}. Available at: {\sf
http://plato. stanford.edu/archives/sum2014/entries/logic-inductive/} (accessed
February~10, 2015).
\bibitem{36-kl}
\Aue{Breiman, L.} 2001. Statistical modeling: The two cultures. \textit{Stat.
Sci.} 16(3):199--231.

\bibitem{38-kl} %37
\Aue{Hastie, T., R.~Tibshirani, J.~Friedman, and J.~Franklin}. 2005. The elements
of statistical learning: Data mining, inference and prediction. \textit{Math.
Intell.} 27(2):83--85.

\bibitem{37-kl} %38
\Aue{Barber, D.} 2010. \textit{Bayesian reasoning and machine learning}.
Cambridge University Press. 720~p.


\bibitem{39-kl}
\Aue{Ferrucci, D., E. Brown, J.~Chu-Carroll, J.~Fan, D.~Gondek,
A.\,A.~Kalyanpur, and C.~Welty}. 2010. Building Watson: An overview of the
DeepQA project. \textit{AI Mag.} 31(3):59--79.
\bibitem{40-kl}
\Aue{Dredze, M., K. Crammer, and F.~Pereira}. 2008. Confidence-weighted linear
classification. \textit{25th Conference (International) on Machine Learning
Proceedings}. Helsinki. 264--271.
\bibitem{41-kl}
\Aue{Starkman, G.\,D., R. Trotta, and P.\,M.~Vaudrevange}. 2008. Introducing doubt
in Bayesian model comparison. arXiv preprint arXiv:0811.2415.
\bibitem{42-kl}
\Aue{March, M.\,C.} 2013. Advanced statistical methods for astrophysical probes of
cosmology. {Springer Theses}. Vol.~20. 177~p.
\bibitem{43-kl}
\Aue{Porto, F.} 2013. Big data in astronomy. The LIneA-DEXL case.
\textit{EMC Summer School on BIG DATA~--- NCE/UFRJ}.
Available at: {\sf
http://www.slideshare.net/ fabiomporto/emc-2013-big-data-in-astronomy} (accessed
February~10, 2015).
\bibitem{44-kl}
\Aue{Racunas, S.\,A., N.\,H.~Shah, I.~Albert, and N.\,V.~Fedoroff}. 2004. Hybrow:
A~prototype system for computer-aided hypothesis evaluation.
\textit{Bioinformatics} 20(1):257--264.
\bibitem{45-kl}
\Aue{Soldatova, L.\,N., A.~Rzhetsky, and R.\,D.~King}. 2011. Representation of
research hypotheses. \textit{J.~Biomed. Semantics} 2(S-2):S9.
\bibitem{46-kl}
\Aue{Callahan, A., M.~Duumontier, and N.~Shah}. 2011. HyQue: Evaluating
hypotheses using Semantic Web technologies. \textit{J.~Biomed. Semantics}
 2(S-2):S3.
\bibitem{47-kl}
\Aue{Gao, Y., J.~Kinoshita, E.~Wu, E.~Miller, R.~Lee, A.~Seaborne, and T.~Clark}.
2006. SWAN: A~distributed knowledge infrastructure for Alzheimer disease
research. \textit{J.~Web Semant.} 4(3):222--228.
\bibitem{48-kl}
\Aue{King, R.\,D., K.\,E. Whelan, F.\,M.~Jones, P.\,G.~Reiser, C.\,H.~Bryant,
S.\,H.~Muggleton, and S.\,G.~Oliver}. 2004. Functional genomic hypothesis
generation and experimentation by a~robot scientist. \textit{Nature}
427(6971):247--252.
\bibitem{49-kl}
\Aue{Porto, F., A.\,M.\,C.~Moura, B.~Gon{\!\fontsize{10pt}{10pt}\selectfont\ptb{\!\c{c}}}alves, R.~Costa, and
S.\,A.~Spaccapietra}. 2012. A~scientific hypothesis conceptual model.
\textit{Advances in conceptual modeling.}
Eds. S.~Castano, P.~Vassiliadis, L.\,V.~Lakshmanan, and M.~Li~Lee.
Lecture notes in computer science ser. Berlin--Heidelberg: Springer.
 7518:101--110.
\bibitem{50-kl}
\Aue{Porto, F., and A.\,M.\,C.~Moura}. 2011. Scientific hypothesis database. Report.
Available at: {\sf
http://livroaberto. ibict.br/bitstream/1/869/1/Scientific\%20Hypothesis\%\linebreak 20Database.pdf} (accessed February~10, 2015).
\bibitem{51-kl}
\Aue{Asgharbeygi, N., P.~Langley, S.~Bay, and K.~Arrigo}. 2006. Inductive revision
of quantitative process models. \textit{Ecol. Model.} 194(1):70--79.
\bibitem{52-kl}
\Aue{Tran, N., C. Baral, V.\,J.~Nagaraj, and L.~Joshi}. 2005. Knowledge-based
integrative framework for hypothesis formation in biochemical networks.
\textit{Data integration in the life sciences}.
Eds. B.~Lud$\ddot{\mbox{a}}$scher and L.~Raschid.
Lecture notes in computer science ser. Berlin--Heidelberg: Springer.
3615:121--136.
\bibitem{53-kl}
\Aue{Sparkes, A., W. Aubrey, E.~Byrne, A.~Clare, M.\,N.~Khan, M.~Liakata, and
R.\,D.~King}. 2010. Towards Robot Scientists for autonomous scientific discovery.
\textit{Autom. Exp.} 2(1). Available at: {\sf
http://www.aejournal.net/content/2/1/1} (accessed February~10, 2015).
\bibitem{54-kl}
Castrillo, J.\,I., and S.\,G.~Oliver, eds. 2011. \textit{Yeast systems biology: Methods and
protocols}. {Methods in molecular biology ser}. Berlin--Heidelberg:
Springer. Vol.~759. 549~p.
\bibitem{55-kl}
\Aue{Plotkin, G.\,D.} 1970. A~note on inductive generalization. \textit{Mach.
Intell.} 5:153--163.
\bibitem{56-kl}
\Aue{Huang, J., L. Antova, C.~Koch, and D.~Olteanu}. 2009. MayBMS: A~probabilistic database management system. \textit{2009 ACM SIGMOD Conference
(International) on Management of Data Proceedings}. Rhode Island. 1071--1074.

\bibitem{59-kl} %57
\Aue{Robin, A., and M.~Cr$\acute{\mbox{e}}$z$\acute{\mbox{e}}$}.
1986. Stellar
populations in the Milky Way~--- a~synthetic model. \textit{Astron.
Astrophys.} 157:71--90.

\bibitem{58-kl}
\Au{Robin, A.\,C., C.~Reyl$\acute{\mbox{e}}$~C., S.~Derri{\!\!\ptb{\`{e}}}re,
and S.~Picaud.} 2006. A~synthetic view on structure and evolution of the Milky Way.
{arXiv preprint astro-ph}/0401052.

 \bibitem{57-kl} %59
\Aue{Czekaj, M.\,A., A.\,C.~Robin, F.~Figueras, X.~Luri, and M.~Haywood}. 2014.
The Besan{$\negthickspace$\!\,\fontsize{10pt}{10pt}\selectfont\ptb{\c{c}}}on Galaxy model \mbox{renewed-I}. Constraints on the local star formation
history from Tycho data. \textit{Astron. Astrophys.} 564:A102.


\bibitem{60-kl}
\Aue{Czekaj, M.\,A.} 2012. Galaxy evolution: A~new version of the
Besan{\fontsize{10pt}{10pt}\selectfont\ptb{\!\c{c}}}on
Galaxy Model constrained with Tycho data. PhD Thesis. Barcelona: Universitet de
Barcelona. 167~p.
\bibitem{61-kl}
\Aue{Martins, A.\,M.\,M.} 2014. Statistical analysis  of large scale surveys for
constraining the Galaxy evolution. PhD Thesis. Barcelona: Universitet de Barcelona.
221~p.
\bibitem{62-kl}
\Aue{Biswal, B.\,B., M.~Mennes, X.\,N.~Zuo, S.~Gohel, C.~Kelly,
S.\,M.~Smith, and C.~Windischberger}. 2010. Toward discovery science of
human brain function. \textit{Proc. Nat. Acad. Sci. USA}
107(10):4734--4739.
\bibitem{63-kl}
\Aue{Craddock, R.\,C., S.~Jbabdi, C.\,G.~Yan, J.\,T.~Vogelstein, F.\,X.~Castellanos,
A.~Di~Martino, and M.\,P.~Milham}. 2013. Imaging human connectomes at the
macroscale. \textit{Nat. Methods} 10(6):524--539.
\bibitem{64-kl}
\Aue{Ginestet, C.\,E., P.~Balanchandran, S.~Rosenberg, and E.\,D.~Kolaczyk}.
2014. Hypothesis testing for network data in functional neuroimaging. arXiv
preprint \mbox{arXiv}:1407.5525.
\bibitem{65-kl}
\Aue{Ginestet, C.\,E., A.\,P.~Fournel, and A.~Simmons}. 2014. Statistical network
analysis for functional MRI: Summary networks and group comparisons.
\textit{Front. Comput. Neurosci.} 8:51. Available at: {\sf
http://www.ncbi.nlm. nih.gov/pmc/articles/PMC4018548/} (accessed February~10,
2015).
\bibitem{66-kl}
\Aue{Yan, C.\,G., R.\,C.~Craddock, X.\,N.~Zuo, Y.\,F.~Zang, and M.\,P.~Milham}.
2013. Standardizing the intrinsic bra towards robust measurement of inter-individual
variation in 1000 functional connectomes. \textit{Neuroimage} 80:246--262.
\bibitem{67-kl}
\Aue{Marcus, D.\,S., J. Harwel, T.~Olsen, M.~Hodge, M.\,F.~Glasser, F.~Prior, and
D.\,C.~Van~Essen}. 2011. Informatics and data mining tools and strategies for the
human connectome project. \textit{Front. Neuroinform.} 5. Available at:
{\sf http://www.ncbi.nlm.nih.gov/pmc/\linebreak articles/PMC3127103/} (accessed
February~10, 2015).

\columnbreak

\bibitem{68-kl}
\Aue{Marcus, D.\,S., T.\,R. Olsen, M.~Ramaratnam, and R.\,L.~Buckner}. 2007. The
extensible neuroimaging archive toolkit. \textit{Neuroinformatics} 5(1):11--33.
\bibitem{69-kl}
\Aue{Brun, A.} 2006. {Manifold learning and representations for image analysis
and visualization}. Department of Biomedical Engineering,
Link$\ddot{\mbox{o}}$pings Universitet. 104~p.
\bibitem{70-kl}
\Aue{Mahmoudi, A., S.~Takerkart, F.~Regragui, D.~Boussaoud, and A.~Brovelli}.
2012. Multivoxel pattern analysis for fMRI data: A~review. \textit{Comput.
Math. Methods Med}. Available at: {\sf
http://www.hindawi.com/journals/ cmmm/2012/961257/} (accessed February~10,
2015).
\bibitem{71-kl}
\Aue{Van Horn, J.\,D., and A.\,W.~Toga}. 2014. Human neuroimaging as a~``Big
Data'' science. \textit{Brain Imaging Behavior} 8(2):323--331.
\bibitem{72-kl}
\Aue{Hillebrandt, H., K.\,J. Friston, and S.\,J.~Blakemore}. 2014. Effective
connectivity during animacy perception-dynamic causal modelling of Human
Connectome Project data. \textit{Sci. Rep.} 4. Available at: {\sf
http://www.ncbi.nlm. nih.gov/pmc/articles/PMC4150124/} (accessed February~10,
2016).
\bibitem{73-kl}
\Aue{Lappalainen, J., M.\,A. Sicilia, and B.~Hern$\acute{\mbox{a}}$ndez}. 2013.
Automatic hypothesis checking using eScience Research Infrastructures, ontologies,
and linked data: A~case study in climate change research. \textit{Procedia Comput.
Sci.} 18:1172--1178.
\bibitem{74-kl}
\Aue{Lenten, L.\,J., and I.\,A.~Moosa}. 2003. An empirical investigation into
long-term climate change in Australia. \textit{Environ. Modell. Softw.}
18(1):59--70.
\bibitem{75-kl}
\Aue{Borges, M.\,R.} 2010. Efficient market hypothesis in European stock markets.
\textit{Eur. J.~Financ.} 16(7):711--726.
\bibitem{76-kl}
\Aue{Bollen, J., H.~Mao, and X.~Zeng}. 2011. Twitter mood predicts the stock
market. \textit{J.~Comput. Sci.} 2(1):1--8.
\bibitem{77-kl}
\Aue{Spangler, S., A.\,D.~Wilkins, B.\,J.~Bachman, \textit{et al}.} 2014. Automated
hypothesis generation based on mining scientific literature. \textit{KDD'14
Proceedings}. New York. 1877--1886.
\bibitem{78-kl}
\Aue{Zhou, D., O. Bousquet, T.\,N.~Lal, J.~Weston, and
B.~Sch$\ddot{\mbox{o}}$lkopf}. 2004. Learning with local and global consistency.
\textit{Adv. Neur. Inform. Proc. Syst.} 16(16):321--328.

\end{thebibliography} } }

\end{multicols}

\vspace*{-3pt}

\hfill{\small\textit{Received February 10, 2015}}

\vspace*{-36pt}

\Contr

\noindent
\textbf{Kalinichenko Leonid A.} (b.\ 1937)~--- Doctor of Science in physics and mathematics, professor;
Head of Laboratory, Institute of Informatics Problems, Russian Academy of Sciences; 44-2 Vavilov Str.,
Moscow 119333, Russian Federation; professor, Faculty of Computational Mathematics and
Cybernetics, M.\,V.~Lomonosov Moscow State University, 1-52 Leninskiye Gory, GSP-1, Moscow
119991, Russian Federation; leonidandk@gmail.com

\vspace*{3pt}

\noindent
\textbf{Kovalev Dmitry Yu.} (b.\ 1988)~--- junior scientist, Institute of Informatics Problems, Russian
Academy of Sciences, 44-2 Vavilov Str., Moscow 119333, Russian Federation;
dkovalev@ipiran.ru


\vspace*{3pt}

\noindent
\textbf{Kovaleva Dana A.} (b.\ 1973)~--- Candidate of Science (PhD)
in physics and mathematics, scientist, Institute of
Astronomy, Russian Academy of Sciences, 48 Pyatnitskaya Str., Moscow 119017, Russian Federation;
dana@inasan.ru

\vspace*{3pt}

\noindent
\textbf{Malkov Oleg Yu.} (b.\ 1961)~---
Doctor of Science in physics and mathematics, associate professor; Head of Department, Institute of
Astronomy, Russian Academy of Sciences; 48 Pyatnitskaya Str., Moscow 119017, Russian Federation;
professor, Faculty of Physics, M.\,V.~Lomonosov Moscow State University, 1-52 Leninskiye Gory, GSP-1,
Moscow 119991, Russian Federation; malkov@inasan.ru

%\vspace*{8pt}

%\hrule

%\vspace*{2pt}

%\hrule

%\vspace*{-6pt}

\newpage

\vspace*{-18pt}


\def\tit{МЕТОДЫ И~СРЕДСТВА ПОДДЕРЖКИ ИССЛЕДОВАНИЙ, ДВИЖИМЫХ ГИПОТЕЗАМИ: ОБЗОР}

\def\aut{Л.\,А.~Калиниченко$^1$, Д.\,Ю.~Ковалев$^2$, Д.\,А.~Ковалева$^3$, О.\,Ю.~Малков$^4$}


\def\titkol{Методы и средства поддержки исследований, движимых гипотезами: Обзор}

\def\autkol{Л.\,А.~Калиниченко, Д.\,Ю.~Ковалев, Д.\,А.~Ковалева, О.\,Ю.~Малков}

%{\renewcommand{\thefootnote}{\fnsymbol{footnote}}
%\footnotetext[1]{Работа проводится при финансовой поддержке Программы
%стратегического развития Петрозаводского государственного университета в рамках
%на\-уч\-но-ис\-сле\-до\-ва\-тель\-ской деятельности.}}


\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-12pt}

\noindent
$^1$Институт проблем информатики Российской академии наук; leonidandk@gmail.com

\noindent
$^2$Институт проблем информатики Российской академии наук; dkovalev@ipiran.ru

\noindent
$^3$Институт астрономии Российской академии наук; dana@inasan.ru

\noindent
$^4$Институт астрономии Российской академии наук;  malkov@inasan.ru


\vspace*{6pt}

\def\leftfootline{\small{\textbf{\thepage}
\hfill ИНФОРМАТИКА И ЕЁ ПРИМЕНЕНИЯ\ \ \ том\ 9\ \ \ выпуск\ 1\ \ \ 2015}
}%
 \def\rightfootline{\small{ИНФОРМАТИКА И ЕЁ ПРИМЕНЕНИЯ\ \ \ том\ 9\ \ \ выпуск\ 1\ \ \ 2015
\hfill \textbf{\thepage}}}

\Abst{Исследования с интенсивным использованием данных (ИИИД), развиваемые в рамках новой
парадигмы изучения естественных явлений, именуемой Четвертой парадигмой, придают особое
значение все возрастающей роли, которую играют данные, полученные в результате наблюдений,
экспериментов или компьютерного моделирования,  практически во всех областях анализа и
накопления информации. Главной целью ИИИД является извлечение (вывод) знаний из данных.
Целью настоящей работы является обзор существующих подходов, методов и инфраструктур анализа
данных в ИИИД с акцентом на роли гипотез в процессе анализа информации и эффективной
поддержки формирования, оценки и выбора гипотез при моделировании естественных явлений и
проведении экспериментов. Статья включает введение в разнообразные понятия, методы и средства
эффективной организации  движимых гипотезами экспериментов в ИИИД.}

\KW{исследования с интенсивным использованием данных; Четвертая парадигма; гипотезы; модели;
теории; ги\-по\-те\-ти\-ко-де\-дук\-тив\-ный метод; проверка гипотез; решетка гипотез; модель
Галактики, анализ коннектома; автоматизированная генерация гипотез}

\DOI{10.14357/19922264150104}

%\vspace*{6pt}


 \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily Литература}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
{%\baselineskip=10.8pt
\begin{thebibliography}{99}
\bibitem{1-kl-1}
The Fourth paradigm: Data-intensive scientific discovery~/
Eds. T.~Hey, S.~Tansley, K.~Tolle.~--- Redmond, Microsoft Research, 2009.  252~p.
\bibitem{2-kl-1}
\Au{McComas W.\,F.} The principal elements of the nature of science: Dispelling the
myths of science~// Nature of science in science education: Rationales and
strategies~/ Ed. W.\,F.~McComas.~--- Kluwer Academic Publs., 1998. P.~53--70.
\bibitem{3-kl-1}
\Au{Lakshmana Rao J.\,R.} Scientific `Laws', `Hypotheses' and `Theories'~//
Meanings Distinctions Reson., 1998. Vol.~3. P.~69--74.
\bibitem{4-kl-1}
\Au{Poincar$\acute{\mbox{e}}$ H.} The foundations of science: Science and
hypothesis, the value of science,  science and method. The Project Gutenberg
EBook, 2012. No.~39713. P.~554.
{\sf http://www.gutenberg.org/files/39713/39713-8.txt}.
\bibitem{5-kl-1}
\Au{Bacon F.} The new organon~// Great books of the Western World. Vol.~30. The works of
Francis Bacon~/ Ed.\ R.\,M.~Hutchins.~--- Chicago, Encyclopedia Britannica, Inc.,
1952.  P.~107--195.
\bibitem{6-kl-1}
\Au{Menzies T.} Applications of abduction: Knowledge-level modeling~//
Int. J.~Hum.-Comput. St., 1996. Vol.~45. No.\,3. P.~305--335.
\bibitem{7-kl-1}
\Au{Haber J.} Research questions, hypotheses, and clinical questions~// Evolve
resources for nursing research.~--- 7th ed.~--- Elsevier, 2010. P.~27--55.
\bibitem{8-kl-1}
\Au{Popper K.} The logic of scientific discovery.~---
London\,--\,New York: Routledge, Taylor \& Francis,
2005. 545~p. {\sf http://strangebeautiful.com/other-texts/popper-logic-scientific-discovery.pdf}.
\bibitem{9-kl-1}
\Au{Kerlinger F.\,N.,  Lee H.\,B.} Foundations of behavioral research: Educational
and psychological inquiry.~--- New York: Holt, Rinehart and Winston, 1964. 739~p.
\bibitem{10-kl-1}
\Au{Hempel C.\,G.} Fundamentals of concept formation in empirical science~// Int.
Encyclopedia Unified Sci., 1952. Vol.~2. No.\,7.
{\sf
http://www.iep.utm.edu/hempel/}.


\bibitem{12-kl-1} %11
\Au{Porto F., Spaccapietra~S.} Data model for scientific models and hypotheses~//
Evolution Conceptual Modeling, 2011, Vol.~6520. P.~285--305.
\bibitem{11-kl-1} %12
\Au{Gon\/{\!\fontsize{10pt}{10pt}\selectfont\ptb{\!\!\c{c}}}alves~B., Porto~F.} A~lattice-theoretic approach for
representing and managing hypothesis-driven research~// AMW, 2013.

\bibitem{13-kl-1}
\Au{Gon\/{\!\fontsize{10pt}{10pt}\selectfont\ptb{\!\!\c{c}}}alves~B., Porto~F., Moura~A.\,M.\,C.} On the semantic
engineering of scientific hypotheses as linked data~// 2nd Workshop (International)
on Linked Science Proceedings, 2012.
\bibitem{14-kl-1}
\Au{Woodward J.} Scientific explanation~// The Stanford Encyclopedia of
Philosophy, 2011. {\sf http://plato.stanford.edu/ archives/win2011/entries/scientific-explanation/}.
\bibitem{15-kl-1}
Scientific discovery: Case studies~/
Ed. T.~Nickles.~--- Taylor \& Francis, 1980. Vol.~2. 501~p.
\bibitem{16-kl-1}
\Au{Schickore J.} Scientific discovery~// The Stanford Encyclopedia of Philosophy,
2014. {\sf http://plato.stanford.edu/ archives/spr2014/entries/scientific-discovery/}.
\bibitem{17-kl-1}
\Au{Kakas A.\,C., Kowalski R.\,A.,  Toni~F.} Abductive logic programming~//
J.~Logic Comput., 1993.  Vol.~2. No.\,6.  P.~719--770.
\bibitem{18-kl-1}
\Au{Kakas A.\,C., Michael~A.,  Mourlas~C.} ACLP: Abductive constraint logic
programming~// J.~Logic Program., 2000. Vol.~44. No.\,1. P.~129--177.
\bibitem{19-kl-1}
\Au{Van Nuffelen B., Kakas A.} A-system: Declarative programming with
abduction~// Logic programming and nonmotonic reasoning~/
Eds. T.~Eiter, W.~Faber, M.~Truszczy$\acute{\mbox{n}}$ski.~---
Lecture notes in computer science ser.~--- Berlin--Heidelberg:
Springer, 2001. Vol.~2173. P.~393--397.
\bibitem{20-kl-1}
\Au{Alferes J.\,J., Pereira L.\,M., Swift~T.} Abduction in well-founded semantics and
generalized stable models via tabled dual programs~// Theor. Pract. Log. Prog.,
2004. Vol.~4. No.\,4.
P.~383--428.
\bibitem{21-kl-1}
\Au{Ray O., Kakas A.} ProLogICA: A~practical system for Abductive Logic
Programming~// 11th Workshop (International) on Non-Monotonic Reasoning
Proceedings, 2006. P.~304--312.
\bibitem{22-kl-1}
\Au{Citrigno S., Eiter T., Faber~W., Gottlob~G., Koch~C., Leone~N., Scarcello~F.}
The dlv system: Model generator and application frontends~// 12th Workshop on
Logic Programming Proceedings, 1997.  P.~128--137.
\bibitem{23-kl-1}
\Au{King R.\,D., Liakata M., Lu~C., Oliver~S.\,G., Soldatova~L.\,N.} On the
formalization and reuse of scientific research~// J.~Roy. Soc. Interface,
2011. Vol.~8. No.\,63.  P.~1440--1448.
\bibitem{24-kl-1}
\Au{Tamaddoni-Nezhad A., Chaleil~R., Kakas~A., Muggleton~S.\,H.} Application of
abductive ILP to learning metabolic network inhibition from temporal data~//
Mach. Learn., 2006. Vol.~64.  P.~209--230.
\bibitem{25-kl-1}
\Au{Inoue K., Sato T., Ishihata M., Kameya~Y., Nabeshima~H.} Evaluating
abductive hypotheses using and EM algorithm on BDDs~// IJCAI-09 Proceedings,
2009. P.~810--815.
\bibitem{26-kl-1}
\Au{Bartha P.} Analogy and analogical reasoning~// The Stanford Encyclopedia of
Philosophy, 2013. {\sf
http://plato. stanford.edu/archives/fall2013/entries/reasoning-analogy/}.
\bibitem{27-kl-1}
\Au{Ivezi$\acute{\mbox{c}}$~{\ptb{\v{Z}}}., Connolly~A.\,J., VanderPlas~J.\,T.,
Gray~A.} Statistics, data mining, and machine learning in astronomy: A~practical
Python guide for the analysis of survey data.~--- Princeton University Press, 2014.
552~p.
\bibitem{28-kl-1}
\Au{Sivia D.\,S., Skilling~J.} Data analysis. A~Bayesian tutorial.~--- New
York: Oxford University Press Inc., 2006.  264~p.
\bibitem{29-kl-1}
\Au{Field A.} Discovering statistics using IBM SPSS statistics.~---
4th ed.~--- Sage, 2013.  915~p.
\bibitem{30-kl-1}
IBM SPSS Statistics for Windows, Version 22.0. Armonk, N.Y.: IBM Corp. IBM
SPSS Statistics base, 2013.
{\sf
https://www.uio.no/tjenester/it/forskning/statistikk/
hjelp/programveilednigner/ibm\_spss\_statistics\_brief\_ guide-2.pdf}.

\bibitem{31-kl-1}
\Au{Ihaka R., Gentleman R.} R:~A~language for data analysis and graphics~//
J.~Comput. Graph. Stat., 1996. Vol.~5. No.\,3. P.~299--314.
\bibitem{32-kl-1}
\Au{March M.\,C., Starkman G.,D., Trotta~R., Vaudrevange~P.\,M.} Should we
doubt the cosmological constant?~// Mon. Not. Roy. Astron. Soc.,
2011. Vol.~410. No.\,4. P.~2488--2496.
\bibitem{33-kl-1}
\Au{Rouder J.\,N., Speckman~P.\,L., Sun~D., Morey~R.\,D., Iverson~G.} Bayesian t
tests for accepting and rejecting the null hypothesis~// Psychon. Bull.
Rev., 2009.  Vol.~16. No.\,2. P.~225--237.
\bibitem{34-kl-1}
\Au{Weber M.} Experiment in biology~// The Stanford Encyclopedia of Philosophy,
2014. {\sf  http://plato.stanford.edu/ archives/fall2014/entries/biology-experiment/}.
\bibitem{35-kl-1}
\Au{Hawthorne J.} Inductive logic~// The Stanford Encyclopedia of Philosophy,
2014. {\sf http://plato.stanford.edu/ archives/sum2014/entries/logic-inductive/}.
\bibitem{36-kl-1}
\Au{Breiman L.} Statistical modeling: The two cultures~// Stat. Sci., 2001.
Vol.~16. No.\,3. P.~199--231.

\bibitem{38-kl-1} %37
\Au{Hastie T., Tibshirani R., Friedman~J.,  Franklin~J.} The elements of statistical
learning: Data mining, inference and prediction~// Math. Intell.,
2005. Vol.~27. No.\,2. P.~83--85.

\bibitem{37-kl-1} %38
\Au{Barber D.} Bayesian reasoning and machine learning.~--- Cambridge University
Press, 2010.  720~p.

\bibitem{39-kl-1}
\Au{Ferrucci D., Brown E., Chu-Carroll~J., Fan~J., Gondek~D., Kalyanpur~A.\,A.,
Welty~C.} Building Watson: An overview of the DeepQA project~// AI Mag.,
2010. Vol.~31. No.\,3. P.~59--79.
\bibitem{40-kl-1}
\Au{Dredze M, Crammer K., Pereira~F.} Confidence-weighted linear
classification~// 25th Conference (International) on Machine Learning Proceedings.
Helsinki, Finland, 2008.  P.~264--271.
\bibitem{41-kl-1}
\Au{Starkman G.\,D., Trotta~R., Vaudrevange~P.\,M.} Introducing doubt in Bayesian
model comparison. arXiv preprint arXiv:0811.2415, 2008.
\bibitem{42-kl-1}
\Au{March M.\,C.} Advanced statistical methods for astrophysical probes of
cosmology. Springer Theses, 2013.  Vol.~20. 177~p.
\bibitem{43-kl-1}
\Au{Porto F.} Big data in astronomy. The LIneA-DEXL case~// Presentation at the
EMC Summer School on BIG DATA~--- NCE/UFRJ, 2013.
{\sf
http://www.slideshare.net/ fabiomporto/emc-2013-big-data-in-astronomy}.
\bibitem{44-kl-1}
\Au{Racunas S.\,A., Shah~N.\,H., Albert~I., Fedoroff~N.\,V.} Hybrow: A~prototype
system for computer-aided hypothesis evaluation~// Bioinformatics, 2004. Vol.~20.
No.\,1. P.~257--264.
\bibitem{45-kl-1}
\Au{Soldatova L.\,N., Rzhetsky~A., King~R.\,D.} Representation of research
hypotheses~// J.~Biomed. Semantics, 2011. Vol.~2. No.\,S-2. P.~S9.
\bibitem{46-kl-1}
\Au{Callahan A., Duumontier~M., Shah~N.} HyQue: Evaluating hypotheses using
Semantic Web technologies~// J.~Biomed. Semantics, 2011. Vol.~2. No.\,S-2.
P.~S3.
\bibitem{47-kl-1}
\Au{Gao Y., Kinoshita J., Wu~E., Miller~E., Lee~R., Seaborne~A., Clark~T.}
SWAN: A~distributed knowledge infrastructure for Alzheimer disease
research~// J.~Web Semant., 2006. Vol.~4. No.\,3. P.~222--228.
\bibitem{48-kl-1}
\Au{King R.\,D., Whelan K.\,E., Jones~F.\,M., Reiser~P.\,G., Bryant~C.\,H.,
Muggleton~S.\,H., Oliver~S.\,G.} Functional genomic hypothesis generation and
experimentation by a~robot scientist~// Nature, 2004.  Vol.~427. No.\,6971.
P.~247--252.
\bibitem{49-kl-1}
\Au{Porto F., Moura~A.\,M.\,C., Gon\/{\!\fontsize{10pt}{10pt}\selectfont\ptb{\!\!\c{c}}}alves~B., Costa~R., Spaccapietra~S.\,A.}
A~scientific hypothesis conceptual model~//
{Advances in conceptual modeling}~/
Eds. S.~Castano, P.~Vassiliadis, L.\,V.~Lakshmanan, M.~Li~Lee.~---
Lecture notes in computer science ser.~--- Berlin--Heidelberg: Springer, 2012.
Vol.~7518. P.~101--110.
\bibitem{50-kl-1}
\Au{Porto F., Moura A.\,M.\,C.} Scientific hypothesis database. Report, 2011.
{\sf
http://livroaberto.ibict.br/bitstream/1/ 869/1/Scientific\%20Hypothesis\%20Database.pdf}.
\bibitem{51-kl-1}
\Au{Asgharbeygi N., Langley P., Bay~S., Arrigo~K.} Inductive revision of
quantitative process models~// Ecol. Model., 2006. Vol.~194. No.\,1.
P.~70--79.
\bibitem{52-kl-1}
\Au{Tran N., Baral C., Nagaraj~V.\,J., Joshi~L.} Knowledge-based integrative
framework for hypothesis formation in biochemical networks~// Data
integration in the life sciences~/
Eds. B.~Lud$\ddot{\mbox{a}}$scher, L.~Raschid.~---
Lecture notes in computer science ser. Berlin--Heidelberg: Springer,
2005. Vol.~3615. P.~121--136.

\bibitem{53-kl-1}
\Au{Sparkes A., Aubrey W., Byrne~E., Clare~A., Khan~M.\,N., Liakata~M.,
King~R.\,D.} Towards Robot Scientists for autonomous scientific discovery~//
Autom. Exp., 2010.  Vol.~2. No.\,1. {\sf
http://www.aejournal.net/content/2/1/1}.
\bibitem{54-kl-1}
Yeast systems biology: Methods and protocols~/
Eds. J.\,I.~Castrillo,  S.\,G.~Oliver.~---  Methods in molecular biology ser.~---
Berlin--Heidelberg: Springer, 2011. Vol.~759. 549~p.
\bibitem{55-kl-1}
\Au{Plotkin G.\,D.} A~note on inductive generalization~// Mach. Intell.,
1970.  Vol.~5. P.~153--163.
\bibitem{56-kl-1}
\Au{Huang J., Antova L., Koch~C., Olteanu~D.} MayBMS: A~probabilistic database
management system~// 2009 ACM SIGMOD Conference (International) on
Management of Data Proceedings, 2009. P.~1071--1074.

\bibitem{59-kl-1} %57
\Au{Robin A., Cr$\acute{\mbox{e}}$z$\acute{\mbox{e}}$~M.} Stellar populations in
the Milky Way~--- a~synthetic model~// Astron. Astrophys., 1986. Vol.~157.
P.~71--90.

\bibitem{58-kl-1}
\Au{Robin A.\,C., Reyl$\acute{\mbox{e}}$~C., Derri{\!\!\!\ptb{\`{e}}}re~S.,
Picaud~S.} A~synthetic view on structure and evolution of the Milky Way.
arXiv preprint astro-ph/0401052, 2004.

\bibitem{57-kl-1} %59
\Au{Czekaj M.\,A., Robin A.\,C., Figueras~F., Luri~X., Haywood~M.}
The Besan{\!\fontsize{10pt}{10pt}\selectfont\ptb{\!\c{c}}}on
Galaxy model renewed-I. Constraints on the local star formation history from Tycho
data~//Astron. Astrophys., 1986. Vol.~564. P.~A102.

\bibitem{60-kl-1}
\Au{Czekaj~M.\,A.} Galaxy evolution: A~new version of the
Besan{\!\fontsize{10pt}{10pt}\selectfont\ptb{\!\c{c}}}on Galaxy
Model constrained with Tycho data. PhD Thesis, 2012. Universitet de
Barcelona, Spain. 167~p.
\bibitem{61-kl-1}
\Au{Martins A.\,M.\,M.} Statistical analysis  of large scale surveys for constraining
the Galaxy evolution. PhD Thesis, 2014. Universitet de Barcelona, Spain.
221~p.
\bibitem{62-kl-1}
\Au{Biswal B.\,B., Mennes~M., Zuo~X.\,N., Gohel~S., Kelly~C., Smith~S.\,M.,
Windischberger~C.} Toward discovery science of human brain function~// Proc.
Nat. Acad. Sci. USA, 2010.  Vol.~107. No.\,10. P.~4734--4739.
\bibitem{63-kl-1}
\Au{Craddock R.\,C., Jbabdi~S., Yan~C.\,G., Vogelstein~J.\,T., Castellanos~F.\,X.,
Di~Martino~A., Milham~M.\,P.} Imaging human connectomes at the macroscale~//
Nat. Methods, 2013. Vol.~10. No.\,6. P.~524--539.
\bibitem{64-kl-1}
\Au{Ginestet C.\,E., Balanchandran~P., Rosenberg~S., Kolaczyk~E.\,D.} Hypothesis
testing for network data in functional neuroimaging. arXiv preprint arXiv:1407.5525,
2014.
\bibitem{65-kl-1}
\Au{Ginestet C.\,E., Fournel~A.\,P., Simmons~A.} Statistical network analysis for
functional MRI: Summary networks and group comparisons~// Front.
Comput. Neurosci., 2014. Vol.~8. P.~51.
{\sf
http://www.ncbi.nlm.nih.gov/ pmc/articles/PMC4018548/}.
\bibitem{66-kl-1}
\Au{Yan C.\,G., Craddock~R.\,C., Zuo~X.\,N., Zang~Y.\,F., Milham~M.\,P.}
Standardizing the intrinsic bra towards robust measurement of inter-individual
variation in 1000 functional connectomes~// Neuroimage, 2013. Vol.~80.
P.~246--262.
\bibitem{67-kl-1}
\Au{Marcus D.\,S., Harwell J., Olsen~T., Hodge~M., Glasser~M.\,F.,
Prior~F., Van Essen~D.\,C.} Informatics and data mining tools and strategies
for the human connectome project~// Front. Neuroinform., 2011.
Vol.~5. {\sf http://www.ncbi.nlm.nih.gov/pmc/\linebreak articles/PMC3127103/}.
\bibitem{68-kl-1}
\Au{Marcus D.\,S., Olsen T.\,R., Ramaratnam~M., Buckner~R.\,L.} The extensible
neuroimaging archive toolkit~// Neuroinformatics, 2007. Vol.~5. No.\,1. P.~11--33.
\bibitem{69-kl-1}
\Au{Brun A.} Manifold learning and representations for image analysis and
visualization. Department of Biomedical Engineering, Link$\ddot{\mbox{o}}$pings
Universitet, 2006. 104~p.
\bibitem{70-kl-1}
\Au{Mahmoudi A., Takerkart~S., Regragui~F., Boussaoud~D., Brovelli~A.}
Multivoxel pattern analysis for fMRI data: A~review~// Comput.
Math. Methods Med., 2012.
{\sf
http://www.hindawi.com/journals/cmmm/2012/\linebreak 961257/}.
\bibitem{71-kl-1}
\Au{Van Horn J.\,D., Toga~A.\,W.} Human neuroimaging as a~``Big Data''
science~// Brain Imaging Behavior, 2014.  Vol.~8. No.\,2.  P.~323--331.
\bibitem{72-kl-1}
\Au{Hillebrandt H., Friston~K.\,J., Blakemore~S.\,J.} Effective connectivity during
animacy perception-dynamic causal modelling of Human Connectome Project data~//
Sci. Rep., 2014.  Vol.~4. {\sf
http://www.ncbi.nlm.nih.gov/ pmc/articles/PMC4150124/}.
\bibitem{73-kl-1}
\Au{Lappalainen J., Sicilia~M.\,$\acute{\mbox{A}}$.,
Hern$\acute{\mbox{a}}$ndez~B.} Automatic hypothesis checking using eScience
Research Infrastructures, ontologies, and linked data: A~case study in climate change
research~// Procedia Comput. Sci., 2013. Vol.~18. P.~1172--1178.
\bibitem{74-kl-1}
\Au{Lenten L.\,J., Moosa I.\,A.} An empirical investigation into long-term climate
change in Australia~// Environ. Modell. Softw., 2003. Vol.~18. No.\,1.
P.~59--70.
\bibitem{75-kl-1}
\Au{Borges M.\,R.} Efficient market hypothesis in European stock markets~//
Eur. J.~Financ., 2010. Vol.~16. No.\,7. P.~711--726.
\bibitem{76-kl-1}
\Au{Bollen J., Mao H., Zeng~X.} Twitter mood predicts the stock market~//
J.~Comput. Sci., 2011. Vol.~2. No.\,1. P.~1--8.
\bibitem{77-kl-1}
\Au{Spangler S., Wilkins A.\,D., Bachman~B.\,J., \textit{et al.}} Automated
hypothesis generation based on mining scientific literature~// KDD'14 Proceedings,
2014. P.~1877--1886.
\bibitem{78-kl-1}
\Au{Zhou D., Bousquet~O., Lal~T.\,N., Weston~J.,  Sch$\ddot{\mbox{o}}$lkopf~B.}
Learning with local and global consistency~// Adv. Neur. Inform.
Proc. Syst., 2004.  Vol.~16. No.\,16. P.~321--328.

\end{thebibliography}
} }

\end{multicols}

 \label{end\stat}

 \vspace*{-3pt}

\hfill{\small\textit{Поступила в редакцию 10.02.2015}}
\renewcommand{\bibname}{\protect\rm Литература}
\renewcommand{\figurename}{\protect\bf Рис.}

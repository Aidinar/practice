\def\stat{popova}

\def\tit{ВЫБОР ОПТИМАЛЬНОЙ МОДЕЛИ КЛАССИФИКАЦИИ ФИЗИЧЕСКОЙ АКТИВНОСТИ
ПО~ИЗМЕРЕНИЯМ АКСЕЛЕРОМЕТРА$^*$}

\def\titkol{Выбор оптимальной модели классификации физической активности
по измерениям акселерометра}

\def\aut{М.\,С.~Попова$^1$, В.\,В.~Стрижов$^2$}

\def\autkol{М.\,С.~Попова, В.\,В.~Стрижов}


\titel{\tit}{\aut}{\autkol}{\titkol}

{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
{Работа поддержана Skolkovo Institute of Science
and Technology (Skoltech) в рамках SkolTech/MITInitiative.}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Московский физико-технический институт,
maria\_popova@phystech.edu}
\footnotetext[2]{Вычислительный центр Российской академии
наук им.~А.\,А. Дородницына, strijov@ccas.com}

\vspace*{2pt}

\Abst{Решается проблема построения
оптимальных устойчивых моделей в~задаче классификации физической
активности человека. Каждый тип физической активности конкретного
человека описывается набором признаков, сгенерированных по временн$\acute{\mbox{ы}}$м
рядам с~акселерометра. В~условиях мультиколлинеарности признаков
выбор устойчивых моделей классификации затруднен из-за необходимости
оценки большого числа параметров этих моделей. Оценка оптимального
значения параметров также затруднена в~связи с~тем, что функция
ошибок имеет большое количество локальных минимумов в~пространстве
параметров. В~работе исследуются модели, принадлежащие классу
двуслойных нейронных сетей. Ставится задача нахождения
Па\-ре\-то-опти\-маль\-но\-го фронта на множестве допустимых моделей.
Предлагаются критерии оптимального, последовательного и~устойчивого
прореживания нейронной сети, критерий наращивания сети, а~также
строится стратегия пошаговой модификации модели с~использованием
предложенных критериев. В~вычислительном эксперименте модели,
порождаемые предложенной стратегией, сравниваются по трем критериям
качества~--- сложности, точности и~устойчивости.}

\vspace*{2pt}

\KW{классификация; нейронные сети; сложность;
устойчивость; оптимальность по Парето; критерии прореживания и~наращивания}

\DOI{10.14357/19922264150107}

\vspace*{6pt}


\vskip 14pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}



\section{Введение}

Для получения точного и~устойчивого прогноза физической активности
человека необходимы\linebreak методы, позволяющие выбирать адекватные модели
из некоторого множества допустимых моделей-пре\-тен\-ден\-тов. Проблема
выбора моделей об\-суж\-да\-ет\-ся в~работах~[1--3]. Настройка параметров\linebreak
универсальной модели является нетривиальной многоэкстремальной
оптимизационной задачей. Предлагается упростить эту задачу,
рассматривая наборы последовательно порождаемых устойчивых моделей
заданной сложности. Модели порождаются путем модификации структуры
искусственной нейронной сети. Решается задача последовательной
модификации нейронной сети. Требуется получить нейронную сеть
с~небольшим числом связей между нейронами, которая достаточно точно
решала бы задачу классификации физической активности человека по
показаниям акселерометра и~обладала бы устойчивостью к возмущениям
данных. Ввиду этого возникает задача минимизации сложности модели
без потери точности классификации~\cite{Myung2000Complexity}.

Существует два базовых подхода к~решению задачи выбора сетей
оптимальной структуры: \textit{наращивание структуры сети} (network
growing)~\cite{MacLeod2001Grow} и~\textit{прореживание структуры сети}
(network pruning)~[6--8].

Согласно первому подходу в качестве начальной модели выбирается сеть
недостаточной сложности, решающая поставленную задачу с~большим
значением функции ошибки, после чего~в~сеть добавляются новые
нейроны и~связи между ними. В~\cite{MacLeod2001Grow} описаны
некоторые методы наращивания, приведен сравнительный анализ
генетических алгоритмов с~алгоритмом байесовской оптимизации.
В~алгоритмах метода прореживания модифицируется многослойная сеть
с~избыточным числом нейронов и~связей между ними. Классическими
алгоритмами прореживания нейронных сетей являются <<optimal brain
damage>>~\cite{LeCun1990Optimal} и~<<optimal brain surgery>>~\cite{Hassibi93}, основанные на вычислении вторых производных
функции ошибки. Также получили развитие \textit{гибридные алгоритмы},
в~которых объединяются оба упомянутых выше подхода~[9--11].

В данной работе предлагается стратегия пошаговой модификации
нейронной сети, комбинирующая этапы добавления и~удаления
па\-ра\-мет\-ров~\cite{Knerr1990Stepwise, Strijov2013Evidence-1}. Стратегия включает
в~себя критерии прореживания и наращивания структуры сети, критерии
останова этапов добавления и~удаления параметров, а~также критерий
останова процедуры модификации. Согласно предложенной стратегии
процедура модификации начинается с~нейронной сети избыточной
сложности и чередует шаги удаления и добавления параметров до тех
пор, пока этот процесс не стабилизируется согласно критерию останова
процедуры модификации. Критерии прореживания и~наращивания позволяют
на каж\-дом шаге процедуры модификации выбирать параметр, добавление
или удаление которого улучшит качество нейронной сети. Качество сети
оценивается по трем критериям: сложности, точ\-ности
и~устойчивости~\cite{Tokmakova2012HyperPar, Leonteva2012Feature,
Zaycev2012Evaluation}. Также предлагается рассматривать процедуру
пошаговой модификации нейронной сети как путь в~многомерном кубе.

В вычислительном эксперименте определяются значения критериев
качества для нейронных сетей, порождаемых предложенной стратегией.
В~качестве тестового примера рассматривается задача классификации
физической активности человека по измерениям акселерометра~\cite{Kwapisz2010Activity}.

\section{Постановка задачи}

Дана выборка $\mathfrak{D} =\left\{ (\mathbf{x}_i,
\mathbf{t}_i) \right\}, i\in\mathcal{I}\hm=\{1 \dots m\}$,
состоящая из~$m$ объектов~$\mathbf{x}$, каждый~из~которых описывается~$n$
признаками,~$\mathbf{x}_i\in\mathbb{R}^n$,
и~принадлежит одному из~$z$ классов~$\mathbf{t}_i\hm\in\{0,1\}^z$.
Также задано разбиение множества индексов выборки $\mathcal{I} \hm
= \mathcal{L}\sqcup \mathcal {T}$ на обучающую
${(\mathbf{x}_i,\mathbf{t}_i)}, i \hm\in \mathcal{L}$,
и~контрольную ${(\mathbf{x}_i,\mathbf{t}_i)}, i \hm\in \mathcal{T}$.
Требуется выбрать устойчивую модель классификации оптимальной сложности.


\smallskip

\noindent
\textbf{Определение 1.}
Моделью назовем отображение
\begin{equation*}
\mathbf{f} : (\mathop{\mathbf{w}}\limits_{k \times 1},
\mathop{\mathbf{x}}\limits_{1 \times n}) \mapsto
\mathop{\mathbf{y}}\limits_{1 \times z}\,,
\end{equation*}
где $\mathbf{w} = \left[w_1, \dots,w_j,\dots,w_k\right]^{\mathrm{T}}$, $j \hm\in \mathcal{J} = \{1,\dots, k\},$~---
вектор параметров модели; $\mathbf{x} \hm\in \mathbb{R}^{n\times m}$~---
матрица плана; $\mathbf{y} \in \{0,1\}^z$~--- зависимая переменная.

\smallskip

Предполагается, что переменная $\mathbf{y}$~--- мультиномиально
распределенная случайная величина, а~переменная~$\mathbf{w}$ имеет нормальное
распределение с~нулевым математическим ожиданием:
\begin{equation}
\label{eq:1}
\mathbf{w} \sim \mathcal{N}\left(\boldsymbol{0}, \mathbf{A}^{-1}\right)\,,
\end{equation}
где $\mathbf{A}^{-1}$~--- ковариационная матрица параметров
общего вида, по\-ло\-жи\-тель\-но-опре\-де\-лен\-ная:
$\mathbf{w}^{\mathrm{T}}\mathbf{A}\mathbf{w} \hm> \boldsymbol{0}$ для любого
$\mathbf{w} \in \mathbb{R}^k$.

В данной работе  рассматриваются модели, принадлежащие классу
двухслойных нейронных сетей c~функциями активации $\tanh$  и~softmax:
\begin{align}
\label{eq14}
\mathbf{a(\mathbf{x})}& =
\mathop{\mathbf{W}_2^{\mathrm{T}}}\limits_{N_h \times z}\mathbf{tanh}(\mathop{\mathbf{W}_1^{\mathrm{T}}}\limits_{n \times N_h}\mathbf{x})\,;
\\
\mathbf{f(\mathbf{x})} &= \fr {\exp(\mathbf{a(\mathbf{x})})}{\sum_{j=1}^n \exp(a_j(\mathbf{x}))}\,.\notag
\end{align}
Вектор~$\mathbf{f}$ интерпретируется как вектор
вероятностей: $f_{\xi}$ есть вероятность того, что вектор~$\mathbf{x}$
принадлежит классу с~номером~${\xi}$:
\begin{equation*}
\mathbf{f(\mathbf{x})} = \{f_\xi\}\,,\enskip
0 \leq f_{\xi} \leq 1\,,\enskip \sum f_{\xi} = 1, \enskip {\xi} = 1\dots z\,.
\end{equation*}
Под вектором параметров двухслойной нейронной сети будем понимать
$\mathbf{w}\hm=\mathbf{vec}\left(\mathbf{W}_1^{\mathrm{T}}|\mathbf{W}_2^{\mathrm{T}}\right)$,
где $\mathbf{W}_1$ и~$\mathbf{W}_2$~--- матрицы весов первого и~второго слоя
нейронной сети~(\ref{eq14}). Вектор $\mathbf{y}\hm=[y_1,\dots,y_{\xi},\dots,y_z]^{\mathrm{T}}$
определим следующим образом:
\begin{equation*}
y_{\xi} = \begin{cases}
1, & \mbox{если}~{\xi} = \mathop{\arg\max}\limits_{\xi \in \{1,\dots,z\}} (p_{\xi})\,;  \\
0 & \mbox{иначе}\,.
\end{cases}
\end{equation*}
Вектор $\mathbf{y}$~--- это вектор метки класса, полученный для
объекта~$\mathbf{x}$ с~помощью построенной модели, в~то время как
вектор~$\mathbf{t}$~-- это вектор метки класса объекта~$\mathbf{x}$ из
выборки~$\mathfrak{D}$.

 Под \textit{структурным} параметром двухслойной нейронной сети
будем понимать количество нейронов в~скрытом слое нейронной сети~--- $N_h$.
Матрица весов первого слоя имеет размерность $n \times N_h$, мат\-ри\-ца весов
второго слоя имеет размерность $N_h \times z$. Далее будем считать, что
структурный параметр фиксирован и~одинаков для всех рассматриваемых моделей.

\smallskip

\noindent
\textbf{Определение 2.}
Параметр $w_j$ модели $\mathbf{f}$ назовем активным, если $w_j \hm\neq 0$.


\smallskip

\noindent
\textbf{Определение 3.}
Структурой $\mathcal{A}$ модели~$\mathbf{f}$ назовем множество
индексов активных параметров этой модели $\mathcal{A} \hm= \{j: w_j \neq 0 \}
\subseteq \mathcal{J}$.


\smallskip

Каждая структура $\mathcal{A} \subseteq \mathcal{J}$ задает некоторую модель
\begin{equation*}
\mathbf{f}_{\mathcal{A}} : \mathbf{\hat{w}}_{\mathcal{A}} \in \mathbb{R}^k\,,
\end{equation*}
где $\mathbf{f}_{\mathcal{A}}$~--- модель со структурой $\mathcal{A}$,
а~$\mathbf{\hat{w}}_{\mathcal{A}}\hm \in \mathbb{R}^k$~--- оптимальный
вектор параметров модели $\mathbf{f}_{\mathcal{A}}$, определение которому
будет дано ниже. Объединение всех $\mathbf{f}_\mathcal{A}$ назовем множеством
допустимых моделей
\begin{equation}
\label{eq:2}
\mathfrak{F} = \bigcup\limits_{{\mathcal{A} \subseteq \mathcal{J}}}\{\mathbf{f}_\mathcal{A}\}\,.
\end{equation}
Оптимальную модель $\mathbf{\hat{f}}_{\mathcal{A}}$ будем выбирать из множества допустимых
моделей~$\mathfrak{F}$.

Согласно гипотезе~(\ref{eq:1}) о~распределении многомерных случайных
величин~$\mathbf{y}$ и~$\mathbf{w}$ в~качестве функции ошибки выберем функцию
\begin{equation*}
%\label{eq:3}
S(\mathbf{w}|\mathcal{K}) = - \sum\limits_{i \in \mathcal{K}}
\sum\limits_{\xi = 1}^{z} t_{i{\xi}}\ln
\left( f_{\xi}(\mathbf{x}_i, \mathbf{w}) \right)\,,
\end{equation*}
максимизирующую логарифм правдоподобия случайной
величины~$\mathbf{y}$ и~заданную на разбиении выборки~$\mathfrak{D}$,
определенном некоторым множеством индексов $\mathcal{K} \subseteq \mathcal{I}$,
$\mathbf{t}_i \hm= [t_{i1},\dots,t_{i\xi},\dots,t_{iz}]^{\mathrm{T}}$.


\smallskip

\noindent
\textbf{Определение 4.}
Оптимальным вектором параметров модели~$\mathbf{f}_{\mathcal{A}}$  назовем
такой вектор~$\mathbf{\hat{w}}_{\mathcal{A}}$, который является решением
сле\-ду\-ющей задачи оптимизации:
\begin{equation}
\label{eq:4}
\mathbf{\hat{w}}_{\mathcal{A}} = \mathop{\arg\min}\limits_{{\mathbf{w}_{\mathcal{A}} \in \mathbb{R}^k}}S(\mathbf{w}_\mathcal{A}|\mathcal{L})\,.
\end{equation}


\smallskip

Для оценки качества моделей и~сравнения их друг с~другом введем три
критерия качества~--- сложность, устойчивость и~точность.

\smallskip

\noindent
\textbf{Определение 5.}
Сложностью $C \hm= C(\mathbf{\hat{w}})$ модели~$\mathbf{f}$ c~вектором
параметров $\mathbf{\hat{w}} \hm= [w_1, \dots, w_k]$ назовем мощность
множества активных параметров этой модели
\begin{equation*}
C(\mathbf{w})= \sum\limits_{i=1}^k[w_i \neq 0] = |\mathcal{A}|\,.
\end{equation*}


\smallskip

Чем больше мощность множества активных параметров, тем сложнее модель.
Максимально возможная сложность модели равна размерности пространства
параметров~$k$.

\smallskip

\noindent
\textbf{Определение 6.}
Устойчивостью $\eta \hm= \eta(\mathbf{\hat{w}})$ модели~$\mathbf{f}$
c~вектором параметров $\mathbf{w}$ назовем число~$\eta$, равное числу
обусловленности матрицы $\mathbf{A}$~($\ref{eq:1}$), т.\,е.
\begin{equation*}
\eta(\mathbf{\hat{w}})=\fr{\lambda_{\max}}{\lambda_{\min}}\,,
\end{equation*}
где $\lambda_{\max}$~--- максимальное, а~$\lambda_{\min}$~---
минимальное собственное число матрицы $\mathbf{A}$.


\smallskip

Чем лучше обусловлена матрица $\mathbf{A}$, тем более устойчива модель.
У~абсолютно устойчивой модели $\lambda_{\min} \hm= \lambda_{\max}$, $\eta \hm= 1$.


\smallskip

\noindent
\textbf{Определение 7.}
Под точностью $S$ модели~$\mathbf{f}$ с~вектором параметров~$\mathbf{\hat{w}}$
будем понимать величину функции ошибки~(\ref{eq:2}) на контрольной выборке.

\smallskip

Чем больше значение функции ошибки, тем меньше точность модели.

Введем на множестве допустимых моделей $\mathcal{F}$ отношение
доминирования. Будем говорить, что \textit{модель~$\mathbf{f}'$ доминирует
над моделью~$\mathbf{f}$} и~обозначать $\mathbf{f}' \succ \mathbf{f}$, если
\begin{equation*}
C^\prime \leq C\,; \quad \eta^\prime \leq \eta\,;\quad S^\prime \leq S_b\,,
\end{equation*}
где $C$, $\eta$, $S$ и~$C^\prime$, $\eta^\prime$, $S^\prime$ --- сложность,
устойчивость и~точность
моделей~$\mathbf{f}$ и~$\mathbf{f}'$.

\smallskip

\noindent
\textbf{Определение 8.}
Модель $\mathbf{f} \hm\in \mathcal{F}$ назовем оптимальной по Парето, если
не существует $\mathbf{f^\prime}\hm \in \mathcal{F}$ такой,
что $\mathbf{f^\prime} \succ \mathbf{f}$.

\smallskip

\noindent
\textbf{Определение 9.}
Множество оптимальных по Парето моделей назовем Па\-ре\-то-опти\-маль\-ным
фронтом {POF}$_{\mathfrak{F}}$ множества допустимых моделей~$\mathfrak{F}$.


\smallskip

Задача выбора оптимальной модели состоит в~том, чтобы найти
Па\-ре\-то-опти\-маль\-ный фронт {POF}$_{\mathfrak{F}}$ множества
допустимых моделей~$\mathfrak{F}$.

\section{Стратегия пошаговой модификации модели}

\noindent
\textbf{Определение 10.}
Стратегией пошаговой модификации модели называется процедура
последовательного изменения модели, в~которой на каж\-дом шаге решается
оптимизационная задача вида
\begin{equation*}
\hat{j} = \mathop{\arg \text{opt}}\limits_{{j \in \mathcal{A}}}Q
(\mathbf{\hat{w}}_\mathcal{A})\,,
\end{equation*}
где $Q$~--- один из вышеприведенных критериев качества или их
Па\-ре\-то-опти\-маль\-ный набор.

\smallskip

Стратегия задается следующими математическими объектами:
\begin{itemize}
\item набором критериев оптимизации~--- слож\-ностью, точ\-ностью, устойчивостью $\{C, S, \eta\}$,
\item набором ограничений на структуру и~па\-ра\-мет\-ры
модели~$\mathcal{A} \subseteq \mathcal{J}$,
$\mathbf{w} = \mathbf{\hat{w}}_{\mathcal{A}}$ из~($\ref{eq:4}$),
\item критериями останова шагов удаления~(см.\ ($\ref{eq:11}$))
и~добавления~(см.\ $(\ref{eq:12})$),
\item критерием останова процедуры выбора модели~(см.\ $(\ref{eq:13})$).
\end{itemize}
Действуя согласно стратегии, будем изменять структуру модели, удаляя
из нее элементы и~добавляя их согласно~(\ref{eq:13}).

Для определения индекса параметра~$\hat{j}$, который должен быть
удален из модели или добавлен в~нее, ниже предлагается несколько
критериев оптимизации модели.

\subsection{Критерий оптимального прореживания}

Этот критерий позволяет выяснить индекс параметра, удаление которого
приведет к~минимизации приращения функции ошибки~(\ref{eq:2}).
Для функции ошибки используется локальная аппроксимация вблизи некоторого
локального минимума вектора параметров~$\mathbf{w}_0$:
\begin{multline*}
S(\mathbf{w}_0 + \Delta \mathbf{w}) = S(\mathbf{w}_0) +
\mathbf{g}^{\mathrm{T}}\mathbf{(w}_0)\Delta \mathbf{w} + {}\\
{}+\fr{1}{2}\,\Delta\mathbf{w}^{\mathrm{T}}
\mathbf{H}\Delta \mathbf{w} + O(\parallel\Delta \mathbf{w}\parallel^3)\,,
\end{multline*}
где~$\Delta \mathbf{w}$~---~возмущение вектора параметров~в~данной
точке~$\mathbf{w}_0;\,\mathbf{g(w}_0)$~--- вектор градиента, вычисленный
в~точке $\mathbf{w}_0$; $\mathbf{H} \hm= \mathbf{H(w}_0)$~--- матрица
вторых производных функции ошибки.
Предполагается, что мат\-ри\-ца вторых производных
$\mathbf{H}\hm=\mathbf{H}(\mathbf{w})$~--- диагональная, а~функция ошибки
в~окрестности глобального или локального минимума является квадратичной.
На основании этих гипотез аппроксимация функции ошибки записывается
в~следующем виде:
\begin{equation*}
\Delta S = S(\mathbf{w}_0 + \Delta \mathbf{w}) - S(\mathbf{w}_0) =
\fr{1}{2}\Delta \mathbf{w}^{\mathrm{T}}\mathbf{H}\Delta \mathbf{w}\,.
\end{equation*}

Пусть~$w_j$~--- некоторый параметр. Удаление этого параметра
(присваивание ему нулевого значения) эквивалентно выполнению условия
\begin{equation*}
\mathbf{e}_j^{\mathrm{T}} \Delta \mathbf{w} + w_j = 0\,,
\end{equation*}
где~$\mathbf{e}_j^{\mathrm{T}}$~--- вектор, все элементы
которого равны нулю, за исключением~$j$-го,~который равен единице.
Таким образом, получаем задачу условной минимизации
\begin{equation*}
\Delta S = \fr{1}{2}\,\Delta \mathbf{w}^{\mathrm{T}}\mathbf{H}\Delta\mathbf{w}
\rightarrow \min\,;\enskip \mathbf{e}_j^{\mathrm{T}} \Delta \mathbf{w} + w_j = 0\,.
\end{equation*}
Для решения этой задачи строим лагранжиан
\begin{equation*}
L = \fr{1}{2}\,\Delta \mathbf{w}^{\mathrm{T}}\mathbf{H}\Delta \mathbf{w} -
\lambda_i(\mathbf{e}_j^{\mathrm{T}} \Delta \mathbf{w} + w_j)\,.
\end{equation*}
Продифференцировав~$L$~по~$\Delta \mathbf{w}$, получаем значение
лагранжиана~$L_j$ для элемента~$w_j$:
\begin{equation*}
L_j = \fr {w_j^2}{2[\mathbf{H}^{-1}]_{j,j}}\,,
\end{equation*}
где~$\mathbf{H}^{-1}$~--- матрица, обратная
гессиану~$\mathbf{H}$; $[\mathbf{H}^{-1}]_{j,j}$~--- $j$-й диагональный элемент
этой матрицы. Значение лагранжиана~$L_j$ называется выпуклостью~$w_j$.
Выпуклость~$L_j$ описывает рост среднеквадратичной ошибки, вызываемый удалением
параметра~$w_j$.

Критерию оптимального прореживания отвечает параметр~$w_{\hat{j}}$,
соответствующий минимальному значению выпуклости:
\begin{equation}
\label{eq:6}
\hat{j} = \mathop{\arg\min}\limits_{{j \in \mathcal{A}}}L_j\,.
\end{equation}

\subsection{Критерий последовательного прореживания}

В качестве второго критерия предлагается прос\-той критерий последовательного
удаления па\-ра\-мет\-ров~$w_j$~--- компонент вектора~$\mathbf{w}$. Основной идеей
этого критерия является принцип ло\-каль\-но-опти\-маль\-но\-го выбора~---
критерию отвечает параметр~$w_j$, без которого функция ошибки~(\ref{eq:2})
оказывается минимальной.

Для нахождения параметра, отвечающего этому критерию, решается задача
\begin{equation}
\label{eq:7}
\hat{j} = \mathop{\arg\min}\limits_{{j \in \mathcal{A}}}S(\mathbf{w}_{\mathcal{A}}
\setminus {w_j}|\mathcal{T})\,.
\end{equation}

\subsection{Критерий устойчивого прореживания}

Помимо вышеописанных критериев предлагается критерий устойчивого прореживания,
основанный на модификации метода Белсли~\cite{Belsley2005, Sandulyanu2012Feature}.

Пусть $\mathbf{W}$~--- матрица реализаций оптимального вектора
параметров~$\mathbf{\hat{w}}$, определенного в~($\ref{eq:4}$)
и~рассматриваемого согласно~($\ref{eq:2}$) как
многомерная случайная величина. Пусть эта матрица имеет размерность $r \times k$.
Выполним ее сингулярное разложение:
\begin{equation}
\label{eq:8}
\mathbf{W} = \mathbf{U}\mathbf{S}\mathbf{V}^{\mathrm{T}},
\end{equation}
где $\mathbf{U}$ и $\mathbf{V}$~--- ортогональные матрицы размера
$r \times r$ и $k \times k$, при этом $r$~-- количество оценок,
а~$k$~-- размерность вектора параметров~$\mathbf{w}$; $\mathbf{\Lambda}$~---
матрица, на диагонали которой стоят сингулярные числа матрицы~$\mathbf{W}$.

По определению ковариационная матрица вектора параметров~$\mathbf{w}$
вычисляется как
\begin{multline*}
\mathbf{A}^{-1} = \text{\bf{cov}}(\mathbf{W}) =
 \mathsf{E}(\mathbf{W}^{\mathrm{T}}\mathbf{W}) -
\mathsf{E}(\mathbf{W})\mathsf{E}(\mathbf{W}^{\mathrm{T}}) ={}\\
{}= \mathsf{E}(\mathbf{W}^{\mathrm{T}}\mathbf{W})\,.
\end{multline*}
Последнее равенство выполняется в~силу предположения о~том,
что математическое ожидание вектора параметров равно нулю:
$\mathsf{E}(\mathbf{w}) \hm= \boldsymbol{0}$. По матрице реализаций~$\mathbf{W}$
многомерной случайной величины~$\mathbf{w}$ ковариационная матрица может быть
оценена следующим образом:
\begin{equation*}
\mathbf{A}^{-1} = \fr{1}{r}\,\mathbf{W}\mathbf{W}^{\mathrm{T}}.
\end{equation*}
У ковариационной матрицы есть нулевые строки с~индексами из
множества $\mathcal{J}\backslash\mathcal{A}$, где $\mathcal{J}$~---
множество индексов всех параметров модели,
а~$\mathcal{A}$~--- множество индексов активных параметров. Таким образом,
ковариационная матрица является неполноранговой.

Используя сингулярное разложение~(\ref{eq:8}) матрицы~$\mathbf{W}$,
получим выражение для матрицы $\mathbf{A}^{-1}$:
\begin{multline*}
\mathbf{A}^{-1} = (\mathbf{W}\mathbf{W}^{\mathrm{T}}) =
(\mathbf{U}\mathbf{\Lambda}\mathbf{V}^{\mathrm{T}}\mathbf{V}\mathbf{\Lambda}^{\mathrm{T}}
\mathbf{U}^{\mathrm{T}}) = {}\\
{}=(\mathbf{U}\mathbf{\Lambda}\mathbf{\Lambda}^{\mathrm{T}}\mathbf{U}^{\mathrm{T}}) =
\mathbf{U}\mathbf{\Lambda}^{2}\mathbf{U}^{\mathrm{T}}.
\end{multline*}

Индексом обусловленности $\eta_{\zeta}$ назовем отношение
максимального элемента~$\lambda_{\max}$ матрицы~$\mathbf{\Lambda}$
к~${\zeta}$-му по величине элементу~$\lambda_{\zeta}$ этой матрицы:
\begin{equation*}
\eta_{\zeta} = \fr{\lambda_{\max}}{\lambda_{\zeta}}\,.
\end{equation*}

Так как ковариационная матрица $\mathbf{A}^{-1}$
неполноранговая, то некоторые значения индексов обу\-слов\-лен\-ности не определены.
Чтобы избежать этой проблемы, исключим из рассмотрения параметры с~дисперсией,
меньшей некоторого порога~$\alpha$, и~добавим к~каж\-до\-му элементу, стоящему
на диагонали ковариационной матрицы, небольшое число~$\tau$.

Оценками дисперсии параметров будут диагональные элементы~$\mathbf{A}^{-1}$:
\begin{equation*}
\sigma(w_{\zeta}) = \mathbf{A}^{-1}_{\zeta \zeta}\,.
\end{equation*}

Долевой коэффицент $q_{{\zeta}j}$ определим как вклад $j$-го признака
в~дисперсию ${\zeta}$-го элемента вектора параметров~$\mathbf{w}$:
\begin{equation*}
q_{{\zeta}j} = \fr {u_{{\zeta}j}^2\lambda_{jj}^2}{\sigma(w_{\zeta})}\,.
\end{equation*}

Находим индексы обусловленности и~долевые коэффициенты для набора
активных параметров~$\mathcal{A}$. Большие значения индексов обусловленности
указывают на зависимость между признаками. Поэтому для нахождения параметра,
отвечающего этому критерию прореживания, находим максимальный индекс
обусловленности
\begin{equation*}
\hat{{\zeta}} = \mathop{\text{argmax}}\limits_{{{\zeta} \in
\mathcal{A}}}\eta_{\zeta}\,.
\end{equation*}
Затем находим максимальный долевой коэффициент, соответствующий
найденному максимальному индексу обусловленности~$\eta_{\hat{{\zeta}}}$:
\begin{equation}
\label{eq:9}
\hat{j} = \mathop{\text{argmax}}\limits_{{j \in \mathcal{A}}}q_{\hat{{\zeta}}j}\,.
\end{equation}
Параметр $w_{\hat{j}}$ и есть параметр, отвечающий критерию устойчивого
прореживания.

\subsection{Критерий последовательного наращивания}

Критерий последовательного добавления параметров, как
и~критерий~($\ref{eq:7}$), основан на принципе ло\-каль\-но-опти\-маль\-но\-го
выбора~--- критерию отвечает параметр, при добавлении которого
в~сеть функция ошибки~(\ref{eq:2}) минимальна.

Для нахождения параметра, отвечающего этому критерию, решается задача
\begin{equation*}
%\label{eq:10}
\hat{j} = \mathop{\text{argmin}}\limits_{{j \in \mathcal{J} \backslash \mathcal{A}}}S(\mathbf{w}_{\mathcal{A}} \cup {w_j}|\mathcal{T})\,.
\end{equation*}

\subsection{Описание базовой стратегии}

Стратегия пошаговой модификации модели состоит из двух этапов~--- Del
и~Add. Перед началом процедуры модификации все параметры модели активны.

\textbf{Этап Del.} Ищем параметр с~индексом~$\hat{j}$, отвечающий одному
из критериев прореживания~($\ref{eq:6}$), ($\ref{eq:7}$) или~($\ref{eq:8}$),
и~удаляем его из множества активных параметров:
\begin{equation*}
\mathcal{A} = \mathcal{A} \backslash \hat{j}\,.
\end{equation*}
Этап Del повторяем до тех пор, пока ошибка
$S(\mathbf{w}_{\mathcal{A}}|\mathcal{T})$ не превысит свое
минимальное значение на данном этапе более чем на некоторое заданное
значение~$\delta S_1$. Критерием останова шага Del является
следующее условие:
\begin{equation}
\label{eq:11}
S(\mathbf{\hat{w}}_{\mathcal{A}}|\mathcal{T}) \geq S_{\min} + \delta S_1\,,
\end{equation}
где $S_{\min}$~--- некоторое заданное значение.

\textbf{Этап Add.} В модели ищем параметр $\hat{j}$,
отвечающий критерию наращивания~($\ref{eq:9}$), и~добавляем найденный параметр
во множество активных параметров:
\begin{equation*}
\mathcal{A} = \mathcal{A} \cup {\hat{j}}\,.
\end{equation*}
Критерием останова шага Add является выполнение условия
\begin{equation}
\label{eq:12}
S(\mathbf{\hat{w}}_{\mathcal{A}}|\mathcal{T}) \geq S_{\min} + \delta S_2\,,
\end{equation}
где $S_{\min}$~--- некоторое заданное значение. На рис.~1
приведен график, демонстрирующий изменение функции ошибки при удалении
параметров из модели. Аналогичным образом ведет себя фукнция ошибки при
добавлении параметров в~модель. Из графика видно, что эта зависимость имеет
минимум, а~значит модели с~большим чис\-лом параметров
не являются наиболее точными. На рис.~2 показано, как
согласно критериям останова~($\ref{eq:11}$) и~($\ref{eq:12}$) сменяются
шаги удаления и~добавления.



Процедура модификации продолжается до тех пор, пока процесс не стабилизируется.
В~качестве критерия стабилизации предлагается использовать энтропию изменения
структуры модели:
\begin{equation}
\label{eq:13}
H(\mathcal{A},\mathcal{A'}) =
-\sum\limits_{j=1}^{k} {\rho(a_j, a_j')\ln(\rho(a_j, a_j'))}
\end{equation}

\begin{center}  %fig1
\vspace*{1pt}
\mbox{%
 \epsfxsize=80mm %476mm
 \epsfbox{pop-1.eps}
 }
\end{center}

%\vspace*{-3pt}

\noindent
{{\figurename~1}\ \ \small{Изменение функции ошибки при удалении параметров из модели}}


\vspace*{6pt}


\addtocounter{figure}{1}

\begin{center}  %fig2
\vspace*{1pt}
\mbox{%
 \epsfxsize=80mm %.475mm
 \epsfbox{pop-2.eps}
 }

\vspace*{6pt}


{{\figurename~2}\ \ \small{Смена шагов Del и Add}}
\end{center}


\vspace*{9pt}


\addtocounter{figure}{1}




\noindent
множества попарных нормированных расстояний Хэмминга между
элементами наборов $\mathcal{A} \hm= \{a_1,\dots,a_k\}$
и~$\mathcal{A}' \hm=\{a_1',\dots,a_k'\}$, полученных на двух
последовательных итерациях алгоритма следующим образом:
\begin{equation*}
a_j = \begin{cases}
1, & \mbox{если}~w_j \neq 0\,;  \\
0, & \mbox{если}~w_j = 0\,.
\end{cases}
\end{equation*}
Процесс считается стабильным, если энтропия $H(\mathcal{A},\mathcal{A'})$
не превосходит заданного порога.

\section{Путь в {\boldmath{$k$}}-мерном кубе}

В данной задаче будем иметь дело с~вектором параметров размерности~$k$.
Это означает, что существует~$2^k$~вариантов структуры модели.
Из этих~$2^k$~возможных вариантов структуры выбираются оптимальные.
Все варианты можно представить в~виде вершин~$\mathbf{v}$
$k$-мер\-но\-го куба~$\mathfrak{V}$. И~тогда стратегия задает путь~$\mathbf{V}$
по его вершинам. Этот путь заканчивается в некоторой
вершине~$\mathbf{\hat{v}}$, к~которой сходится процедура
модификации. Будем искать оптимальные модели в~некоторой окрестности
вершины~$\mathbf{\hat{v}}$. Так как охватить все возможные варианты
слишком трудоемко, то в~качестве окрестности $\mathbf{\hat{v}}$
будем рассматривать ведущий к~$\mathbf{\hat{v}}$ путь по вершинам
куба, полученный по описанной выше стратегии.

\smallskip

\noindent
\textbf{Пример 1}. В этом примере использовалась
выборка~$\{\mathbf{x}_i, y_i\}$, $i\hm\in\{1,\dots,177\}$. Каждый
объект выборки описывался 6~признаками $\mathbf{\chi}_1, \dots,
\mathbf{\chi}_6$ и~принадлежал одному из трех классов. Схематично
взаимное расположение векторов $\mathbf{\chi}_1, \dots,
\mathbf{\chi}_6$ изображено на рис.~3.


Для классификации такой выборки модифицировалась двухслойная
нейронная сеть с~одним нейроном в~скрытом слое. Совокупное число
параметров такой сети равно девяти. Нейронная сеть модифицировалась
за 11~итераций. На рис.~4 изоб\-ражен путь по вершинам
девятимерного куба. По вертикали отложен номер параметра, по
горизонтали~--- номер итерации. Черная клетка означает, что параметр
с~индексом~$j$~--- активный, белая клетка~---
 параметр неактивный.
Например, на пятой итера-\linebreak\vspace*{-12pt}
\begin{center}  %fig3
\vspace*{12pt}
\mbox{%
 \epsfxsize=74.023mm
 \epsfbox{pop-3.eps}
 }

\vspace*{6pt}


{{\figurename~3}\ \ \small{Данные}}
\end{center}


\vspace*{6pt}


\addtocounter{figure}{1}

\begin{center}  %fig3
\vspace*{1pt}
\mbox{%
 \epsfxsize=78.629mm
 \epsfbox{pop-4.eps}
 }

\vspace*{6pt}


{{\figurename~4}\ \ \small{Путь в кубе}}
\end{center}


%\vspace*{9pt}


\addtocounter{figure}{1}

\noindent
ции из сети был удален параметр~9, а~на
одиннадцатой итерации этот параметр был снова добавлен в~сеть.

\section{Вычислительный эксперимент}

С целью получить значение критериев качества описанной стратегии был
проведен вычислительный эксперимент. Использовались данные
акселерометра мобильного телефона. Данные состояли из~5418~векторов
признаков, которые были получены в~результате обработки
соответствующих временн$\acute{\mbox{ы}}$х рядов. Было выделено 43~признака
и~6~классов физической активности: ходьба, бег, сидение, стояние, подъем
и~спуск. Временн$\acute{\mbox{ы}}$е ряды записывались акселерометром мобильного
телефона, который находился в~кармане у~человека, выполняющего один
из типов физической активности. Для выделения признаков временн$\acute{\mbox{ы}}$е
ряды разделялись на десятисекундные сегменты. Из этих сегментов
извлекались признаки, такие как проекции среднего ускорения на
координатные оси, среднеквадратические отклонения от проекций
среднего ускорения на каждую из трех координатных осей, время между
пиками синусоидального сигнала в~миллисекундах и~др. С~более
подробным описанием признаков и~процессом их генерации можно
ознакомиться в~\cite{Kwapisz2010Activity}.



В вычислительном эксперименте оптимизировалась двухслойная нейронная
сеть с~пятью нейронами в~скрытом слое. Размерность вектора
па\-ра\-метров такой модели $k\hm=245$. Нейронная сеть\linebreak оптимизировалась по
стратегии, описанной в~разд.~3. Был получен набор из 771~модели.
В~процедуре модификации использовался каждый из трех критериев
прореживания~--- оптимального, последовательного и~устойчивого. Для
всех моделей были вычислены значения критериев качества. Был\linebreak
построен Па\-ре\-то-опти\-маль\-ный фронт трех кри\-териев. На
рис.~5
изображены все полученные модели. Пустыми значками обозначены модели,
\mbox{которые} были получены по стратегии с~применением критерия
устойчивого прореживания, серыми значками~--- критерия
последовательного прореживания, черными
 значками~--- оптимального
прореживания. Па\-ре\-то-опти\-маль\-ные \mbox{модели} обозначены черными крестиками.
 Из
рис.~5,\,\textit{а} видно, что самые устойчивые модели получаются при
использовании критерия устойчивого прореживания. В~таблице приведены
значения критериев качества моделей, \mbox{которые} являются точками
останова процедуры модификации для каждого их трех критериев
прореживания.



\begin{center}  %fig5
\vspace*{1pt}
\mbox{%
 \epsfxsize=77.734mm
 \epsfbox{pop-5.eps}
 }
\end{center}

\vspace*{-12pt}

\noindent
{{\figurename~5}\ \ \small{Множество моделей в координатах <<устой\-чивость--слож\-ность>>~(\textit{а}),
<<точ\-ность--слож\-ность>>~(\textit{б}) и~<<точ\-ность--устой\-чивость>>~(\textit{в})}}


\vspace*{6pt}


\addtocounter{figure}{1}

\begin{table*}\small
\begin{center}
%\vspace*{2ex}


\begin{tabular}{|l|c|c|c|}
\multicolumn{4}{c}{Сложность, точность и устойчивость моделей}\\
\multicolumn{4}{c}{\ }\\[-5pt]
        \hline
        \multicolumn{1}{|c|}{Стратегия}& Cложность & Точность & Устойчивость \\
        \hline
&&&\\[-10pt]
        Оптимальное прореживавние & 50 & 877 & $1{,}2\cdot 10^{6}$ \\
%        \hline
        Последовательное прореживавние & 36 & 870 & $2{,}0 \cdot 10^{6}$ \\
 %       \hline
        Устойчивое прореживание & 50 & 866 & $6{,}\cdot10^{5}$ \\
        \hline
\end{tabular}
\end{center}
\vspace*{-3pt}
\end{table*}


На рис.~6 приведена интерпретация полученных
результатов. В~верхней области графика Па\-ре\-то-оп\-ти\-маль\-ные модели не
интересны для рассмотрения, так как в~этой области имеет место
недо\-обуче\-ние~--- модели излишне сложны. Па\-ре\-то-опти\-маль\-ные
 модели
с~незначительной слож\-ностью находятся в~нижней области графика.

\begin{center}  %fig6
\vspace*{1pt}
\mbox{%
 \epsfxsize=77.724mm
 \epsfbox{pop-8.eps}
 }

\vspace*{3pt}


{{\figurename~6}\ \ \small{Интерпретация результатов}}
\end{center}


\vspace*{9pt}


\addtocounter{figure}{1}


Также была визуализирована процедура пошаговой модификации модели
как путь в $k$-мерном кубе. На рис.~7, так же как и~в~примере~1,
по вертикали отложен номер параметра, по горизонтали~--- номер
итерации. Черная клетка означает, что параметр активный, белая
клетка~--- параметр неактивный. На рис.~6, 7,\,\textit{а} и~7,\,\textit{б}  указана
последовательность, в~которой параметры удалялись из модели
и~до\-бав\-ля\-лись в~нее. Из рис.~7,\,\textit{б} и~7,\,\textit{в}
видно, что
стратегия с~критериями оптимального и~последовательного
прореживания, которые выбирают для удаления параметр, минимизирующий
функцию ошибки, остав\-ля\-ет в~моделях параметры с~номерами с~216 по~245.
Это связано с~тем, что параметры с~такими номерами относятся ко
второму слою нейронной сети, а~удаление большого числа параметров
второго слоя приводит к~росту функции ошибки.



\section{Заключение}

В работе была предложена стратегия пошаговой модификации моделей
классификации согласно трем критериям качества~--- сложности,
точности и~устойчивости. В~рамках стратегии были предложены критерии
добавления и~удаления параметров в~модель, критерии останова шагов
добавления и~удаления, а~также критерий останова
 процедуры
модификации. Процедура пошаговой модификации модели была рассмотрена
и~визуали-\linebreak\vspace*{-12pt}

\begin{center}  %fig7
\vspace*{1pt}
\mbox{%
 \epsfxsize=77.709mm
 \epsfbox{pop-9.eps}
 }
\end{center}

\vspace*{-9pt}


\noindent
{{\figurename~7}\ \ \small{Путь в кубе: устойчивое~(\textit{а}); последовательное~(\textit{б})
и~оптимальное~(\textit{в})  прореживания}}



%\vspace*{9pt}


\addtocounter{figure}{1}



\noindent
зирована как путь в~многомерном кубе. Был проведен
вычислительный эксперимент, в~ходе которого был получен набор
моделей и~найден Па\-ре\-то-опти\-маль\-ный фронт критериев качества этого
набора. Вычислительный эксперимент показал, что наилучшие по
рассматриваемым критериям качества модели получаются при
использовании критерия устойчивого прореживания. Это связано с~тем,
что\linebreak критерий устойчивого прореживания позволяет получать более
устойчивые модели, удаляя коррелирующие параметры и~тем самым
повышая устойчивость и~обобщающую способность модели классифика\-ции.
Программная реализация стратегии пошаговой модификации нейронной
сети в~среде разработки MatLab находится в~свободном доступе~\cite{StrategyCode}.



{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{Vizilter2012learnong} %1
\Au{Визильтер Ю.\,В., Горбацевич В.\,С., Каратеев~С.\,Л., Кост\-ро\-мов~Н.\,А.}
Обучение алгоритмов выделения кожи на цветных изображениях лиц~// Информатика
и~её применения, 2012. Т.~6. Вып.~1. С.~109--113.



\bibitem{Tokmakova2012HyperPar} %3
\Au{Токмакова~А.\,А., Стрижов~В.\,В.} Оценивание гиперпараметров линейных
и~регрессионных моделей при отборе шумовых и~коррелирующих признаков~//
Информатика и~её применения, 2012. Т.~6. Вып.~4. С.~66--75.

\bibitem{Haplanov2013assimptothic} %2
\Au{Хапланов~А.\,Ю.} Асимптотическая нормальность оценки параметров
многомерной логистической регрессии~// Информатика и~её применения, 2013. Т.~7.
Вып.~2. С.~69--74.

\bibitem{Myung2000Complexity} %4
\Au{Myung~I.\,J.} The importance of complexity in model selection~//
J.~Math. Psychol., 2000. Vol.~44. No.\,1. P.~190--204.

\bibitem{MacLeod2001Grow} %5
\Au{MacLeod~C., Maxwell~M.} Incremental evolution in ANNs: Neural
nets which grow~// Artif. Intell. Rev., 2001. Vol.~16. No.\,3. P.~201--224.

\bibitem{Karnin1990Simple} %6
\Au{Karnin~E.\,D.} A simple procedure for pruning back-propagation
trained neural networks~// IEEE Trans. Neural Networks, 1990. Vol.~1. No.\,2. P.~239--242.

\bibitem{LeCun1990Optimal} %7
\Au{LeCun~Y., Denker~L.\,S., Solla~S.\,A.} Optimal brain damage~//
Adv. Neur. Inform. Processing Syst., 1990. Vol.~2. No.\,2. P.~598--605.

\bibitem{Hassibi93} %8
\Au{Hassibi~B., Stork~D.\,G., Woff~G.\,J.} Optimal brain surgeon and
general network pruning~// IEEE  Conference (International) on Neural Networks
Proceedings, 1993. Vol.~1. P.~293--299.

\bibitem{Han2011Water} %9
 \Au{Hong-Gui~H., Qi-li~C., Jun-Fei~Q.} An efficient self-organizing {RBF}
neural network for water quality prediction~// Neural Networks, 2011.
Vol.~24. No.\,7. P.~717--725.


\bibitem{Yang2012PruningAlgorithm} %10
\Au{Yang~S., Chen~Y.} An evolutionary constructive and pruning algorithm
for artificial neural networks and its prediction applications~// Neurocomputing,
2012. Vol.~86. P.~140--149.

\bibitem{Pu2013PruningAlgorithm} %11
\Au{Pu~X., Pengfei Sun~P.} A~new hybrid pruning neural network algorithm based on
sensitivity analysis for stock market forcast~// J.~Inform.
Comput. Sci., 2013. Vol.~3. P.~883--892.


\bibitem{Knerr1990Stepwise} %12
\Au{Knerr~S., Personnaz~L., Dreyfus~G.} Single-layer learning revisited:
A~stepwise procedure for building and training a neural network~//
Neurocomputing Algorithms Architectures Applications, 1990.
Vol.~68. No.\,1. P.~41--50.

\bibitem{Strijov2013Evidence} %13
\Au{Strijov~V., Krymova~E., Weber~S.\,V.} Evidence optimization for consequently
generated models~// Math.  Comput. Modell., 2010. Vol.~57. No.\,1--2. P.~50--56.

\bibitem{Leonteva2012Feature} %14
\Au{Леонтьева~Л.\,Н.} Последовательный выбор признаков при восстановлении
регрессии~// Машинное обучение и~анализ данных, 2012. Т.~1. №\,3. С.~335--346.

\bibitem{Zaycev2012Evaluation} %15
\Au{Зайцев~А.\,А., Токмакова~А.\,А.} Оценка гиперпараметров
линейных регрессионных моделей методом максимального правдоподобия
при отборе шумовых и~коррелирующих признаков~// Машинное обучение
и~анализ данных, 2012. Т.~1. №\,3. С.~347--353.

\bibitem{Kwapisz2010Activity} %16
\Au{Kwapisz~J.\,R., Weiss~G.\,M., Moore~S.} Activity recognition using cell
phone accelerometers~// SIGKDD Explorations, 2010. Vol.~12. No\,2. P.~74--82.

\bibitem{Belsley2005} %17
\Au{Belsley~D.\,A., Kuh~E., Welsch~R.\,E.} Regression diagnostics:
Identifying influential data and sources of collinearity.~--~New York:
John Wiley and Sons, 2005. 302~p.

\bibitem{Sandulyanu2012Feature} %18
\Au{Сандуляну~Л.\,Н., Стрижов~В.\,В.} Выбор признаков
в~авторегрессионных задачах прогнозирования~// Информационные технологии, 2012.
Т.~7. С.~11--15.

\bibitem{StrategyCode} %19
\Au{Попова~М.\,С.} Реализация стратегии пошаговой модификации нейронной сети~//
Algorithms Machine Learning, 2014.
{\sf http://sourceforge.net/p/mlalgorithms/
code/HEAD/tree/Group174/Popova2014OptimalMode lSelection/code/main.m}.
 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-1pt}

\hfill{\small\textit{Поступила в редакцию 10.08.14}}

\newpage

%\vspace*{12pt}

%\hrule

%\vspace*{2pt}

%\hrule

%\vspace*{12pt}

\def\tit{SELECTION OF~OPTIMAL PHYSICAL ACTIVITY CLASSIFICATION MODEL USING MEASUREMENTS
OF~ACCELEROMETER}

\def\titkol{Selection of~optimal physical activity classification model using measurements
of~accelerometer}

\def\aut{M.~Popova$^1$ and V.~Strijov$^2$}

\def\autkol{M.~Popova and V.~Strijov}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-9pt}

\noindent
$^1$Moscow Institute of Physics and Technology,
9 Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian\linebreak
$\hphantom{^1}$Federation

\noindent
$^2$Dorodnicyn Computing Center, Russian Academy of Sciences,
40~Vavilov Str.,  Moscow 119333, Russian\linebreak
$\hphantom{^1}$Federation


\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2015\ \ \ volume~9\ \ \ issue\ 1}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2015\ \ \ volume~9\ \ \ issue\ 1
\hfill \textbf{\thepage}}}

\vspace*{3pt}


\Abste{The paper solves the problem of selecting optimal stable models
for classification of physical activity. Each type of physical activity of
a~particular person is described by a~set of features generated from an
accelerometer time series. In conditions of feature's multicollinearity,
selection of stable models is hampered by the need to evaluate a~large number
of parameters of these models. Evaluation of optimal parameter values
is also difficult due to the fact that the error function has a~large number
of local minima in the parameter space. In the paper, the optimal models from
the class of two-layer artificial neural networks are chosen. The problem
of finding the Pareto optimal front of the set of models is solved.
The paper presents a~stepwise strategy of building optimal stable models.
The strategy includes steps of deleting and adding parameters, criteria of
 pruning and growing the model and criteria of breaking the process of building.
The computational experiment compares the models generated by the proposed strategy
on three quality criteria~--- complexity, accuracy, and stability.}

\KWE{classification; artificial neural networks; complexity;
accuracy; stability; Pareto efficiency; growing and pruning criteria}


\DOI{10.14357/19922264150107}

\Ack
\noindent
The research was supported by Skolkovo Institute of Science and Technology
(Skoltech) in the frame of SkolTech/MITInitiative.


%\vspace*{3pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}



{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}

\bibitem{Vizilter2012learnong-1}
\Aue{Vizilter,~Y., V.~Gorbatcevich, S.~Karateev, and N.~Kostromov.}
2012.
Obuchenie algoritmov vydeleniya kozhi na tsvetnykh izobrazheniyakh lits
[Teaching of skin extraction algorithms for human face color images].
\textit{Informatika i~ee Primeneniya}~--- \textit{Inform. Appl.} 6(1):109--113.



\bibitem{Tokmakova2012HyperPar-1} %3
\Aue{Tokmakova,~A.\,A., and V.\,V.~Strizhov}.
2012. Otsenivanie giperparametrov lineynykh i regressionnykh modeley
pri otbore shumovykh i korreliruyushchikh priznakov
[Estimation of linear model hyperparameters for noise or correlated
feature selection poblem].
\textit{Informatika i~ee Primeneniya}~--- \textit{Inform. Appl.} 6(4):66--75.

\bibitem{Haplanov2013assimptothic-1} %2
  \Aue{Khaplanov, A.\,Yu.}
2013. Asimptoticheskaya normal'nost' otsenki parametrov mnogomernoy
logisticheskoy regressii
[Asymptotic normality of the estimation of the multivariate logistic
regression]. \textit{Informatika i~ee Primeneniya}~--- \textit{Inform. Appl.} 7(2):69--74.


\bibitem{Myung2000Complexity-1} %4
\Aue{Myung,~I.\,J.} 2000. The
importance of complexity in model selection.
\textit{J.~Math. Psychol.} 44(1):190--204.

\bibitem{MacLeod2001Grow-1} %5
\Aue{MacLeod,~C., and M.~Maxwell}. 2001.
Incremental evolution in ANNs: Neural nets which grow.
\textit{Artif. Intell. Rev.} 16(3):201--224.
\bibitem{Karnin1990Simple-1} %6
\Aue{Karnin,~E.\,D.} 1990. A~simple procedure for pruning back-propagation
trained neural networks. \textit{IEEE Trans. Neural Networks} 1(2):239--242.
\bibitem{LeCun1990Optimal-1} %7
\Aue{LeCun,~Y., L.\,S.~Denker, and S.\,A.~Solla}.
1990. Optimal brain damage. \textit{Adv. Neur. Inform. Processing
Syst.} 2(2):598--605.

\bibitem{Hassibi93-1} %8
\Aue{Hassibi,~B., D.\,G.~Stork, and G.\,J.~Woff}.
1993. Optimal brain surgeon and general network pruning.
\textit{IEEE Conference (International) on Neural Networks Proceedings}. 293--299.

\bibitem{Han2011Water-1} %9
\Aue{Hong-Gui,~H., C.~Qi-li, and Q.~Jun-Fei}. 2011.
An efficient self-organizing {RBF} neural network for water quality prediction.
\textit{Neural Networks} 24(7):717--725.

\bibitem{Yang2012PruningAlgorithm-1} %10
\Aue{Yang,~S., and Y.~Chen}.
2012. An evolutionary constructive and pruning algorithm for artificial
neural networks and its prediction applications.
\textit{Neurocomputing} 86(1):140--149.
{\looseness=1

}

\bibitem{Pu2013PruningAlgorithm-1} %11
\Aue{Pu,~X., and P.~Pengfei-Sun}.
2013. A~new hybrid pruning neural network algorithm based on sensitivity analysis
for stock market forcast. \textit{J.~Inform. Comput. Sci.} 3(1):883--892.
{\looseness=1

}

\bibitem{Knerr1990Stepwise-1} %12
\Aue{Knerr,~S., L.~Personnaz, and G.~Dreyfus}.
1990. Single-layer learning revisited: A~stepwise procedure for building
 and training a neural network.
\textit{Neurocomputing Algorithms Architectures Applications} 68(1):41--50.



\bibitem{Strijov2013Evidence-1} %13
\Aue{Strijov,~V., E.~Krymova, and S.~Weber}.
2013. Evidence optimization for consequently generated models.
\textit{Math. Comput. Modell.} 57(1-2):50--56.

\bibitem{Leonteva2012Feature-1} %14
\Aue{Leont'eva,~L.\,N.}
2012. Posledovatel'nyy vybor priznakov pri vosstanovlenii regressii
[Feature selection in autoregression forecasting].
\textit{J.~Machine Learning Data Analysis} 1(3):335--346.

\bibitem{Zaycev2012Evaluation-1} %15
\Aue{Zaytsev,~A.\,A., and A.\,A.~Tokmakova}.
2012. Otsenka giperparametrov lineynykh regressionnykh modeley
metodom maksimal'nogo pravdopodobiya pri otbore shumovykh
i~korreliruyushchikh priznakov
[Estimation regression model hyperparameters using
    maximum likelihood]. \textit{J.~Machine Learning Data Analysis} 1(3):347--353.

\columnbreak

\bibitem{Kwapisz2010Activity-1} %16
\Aue{Kwapisz,~J.\,R., G.\,M.~Weiss, and S.~Moore}.
2010. Activity recognition using cell phone accelerometers.
\textit{SIGKDD Explorations} 12(2):74--82.
\bibitem{Belsley2005-1} %17
\Aue{Belsley,~D.\,A., E.~Kuh, R.\,E.~Welsch}.
2005. \textit{Regression diagnostics: Identifying influential data and sources of
collinearity}. New York: John Wiley and Sons. 302~p.
\bibitem{Sandulyanu2012Feature-1} %18
\Aue{Sanduljanu,~L.\,N., and V.\,V.~Strizhov}.
2012. Vybor priznakov v avtoregressionnykh zadachakh prognozirovaniya
[Feature selection in autoregression forecasting].
\textit{Information Technologies} 7:11--15.
\bibitem{StrategyCode-1} %19
\Aue{Popova,~M.\,S.}
2014. Realizatsiya strategii poshagovoy modifikatsii neyronnoy seti
[Realization of a~stepwise strategy for neural network modification].
Available at: {\sf
http://sourceforge.net/p/mlalgorithms/code/HEAD/
tree/Group174/Popova2014OptimalModelSelection/\linebreak code/main.m} (accessed February~10, 2015).
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-3pt}

\hfill{\small\textit{Received August 10, 2014}}

%\vspace*{-18pt}


\Contr

\noindent
\textbf{Popova Maria S.} (b.\ 1994)~--- student, Moscow Institute of Physics and Technology,
9 Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian Federation;
maria\_popova@phystech.edu

\vspace*{3pt}

\noindent
\textbf{Strijov Vadim V.} (b.\ 1967)~--- Candidate of science (PhD)
in physics and mathematics; associate professor,
Moscow Institute of Physics and Technology,
9 Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian Federation;
leading scientist, Dorodnicyn Computing Center, Russian Academy of Sciences,
40~Vavilov Str.,  Moscow 119333, Russian Federation;  strijov@ccas.com

\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература}
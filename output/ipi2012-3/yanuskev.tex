
 \renewcommand{\figurename}{\protect\bf Figure}
\renewcommand{\tablename}{\protect\bf Table}
\renewcommand{\bibname}{\protect\rmfamily References}

\def\stat{yan}

\def\tit{ABOUT THE RATE OF~CONVERGENCE OF~ONE U-STATISTIC}

\def\titkol{About the rate of~convergence of~one U-statistic}

\def\autkol{O.~Yanushkevichiene and R.~Yanushkevichius}
\def\aut{O.~Yanushkevichiene$^{1}$ and R.~Yanushkevichius$^{2}$}

\titel{\tit}{\aut}{\autkol}{\titkol}

%{\renewcommand{\thefootnote}{\fnsymbol{footnote}}\footnotetext[1]
%{Received by the editors November, 2011. 1991 \textit{Mathematics Subject Classification}.
%Primary 62E20; Secondary 60F05.}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Vilnius University, Institute of Mathematics and Informatics, Lithuania, olgjan@zebra.lt}
\footnotetext[2]{Lithuanian University of Educational Sciences, Vilnius, Lithuania}

\vspace*{-10pt}

\Abste{This paper pertains to the analysis of the asymptotic behavior of 
U-statistics, which are important in the construction and application of 
modern statistical methods for studying information systems.}

\vspace*{-1pt}

\KWE{rates of convergence; Berry--Esseen bound; quadratic form; second-degree U-statistics; 
Kolmogorov distance}

\vskip 14pt plus 9pt minus 6pt

      \thispagestyle{headings}

      \begin{multicols}{2}

            \label{st\stat}


\section{Introduction and~Formulation of~the~Result}

\noindent
Analysis of modern information systems is impossible without the use of statistical 
methods. Function exploring the characteristics of the traffic is usually characterized 
by a symmetry property with respect to the arguments. This makes it especially important 
to study the behavior of U-statistics. This paper pertains to the investigation of the 
limit distribution of the second-order U-statistic.

Let $X,X_1,\ldots ,X_n$ be independent identically distributed (i.i.d.)\
random variables taking values in a measurable space  $(\Theta, \
\mathcal{R})$. Let $h: \ \Theta^{2}\rightarrow\mathbf{R}$ be real-valued
measurable functions. Let $h$ be symmetric, that is,
$h(x,y)=h(y,x)$ for all $x,y\in\Theta$. Consider the \mbox{U-statistic}

\vspace*{2pt}

\noindent
\begin{equation}
\label{e1-y}
  Z=Z(X_1,\ldots ,X_n)=n^{-1} \sum\limits_{1\leq i<k\leq n}h(X_i,X_k)
\end{equation}
assuming that $ \beta_{2}=\mathbf{E}|h(X,X_1)|^{2}>0$, 
$\beta_{3}$\linebreak $=\mathbf{E}|h(X,X_1)|^{3} <\infty$, and 
$\mathbf{E}h(x,X)=0$ for all  $x \in\Theta$.

    The condition $\beta_{2}>0$ ensures
    that the quadratic part of the statistic is not asymptotically negligible and,
    therefore,~$Z$ is not asymptotically normal. To be more precise, the asymptotic distribution
    of $Z$ is non-Gaussian and is given by the distribution of the random variable
    
    \vspace*{2pt}
    
    \noindent
  \begin{equation*}
%  \label{5-y}
    Z_0=\fr{1}{2}\sum\limits_{i\geq 1}q_i(\eta_i^{2}-1)\,. 
    \end{equation*}
    Here, $\eta_i$ is the sequence of i.i.d.\ standard normal random variables and $q_1,q_2,\ldots$
    denote the eigenvalues of
    the Hilbert--Schmidt operator, say~$Q$, associated with the kernel~$h$ (see~[1] for
    detailed definitions). Without loss
    of generality, let assume throughout that $|q_1|\geq |q_2|\geq\ldots$

    Let
    
    \noindent
    \begin{equation*}
    \label{o}
    F(x)=\mathbf{P}\{Z\leq x\}\,; \enskip F_0(x)=\mathbf{P}\left\{Z_0\leq x\right\}\,.
\end{equation*}
     Let write
     
     \noindent
     \begin{equation*}
%     \label{o1}
    \Delta_{n}=\rho(Z,Z_0)=\sup\limits_{x}|F(x)-F_0(x)|\,.
    \end{equation*}

    Korolyuk and Borovskikh proved~\cite{3-y} 
    that $\Delta_{n}$\linebreak $ =o(n^{-1/2})$ if $|q_i| >0$ for all~$i$.
    Under the condition $|q_9| >0$, an optimal bound $\Delta_{n} =O(n^{-1})$ was established
    by Bentkus and G$\ddot{\mbox{o}}$tze~\cite{1-y}. From a construction in~\cite{1-y}, it is
    clear that the bounds for $\Delta_{n}$ are related to the estimates of
    the convergence rate in Hilbert spaces. Therefore, if $|q_k| >0$, a result of
    Senatov~\cite{6-y} implies a lower bound for $\Delta_{n}$, namely,
    $\liminf\limits_{n\rightarrow\infty}\Delta_{n}n^{k/12}(q_1\cdots q_k)^{1/2} >0,$ then $k \leq6$. 
    In the considered case, $k =1$ and the lower bound is $n^{k/12}$. The rate of convergence of such 
    order was got in~\cite{9-y} for the case, when the expression~(\ref{e1-y}) 
    has not only the quadratic part, but also the linear part. Now, the question is: 
    what order of the rate of convergence will be, if the linear part does not exist? 
    The theorem bellow answers this question.

    By $c$, positive absolute constants which may
    differ from line to line or from formula to formula are denoted. The 
    following theorem is the authors' result.
    
\smallskip

\noindent
\textbf{Theorem~1.} \textit{If $q_1>0$, one gets}

\noindent
   \begin{equation}\label{6-y}
    \Delta_{n}\leq \fr{c\beta_3^{1/6}}{\sqrt{q_1}n^{1/14}}+\fr{c}{n^{1/4}}
    \left(\sum\limits_{i\geq 1}\left\vert\fr{q_i}{q_1}\right\vert\right)^{1/2}+
    \fr{c}{\sqrt{q_1}n}\,.
    \end{equation}

\smallskip


\noindent
\textbf{Remark~1.} If on the right-hand side of~(\ref{6-y}) the sum $\sum\limits_{i \geq
    1}|q_i|$ is divergent, i.\,e.,
    
    \noindent
    \begin{equation}
    \label{r}
    \sum\limits_{i\geq 1}|q_i|=\infty\,,  
    \end{equation}
    then $\Delta_{n}=O(n^{-1}).$

\smallskip

    Indeed, in principle, the series on the right-hand side of~(\ref{6-y}) 
    can diverge. Then, the estimate~(\ref{6-y}) is naturally
    true, but useless. However, if~(\ref{r}) holds, then $|q_9| >0$,
    since
    $|q_1| \geq |q_2| \geq\cdots\geq |q_9| \geq\cdots\geq |q_n| \geq0$. Then,
let use Theorem~1.1 from~\cite{1-y}. The theorem states that
    if $|q_9| >0$, then
     $\Delta_{n} =O(n^{-1})$. That is, if the series mentioned is
    divergent,then estimate~(\ref{6-y}) can be replaced by the
    improved estimate from~\cite{1-y}.

%\pagebreak

    In the one-dimensional case, that is, in the case
    $q_2 =q_3=\cdots =0$,
    the bound~(\ref{6-y}) $\Delta_{n} =O(n^{-1/14})$ improves to $\Delta_{n} =O(n^{-1/4})$, and
    the rate $O(n^{-1/4})$ is best possible~\cite{8-y}.
    The result of Senatov~\cite{6-y} shows that the optimal order of bound~(\ref{6-y}) 
    is $O(n^{-1/12})$.

    The authors' results answer the question of V.~Bentkus, who
    also suggested an idea of the proof.
    
    \vspace*{-6pt}


    \section{Proof}

    \vspace*{-2pt}

    \subsection{Lemma}

\noindent
    To prove the theorem, one needs the following lemma.

\smallskip

\noindent
\textbf{Lemma~1.} \textit{Let $\eta$ be a standard Gaussian random variable and $q_1>0$.
    Then the distribution function
    $H(x)=\textbf{P}\{q_1\eta^2<x\}$ satisfies the Lipschitz condition
       \begin{equation}
       \label{6l}
   \sup\limits_{x}|H(x+\varepsilon)-H(x)|\leq c\sqrt{\fr{\varepsilon}{q_1}}
    \end{equation}
    where} $\varepsilon >0$.

\smallskip

\noindent
P\,r\,o\,o\,f\ \ of Lemma~1.
    The distribution of
    $\eta^2$ has the density
    $$ 
    f(x)=
    \begin{cases}
    \fr{1}{\sqrt{2}\Gamma(1/2)\sqrt{x}}e^{-x/2} &\ \mbox{for} \ x>0\,;\\
    0 &\ \mbox{for} \ x\leq 0\,.
    \end{cases}
    $$
    The function $f(u)$ is decreasing for $u >0$.
    One can write:
    $$\sup\limits_{x}|H(x+\varepsilon)-H(x)|\leq
    c\int\limits^{\varepsilon/q_1}_{0}\fr{e^{-u}}{\sqrt{u}}\,du\leq c\sqrt{
    \fr{\varepsilon}{q_1}}\,.
    $$
    The lemma is proved.

\smallskip

\noindent
\textbf{Lemma~2.} \textit{Let $\eta$ be a standard Gaussian random variable and $q_1 >0$.
    Then the the following inequality is true:}
      \begin{equation*} %multline*}
%      \label{le}
   \sup\limits_{x}\left\vert\mathbf{P}(q_1\eta_1^{2}- \fr{1}{n}q_1\eta_1^{2}\leq x)-
   \mathbf{P}(q_1\eta_1^{2}\leq x)\right\vert %\\
\leq
\fr{c}{\sqrt{q_1}n}\,.
    \end{equation*} %multline*}
   
\smallskip

\noindent
P\,r\,o\,o\,f\ \ of Lemma~2.
    It is easy to see that
\begin{multline*}
\left\vert\mathbf{P}\left(q_1\left(1-\fr{1}{n}\right)\eta_1^{2}\leq x\right)-
\mathbf{P}(q_1\eta_1^{2}\leq
    x)\right\vert\\
    {}\leq c\int\limits_{0}^{2/\sqrt{q_1}n}e^{-t^{2}/2}\,dt\leq \fr{c}{\sqrt{q_1}\,n}\,.
    \end{multline*}
    The lemma is proved.

\columnbreak

It is necessary to prove the authors' theorem, i.\,e., the bound~(\ref{6-y}).
    Let $G$, $G_i$, $1 \leq i \leq n$, be i.i.d.\ Gaussian random vectors
    $G_i =(G_{1,i},G_{2,i},\ldots)$ with
    values in $\mathbb{R}^{\infty}$ where $G_{1,i},G_{2,i},\ldots$ denote i.i.d.\ standard normal
    random variables. Let assume throughout that
\begin{equation}
\left.
\begin{array}{rl}
     \mathbf{E}h(x,G)&=0\,;\\[9pt]
     \mathbf{E}h(x,G)h(y,G)&=\mathbf{E}h(x,X)h(y,X)
    \end{array}
 \right\}
 \label{m}
    \end{equation}
for~all $x\in\Theta$.
    Note that the possibility of selecting Gaussian random
    variables so that Eqs.~(\ref{m}) are valid is substantiated
    in~[1, p.~461].

     According to the triangle inequality, one has
     \begin{multline}
     \label{7-y}
    \rho (Z,Z_0)\leq \rho(Z(X_1,\ldots,X_n),Z(G_1,\ldots,G_n))\\
    {}+\rho(Z(G_1,\ldots,G_n),Z_0)\,.
     \end{multline}
     In view of~(\ref{7-y}), to prove~(\ref{6-y}),  it is sufficient to establish that
     \begin{multline}
   \rho(Z(X_1,\ldots,X_n),Z(G_1,\ldots,G_n))\\
   {}\leq cq_1^{-1/2}\beta_3^{1/6}n^{-1/14}
        \label{a-y}
     \end{multline}
     and
    \begin{multline}
   \rho(Z(G_1,\ldots,G_n),Z_0)\\
   {}\leq \fr{c}{n^{1/4}}
    \left(\sum\limits_{i\geq 1}\left\vert\fr{q_i}{q_1}\right\vert\right)^{1/2}
+
    \fr{c}{\sqrt{q_1}n}\,,
        \label{b}
     \end{multline}

     \subsection{Proof of inequality~(\ref{a-y})}

\noindent
Let estimate $\rho(Z(X_1,\ldots,X_n),Z(G_1,\ldots,G_n))$.
    It is to verify using lemma~1 that the distribution function of
    $Z(G_1,\ldots,G_n)$ satisfies the Lipschitz condition with the exponent~$1/2$.

    Let prove that, for any $\varepsilon  >0$, one has
         \begin{equation}
         \label{c4}
\rho(Z(X_1,\ldots,X_n),Z(G_1,\ldots,G_n))\leq \fr{c}{\sqrt{q_1}}\sqrt{\varepsilon}+\Delta\!
    \end{equation}
where
    $$
    \Delta=\max\limits_\varphi |\mathbf{E}\varphi(Z(X_1,\ldots,X_n))-
    \mathbf{E}\varphi(Z(G_1,\ldots,G_n))|\,,  
    $$
    and maximum is taken over all infinite differentiable $\varphi$ such
    that $0 \leq\varphi(u) \leq1$ and
      $|\varphi^{(k)}(u)| \leq
    c_1/\varepsilon^{k}$, $k =1,2,3$:
    \begin{equation}
    \label{c2}
\varphi(u)=
    \begin{cases}
    1\,, &\mbox{if}\  u\leq x-\epsilon\,; \\[6pt]
    0\,, &\mbox{if}\ u\geq x\,,
    \end{cases}
\end{equation}
    or
    
    \noindent
    \begin{equation}
    \label{c3}
    \varphi(u)=
    \begin{cases}
    1\,, &\mbox{if}\ u\leq x\,; \\[6pt]
    0\,, &\mbox{if}\ u\geq x+\epsilon\,. 
    \end{cases}
    \end{equation}
Let write $\delta^{*} =P\{Z(X_1,\ldots,X_n) \leq x\}$\linebreak $ -\;P\{Z(G_1,\ldots,G_n) \leq x\}$.
    Let prove~(\ref{c4}) in the case
    $\delta^{*} \geq 0.$  Let 
    take a function $\varphi$ such that~(\ref{c2}) holds. Then
    
    \noindent
    \begin{multline*}
\delta^{*}  = EI\{Z(X_1,\ldots,X_n)\leq x\}\\
{}-P\{Z(G_1,\ldots,G_n)\leq x\} \\
 \leq 
    |E\varphi(Z(X_1,\ldots,X_n))-E\varphi(Z(G_1,\ldots,G_n))|\\
    {}+|E\varphi(Z(G_1,\ldots,G_n))-
    P\{Z(G_1,\ldots,G_n)\leq
    x\}|  \\
     \leq 
    \Delta+P\{x\leq Z(G_1,\ldots,G_n)\leq x+\varepsilon\}\,. 
    \end{multline*}
    Using the Lipschitz condition, one gets~(\ref{c4}).

    The proof is similar if $\delta^{*} <0$, taking a function~$\varphi$ satisfying~(\ref{c3})
    instead of~(\ref{c2}).

    Let estimate
    
    \vspace*{1pt}
    
    \noindent
    $$
    \Delta^{*}(\varphi)=|\mathbf{E}\varphi(Z(X_1,\ldots,X_n))-
    \mathbf{E}\varphi(Z(G_1,\ldots,G_n))|\,.
    $$
    It is easy to see that
    
    \noindent
    \begin{multline}
    \Delta^{*}(\varphi)\leq |\mathbf{E}\varphi(Z(X_1,\ldots,X_n))\\
    {}-
    \mathbf{E}\varphi(Z(X_1,\ldots,X_{n-1},G_n))|\\
{}+
    |\mathbf{E}\varphi(Z(X_1,\ldots,X_{n-1},G_n))\\
    {}-
    \mathbf{E}\varphi(Z(X_1,\ldots,X_{n-2},G_{n-1},G_n))|+\ldots{}
\\
\ldots{} +
     |\mathbf{E}\varphi(Z(X_1,G_{2},\ldots,G_n))\\
     {}-
     \mathbf{E}\varphi(Z(G_1,\ldots,G_n))|=\Delta^{*}_{1,n}+\ldots+\Delta^{*}_{n,n}\,.
     \label{e8-y}
     \end{multline}
One has

\noindent
    \begin{multline*}
    Z(X_1,\ldots,X_n)=\fr{1}{n}\{h(X_1,X_2)+\cdots{}\\
\cdots +h(X_1,X_{n-1})+h(X_1,X_n)+\cdots\\
\cdots+h(X_{n-2},X_{n-1})+h(X_{n-2},X_n)+{}\\
{}+h(X_{n-1},X_n)\}\,.
\end{multline*}
    one designates:
    
    \noindent
    \begin{multline*}
    w=\fr{1}{n}\{h(X_1,X_2)+\cdots+h(X_1,X_{n-1})+
    \cdots{}\\
    {}\cdots +h(X_{n-2},X_{n-1})\}
    \end{multline*}
    and
    \begin{multline*}
    l_n=\fr{1}{n}\left\{h(X_1,X_n)+\cdots+h(X_{n-2},X_n)\right.\\
\left.    {}+h(X_{n-1},X_n)\right\}\,.
    \end{multline*}
    Replacing $X_n$ by $G_n$, one gets:
    
    \noindent
    \begin{multline*}
    Z(X_1,\ldots,X_{n-1},G_n)=w+\fr{1}{n}\{h(X_1,G_n)+\cdots{}\\
{}    \cdots+h(X_{n-2},G_n)+
    h(X_{n-1},G_n)\}
    {}= w+l_n^{*}\,.
    \end{multline*}
    Let expand into the Taylor series:
    \begin{multline*}
    \varphi(x+y)=\varphi(x)+\varphi^{'}(x)y+\fr{1}{2}\varphi^{''}(x)y^{2}\\
    {}+\fr{1}{2}
    \mathbf{E}\varphi^{'''}(x+\tau y)(1-\tau)^{2}y^3\,. 
    \end{multline*}
    Here, $\tau$ is the random
    variable uniformly distributed in $[0,1]$ and independent of
    all the other random variables. Let replace $x$ by $w$ and $y$ by $l_n$, respectively.
Let write

\noindent
     \begin{multline*}
    \Delta^{*}_{1,n}\\
    {}=\left\vert\mathbf{E}\varphi(Z(X_1,...,X_n))-
    \mathbf{E}\varphi(Z(X_1,\ldots,X_{n-1},G_n))\right\vert\\
    {}=
    \left\vert\vphantom{\fr{1}{2}}
    \mathbf{E}\varphi(w)-\mathbf{E}\varphi(w)+\mathbf{E}\varphi^{'}(w)l_n-
    \mathbf{E}\varphi^{'}(w)l_n^{*}\right.\\
    {}+
    \fr{1}{2}\,\mathbf{E}\varphi^{''}(w)l_n^2-
    \fr{1}{2}\,\mathbf{E}\varphi^{''}(w)(l_n^{*})^2\\
{}+
    \fr{1}{2}\mathbf{E}\varphi^{'''}(w+l_n\tau )l_n^3(1-\tau)^{2}\\
\left.    {}-
    \fr{1}{2}\mathbf{E}\varphi^{'''}(w+l_n^{*}\tau)(l_n^{*})^3(1-\tau)^{2}\right\vert\,.
    \end{multline*}
    Now, let condition $X_1,\ldots,X_{n-1},\tau$:
    
    \noindent
    \begin{multline*}
    \Delta^{*}_{1,n}=
    \Big|\mathbf{E}_{X_1,\ldots,X_{n-1},\tau} \mathbf{E}_{X_{n}}\varphi^{'}(w)l_n\\
    {}-
    \mathbf{E}_{X_1,\ldots,X_{n-1},\tau}
    \mathbf{E}_{X_{n}}\varphi^{'}(w)l_n^{*}\\
{}+\fr{1}{2}\,
    \mathbf{E}_{X_1,\ldots,X_{n-1},\tau} \mathbf{E}_{X_{n}}\varphi^{''}(w)l_n^2\\
    {}-
    \fr{1}{2}\,\mathbf{E}_{X_1,\ldots,X_{n-1},\tau} \mathbf{E}_{X_{n}}\varphi^{''}(w)(l_n^{*})^2\\
{}+
    \fr{1}{2}\,\mathbf{E}\varphi^{'''}(w+l_n\tau )l_n^3(1-\tau)^{2}\\
    {}-
    \fr{1}{2}\,\mathbf{E}\varphi^{'''}(w+l_n^{*}\tau)(l_n^{*})^3(1-\tau)^{2}\Big|\,.
    \end{multline*}
    Using~(\ref{m}), one gets:
    
    \noindent
     \begin{multline}
     \label{e11-y}
    \Delta^{*}_{1,n}=\left\vert\fr{1}{2}\,\mathbf{E}\varphi^{'''}(w+l_n\tau)l_n^3(1-\tau)^{2}\right.\\
\left.    {}-
    \fr{1}{2}\,\mathbf{E}\varphi^{'''}(w+l_n^{*}\tau)(l_n^{*})^3(1-\tau)^{2}\right\vert\,.
    \end{multline}
    It is easy to see that
    
    \noindent
    \begin{multline*}
    |\mathbf{E}\varphi^{'''}(w+l_n\tau)l_n^3(1-\tau)^{2}|\leq 
    \fr{c}{\varepsilon^{3}}\mathbf{E} \,    \mathbf{I}(x\leq w+\tau l_n\\
    {}\leq x
    +\varepsilon)|l_{n}|^{3}\leq \fr{c}{\varepsilon^{3}}\,\mathbf{E} 
    \left\vert l_{n}\right\vert^{3}\,.
    \end{multline*}

    From theorem~20 in~[5, p.~89], one gets:
    
    \noindent
     \begin{multline}
     \label{11d}
     \mathbf{E}|l_{n}|^{3}=\left\vert\mathbf{E}\left(h(X_1,X_n)+
     \cdots+h(X_{n-2},X_n)\right.\right.\\
\left.\left.     {}+h(X_{n-1},X_n)\right)\right\vert^{3}
    \leq c(n-1)^{3/2}\beta_3\,.
    \end{multline}
    

   Combining~(\ref{c4}), (\ref{e8-y})--(\ref{11d}), one has
   
   \noindent
   \begin{multline*}
   \rho(Z(X_1,\ldots,X_n),Z(G_1,\ldots,G_n))\\
   {}\leq 
   \fr{c}{\sqrt{q_1}}\sqrt{\varepsilon}+\fr{c}{\varepsilon^{3}}(n-1)^{3/2}\beta_3\,.
   \end{multline*}
   
   \pagebreak


\noindent
   Let $\varepsilon= n^{-1/7}\beta_3^{1/3}$, then one gets:
  \begin{multline*}
   \rho(Z(X_1,\ldots,X_n),Z(G_1,\ldots,G_n))\\
   {}\leq
   c_2q_1^{-1/2}\beta_3^{1/6}n^{-1/14}\,.
   \end{multline*}
   
    \subsection{Proof of inequality~(\ref{b})}

\noindent
   Now, let estimate $\rho(Z(G_1,\ldots,G_n),Z_0).$

   It has been shown in~\cite{1-y} that it is possible to represent the statistic
   $Z(G_1,\ldots,G_n)$ in the form:
   $$
   Z(G_1,\ldots,G_n)=n^{-1} \sum\limits_{1\leq i<k\leq n}\langle Q G_i,G_k\rangle
$$
    where $a=(a_i)_{i\geq 1}$ is some constant. Let $G_{i,j}$,  $i \geq 1$ be the components
    of vector $G_{j}$. Then, one can rewrite this expression in the form:
    $$
    Z(G_1,\ldots,G_n)= n^{-1}\sum\limits_{i\geq 1}q_i
    \sum\limits_{1\leq j<k\leq n} G_{i,j}G_{i,k}
    $$
    where $G_{i,j}$ are the normally distributed random variables with the moments $(0,1)$.
        It is easy to see that
        $$
    2n^{-1}q_i\sum\limits_{1\leq j<k\leq n} G_{i,j}G_{i,k}=(n-1)q_i\overline{G}_i^{2}-
    q_iS_i^{2}
        $$
    where
    $$
    \overline{G}_i=n^{-1}\sum\limits_{1\leq j\leq n}G_{i,j} \
    \mbox{and} \ S_i^{2}=n^{-1}\sum\limits_{1\leq j\leq n}G_{i,j}^{2}-\overline{G}_i^{2}\,.
    $$
    It is well known~\cite{2-y} that random variables $\overline{G}_i$ and $S_i$
    are independent. The random variables $\overline{G}_i$ can be written in the form
    $\overline{G}_i =\eta_i/\sqrt{n}$; so, one can write:
    $$
    n^{-1}q_i\sum\limits_{1\leq j<k\leq n} G_{i,j}G_{i,k}=
    \fr{n-1}{n}\,q_i\eta_i^{2}-q_iS_i^{2}\,.
    $$
    Then
    \begin{multline*}
    Z(G_1,\ldots,G_n)= \sum\limits_{i\geq 1}\left(
    \fr{n-1}{2n}q_i\eta_i^{2}-\fr{q_i}{2}\,S_i^{2}\right)\\
{}=\sum\limits_{i\geq 1}\left(\fr{1}{2}\,q_i(\eta_i^{2}-1)-
    \fr{1}{2n}\,q_i\eta_i^{2}-\fr{q_i}{2}\left(S_i^{2}-1\right)\right)\,.
    \end{multline*}
    Assume
    \begin{multline*}
    \rho(Z(G_1,\ldots,G_n),Z_0)
=\sup\limits_{x}
    \left\vert\mathbf{P}\left(\sum\limits_{i\geq 1}\left(\fr{1}{2}q_i
    (\eta_i^{2}-1)\right.\right.\right.\\
%    \label{12-y}
\left.\left.    {}-
    \fr{1}{2n}\,q_i\eta_i^{2}-\fr{q_i}{2}\left(S_i^{2}-1\right) \right)\leq x
    \vphantom{\sum\limits_{i\leq1}}\right)\hspace*{3mm}
        \end{multline*}
    
\noindent
        \begin{multline}
\label{12-y}
\hspace*{5mm}\left.    {}-
    \mathbf{P}\left(\fr{1}{2}\sum\limits_{i\geq 1}q_i(\eta_i^{2}-1)\leq
    x\right)\right\vert \\
{}\leq\sup\limits_{x}\left\vert\mathbf{P}\left(\sum\limits_{i\geq 1}\left(\fr{1}{2}\,q_i
    (\eta_i^{2}-1)\right.\right.\right.\\
\left.\left.    {}-
    \fr{1}{2n}\,q_i\eta_i^{2}-\fr{q_i}{2}\left(S_i^{2}-1\right)
    \right)\leq x     
    \vphantom{\sum\limits_{i\leq1}}
    \right)\\
\left.    {}-
    \mathbf{P}\left(\sum\limits_{i\geq 1}\left(\fr{1}{2}\,q_i(\eta_i^{2}-1)-
    \fr{1}{2n}\,q_i\eta_i^{2}\right)\leq x\right)\right\vert
\\
  {}+\sup\limits_{x}\left\vert\mathbf{P}\left(\sum\limits_{i\geq 1}\left(\fr{1}{2}\,q_i
  \left(\eta_i^{2}-1\right)-
    \fr{1}{2n}\,q_i\eta_i^{2}\right)\leq x\right)\right.\\
\left.    {}-
    \mathbf{P}\left(\fr{1}{2}\sum\limits_{1\leq i}q_i(\eta_i^{2}-1)\leq
    x\right)\right\vert=  v_1+v_2\,.
    \end{multline}
    Let estimate $v_1.$ Using the independence of $S_i$,  $\eta_i$,  and $\eta_j$, $i \neq j$
    and inequality~(\ref{6l}), one obtains:
    $$
    v_1\leq c\left(|q_1|^{-1}\sum\limits_{i\geq 1}|q_i|\mathbf{E}|S_1^{2}-1|\right)^{{1}/{2}}\,.
    $$
    It follows that
    \begin{multline*}
\hspace*{-11.82176pt}\left(\mathbf{E}|S_1^{2}-1|\right)^{{1}/{2}}=\left(\mathbf{E}|
    n^{-1}\sum_{1\leq j\leq n}(G_{1,j}^{2}-1)-\overline{G}_1^{2}|\right)^{{1}/{2}}\\
    {}\leq
    c\left(\mathbf{E}|
    n^{-1}\sum\limits_{1\leq j\leq
    n}(G_{1,j}^{2}-1)|\right)^{{1}/{2}}+\left(\mathbf{E}|\overline{G}_1^{2}|\right)^{{1}/{2}}\,.
    \end{multline*}
    Let estimate the summands. One has:
    $$
    \left(\mathbf{E}|  \overline{G}_1^{2}|\right)^{{1}/{2}}=n^{-1/4}\left(\mathbf{E}|
    \eta_i^{2}|\right)^{{1}/{2}}=n^{-1/4}\,.
    $$
    Also, it is easy to see that
    \begin{multline*}
    \left(\mathbf{E}|n^{-1}\sum\limits_{1\leq j\leq
    n}(G_{1,j}^{2}-1)|\right)^{{1}/{2}}\\
    {}=
    n^{-1/4}\left(\mathbf{E}\left\vert\sum\limits_{1\leq j\leq n}
    \fr{(G_{1,j}^{2}-1)}{\sqrt{n}}\right\vert\right)^{{1}/{2}}\\
   {}\leq        n^{-1/4}\left(\mathbf{E}\left(\sum\limits_{1\leq j\leq
    n}\fr{(G_{i,j}^{2}-1)}{\sqrt{n}}\right)^{2}\right)^{{1}/{4}}\leq cn^{-1/4}\,.
    \end{multline*}
    Consequently,
    $$
    v_1\leq c\left(\sum\limits_{i\geq 1}\left\vert\fr{q_i}{q_1}\right\vert\right)^{1/2}n^{-1/4}\,.
    $$
    Let estimate the second summand in~(\ref{12-y}):
    \begin{multline*}
    v_2=\sup\limits_{x}\left\vert\mathbf{P}\left(\sum\limits_{i\geq 1}\left(q_i(\eta_i^{2}-1)-
    \fr{1}{n}\,q_i\eta_i^{2}\right)\leq x\right)\right.\\
\left.    {}-
    \mathbf{P}\left(q_i(\eta_i^{2}-1)\leq
    x\right)
    \vphantom{\fr{1}{n}\sum\limits_{i\geq 1}}\right\vert\,.
    \end{multline*}
    Denote
    $$
    \theta=-\fr{1}{n}\,q_1\eta_1^{2}\,; \
    T=\sum\limits_{i\geq 2}q_i(\eta_i^{2}-1)\,; \ 
    Y=-  \fr{1}{n}\sum\limits_{i\geq 2}q_i\eta_i^{2}\,.
    $$
    Let $F_{1}(u)=\mathbf{P}(q_1(\eta_1^{2}-1)+
    \theta\leq x)$, $F(u)=\mathbf{P}(q_1(\eta_1^{2}$\linebreak $-\;1)\leq x)$,
    then
    $$
    v_2=\sup\limits_{x}|\mathbf{E} \, F_{1}(x-T-Y)-\mathbf{E} \,
    F(x-T)|\,.
    $$
    Using triangle inequality, one finds:
\begin{multline}
\label{do}
v_2\leq \sup\limits_{x}\left\vert\mathbf{E} \, F_{1}(x-T-Y)-\mathbf{E} \,
    F_{1}(x-T)\right\vert\\
    {}+\sup\limits_{x}\left\vert\mathbf{E} \, F_{1}(x-T)-\mathbf{E} \,
    F(x-T)\right\vert\,.
    \end{multline}
    One has:
    \begin{multline*}
    \hspace*{-4pt}\sup\limits_{x}|\mathbf{E} \, F_{1}(x-T-Y)-\mathbf{E} \,
    F_{1}(x-T)|\leq c \left\vert q_1\right\vert^{-1/2}\sqrt{\mathbf{E} \, Y}\\
    {}\leq c\left\vert q_1\right\vert^{-1/2}
    (\mathbf{E} \, Y^{2})^{1/4}\,.
    \end{multline*}
    It is easy to see that
    $$
    \mathbf{E} \ (Y^{2})^{1/4}\leq  cn^{-1/2}\left(\sum_{i\geq 2}q_i^{2}\right)^{1/4}\,.
    $$
    For the second summand in~(\ref{do}), let apply Lemma~2:
    $$
    \sup\limits_{x}|\mathbf{E} \, F_{1}(x-T)-\mathbf{E} \,
    F(x-T)| \leq \fr{c}{\sqrt{q_1}n}\,.
    $$
    Finally, one has:
    $$
    v_2\leq \fr{c}{\sqrt{q_1}n}+
    cn^{-1/2}\left(\sum\limits_{i\geq 2}\left(\fr{q_i}{q_1}\right)^{2}\right)^{1/4}\,.
    $$
    Consequently,
    $$
    \rho(Z(G_1,\ldots,G_n),Z_0)\leq \fr{c}{n^{1/4}}
    \left(\sum\limits_{i\geq 1}\left\vert\fr{q_i}{q_1}\right\vert\right)^{1/2}+\fr{c}{\sqrt{q_1}n}\,.
    $$
     The theorem is proved.

  {\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{9}
  
    \bibitem{1-y}
    \Au{Bentkus~V., G$\ddot{\mbox{o}}$tze F.}
    Optimal bounds in non-Gaussian limit
      theorems for U-statistics~// Ann. Prob., 1999.
      Vol.~27. No.\,1. P.~454--521.


\bibitem{3-y} %2
\Au{Korolyuk V.\,S., Borovskikh~Yu.\,V.}
Rate of convergence of degenerate von Mises functionals~// Theory Prob. Appl., 
1988. Vol.~33. No.\,1. P.~125--135.

%\bibitem{4-y}
%\Au{Nagaev S.\,V., Chebotarev V.\,I.}
%Estimates of the rate of
%convergence in the central limit theorem in space $l_{2}$~//
%Math. Anal. Smezhn. voprpsy mat.~--- Novosibirsk: Nauka, 1978.
%P.~153--182. [In Russian.]



\bibitem{6-y} %3
\Au{Senatov V.\,V.}
Qualitative effects in the estimates of the
convergence rate in the central limit theorem in
multidimensional spaces~// Proceedings of the Steklov Inst. of
      Math., 1996. Vol.~215. No.\,4. 

%\bibitem{7-y}
%\Au{Yanushkevichiene O.}
%On the rate of convergence of second-degree
%random polynomials~// J. Math. Sci., 1998. Vol.~92. No.\,3.
%P.~3955--3959.

\bibitem{9-y} %4
\Au{Yanushkevichiene O.}
Asymptotic rate of convergence in the degenerate U-statistics of second order~//
Banach Center Publs., 2010. Vol.~90.  P.~275--284.

%\bibitem{5-y}
%\Au{Petrov V.\,V.}
%Limit theorems for the summs of random variables.~--- Moscow: Nauka, 1987.
%[In Russian.]

\bibitem{8-y} %5
\Au{Yanushkevichiene O.}
Optimal rates of convergence of second degree
polynomials in several metrics~// J.~Math. Sci., 2006.
Vol.~138, No.\,1. P.~5472--5479.

\bibitem{2-y} %6
\Au{Cramer H.} 
Mathematical methods of statisics.~--- Stockholm, 1946.


\end{thebibliography}
}
}


\end{multicols}

\vspace*{6pt}

\hrule

\vspace*{6pt}


\def\tit{О СКОРОСТИ СХОДИМОСТИ НЕКОТОРОЙ U-СТАТИСТИКИ}

\def\aut{О.\,Л.~Янушкявичене$^1$, Р.~Янушкявичюс$^2$}

\titelr{\tit}{\aut}

\vspace*{6pt}

\noindent
$^1$Институт математики и информатики Вильнюсского университета, Литва, olgjan@zebra.lt\\
\noindent
$^2$Литовский  университет эдукологии, Вильнюс, Литва 


\Abst{Анализируется асимптотическое поведение U-ста\-ти\-стик, которые важ\-ны для
конструирования статистических методов, применяемых для изучения информационных
систем.}


\label{end\stat}

\KW{скорость сходимости; неравенство Берри--Эссеена; квадратичная форма, U-статистика второго
порядка; мет\-ри\-ка Колмогорова}


\renewcommand{\figurename}{\protect\bf Рис.}
\renewcommand{\tablename}{\protect\bf Таблица}
\renewcommand{\bibname}{\protect\rmfamily Литература}
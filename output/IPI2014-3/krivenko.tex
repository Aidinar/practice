\def\stat{krivenko}

\def\tit{СРАВНИТЕЛЬНЫЙ АНАЛИЗ ПРОЦЕДУР РЕГРЕССИОННОГО АНАЛИЗА}

\def\titkol{Сравнительный анализ процедур регрессионного анализа}

\def\aut{М.\,П.~Кривенко$^1$}

\def\autkol{М.\,П.~Кривенко}

\titel{\tit}{\aut}{\autkol}{\titkol}

%{\renewcommand{\thefootnote}{\fnsymbol{footnote}}
%\footnotetext[1]{Работа поддержана
%Российским фондом фундаментальных исследований (проекты
%12-07-00115a, 12-07-00109a, 14-07-00041а).}}

\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Институт проблем информатики Российской академии наук, mkrivenko@ipiran.ru}


     \Abst{Рассмотрена задача прогнозирования значений одной переменной по
значениям другой методами регрессионного анализа. В~перечень сравниваемых процедур
регрессионного анализа были включены следующие методы оценивания параметров модели:
обычный наименьших квадратов, наименьших квадратов на основе знаковых статистик,
нулевой корреляции на основе ранговых статистик. Реализация метода наименьших
квадратов на основе знаковых статистик потребовала построения эффективной процедуры
обработки данных, для чего был описан и исследован вариант метода наискорейшего спуска.
В~сложной ситуации ку\-соч\-но-по\-сто\-ян\-ной целевой функции получен
работоспособный вариант соответствующего алгоритма.
      Проведенный сравнительный анализ процедур регрессионного анализа в реальных
условиях, когда данные не подчиняются нормальному распределению, показал
преимущества непараметрических методов. Принимая во внимание алгоритмические
аспекты, в рассматриваемом случае предпочтение надо отдать процедурам, основанным на
рангах, а не на знаках.
      Достоинства непараметрических методов позволяют повысить точность согласования
двух способов измерения хромогранина~A, широко используемого в качестве
иммуногистохимического маркера нейроэндокринной дифференцировки.}

      \KW{регрессионный анализ; ранговые и знаковые процедуры; качество прогноза;
согласование результатов измерений}

\DOI{10.14357/19922264140308}

\vskip 12pt plus 9pt minus 6pt

      \thispagestyle{headings}

      \begin{multicols}{2}

            \label{st\stat}

\section{Введение}

     Если в ходе регрессионного анализа появляются свидетельства того, что
распределение данных существенно отличается от нормального (в част\-ности,
присутствуют аномальные наблюдения), появляют\-ся сомнения в
эффективности общепринятых методов и приходится обращаться к робастным
методам. Соответствующая мотивация дана во множестве работ (см., в
частности, гл.~2 в~[1] или разд.~1 в~[2]). В~данной работе рассматривается
задача линейного регрессионного анализа двухмерных данных и использование
результатов ее решения для прогнозирования значений одной переменной с
помощь другой.

\section{Методы линейного регрессионного анализа}

     Классический параметрический метод подбора вида зависимости между
переменными основывается на методе наименьших квадратов, он обладает
оптимальными свойствами при выполнении условий независимости и
гомоскедастичности. Дополнительное предположение о нормальности
распределения данных приводит к простым статистическим процедурам
описания получающихся оценок.

     Для данных примем следующую модель:
     $$
     Y=XA+E\,,
     $$
где $Y=(y_1, \ldots , y_n)^{\mathrm{T}}$~--- зависимая переменная, $X\hm=
\begin{pmatrix} 1 & \cdots & 1\\
x_1 & \cdots & x_n\end{pmatrix}^{\mathrm{^T}}$~--- независимая переменная
(предиктор), $A\hm= (a_1, a_2)^{\mathrm{T}}$~--- параметры модели, $E\hm=
(\varepsilon_1, \ldots , \varepsilon_n )^{\mathrm{T}}$~--- ошибка
представления~$Y$ с \mbox{помощью}~$X$.

     Обычные оценки наименьших квадратов (OLS-оцен\-ки) являются
решением следующей задачи:
     \begin{equation*}
     S_{\mathrm{OLS}}(a_1, a_2)
     \underset{a_1,a_2}{\Longrightarrow} \min\,.
     \end{equation*}
Здесь
     $S_{\mathrm{OLS}} (a_1,a_2) =\sum\limits_{i=1}^n (y_i -a_1 -x_ia_2)^2$
и имеют вид:
$$
a_2^* = \fr{\sum\limits_{i=1}^n (x_i -\overline{x})y_i} {\sum\limits_{i=1}^n (x_i-
\overline{x} )^2}\,;\quad a_1^*=\overline{y}-a_2^*\overline{x}\,,
$$
где верхняя горизонтальная черта означает выборочное среднее. Здесь и далее
аббревиатуры типа OLS формировались на основе общепринятых
англоязычных терминов.

     Оценки параметров линейной регрессии с помощью непараметрических
методов строятся путем применения идей OLS-оцен\-ки и использования
аналогов обычных статистик (см., например, разд.~8 в~[3]), причем объектом
анализа становятся так называемые остатки, т.\,е.\ величины $y_i\hm - a_1 \hm-
x_i a_2$, $i\hm= 1, \ldots , n$.

     Регрессия на основе линейных ранговых ста\-тистик
     Ман\-на--Уит\-ни--Вил\-кок\-со\-на использует ко\-эффициент корреляции
общего вида с заменой значе\-ний остатков на их ранги, что приводит к
уравнению для получения оценок параметров регрессионной модели,
имеющему следующий вид:
     \begin{equation}
     S_{\mathrm{RZE}} (a_1, a_2)=0\,,
     \label{e1-kri}
     \end{equation}
где
\begin{multline*}
S_{\mathrm{RZE}}(a_1, a_2) = {}\\
{}=\sum\limits_{i=1}^n (x_i- \overline{x}) \,
\mathrm{rank} \left( y_i -a_1 -a_2 (x_i-\overline{x} )\right)\,.
\end{multline*}

     Принимая во внимание инвариантность рангов относительно сдвига,
получаем, что~(1) эквивалентно следующему уравнению для нахождения уже
только оценки для~$a_2$:
     \begin{equation}
     T(a_2) =0\,,
     \label{e2-kri}
     \end{equation}
где
$$
T(a_2) = \sum\limits_{i=1}^n x_i\,\mathrm{rank}\left( y_i -a_2 x_i\right) -
\fr{n(n+1)}{2}\,\overline{x}\,.
$$

     Трудности решения~(2) возникают в силу того, что $T(u)$~---
     ку\-соч\-но-по\-сто\-ян\-ная функция. Если предположить, что $x_j\not=
x_i$ для всех допустимых неравных значений~$i$ и~$j$, и ввести обозначения
$b_{ij} \hm = (y_j-y_i)/(x_j-x_i)$, то оказывается, что функция $T(u)$ не меняет
своего значения при любых~$u$, лежащих между двумя последующими
значениями~$b_{ij}$. При этом в общем случае решения~(2) может и не быть.
В~качестве выхода из создавшегося положения предлагалось рассматривать
$T(u)$ как аппроксимацию некоторой непрерывной функции и использовать
линейную интерполяцию между двумя соседними значениями~$u$, для
которых $T(u)$ принимает различные по знаку значения (см.\ разд.~8.1.2 в~[3]).
Таким образом может быть найдена оценка $a_2^*$ для~$a_2$.

     Но для задачи прогнозирования важна оценка и другого параметра
регрессионной модели. Существуют различные варианты ее получения (см.\
разд.~8.1.5 в~[3]), один из них основывается на предположениях относительно
наблюденных значений и является действенным и простым. Для выбранной
модели данных остатки вида $y_i \hm- a_2 x_i$ для $i\hm= 1, \ldots ,n$ имеют
идентичные распределения с медианой, близкой к~$a_1$. Поэтому идея метода
оценивания~$a_1$ состоит в получении выборочной медианы для
наблюденных значений $y_i\hm- a_2^*x_i$. Это замечание завершает описание
шагов по оцениванию параметров~$a_1$ и~$a_2$.

     Знаковые оценки параметров модели вводятся на основе знаковых
критериев значимости и их статистик (см.\ гл.~3 в~[4]). Они являются решением
задачи
     \begin{equation}
     S_{\mathrm{SLS}}(a_1,a_2) \underset{a_1,a_2}{\Longrightarrow} \min\,,
     \label{e3-kri}
     \end{equation}
где
\begin{multline*}
S_{\mathrm{SLS}}(a_1,a_2) = \left( \sum\limits_{i=1}^n \mathrm{sign} \left( y_i -a_1 -x_i a_2
\right)\right)^2 +{}\\
{}+\left( \sum\limits_{i=1}^n x_i \mathrm{sign} \left( y_i -a_1 -x_i a_2\right)
\right)^2\,.
\end{multline*}

     К сожалению, предложенные в~[4] решения этой задачи даже
применительно к простой двух\-па\-ра\-мет\-ри\-че\-ской регрессионной модели далеки
от завершения. Поэтому для того, чтобы провести сравнительный анализ
методов оценивания, пришлось сначала предложить и исследовать приемы
построения знаковых оценок.

     Введем ряд функций от одной переменной~$u$, зависящих от наборов
векторных параметров $r$, $v$, $w$ (всех или части из них), и исследуем их
свойства. Они будут играть ключевую роль при решении задачи~(3). Начнем с
функции следующего вида:
     $$
     \psi_0 (r,v,w,u) =\sum\limits_{i=1}^n r_i \mathrm{sign} \left( v_i-w_iu\right)\,.
     $$
Выделим в сумме слагаемые с $w_i\hm=0$, что приведет к следующим
представлениям:

\noindent
\begin{multline*}
\psi_0(r,v,w,u) =\sum\limits_{\substack{{i=1,\ldots , n}\\
{w_i=0}}} r_i \mathrm{sign}\left (v_i\right) +{}\\
{}+
\sum\limits_{{\substack{{i=1,\ldots ,n}\\{w_i\not=0}}}} r_i
\mathrm{sign}\left (v_i-w_i u\right)= %{}\\
%{}=
\sum\limits_{\substack{{i=1,\ldots , n}\\
{w_i=0}}} r_i \mathrm{sign}\left(v_i\right) +{}\\
{}+
\sum\limits_{\substack{{i=1,\ldots ,n}\\{w_i\not=0}}} r_i \mathrm{sign}\left (w_i\right)
\mathrm{sign} \left(
\fr{v_i}{w_i}-u\right)={}\\
{}=
\sum\limits_{\substack{{i=1,\ldots , n}\\
{w_i=0}}} r_i \mathrm{sign}\left (v_i\right) +
\sum\limits_{\substack{{i=1,\ldots ,n}\\{w_i\not=0}}} \tilde{r}_i
\mathrm{sign}\left (\tilde{v}_i- u\right)\,,
\end{multline*}
где $\tilde{r} =r_i \mathrm{sign}\left(w_i\right)$; $\tilde{v}_i = v_i/w_i$.

\pagebreak

     Рассмотрим сумму, содержащую только сла\-га\-емые с $w_i\not= 0$, а
именно:
     $$
     \psi_1 (\tilde{r}, \tilde{v}, u) =\sum\limits_{i=1}^m \tilde{r}_i
     \mathrm{sign} \left(
\tilde{v}_i-u\right)\,.
     $$
Упорядочим значения $\tilde{v}_i$ по возрастанию, что даст
последовательность $\{ \tilde{v}_{(i)}\}$. После перестановки индексов
получим эквивалентную запись суммы:
$$
\psi_1(\tilde{r},\tilde{v},u) =\sum\limits_{i=1}^m
\tilde{r}_{i(\tilde{v}_{(i)})}
\mathrm{sign}  \left( \tilde{v}_{(i)}- u\right)\,,
$$
где $i(\tilde{v}_{(i)})$~--- индекс значения $\tilde{v}_{(i)}$ в наборе $\left\{
\tilde{v}_i\right\}$  до
упорядочивания. Тогда верны следующие утверж\-дения:
\begin{itemize}
\item функции $\psi_1(\tilde{r},\tilde{v},u)$ и $\psi_0(r,v,w,u)$ являются
ку\-соч\-но-по\-сто\-ян\-ны\-ми с точками разрыва 1-го рода $\tilde{v}_{(i)}$;
\item $\psi_1 (\tilde{r},\tilde{v}, -\infty) =
\sum\limits_{\substack{{i=1,\ldots ,n}\\{w_i\not=0}}} \tilde{r}_i $
и
$\psi_0(r,v,w,-\infty) \hm= \sum\limits_{\substack{{i=1,\ldots , n}\\
{w_i=0}}} r_i \mathrm{sign}\left(v_i\right) +
\sum\limits_{\substack{{i=1,\ldots ,n}\\{w_i\not=0}}} \tilde{r}_i$;
\item переход через каждую точку $\tilde{v}_{(i)}$ при возрастании~$u$
дважды изменяет значение $\psi_1(\tilde{r}, \tilde{v},u)$ и
$\psi_0(r,v,w,u)$ на величину $\tilde{r}_{i(\tilde{v}_{(i)})}$.
\end{itemize}
Справедливость перечисленных утверждений следует непосредственно из вида
рассматриваемых функций.

     Объединим обозначения для множеств точек, где
     ку\-соч\-но-по\-сто\-ян\-ная функция принимает некоторое значение,
следующим образом: $\langle \tilde{v}_{(i)}, \delta_i\rangle$, где
$\delta_i\hm=0$, когда речь идет о точке, и $\delta_i \hm = \tilde{v}_{(i+1)} \hm-
\tilde{v}_{(i)}\hm>0$, когда речь идет об интервале. Для пар $\langle \tilde{v}_{(i)},
\delta_i\rangle$ действует обычный лексикографический порядок, а для
множества пар определен минимальный элемент.

     Теперь можно перейти к описанию поведения квадратов введенных
функций. Функции вида $\psi_0^2(r,v,w,u)$ или $\psi_1^2(\tilde{r},
\tilde{v},u)$ достигают своего минимума либо в точке $\tilde{v}_{(i)}$, либо на
интервале $\left(\tilde{v}_{(i)}, \tilde{v}_{(i+1)}\right)$ для некоторого
значения~$i$; этот минимум может быть найден с помощью перебора пар
$\langle \tilde{v}_{(i)}, \delta_i\rangle$. Аналогичные выводы справедливы и для
функции
     $$
     \varphi_0(r,v,w,u) =\psi_0^2(1,v,w,u)+\psi_0^2(r,v,w,u)\,,
     $$
      где $1=(1, \ldots , 1)^{\mathrm{T}}$.

     Введем дополнительные ограничения на вектор~$r$ и рассмотрим
следующую функцию:
     $$
      \psi_2(r,v,u) =\sum\limits_{i=1}^m r_i \mathrm{sign} \left(v_i-u\right)\,, \enskip
      r_i>0\ \mbox{для всех } i\,.
      $$
Для нее выполняется дополнительное свойство: переход через каждую точку
$v_{(i)}$ при возрастании~$u$ уменьшает значение $\psi_2(r,v,u)$ дважды
на величину $r_{i(v(i))}$.

     Далее понадобятся свойства функции $\psi_2(1,v,u)$. Она достигает
нулевого значения при $\langle v_{(m/2)}, v_{(m/2)+1}-v_{(m/2)}\rangle$, если
$m$ четно, или при $\langle v_{\lfloor m/2\rfloor +1},0\rangle$, если $m$ нечетно.

     Для $\varphi_2^2(r,v,u)$ поиск минимума перебором точек $v_{(i)}$
можно сделать более эффективным. Дело в том, что функция
$\psi_2^2(r,v,u)$ не возрастает в области, где $\psi_2(r,v,u)\hm>0$, и не
убывает, где $\psi_2(r,v,u)\hm<0$; этим можно воспользоваться для
завершения последовательного перебора пар $\langle v_{(i)}, \delta_i\rangle$ для
$i\hm= 1, 2,\ldots$

     Функция $\psi_2^2(1,v,u)$ достигает своего нулевого минимального
значения при $\langle v_{(m/2)}, v_{(m/2)+1}\hm-v_{(m/2)}\rangle$,
если~$m$~четно, или при $\langle v_{\lfloor m/2\rfloor +1},0\rangle$, если~$m$~нечетно.

     Как следствие, становится возможным сократить перебор и для функции
вида:
     $$
     \varphi_2(r,v,u) =\psi_2^2\left(1,v,u\right)+\psi_2^2(r,v,u)\,.
     $$
Просмотр точек разрыва $v_{(i)}$ для $i\hm=1, 2,\ldots$ следует прекратить для
такого наименьшего $i\hm=k$, при котором $\psi_2(1,v,v_{(k)}) \hm <0$ и
$\psi_2(r,v,v_{(k)})\hm> \psi_2(r,v,v_{(k-1)})$.

     Вернемся к задаче~(3), решение которой будет формироваться
     каким-либо итерационным методом. Приближение на $t$-м шаге итерации обозначим
как $\left( a_1^{(t)}, a_2^{(t)}\right)$ и рассмотрим получение $\left( a_1^{(t+1)},
a_2^{(t+1)}\right)$ на следующем шаге итерации. Зная $\left( a_1^{(t+1)},
a_2^{(t+1)}\right)$, можно вычислить соответствующее значение $S_{\mathrm{SLS}}\left(
a_1^{(t+1)}, a_2^{(t+1)}\right)$ и сравнить его с тем, что получено на
предыдущем шаге итерации. Процесс уточнения оценок будет завершаться при
многократном повторении одинаковых значений целевой функции.

     Уже понятно, что решением оптимизационных задач может быть либо
отдельное значение, либо интервал. Для того чтобы в дальнейшем обеспечить
однозначность при использовании оценок па\-ра\-мет\-ров модели, во втором случае
в качестве решения будем выдавать некоторое значение из интервала,
полученное случайным образом, а именно: если минимум некоторой функции
достигается при значениях $\langle v,\delta\rangle$, где $\delta \hm>0$, то
итоговое решение $v^*\hm= \mathrm{Random}\left (v, v+\delta \right)$, где $\mathrm{Random}\left(c,d\right)$~---
случайная величина, равномерно распределенная на интервале $(a,b)$.

     Руководствуясь условиями далее рассматри\-ва\-емой реальной задачи,
предположим, что $x_i\hm>0$ для $i\hm= 1,\ldots , n$. Теперь опишем
возможные методы решения~(3), ограничившись заданием очередного шага
итерации.

     \smallskip

     \textbf{Метод поэлементно-покоординатного спуска.} Будем сначала
решать задачу
$$
\left(\sum\limits_{i=1}^n \mathrm{sign}\left( y_i-a_1-x_ia_2^{(t)}\right)
\right)^2\underset{a_1}{\Rightarrow}\min
$$
или
$$
\psi_2^2(1,v,u)\underset{u}{\Rightarrow}\min\,,
$$
где $v_i=y_i-x_i a_2^{(t)}$.

     Ранее было указано, что $\min\limits_u \psi_2^2(1,v,u)\hm=0$ и он
достигается
     при $a_1^{(t+1)}\hm\in \left(v_{(n/2)+1}-v_{(n/2)}\right)$, если $n$ четно, или
     при $a_1^{(t+1)} \hm= v_{\lfloor n/2\rfloor +1}$, если $n$ нечетно.

     После этого будем решать задачу
     $$
     \left( \sum\limits_{i=1}^n x_i \mathrm{sign} \left( y_i -a_1^{(t+1)} -x_i
a_2\right)\right)^2 \underset{a_2}{\Rightarrow}\min
     $$
или
$$
\psi_2^2(r,v,u) \underset{u}{\Rightarrow}\min\,,
$$
где $r_i=x_i$; $v_i\hm= (y_i -a_1^{(t+1)})/x_i$.

     В итоге будет получено $\left( a_1^{(t+1)}, a_2^{(t+1)}\right)$.

     Такой вариант алгоритма был предложен в \S\,3.1~[4], где отмечается, что
<<теоретически его сходимость не доказана, но во всех многочисленных
случаях его применений мы получили правильные результаты>>.

     \smallskip

     \textbf{Метод покоординатного спуска.} Будем сначала решать задачу
     $$S_{\mathrm{SLS}} \left(a_1, a_2^{(t)}\right) \underset{a_1}{\Rightarrow}\min\,,
     $$
где
\begin{multline*}
S_{\mathrm{SLS}} \left( a_1,a_2^{(t)}\right) = \left( \sum\limits_{i=1}^n
\mathrm{sign} \left( y_i -a_1 -
x_ia_2^{(t)}\right)\right)^2+{}\\
{}+ \left( \sum\limits_{i=1}^n x_i \mathrm{sign} \left( y_i -a_1 - x_i a_2^{(t)} \right)
\right)^2\,,
\end{multline*}
или
$$
\varphi_0(r,v,1,u) \underset{u}{\Rightarrow}\min\,,
$$
где $r_i= x_i$; $v_i=y_i\hm- x_i a_2^{(t)}$.
Ее решение даст значение $a_1^{(t+1)}$. После чего перейдем к задаче
$$
S_{\mathrm{SLS}} \left( a_1^{(t+1)}, a_2\right) \underset{a_2}{\Rightarrow}\min\,,
$$
где
\begin{multline*}
S_{\mathrm{SLS}} \left(  a_1^{(t+1)},a_2\right) ={}\\
{}=
\left( \sum\limits_{i=1}^n \mathrm{sign}
\left( y_i -a_1^{(t+1)} -x_ia_2\right) \right)^2+{}\\
{}+\left( \sum\limits_{i=1}^n x_i \mathrm{sign} \left( y_i -a_1^{(t+1)} -x_i a_2\right)
\right)^2\,,
\end{multline*}
или
$$
\varphi_0(r,v,1,u)\underset{u}{\Rightarrow}\min\,,
$$
где $r_i =x_i$; $v_i\hm= (y_i-a_1^{(t+1)})/x_i$.

     Таким образом будет получено значение для $a_2^{(t+1)}$.

     \smallskip

     \textbf{Метод <<наискорейшего>> спуска.} Возникающие при
применении описанных методов проблемы побуждали к разработке процедур,
обеспечивающих гарантированное получение минимума функции. Рассмотрим
прямую, проведенную под углом~$\alpha$ к горизонтальной оси и проходящую
через точку $\left( a_1^{(t)}, a_2^{(t)}\right)$. Положение точки $(a_1,a_2)$ на
этой прямой будет определяться относительно $\left( a_1^{(t)}, a_2^{(t)}\right)$
с помощью величины~$u$, т.\,е.\ координаты этой точки принимают вид $\left( u
\cos\alpha \hm+ a_1^{(t)}, u\sin\alpha\hm+a_2^{(t)}\right)$. Тогда получаем:
     \begin{multline*}
     y_i -a_1 -x_ia_2 ={}\\
     {}= y_i -\left( u\cos\alpha + a_1^{(t)}\right) -x_i \left(
u\sin\alpha + a_2^{(t)}\right)={}\\
     {}= y_i -a_1^{(t)} -x_i a_2^{(t)} -\left( \cos\alpha +x_i\sin\alpha\right) u\,.
     \end{multline*}

     В результате приходим к задаче
     \begin{equation}
     \varphi_0(r,v,w,u)\underset{u}{\Rightarrow}\min\,,
     \label{e4-kri}
     \end{equation}
где $r_i=x_i$; $v_i=y_i\hm- a_1^{(t)} \hm- x_i a_2^{(t)}$; $w_i\hm= \cos\alpha
\hm+ x_i\sin\alpha$.

     Решение~(4) даст $\left( a_1^{(t+1)}, a_2^{(t+1)}\right)$. При этом
значение $S_{\mathrm{SLS}} \left( a_1^{(t+1)}, a_2^{(t+1)}\right)$ может относительно
$S_{\mathrm{SLS}} \left( a_1^{(t)}, a_2^{(t)}\right)$ и не уменьшиться. Чтобы по
возможности предотвратить это, найдем пары $\left( a_1^{(t+1)},
a_2^{(t+1)}\right)$ для различных значений~$\alpha$ и выберем среди них ту
оценку параметров модели, которой отвечает наименьшее значение
$S_{\mathrm{SLS}}(a_1,a_2)$. В~данной работе был опробован вариант с набором
углов~$\alpha$ вида
     $ \left\{ 0, (1/m)\pi,\ldots , ((m-1)/m)\pi\right\}$ для $m\hm>1$.
Таким образом, появляется дополнительный параметр алгоритма поиска оценок
параметров модели~--- чис\-ло~$m$ перебираемых углов.

     Была сформулирована и опробована еще одна процедура, реализующая
условно названный метод опорных точек <<наискорейшего>> спуска. Далее
описывается ее основной шаг. Искомое решение $(a_1^*, a_2^*)$ задачи~(3)
есть некоторая прямая на плоскости, разделяющая точки $(x_i,y_i)$ на три
подмножества: точки, для которых $y_i\hm- a_1^* \hm- x_i a_2^*\hm<0$; точки,
для которых $y_i\hm- a_1^* \hm- x_i a_2^*\hm=0$; точки, для которых $y_i\hm-
a_1^*\hm- x_ia_2^*\hm>0$.

Рассмотрим ситуацию, когда некоторая прямая с
коэффициентами $(a_1,a_2)$ проходит по крайне мере через одну точку
$(x_l,y_l)$, т.\,е.\ $y_l\hm-a_1\hm- x_la_2\hm=0$ или $a_1\hm= y_l\hm- x_la_2$.
В~результате имеем:
\begin{multline*}
y_i- a_1  - x_ia_2 = y_i - (y_l- x_la_2)
 - x_ia_2={}\\
 {}= (y_i - y_l)- (x_i- x_l)a_2\,.
 \end{multline*}
  Тогда получаем задачу:
     $$
     \varphi_0(r,v,w,u)\underset{u}{\Rightarrow} \min\,,
     $$
где $r_i = x_i$; $v_i\hm= y_i\hm-y_l$;  $w_i\hm= x_i\hm-x_l$.

     Таким образом, с помощью перебора всех пар $(x_l,y_l)$ может быть
получено решение задачи~(3), правда, при дополнительном условии, что
$y_l\hm- a_1\hm- x_la_2\hm=0$ по крайне мере для одного из $l\hm= 1,\ldots ,n$.
Выяснить, нужно ли анализировать ситуации, когда прямая с коэффициентами
$(a_1,a_2)$ проходит между всеми точками $(x_i,y_i)$, $i\hm= 1,\ldots ,n$, а
также, если в этом есть необходимость, как искать минимум в этом случае,
пока не удалось. При этом процедура продемонстрировала свою
жизнеспособность, хотя и требует дальнейшего развития.

     При найденных оценках параметров регрессии прогнозом для заданного
значения~$x$ называется величина $y^*\hm= a_1^*\hm+ a_2^*x$. При
сравнении различных\linebreak способов получения <<новых>> значений~$y^*$
необходимо определиться с понятием качества прогнозирования и методами
оценивания выбранного\linebreak показате\-ля качества. Обычно для этих целей
используется ошибка прогноза, измеренная как средний квадрат ошибки
прогноза, где ошибка~---\linebreak отличие между предсказанным~$y^*$ и истинным
значени\-ем~$y$.

     В данной работе интерес представляет случай, когда истинное
значение~$y$ отсутствует, а имеется лишь результат его измерения с некоторой
ошибкой. При этом предполагается, что при использовании переменных важны
не столько их точные значения, сколько интервалы (диапазоны) значений, в
которые они попадают. По этой причине в качестве показателя качества был
выбран следующий. Разобьем диапазон возможных значений переменной~$y$
на~$k$~интервалов группировки (эти интервалы не пересекаются, их
объединение дает весь диапазон значений~$y$) и представим отдельное~$y$ с
помощью номера интервала, которому оно принадлежит. Считается, что
прогноз осуществлен с ошибкой (индикатор ошибки равен~1), когда для
некоторой пары $(x,y)$ значения~$y^*$ и~$y$ попадают в различные
интервалы группировки; в противном случае принимается, что прогноз
безошибочен (индикатор~--- 0). Тогда ошибка прогноза есть среднее значение
индикатора ошибки.

     Простейший способ оценивания ошибки прогноза заключается в
повторном использовании данных сначала для оценивания параметров
регрессии, а затем для подсчета выборочного среднего индикатора ошибки.
Понятно, что получающийся результат будет излишне оптимистичен и требует
улучшения, которого можно достичь за счет использования различных частей
исходных данных для подбора значений параметров модели и оценивания
качества этого подбора. С~учетом реально действующего условия на малый
объем исходных данных приходим к следующим подходам:
     \begin{itemize}
\item метод перепроверки~[5], когда вся выборка делится на~$V$
непересекающихся групп приблизительно одинакового объема, далее
удаляем одну группу и используем оставшиеся группы для подбора
параметров модели, а удаленную группу~--- для оценивания качества
подбора, повторяем эту процедуру~$V$~раз, каждый раз удаляя различные
группы данных, находим итоговую оценку качества как среднее $V$
получившихся оценок;
\item бутстреп-ме\-тод~[6], когда на основе исходных данных
параметрическим или непараметрическим путем формируется
бут\-стреп-вы\-бор\-ка и с~ее помощью осуществляется подбор
регрессионной модели, которая используется для оценива\-ния ошибки
прогноза для новой
бут\-стреп-вы\-бор\-ки, подобная двухшаговая процедура повторяется
необходимое число раз, что позволяет получить среднее значение ошибки
прогноза.
\end{itemize}

     Бутстреп-метод, безусловно, имеет более общий характер, нежели метод
перепроверки, но для его реализации фактически остается только
непараметрический подход, так как не существует работоспособных
параметрических моделей для пары случайных переменных при отклонении от
предположения о нормальном распределении (а иначе не имело бы смыс\-ла
затевать исследование непараметрических методов линейного регрессионного
анализа). Непараметрический же бутстреп-метод в случае выборок малого
объема привносит дополнительные вопросы: например, что делать с
повторяющимися данными в бут\-стреп-вы\-бор\-ке. Поэтому в данной работе
использовался метод перепроверки при $V\hm=n$ (осуществляется исключение
единственного наблюдения для оценки ошибки прогноза).

\section{Эксперименты}

Совместно со специалистами ФГБУ <<НИИ урологии>> Минздрава России
Сивковым~А.\,В., Кешишевым~Н.\,Г., Ковченко~Г.\,А., Никоновой~Л.\,М.
реша\-лась задача согласования двух способов определения
хромогранина А (CgA)~--- иммуногистохимического маркера
нейроэндокринной дифференцировки. В~России официально зарегистрированы
две соответствующие тест-сис\-те\-мы: одна~--- компании DAKO, а другая~---
компании Euro-Diagnostica (ED). Для исследования диагностической цен\-ности
двух тест-сис\-тем был проведен сравнительный статистический анализ
показателей CgA при различных заболеваниях. Все пациенты были разделены
на следующие группы: контрольную, в которую вошли здоровые мужчины
(контрольная группа) и еще 7~групп с различными заболеваниями предстательной
железы. Всего было проанализировано 113~мужчин, из которых 29~--- это
контрольная группа. Результаты исследований показали, что референсные
значения принципиально разнились.
С~по\-мощью линейного
регрессионного анализа была описана функциональная связь между
измерениями, полученными тест-сис\-те\-ма\-ми DAKO и ED (далее~--- CgA\_D
и CgA\_E соответственно). Наличие альтернативных методов измерения одного
и того же показателя привело к необходимости их сравнительного анализа, при
этом преследовались следующие цели:
     \begin{itemize}
\item принять решение о применении в повседневной практике обоих
методов или о предпочтении одного из них (например, с позиций
доступности, точности, стоимости);
\item иметь возможность одновременного использования результатов
измерений, полученных различными методами.
\end{itemize}

     Проведенные исследования показали, что эффективность использования
обеих тест-сис\-тем практически одинакова. При этом референсные значения
для системы DAKO требуют корректировки, для чего можно использовать
выявленную функциональную связь между измерениями, полученными
различными способами. Эта связь также позволяет использовать в диагностике
обе тест-сис\-темы.

     При формировании статистических моделей для собранных данных было
выяснено, в част\-ности, следующее: для измерений CgA\_D и
CgA\_E нет предпосылок принять гипотезу о нормальном распределении, что
побуждает к осторожности при использовании методов, ориентированных на
эту модель данных.

     В контексте данной работы становится интересным, можно ли повысить
качество прогноза CgA\_D по CgA\_E, отказываясь от модели нормального
распределения.

     Для сравнительного анализа были взяты следующие методы оценивания:
     \begin{itemize}
\item обычный наименьших квадратов (OLS);\\[-9pt]
\item наименьших квадратов на основе знаковых статистик 
(SLS);\\[-9pt]
\item нулевой корреляции на основе ранговых статистик (RZE).
\end{itemize}

Анализ остатков для OLS-оцен\-ки с помощью гис\-то\-грам\-мы,
приведенной на рисунке, под\-тверж\-да\-ет
сомнения в использовании модели
нормального распределения (наличие явной асимметрии).

      \begin{figure*} %fig1
      \vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=140mm
\epsfbox{kri-1.eps}
}
\vspace*{4pt}

\noindent
      {\small Гистограмма остатков для OLS-оценки}
      \end{center}
%      \vspace*{-3mm}
      \end{figure*}

     Наибольшую трудность в смысле реализации представляет получение
SLS-оце\-нок. Дело в том, что заверения авторов~[4] о сходимости
итерационного процесса не подтвердились: часто алгоритм сходился к
решению, которое не было наилучшим, или его шаги сначала приводили к
уменьшению целевой функции, а затем к обратному результату~--- росту
значений $S_{\mathrm{SLS}}(a_1,a_2)$. Отказ от поэлементной оптимизации и
использование классического метода покоординатного спуска давал еще более
плачевные результаты. Причина такого положения дел кроется в разрывности
функции $S_{\mathrm{SLS}}(a_1,a_2)$. В~предлагаемом методе <<наискорейшего>>
спуска за счет перебора углов~$\alpha$ удалось преодолеть описанную
трудность. При этом работоспособная реа\-лизация была достигнута при
следующей схеме \mbox{задания} значений рассматриваемых углов: на первом шаге
итерации $m\hm= 11$, на втором~--- 101, на \mbox{третьем}~--- 997, на четвертом~---
10\,007, на пятом~--- $m\hm= 100\,003$. Дальнейшего увеличения номера
итерационного шага не понадобилось, так как при всех последующих
экспериментах по крайне мере на двух последних итерациях целевая функция
своего значения не меняла. Основные характеристики принятых значений~$m$
(простые числа в качестве значений, возрастание при увеличении номера шага
итерации, увеличение на порядок) были уста\-нов\-ле\-ны экспериментально.
В~качестве начального приближения при получении SLS-оцен\-ки бралась
OLS-оценка.

     Оценивание качества прогноза проводилось методом перепроверки
следующим образом. Пусть из массива исходных данных исключено $i$-е
наблюдение, $i\hm=1, \ldots ,n$. Найдем по оставшимся данным\linebreak\vspace*{-12pt}

\noindent
{\small
\vspace*{-3pt}
\begin{center}
\tabcolsep=10pt
\begin{tabular}{|c|c|c|c|}
\multicolumn{4}{c}{
\tabcolsep=-10pt\begin{tabular}{l}Частота успешных прогнозов для\\ различных оценок\end{tabular}}\\[9pt]
\hline
$k$&OLS&SLS&RZE\\
\hline
\hphantom{9}3&97&98&98\\
\hphantom{9}4&91&91&91\\
\hphantom{9}5&85&84&84\\
\hphantom{9}6&76&77&77\\
\hphantom{9}7&68&70&70\\
\hphantom{9}8&62&62&62\\
\hphantom{9}9&64&64&64\\
10&58&57&57\\
11&50&52&51\\
12&45&50&50\\
13&48&49&50\\
14&39&41&41\\
15&35&38&38\\
16&38&40&41\\
17&32&35&36\\
18&27&30&32\\
19&27&31&31\\
20&30&30&30\\
\hline
\end{tabular}
\end{center}}
%\end{table*}

\vspace*{7pt}

\noindent одну из
сравниваемых оценок зависимости и с ее помощью построим прогноз для
исключенного наблюдения, полученный результат сравним с известным
исключенным значением маркера. Если они попадают в один из $k$ интервалов
группировки, то пометим результат проведенного эксперимента как успешный,
в противном случае~--- как неуспешный. По всем $n$ подобным
экспериментам посчитаем относительную частоту полученных успехов.
Соответствующие результаты приведены в таблице.
{ %\looseness=1

}



Из полученных результатов видно, что прогноз на основе SLS-оцен\-ки лучше,
чем с помощью OLS-оцен\-ки (частота случаев, когда SLS не хуже OLS,
составляет 89\%), а прогноз на основе RZE-оцен\-ки лучше, чем с помощью
SLS-оцен\-ки (частота случаев, когда RZE не хуже SLS, составляет 94\%).

     Для верификации полученного вывода о целесообразности отказа от
модели нормального распределения в пользу предположений общего харак\-тера
можно проделать следующее: предположить, что распределение данных есть
нормальное с па\-ра\-мет\-ра\-ми, совпадающими с соответствующими выборочными
характеристиками (моментами и регрессионной связью), и смоделировать для
него \mbox{набор}
 наблюденных значений, к которым при\-менить\linebreak описанную ранее
процедуру обработки данных, а~именно: нахождение OLS-, SLS-, \mbox{RZE-оце\-нок}
и сравнение качества прогноза с помощью метода перепроверки. Тогда если
полученные выводы
 о нецелесообразности использования модели нормального
распределения верны, то для смоделированных данных никаких преимуществ
перехода  к непараметрическим методам анализа не должно быть обнаружено.
Это полностью подтвердилось, в~част\-ности частота случаев, когда SLS не хуже
OLS, составляет 44\%, а частота случаев, когда RZE не хуже OLS, составляет
56\%.

     Таким образом, обобщение модели данных приводит при ограниченном
объеме наблюдений не к снижению, а к повышению качества принимаемых
решений, что становится дополнительным доводом для отказа от
использования нормального распределения.

\vspace*{-6pt}

\section{Заключение}

     В перечень сравниваемых процедур регрессионного анализа были
включены следующие методы оценивания параметров модели: обычный
наименьших квадратов, наименьших квадратов на основе знаковых статистик,
нулевой корреляции на основе ранговых статистик. Реализация метода
наименьших квадратов на основе знаковых статистик потребовала построения
эффективной процедуры обработки данных, для чего был описан и исследован
вариант метода <<наискорейшего>> спуска. В~сложной ситуации
     ку\-соч\-но-по\-сто\-ян\-ной целевой функции получен работоспособный
вариант соответствующего алгоритма.

     Проведенный сравнительный анализ процедур регрессионного анализа в
реальных условиях, когда данные не подчиняются нормальному
распределению, показал преимущества непараметрических методов. Принимая
во внимание алгоритмические аспекты, в рассматриваемом случае
предпочтение надо отдать процедурам, основанным на рангах, а не на знаках.

     Рассмотренные непараметрические процедуры содержат элементы,
носящие скорее эвристический, нежели теоретически обоснованный характер
(например, аналогия как принцип построения непараметрических методов,
принцип подбора значений параметра~$a_1$ в ранговых процедурах и~т.\,п.).
Это побуждает к дальнейшему развитию предлагаемых решений (например, в
духе разд.~4 и~5~\cite{8-kri}), что, в свою очередь, влечет за собой
необходимость в проведении обширных экспериментов по сравнительному
анализу получающихся модификаций методов и процедур обработки данных.

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{9}
\bibitem{1-kri}
\Au{Ryan T.\,P.} Modern regression methods.~--- 2nd ed.~--- Hoboken, NJ: Wiley,
2008. 672~p.
\bibitem{2-kri}
\Au{Heritier S., Cantoni E., Copt~S., Victoria-Feser~M.-P.} Robust methods in
biostatistics.~--- Chichester, U.K.: Wiley, 2009. 294~p.
\bibitem{3-kri}
\Au{Sprent P., Smeeton N.\,C.} Applied nonparametric statistical methods.~---  3rd
ed.~--- London, U.K.: Chapman\,\&\,Hall/CRC, 2001. 470~p.
\bibitem{4-kri}
\Au{Болдин М.\,В., Симонова Г.\,И., Тюрин~Ю.\,Н.} Знаковый статистический
анализ линейных моделей.~--- М.: Наука, 1997. 288~с.
\bibitem{5-kri}
\Au{Stone M.} Cross-validatory choice and assessment of statistical predictions (with
discussion)~// J.~Roy. Statist. Soc. Ser. B, 1974. Vol.~36. No.\,2. P.~111--147.
\bibitem{6-kri}
\Au{Efron B.} Bootstrap methods: Another look at the jackknife~// Ann. Statist.,
1979. Vol.~7. No.\,1. P.~1--26.
%\bibitem{7-kri}
%\Au{Сивков А.\,В., Кешишев Н.\,Г., Кривенко~М.\,П., Ковченко~Г.\,А.,
%Никонова~Л.\,М.} Согласование результатов определения уровня
%хромогранина~А различными тест-сис\-те\-ма\-ми: DAKO и Euro-Diagnostica
%(не опуб\-ли\-ковано).
\bibitem{8-kri}
\Au{Maronna R.\,A., Martin D.\,R., Yohai~V.\,J.} Robust statistics: Theory and
methods.~--- Chichester, U.K.: Wiley, 2006. 436~p.

\end{thebibliography}
} }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в редакцию 14.07.14}}

%\newpage


\vspace*{12pt}

\hrule

\vspace*{2pt}

\hrule


\def\tit{COMPARATIVE ANALYSIS OF~REGRESSION ANALYSIS PROCEDURES}

\def\titkol{Comparative analysis of~regression analysis procedures}

\def\aut{M.\,P.~Krivenko}
\def\autkol{M.\,P.~Krivenko}


\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-9pt}

\noindent
Institute of Informatics Problems, Russian Academy of Sciences,
44-2 Vavilov Str., Moscow 119333, Russian\\
Federation



\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND APPLICATIONS\ \ \ 2014\ \ \ volume~8\ \ \ issue\ 2}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND APPLICATIONS\ \ \ 2014\ \ \ volume~8\ \ \ issue\ 3
\hfill \textbf{\thepage}}}

\vspace*{6pt}

\Abste{The article considers the problem of forecasting the
values of one variable from the values of another variable using regression
analysis techniques. The list of compared regression analysis procedures
included the following parameter estimation methods: ordinary least squares,
least squares based on the sign statistics, and
zero correlation based on rank
statistics. To implement the method of least squares on the basis of sign
statistics, it is necessary to construct efficient data processing procedures.
An efficient variant of the corresponding algorithm was realized for
a difficult situation, when the goal function is piecewise constant.
The comparative analysis of regression analysis procedures in real conditions,
when  data are not normally distributed, showed that nonparametric techniques
are advantageous. In this case, taking into account algorithmic aspects,
the preference should be given to procedures based on rank, not on signs.
Advantages of nonparametric methods can improve the accuracy of measuring chromogranin A, widely used as an immunohistochemical
marker of neuroendocrine differentiation.
}

\KWE{regression analysis; rank and sign-based procedures;
prediction quality; adjustment of measurement results}


\DOI{10.14357/19922264140308}

%\Ack
%\noindent

\vspace*{9pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}



{\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{References}
\begin{thebibliography}{9}

\bibitem{1-k1-1}
\Aue{Ryan, T.\,P.} 2008. \textit{Modern regression methods}. 2nd ed.
Hoboken, NJ: Wiley. 672~p.
\bibitem{2-k1-1}
\Aue{Heritier, S., E. Cantoni, S.~Copt, and M.-P.~Victoria-Feser}.
2009. \textit{Robust methods in biostatistics}. Chichester, U.K.: Wiley. 294~p.
\bibitem{3-k1-1}
\Aue{Sprent, P., and N.\,C.~Smeeton}. 2001. \textit{Applied
nonparametric statistical methods}. 3rd ed. London, U.K.: Chapman \& Hall/CRC. 470~p.
\bibitem{4-k1-1}
\Aue{Boldin, M.\,V., G.\,I.~Simonova, and Yu.\,N.~Tyurin}. 1997.
\textit{Sign-based methods in linear models}. Providence: AMS. 236~p.
\bibitem{5-k1-1}
\Aue{Stone, M.}  1974.
Cross-validatory choice and assessment of statistical predictions
(with discussion). \textit{J.~Roy. Statist. Soc. ser. B.} 36(2):111--147.
\bibitem{6-k1-1}
\Aue{Efron, B.} 1979. Bootstrap methods: Another look at the jackknife.
\textit{Ann. Statist.} 7(1):1--26.
%\bibitem{7-k1-1}
%\Aue{Sivkov, A.\,V., N.\,G.~Keshishev, M.\,P.~Krivenko, G.\,A.~Kovchenko,
%and L.\,M.~Nikonova}. 2014. Soglasovanie rezul'tatov opredeleniya urovnya
%khromogranina~A razlichnymi test-sistemami [Concordance of the results
%of determining the level chromogranin~A with various test systems].
%\textit{DAKO and Euro-Diagnostica} [In Russian, unpubl.]
\bibitem{8-k1-1}
\Aue{Maronna, R.\,A., D.\,R.~Martin, and V.\,J.~Yohai}.
2006. \textit{Robust statistics: Theory and methods}. Chichester, U.K.: Wiley. 436~p.


\end{thebibliography}
} }


\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Received July 14, 2014}}

\vspace*{-12pt}

\Contrl

\noindent
\textbf{Krivenko Michail P.} (b.\ 1946)~---
Doctor of Science in technology, principal scientist,
Institute of Informatics Problems, Russian Academy of Sciences,
44-2 Vavilov Str., Moscow 119333, Russian Federation;
mkrivenko@ipiran.ru

 \label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература}
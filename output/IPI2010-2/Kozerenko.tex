\def\stat{kozerenko}

\def\tit{ЛИНГВИСТИЧЕСКИЕ ФИЛЬТРЫ В~СТАТИСТИЧЕСКИХ 
МОДЕЛЯХ МАШИННОГО ПЕРЕВОДА}

\def\titkol{Лингвистические фильтры в~статистических 
моделях машинного перевода}

\def\autkol{Е.\,Б.~Козеренко}
\def\aut{Е.\,Б.~Козеренко$^1$}

\titel{\tit}{\aut}{\autkol}{\titkol}

%{\renewcommand{\thefootnote}{\fnsymbol{footnote}}\footnotetext[1]
%{Исследования выполнены при частичной поддержке РФФИ, гранты 08-01-00567, 08-01-91205, 09-01-12180.}}

\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Институт проблем информатики Российской академии наук, kozerenko@mail.ru}


\Abst{Рассмотрены задачи создания лингвистических фильтров в 
статистических моделях машинного перевода и вопросы совершенствования 
механизмов выравнивания параллельных текстов для повышения точности и 
адекватности переводов. Приведены статистические и эвристические модели 
выравнивания и перевода. Предложены решения на основе гибридной 
грамматики, включающей лингвистические правила и вероятностные 
характеристики языковых структур.}

\KW{статистические модели; машинный перевод; параллельные тексты; 
выравнивание; лингвистические фильтры}

       \vskip 24pt plus 9pt minus 6pt

      \thispagestyle{headings}

      \begin{multicols}{2}
      
            \label{st\stat}

\section{Введение}
     
     В машинном переводе, основанном на статистических методах, или 
статистическом машинном переводе (СМП), задача перевода с одного 
естественного языка на другой рассматривается как задача машинного 
обучения. Это означает, что через обучение на очень большом числе 
образцов переводов, выполненных людьми-переводчиками, алгоритмы СМП 
осваивают правила перевода автоматически. Машинный перевод на основе 
статистики был впервые предложен в~[1, 2]. Применение статистических 
моделей чрезвычайно продвинуло развитие машинного перевода за 
последние два десятилетия, однако в дальнейшем появляются новые 
идеи и методы, направленные на создание систем, эффективно сочетающих 
разные модели. 
     
     В последние годы все более отчетливой становится тенденция к 
выработке новых подходов к построению моделей машинного перевода, в 
которых учтены сильные стороны как статистических моделей, так и 
подходов на основе лингвистических правил. Встречное движение к 
созданию гибридных моделей обработки естественного языка идет как со 
стороны статистических подходов к машинному переводу, когда в чисто 
статистические модели встраиваются элементы грамматик, так и со стороны 
традиционных методов машинного перевода, в которых правила анализа и 
перевода дополняются статистическими данными. Эти данные учитываются 
при принятии решений <<машиной перевода>>, особенно для разрешения 
неоднозначности языковых объектов.
     
     Истоки стохастической исследовательской парадигмы применительно к 
задачам обработки естественного языка находятся в проектах разработки 
алгоритмов распознавания речи, символов, ис\-прав\-ле\-ния орфографии. 
Основным методом решения многих задач, в частности определения и 
разметки частей речи, вероятностного грамматического разбора, является 
правило Байеса. В архитектуре стохастических систем в основном 
используется алгоритм динамического программирования.
     
     Машинное обучение в значительной степени основано на 
стохастической исследовательской парадигме. Алгоритмы обучения могут 
быть двух типов: неуправляемые и управляемые. Неуправляемый алгоритм 
должен вывести модель, пригодную для обобщения новых данных, которые 
ему ранее не предъявлялись, и этот вывод должен быть основан только на 
данных. Управляемый же алгоритм обуча\-ет\-ся на множестве правильных 
ответов на данные из обучающей выборки таким образом, что выведенная 
модель дает более точные решения. Целью машинного обучения является 
автоматический вывод модели для некоторой области на основе данных из 
этой области. Таким образом, система, обучаемая, например, синтаксическим 
правилам, должна быть обеспечена базовым набором правил фразовых 
структур. В~последнее время исследователи стали уделять больше внимания 
построению $N$-граммов, отражающих сложности синтаксических и 
семантических структур~[3, 4], применению $N$-граммов переменной 
длины~[5], включению семантической информации в $N$-граммы. 
В~работе~[6] дается детальное описание подхода к созданию 
статистического машинного перевода, основанного на $N$-граммах 
двуязычных единиц, называемых <<кортежами>>, а также четырех 
специальных атрибутных функций. 
\pagebreak
     
     Статистические модели перевода строятся на основе данных, 
получаемых из корпусов параллельных текстов на разных языках. Обычно 
сравнение текстов производится для языковых пар. Текст на языке, с 
которого необходимо осуществить перевод, называют исходным, а текст, 
который является его переводом, называют целевым. Соответственно говорят 
об исходном языке и целевом языке (или языке перевода). 
     
     Основным способом получения данных о со\-ответствии между 
исходным и целевым текстами и языками служит процедура выравнивания 
текстов. Результат этой процедуры также называется выравниванием и 
обозначается через~$A$\linebreak (alignment~--- выравнивание). Вероятностные 
характеристики выравниваний используются для создания алгоритмов 
перевода в статистических моде\-лях машинного перевода. Таким образом, 
вы\-рав\-ни\-ва\-ния и распределения вероятностей являются ключевыми 
понятиями при описании этих моделей.
{\looseness=-1

}
     
     В данной статье используются следующие нотации: символ~$P$ 
используется для обозначения распределений вероятности в самом общем 
смысле, а символ~$p$ используется для обозначения распределений 
вероятности на основе некоторой особой\linebreak модели. Основное внимание 
уделяется описанию различных методов выравнивания параллельных текстов 
на разных языках, поскольку результаты выравнивания определяют точность 
и адекватность перевода. При этом все более возрастает роль 
\textit{линг\-ви\-сти\-ческих фильтров}, которые вводятся в виде структур данных 
и правил в статистические модели перевода. 
     
     Рассматриваемые модели иллюстрируются исходя из двуязычной 
ситуации: в фокусе внимания находится языковая пара 
     \textit{русский}--\textit{английский}. Однако сходные методы 
применимы и при рассмотрении выравниваний и переводов с русского языка 
на французский, немецкий и другие европейские языки. 

\section{Методы выравнивания параллельных текстов}
  
     Статистические подходы к выравниванию параллельных текстов 
направлены на то, чтобы найти наиболее вероятный вариант 
выравнивания~$A$ для двух заданных параллельных текстов~$S$ и~$T$:
     \begin{equation*}
     \underset{A}{\mathrm{arg}\,\max}\,P\left(A\vert S,T\right) 
=\underset{A}{\mathrm{arg}\,\max}\,P \left( A,S,T\right)\,.
%     \label{e1koz}
     \end{equation*}
     
     Для того чтобы оценить значения вероятностей, указанных в этом 
выражении, чаще всего применяются методы, которые представляют 
параллельные тексты в виде последовательности выравниваемых цепочек 
предложений $(B _1,\ldots ,B_K)$. При этом предполагается, что 
вероятность одной цепочки не зависит от вероятностей других цепочек, а 
зависит только от предложений в данной цепочке~[7]. Тогда

\noindent
     \begin{equation*}
     P(A,S,T)\approx \prod\limits_{k=1}^K P(B_k)\,.
%     \label{e2koz}
     \end{equation*}
     
     Этот метод просто учитывает длину предложения на исходном языке и 
на языке перевода, измеренную в символах. Предполагается, что более 
длинное предложение одного языка будет соответствовать более длинному 
предложению другого языка. Такой подход дает вполне устойчивые 
результаты для сходных языков и буквального перевода.
     
     Более тонкие механизмы сопоставления обеспечиваются методами 
лексического выравнивания. Так, в работе~[8] представлен метод 
выравнивания посредством создания модели последовательного пословного 
перевода. Наилучшим результатом выравнивания будет тот, который 
максимизирует вероятность порождения корпуса при заданной модели 
перевода. Для выравнивания двух текстов~$S$ и~$T$ необходимо разбить их 
и представить в виде последовательности цепочек предложений. Цепочка 
содержит ноль или более предложений на каждом из языков, а 
последовательность цепочек покрывает весь корпус

\vspace*{4pt}

\noindent
     \begin{equation*}
     B_k=\left( S_{a_k},\ldots ,S_{b_k};\,t_{c_k},\ldots ,t_{d_k}\right)\,.
%     \label{e3koz}
     \end{equation*}
     
     Затем наиболее вероятное выравнивание $A=$\linebreak $=B_1,\ldots , B_{m_A}$ 
данного корпуса определяется следующим выражением (при этом цепочки 
предложений не зависят друг от друга):

\vspace*{4pt}

\noindent
     \begin{equation*}
     \underset{A}{\mathrm{arg}\,\max}\,P\left(S,T,A\right ) = 
\underset{A}{\mathrm{arg}\,\max}\,P\left(L\right)\prod\limits_{k=1}^{m_A} 
P(B_k)\,,
%     \label{e4koz}
     \end{equation*}
где $P(L)$ означает вероятность того, что порождается выравнивание~$L$ 
цепочек. Модель перевода, используемая при этом подходе, предельно 
упрощена и не учитывает фактор порядка слов в предложении и возможность 
того, что слову в исходном тексте может соответствовать более чем одно 
слово в тексте перевода. В~этой модели используются цепочки слов, при 
этом они ограничены соответствиями 1:1, 0:1 и~1:0. Суть модели 
заключается в том, что если некоторое слово обычно переводится словом 
другого языка, то вероятность соответствия цепочек слов~1:1 будет 
высокой~--- значительно выше, чем произведение вероятностей соответствий 
1:0 и~0:1 цепочек слов, использующих это рассматриваемое\linebreak\vspace*{-12pt}
\pagebreak

\noindent
 слово. При этом 
программа выбирает наиболее вероятный вариант выравнивания.
     
     Модель перевода, основанная на пословном выравнивании (например, 
русского и английского параллельных текстов) будет выглядеть следующим 
образом:
     \begin{equation*}
     P\left(r\vert e\right) =\fr{1}{Z}\sum\limits_{a_1=0}^l \cdots 
\sum\limits_{a_m=0}^l \prod\limits_{j=1}^m P\left(r_j\vert e_{a_j}\right)\,,
%     \label{e5koz}
     \end{equation*}
где $e$~--- предложение на английском языке; $l$~--- длина~$e$, 
выраженная в словах; $r$~--- предложение на русском языке; $m$~--- 
длина~$r$; $r_j$~--- $j$-е слово в~$r$; $a_j$~--- позиция в~$e$, с которой 
выравнивается~$r_j$; $P(w_r\vert w_e)$~--- вероятность перевода, т.\,е.\ 
вероятность того, что~$w_r$ окажется в предложении на русском языке, если 
соответствующее~$w_e$ встречается в английском предложении; $Z$~--- 
константа нормализации.
     
     Для конкретного выравнивания перемножаются $m$ вероятностей 
переводов, при этом отдельные переводы не зависят один от другого. Так, 
если необходимо вычислить вероятность
     \begin{multline*}
     P(\mathrm{Колумб}\vert \mathrm{Columbus})\times P(\mathrm{открыл}\vert 
\mathrm{discovered})\times{}\\
{}\times P(\textrm{Америку}\vert \mathrm{America})
     \end{multline*}
для выравнивания $P(\mathrm{Колумб}\vert \mathrm{Columbus})$, 
$(\mathrm{открыл}\vert \mathrm{discovered})$, $(\textrm{Америку}\vert 
\mathrm{America})$ следует 
перемножить вероятности этих трех переводных соответствий. При этом для 
каждого выравнивания делаются два упрощающих допущения: каждое 
русское слово порождается ровно одним английским словом (которое может 
быть нулевым, т.\,е.\ отсутствовать) и порождение каждого русского слова не 
зависит от других порождаемых слов в русском предложении.
     
     Однако выше описанный подход, основанный на пословном 
сопоставлении и никак не учитывающий связи между словами и фразами, не 
дает оптимальных результатов при выравнивании рус\-ско- и 
англоязычных текстов, поскольку между этими языками имеются 
определенные структурные различия и при переводе могут осуществляться 
значительные трансформации.
     
     Если рассматриваемые языки структурно отличаются, применяются 
методы, ориентированные на привлечение грамматических знаний, например 
используются методы выравнивания по словам, относящимся к значимым 
частям речи~[9]. При этом служебные слова не учитываются. Для 
использования этих методов необходимо произвести разметку параллельных 
текстов по частям речи.
      
      Методы выравнивания параллельных текстов для создания 
статистических моделей перевода, как правило, разрабатывались на основе 
сопоставления слов: каждому слову в цепочке на исходном языке 
необходимо было найти соответствующее слово в цепочке на целевом языке 
(т.\,е.\ языке перевода) и в обратном порядке. Однако довольно часто бывает 
трудно определить, какие слова в целевой цепочке и цепочке исходной 
соответствуют друг другу. Особые проблемы возникают при попытках 
выравнивания слов внутри идиом, в случае переводческих трансформаций, 
при вольном переводе и при опущении служебных слов. 
      
      Выравнивание двух цепочек слов может происходить весьма сложным 
образом. Очень часто приходится учитывать различные перестановки слов, 
опущения, вставки и межуровневые выравнивания <<слово--фраза>>, когда 
одному слову в исходном тексте соответствует целая фраза в целевом тексте, 
или наоборот. Самое общее определение пословного выравнивания 
приводится в~[10]. Пусть даны две цепочки слов: одна на исходном языке 
(например, русском~---$r$) $r_1^J =r_1, \ldots , r_j,\ldots , r_J$, а другая~--- на 
целевом языке (английском~--- $e$) $e_1^I=e_1,\ldots , e_i,\ldots ,e_I$, и для 
этих цепочек необходимо установить выравнивание. Выравнивание между 
двумя цепочками слов~--- это подмножество декартова произведения 
позиций слов, т.\,е.\ выравнивание~$A$ определяется как
      \begin{equation*}
      A\subseteq \left \{\left(j,i\right):\ j=1,\ldots ,J;\ i=1,\ldots ,I\right \}\,.
%      \label{e6koz}
      \end{equation*}
      
      В машинном переводе, основанном на статистических методах, 
делается попытка построения модели вероятности перевода $P(r_1^J\vert 
e_1^I)$, которая описывает соотношение между некоторой цепочкой~$r_1^J$ 
на исходном языке и цепочкой~$e_1^I$ на целевом языке. В~статистических 
моделях выравнивания текстов $P(r_1^J,\,a_1^J\vert e_1^I)$ вводится 
<<скрытое>> выравнивание~$a_1^J$, которое описывает отображение из 
исходной позиции~$j$ в целевую позицию~$a_j$. Соотношение между 
моделью перевода и моделью выравнивания задается следующим образом:
      \begin{equation*}
      P\left(r_1^J\vert e_1^I\right)=\sum\limits_{a_1^J} P\left( 
r_1^J,\,a_1^J\vert e_1^I\right)\,.
%      \label{e7koz}
      \end{equation*}
      
      Выравнивание~$a_1^J$ может содержать выравнивания $a_j=0$ с 
пустым словом~$e_0$ для тех слов исходного языка, которые не были 
выравнены ни с каким словом целевого языка.
      
      В целом статистическая модель зависит от множества неизвестных 
параметров~$\theta$, которые извлекаются из обучающего набора данных в 
процессе обучения. Для того чтобы выразить зависимость модели от 
множества параметров, используется следующее представление:
      \begin{equation*}
      P\left( r_1^J,\,a_1^J\vert e_1^I\right) =p_\theta\left( r_1^J,\,a_1^J\vert 
e_1^I\right)\,.
%      \label{e8koz}
      \end{equation*}
      
      Искусство статистического моделирования состоит в том, чтобы 
разработать специфические статистические модели, которые бы отражали 
наиболее важные свойства рассматриваемой проблемной области. Так, 
статистическая модель выравнивания должна адекватно описывать 
соотношение между цепочкой на исходном и целевом языках.
      
      Для выявления неизвестных параметров~$\theta$  задается 
обучающий корпус параллельных текстов, содержащий $S$~пар 
предложений $\{(r_s,e_s):\ s=$\linebreak $=1,\ldots , S\}$. Для каждой пары $(r_s, e_s)$ 
переменная выравнивания обозначается как $a=a_1^J$. Неизвестные 
параметры определяются путем максимизации сходства параллельных 
текстов в корпусе:
      \begin{equation*}
      \hat{\theta} =\underset{\theta}{\mathrm{arg}\,\max}\prod\limits_{s=1}^S 
\sum\limits_{a} p_\theta\left(r_s,\,a\vert e_s\right)\,.
%      \label{e9koz}
      \end{equation*}
      
      Как правило, для подобных моделей максимизация осуществляется на 
основе алгоритма максимизации ожидания~[11] или ему подобных. Такой 
алгоритм полезен для решения задачи оценки параметров, но не является 
абсолютно необходимым для статистического подхода.
      
      Итак, несмотря на то что для некоторой заданной пары предложений 
существует большое число выравниваний, всегда можно найти наилучшее 
выравнивание
      \begin{equation*}
      \hat{a}_1^J 
=\underset{a_1^J}{\mathrm{arg}\,\max}\,p_{\hat{\theta}}\left( r_1^J,\,a_1^J\vert 
e_1^I\right)\,.
%      \label{e10koz}
      \end{equation*}
      
      Выравнивание $\hat{a}_1^J$ также называется выравниванием 
\textit{Витерби} для пары предложений~$(r_1^J,e_1^I)$. Оценка качества 
выравнивания Витерби осуществляется путем сравнения с некоторым 
эталонным выравниванием, проведенным вручную. Параметры 
статистических моделей выравнивания оптимизируются с учетом критерия 
максимального правдоподобия, который далеко не всегда отражает качество 
выравнивания.
      
\section{Сопоставление статистических и эвристических моделей 
выравнивания}
      
      Наиболее известными статистическими моделями выравнивания 
параллельных текстов являются модели, приведенные в~\cite{1koz, 2koz, 
10koz}, а также скрытая марковская модель выравнивания, описанная в~[12]. 
Каждая из этих моделей предлагает свое, отличное от других, разложение 
вероятности $P(r_1^J,\,a_1^J\vert e_1^I)$.
      
      Значительно более простые методы нахождения выравниваний по 
словам (получившие название \textit{эвристических}) используют функцию 
сходства между типами двух языков~[13--15]. В~качестве 
такой функции сходства используются варианты коэффициента 
Дайса~\cite{16koz}. Для каждой пары предложений формируется матрица, в 
которой пред\-став\-ле\-ны меры ассоциаций между каждым словом в каждой 
позиции:
      \begin{equation*}
      \mathrm{dice}(i,j) = \fr{2C(e_i,r_j)}{C(e_i) C(r_j)}\,,
%      \label{e11koz}
      \end{equation*}
где $C(e,r)$ обозначает частоту совместной встречаемости~$e$ и~$r$ в 
обучающем корпусе параллельных текстов. Соответственно $C(e)$ означает 
частоту появления~$e$ в предложениях на целевом языке, а\linebreak $C(r)$~--- 
частоту~$r$ в предложениях на исходном языке. Выравнивание слов из такой 
матрицы мер ассоциаций получают посредством применения подходящего 
эвристического метода. Один из таких\linebreak методов заключается в том, чтобы 
выбрать в качестве выравнивания $a_j=i$ для позиции $j$ такое слово, у 
которого мера ассоциации является наибольшей:
\begin{equation*}
a_j=\mathrm{arg}\,\max\left \{\mathrm{dice}(i,j)\right \}\,.
%\label{e12koz}
\end{equation*}
     
     Развитие этого метода дается в~\cite{15koz}. Суть его состоит в 
итерационном нахождении выравниваний с наибольшей мерой 
ассоциативности на каждом шаге.
     
     Основное достоинство эвристических моделей~--- их простота, 
поэтому они широко применяются для пословного выравнивания и 
результаты подробно описаны в литературе. Однако то обстоятельство, что в 
эвристических моделях функция сходства задается весьма произвольно, 
делает их менее состоятельными, чем статистические модели.
     
     Наиболее распространенной статистической моделью, которая 
применяется для выравнивания параллельных текстов, является скрытая 
марковская модель. Модель выравнивания $P(r_1^J,\,a_1^J\vert e_1^I)$ 
можно структурировать без потери общности следующим образом:
     \begin{multline*}
     P\left( r_1^J,\,a_1^J\vert e_1^I\right) ={}\\
     {}=P\left(J\vert 
e_1^I\right)\prod\limits_{j=1}^J P\left(r_j,\,a_j\vert r_1^{j-1},\,a_1^{j-
1},\,e_1^I\right)={}\\
     {}=P\left(J\vert e_1^I\right)\prod\limits_{j=1}^J P\left(a_j\vert r_1^{j-
1},\,a_1^{j-1},\,e_1^I\right) \times{}\\
{}\times P\left( r_j\vert r_1^{j-1},\,a_1^j,\,e_1^I\right)\,.
%     \label{e13koz}
     \end{multline*}
     
     При использовании этого разложения получаются три различные 
вероятности:  длины~$P(J\vert e_1^I)$,  
выравнивания $P(a_j\vert r_1^{j-1},\,a_1^{j-1},\,e_1^I)$ и  
лексикона $P(r_j\vert r_1^{j-1},\,a_1^j,\,e_1^I)$. В~скрытой марковской 
модели выравнивания предполагается зависимость первого порядка для 
выравниваний~$a_j$, а также то, что вероятность лексикона зависит только 
от слова в позиции~$a_j$:
     \begin{align*}
     P\left(a_j\vert r_1^{j-1},\,a_1^{j-1},\,e_1^I\right) &=p\left(a_j\vert a_{j-
1},\,I\right)\,;\\[6pt]
     P\left(r_j\vert r_1^{j-1},\,a_1^{j},\,e_1^I\right) &=p\left(r_j\vert 
e_{a_j},\right)\,.
     \end{align*}
     
     Если принять простую модель длины $P(J\vert e_1^I)=$\linebreak $=p(J\vert I)$, то для 
$p(r_1^J\vert e_1^I)$ получается следующее разложение на основе скрытой 
марковской модели:
     \begin{equation*}
     p\left( r_1^J\vert e_1^I\right) =p\left(J\vert 
I\right)\sum\limits_{a_1^J}\prod\limits_{j=1}^J \left [p\left( a_{j-1},\,I\right)
p\left(r_j\vert e_{a_j}\right)\right]
%     \label{e16koz}
     \end{equation*}
с вероятностью выравнивания $p(i\vert i^\prime,\,I)$ и вероятностью перевода 
$p(r\vert e)$. Для того чтобы сделать параметры выравнивания независимыми 
от абсолютных значений позиций слов, принимается, что вероятности 
выравниваний $p(i\vert i^\prime,\,I)$ зависят только от ширины шага $(i-
i^\prime)$. Используя множество не\-от\-ри\-ца\-тель\-ных параметров $\{c(i-
i^\prime)\}$, можно записать вероятности выравнивания в следующем виде:
\begin{equation*}
p\left(i\vert i^\prime,\,I\right) =\fr{c(i-
i^\prime)}{\sum\limits^I_{i^{\prime\prime}=1} c(i^{\prime\prime}-i^\prime)}\,.
%\label{e17koz}
\end{equation*}
Этот вид обеспечивает то, что вероятности выравнивания удовлетворяют 
ограничению нормализации для каждой обусловливающей позиции 
слова~$i^\prime$, $i^\prime=1,\ldots ,I$. Эта модель называется также 
однородной скрытой марковской моделью~\cite{12koz}. Подобная идея была 
предложена в работе~\cite{17koz}.
     
     В исходной формулировке скрытой марковской модели выравнивания 
отсутствует пустое слово, которое порождает слово исходного текста, не 
име\-ющее прямого соответствия в виде слова в целевом тексте (т.\,е.\ у этого 
слова нет непосредственного пословного выравнивания). 

В~работе~\cite{18koz} вводится пустое слово и расширяется сеть скрытой 
марковской модели посредством $I$~пустых слов~$e^{2I}_{I+1}$. Целевое 
слово~$e_i$ имеет соответствующее пустое слово~$e_{i+1}$ (т.\,е.\ позиция 
пустого слова кодирует ранее пройденное целевое слово). На переходы в 
сети скрытой марковской модели $(i\leq I,\,i^\prime\leq I)$ накладываются 
следующие ограничения, в которых участвует пустое 
слово~$e_0$:

\noindent
     \begin{align*}
     p\left( i+I\vert i^\prime,\,I\right) & 
=p_0\delta\left(i,\,i^\prime\right)\,;\\
     p\left( i+I\vert i^\prime+I,\,I\right) & 
=p_0\delta\left(i,\,i^\prime\right)\,;\\
     p\left( i\vert i^\prime+I,\,I\right) & =p\left(i\vert 
i^\prime,\,I\right)\,,
     \end{align*}
где $\delta (i,i^\prime)$~--- функция Кронекера, которая равна 
единице, если $i=i^\prime$, и нулю~--- в противном случае.
Параметр $p_0$~--- это вероятность перехода к пустому слову, которая 
должна оптимизироваться на предоставляемых данных. В~экспериментах, 
описанных в~\cite{18koz}, устанавливается $p_0=0{,}2$.
     
     Скрытая марковская модель основана на зависимостях первого порядка 
$p(i=a_j\vert a_{j-1},\,I)$ для распределения выравнивания. Также довольно 
распространенными являются две модели, использующие зависимости 
нулевого порядка $p(i=$\linebreak $=a_j\vert j,\,I,\,J)$:
     \begin{enumerate}[(1)]
\item в модели~1 используется единообразное распределение $p(i\vert 
j,\,I,J)=1/(I+1)$:
\begin{equation*}
P\left( r_1^J,\,a_1^J\vert e_1^I\right) = \fr{p(J\vert 
I)}{I+1}\prod\limits_{j=1}^J p\left(r_j\vert e_{a_j}\right)\,,
%\label{e21koz}
\end{equation*}
из чего следует, что порядок слов не влияет на вероятность выравнивания;
\item в модели~2 распределение представлено следующим образом:
\begin{multline*}
P\left( r_1^J,\,a_1^J\vert e_1^I\right) ={}\\
{}=p(J\vert I)\prod\limits_{j=1}^J\left[ 
p\left(a_j\vert j,\,I,\,J\right) p \left(r_j\vert e_{a_j}\right)\right]\,.
%\label{e22koz}
\end{multline*}
      
      Для того чтобы сократить число параметров выравнивания, в модели 
не учитывают зависимость от~$J$ и используют распределение $p(a_j\vert 
j,\,I)$ вместо $p(a_j\vert j,\,I,\,J)$.
      \end{enumerate}

\section{Методы перевода на основе выравниваний по фразам}
      
      Модель перевода по фразам, или модель перевода на основе шаблонов 
выравнивания~\cite{19koz}, и другие сходные модели очень сильно 
продвинули развитие технологии машинного перевода за счет расширения 
базовых единиц перевода от слов к фразам, т.\,е.\ подстрокам произвольного 
размера. Однако далеко не всегда фразы этой модели СМП являются 
фразами в смысле какой-либо существующей синтаксической теории или 
формальной грамматики, например, фразой может считаться 
``\textit{alignments the}'' и~т.\,п. Однако переход к уровню фраз при 
формировании данных для СМП позволил представить в модели локальные 
перестановки, переводы многословных выражений, вставки и опущения, 
которые определяются локальным контекстом. Все эти возможности делают 
модель, основанную на фразах, простым и очень мощным инструментом 
перевода. Базовая модель фразового перевода является частным случаем 
модели канала с шумами~\cite{2koz}. Так, перевод предложения с русского 
языка на английский будет представлен следующим образом:
      \begin{multline*}
      \underset{e}{\mathrm{arg}\,\max}\,P\left(e\vert r\right) 
=\underset{e}{\mathrm{arg}\,\max}\,P\left(e,\, r\right)={}\\ 
{}=\underset{e}{\mathrm{arg}\,\max}\left ( P(e) P(r\vert e)\right)\,.
%      \label{e23koz}
      \end{multline*}
      
      Модель перевода по фразам~$P(r\vert e)$ <<кодирует>> $e$ в~$r$, 
выполняя следующие шаги:
      \begin{enumerate}[1)]
\item сегментирует $e$ на фразы $\overline{e}_1\ldots \overline{e}_I$, 
обычно единообразным распределением по всем сегментациям;
\item перегруппировывает $\overline{e}_i$ в соответствии с некоторой 
моделью искажения;
\item переводит каждую из $\overline{e}_i$ на русский язык в 
соответствии с моделью~$P(\overline{r}\vert\overline{e})$, которая 
оценивается на основании обучающих данных.
\end{enumerate}

      Другие модели, основанные на фразах, пред\-став\-ля\-ют совместное 
распределение~$P(e,r)$~\cite{20koz} или превращают~$P(e)$ и~$P(r\vert e)$ 
в атрибуты модели~\cite{21koz}. Но базовая архитектура сегментации (или 
по\-рож\-де\-ния), переупорядочивания фраз и фразового перевода остается такой 
же. Модели на основе фраз могут надежно выполнять переводы, которые 
локализованы в подцепочках и которые являются достаточно частотными, 
чтобы их можно было обнаружить в процессе обучения.
     
     На современном этапе развития исследований и разработок в области 
машинного перевода и обработки текстовых знаний все больше назревает\linebreak 
потребность в подходах, основанных на линг\-висти\-че\-ских знаниях. Это 
осознается и сторонниками\- статистических подходов. Например, в 
работе~\cite{22koz} описан двухступенчатый метод автоматического 
извлечения переводных шаблонов из параллельных текстов на английском и 
китайском языках. Этот метод основан на алгоритме индукции грамматики и 
алгоритме выравнивания с использованием скобочной трансдукционной 
грамматики. Однако сами авторы указывают, что, поскольку в данной 
модели не заложены предварительные синтаксические знания, 
грамматическая правильность результата не может быть гарантирована.
      
      Основные подходы к статистическому машинному переводу, в 
которых используется синтаксис, различаются по тем синтаксическим 
теориям, которых они придерживаются, и системам аннотирования, 
построенным в соответствии с той или иной теорией. 
      
\vspace*{6pt}
\section{Вероятностные методы грамматического разбора 
предложений}
\vspace*{3pt}
     
     Статистические методы обработки естественного языка расширяют 
схему основных суще\-ст\-ву\-ющих подходов к машинному переводу~--- 
прямого перевода, переноса (трансфера) и подхода на основе 
     языка-посредника (интерлингвы)~\cite{23koz, 24koz}. 
     
     Значения вероятностей для каждого возможного варианта 
грамматического разбора (т.\,е.\ развертывания нетерминального узла) 
вычисляются на основе частот встречаемости таких вариантов разбора в 
существующих текстовых корпусах с синтаксической разметкой (treebanks). 
Значения вероятностей для вариантов разбора могут быть также получены и 
в виде лингвистических экспертных оценок.
     
     Для любой системы обработки естественного языка необходимо 
проектирование модуля определения и разметки частей речи (тэггера). 
Стохастические тэггеры появились в 1980-е~гг. Их общая идея 
заключается в выборе наиболее вероятного тэга (т.\,е.\ частеречной метки) 
для данного слова. Чаще всего для вероятностных тэггеров используются 
марковские модели. К~примеру, для некоторого предложения или 
последовательности слов выбирается последовательность тэгов, которая 
максимизирует следующую формулу:
     \begin{equation*}
     P(\mathrm{слово}\vert \mathrm{тэг})\times P(\mathrm{тэг}\vert 
\mathrm{предыдущие}\ n\ \mathrm{тэгов})\,.
%     \label{e24koz}
     \end{equation*}
     
     Еще один подход к машинному обучению, основанный на правилах и 
стохастическом тэггировании (разметке частей речи), известен как обучение, 
основанное на трансформациях (Transformation-Based Learning, TBL~--- 
метод управляемого обуче\-ния с использованием заранее 
размеченного обуча\-юще\-го корпуса).
     
     Для вероятностного грамматического разбора применяются 
стохастические грамматики:
     \begin{itemize}
\item \textit{вероятностная контекстно-свободная грамматика} 
определяется в виде $G = (N, T, P, S, D)$, где $N$~--- множество 
нетерминальных символов; $T$~--- множество терминальных символов; 
$P$~--- множество продукций вида $A\rightarrow b$; $A$~--- это 
нетерминальный символ; $b$~--- цепочка символов; $S$~--- специальный 
исходный символ; $D$~--- функция, приписывающая значения ве\-ро\-ят\-ности 
каждому правилу из множества~$P$. 

Как получить необходимые данные для 
вероятностной контекстно-свободной грамматики? Один из путей~--- 
использование корпуса синтаксически размеченных предложений. Такой 
корпус называется банком синтаксических деревьев (treebank). Например, 
Penn Treebank~\cite{25koz} содержит деревья разбора для ряда текстовых 
корпусов (Brown Corpus, Switchboard corpus). Если задан банк деревьев 
разбора, то вероятность каждой развертки некоторого нетерминального узла 
может быть вычислена путем подсчета частоты ее встречаемости с 
по\-сле\-ду\-ющей нормализацией:
\begin{multline*}
P\left(\alpha\rightarrow\beta\vert\alpha\right) 
=\fr{\mathrm{Count}\left(\alpha\rightarrow\beta\right)}{\sum\limits_{\gamma} 
\mathrm{Count}\left(\alpha\rightarrow\gamma\right)}={}\\
{}=\fr{ \mathrm{Count}\left(\alpha\rightarrow\beta\right)}{\mathrm{Count}\left(\alpha\right)}\,;
%\label{e25koz}
\end{multline*}
\item \textit{вероятностная грамматика замещения деревьев} является 
обобщением вероятностной контекстно-свободной грамматики, при этом 
более мощной стохастически, поскольку она дает возможность приписывать 
значения ве\-ро\-ят\-ности фрагментам или даже целым схемам разбора.
\end{itemize}

     Рассмотрим, каким образом значения ве\-ро\-ят\-ности используются в 
процессе грамматического разбора. Например, вероятностная 
     контекст\-но-сво\-бод\-ная грамматика (PCFG~--- Probabilistic Context Free 
Grammar) и вероятностная грамматика замещения деревьев (PTSG~--- 
Probabilistic Tree Substitution Grammar) присваивают вероятность~$P$ 
каж\-дому дереву разбора~$T$ (т.\,е.\ каждому деривату) предложения~$S$. 
Эта информация является ключевой для разрешения неоднозначности 
синтаксических структур. Вероятность каждого возможного дерева 
разбора~$T$ определяется как произведение вероятностей всех правил~$r$, 
используемых для развертывания каждого узла~$n$ в дереве разбора
     \begin{equation}
     P(T,S) =\prod\limits_{n\in T} p(r(n))\,.
     \label{e26koz}
     \end{equation}
     
     Вероятность однозначного предложения (т.\,е.\ предложения, где не 
требуется разрешать неоднозначность) равна вероятности единственного 
дерева разбора для этого предложения, т.\,е.\linebreak $P(T,S) = P(T)$. Вероятность же 
неоднозначного предложения равна сумме вероятностей всех воз-\linebreak\vspace*{-12pt}
\columnbreak

\noindent
можных 
деревьев разбора~$\tau(S)$ данного предло\-жения
     \begin{equation*}
     P(S) =\sum\limits_{T\in\tau(S)} P(T,S)=\sum\limits_{T\in\tau(S)}P(T)\,.
%     \label{e27koz}
     \end{equation*}
%     \columnbreak
     
     Вероятность полного разбора предложения вычисляется с учетом 
категориальной информации для каждой головной вершины каждого узла. 
Пусть $n$~--- синтаксическая категория некоторого узла~$N$; $h(n)$~--- 
головная вершина узла~$n$; $m(n)$~--- материнский узел для~$n$. Тогда 
будем вычислять вероятность $p(r(n)\vert n, h(n))$, а для этого преобразуем 
выражение~(\ref{e26koz}) таким образом, чтобы каждое правило 
обусловливалось своей головной вершиной:
     \begin{equation*}
     P(T,S) =\prod\limits_{n\in T} p(r(n)\vert n,\,h(n))\times p(h(n)\vert 
n,\,h(m(n)))\,.
%     \label{e28koz}
     \end{equation*}
     
     Одна из центральных проблем повышения качества машинного 
перевода~--- это включение в модель таких языковых фактов, как 
\textit{перефразы}~\cite{26koz}, возможные отношения синонимии 
синтаксических структур, в частности глагольных фраз в активном и 
пассивном залоге, номинализация~\cite{27koz}. В~указанных работах 
предлагаются решения для отдельных видов структур, однако необходима 
возможно полная типизация синонимических языковых структур для 
выравнивания параллельных текстов и извлечения новых правил на основе 
методов машинного обучения.
     
\section{Лингвистические фильтры на~основе грамматики 
когнитивного трансфера}

     В сформулированной ранее \textit{когнитивной трансферной 
грамматике} (КТГ)~[28--30] %\cite{28koz}--\cite{30koz} 
функциональные значения 
языковых структур определяются категориальными значениями головных 
вершин.\linebreak Вероятностные характеристики вводятся в правила унификационной 
грамматики в виде весов, присваиваемых деревьям разбора. 
     
     В КТГ элементарными структурами являются 
\textit{трансфемы}~\cite{30koz}. \textit{Трансфема}~--- это едини\-ца 
когнитивного переноса, устанавливающая функ\-ци\-о\-наль\-но-семантическое 
соответствие между структурами исходного языка~$L_S$ и структурами 
целевого языка~$L_T$. Для выравнивания параллельных текстов трансфемы 
задаются как правила переписывания, в которых в левой части стоит 
нетерминальный символ, а в правой~--- выравненные пары цепочек 
терминальных и нетерминальных символов, принадлежащих исходному и 
целевому языкам:
\pagebreak

\noindent
     \begin{equation*}
     T\rightarrow \langle\rho,\,\alpha,\,\sim\rangle\,,
%     \label{e29koz}
     \end{equation*}
где $T$~--- нетерминальный символ; $\rho$ и~$\alpha$~--- цепочки 
терминальных и нетерминальных символов, принадлежащих русскому и 
английскому языкам, $\sim$~--- символ соответствия между 
нетерминальными символами, входящими в~$\rho$, и нетерминальными 
символами, входящими в~$\alpha$. При выравнивании параллельных текстов 
на основе когнитивной трансферной грамматики процесс деривации 
начинается с пары связанных исходных символов~$S_\rho$ и~$S_\alpha$, 
далее на каждом шаге связанные нетерминальные символы попарно 
переписываются с использованием двух компонентов единого правила. 
     
     Для автоматического извлечения правил из па\-раллельных текстов на 
основе когнитивной трансферной грамматики необходимо предварительно\linebreak 
выравнять тексты по предложениям и словам. Извле\-каемые правила 
опираются на пословные выравнивания таким образом, что вначале 
идентифицируются исходные фразовые пары с использованием такого же 
критерия, как и большинство статистических моделей перевода, основанных 
на фразовом подходе~\cite{19koz}: должно быть хотя бы одно 
слово внутри фразы на одном языке, выравненное с некоторым словом 
внутри фразы на другом языке, но никакое слово внутри фразы на одном 
языке не может быть выравнено ни с каким словом за пределами парной ей 
фразы на другом языке.
{ %\looseness=1

}
     
     \medskip
     \noindent
     \textbf{Определение 1.}
     \textit{Пусть дана пара предложений $\langle r,\,e,\,\sim\rangle$, 
выравненных по словам, пусть~$r_i^j$ обозначает подцепочку~$r$ от 
позиции~$i$ до позиции~$j$ включительно, а соответственно, 
$e_{i^\prime}^{j^\prime}$~--- подцепочку~$e$ от позиции~$i^\prime$ до 
позиции~$j^\prime$ включительно. Тогда правило $\langle 
r_i^j,\,e_{i^\prime}^{j^\prime},\,\sim\rangle$~--- это исходная фразовая пара, 
если и только если:}
     \begin{align*}
     1)\ & r_k\sim e_{k^\prime}\ \mbox{для некоторого}\ k\in [i,j]\ \mbox{и}\ 
k^\prime\in [i^\prime,j^\prime]\,;\\
     2)\ & r_k\not \sim e_{k^\prime}\ \mbox{для всех}\ k\in[i,j]\ \mbox{и}\ 
k^\prime \notin [i^\prime,j^\prime]\,;\\
     3)\ & r_k\not\sim e_{k^\prime}\ \mbox{для всех}\ k\notin [i,j]\ \mbox{и}\ 
k^\prime\in [i^\prime,j^\prime]\,.
     \end{align*}
     
     
     Для того чтобы продолжить извлечение правил из выделенных фраз, 
находим фразы, которые содержат другие фразы, и заменяем их 
нетерминальными символами. Таким образом осуществляется механизм 
вложенности правил, отображающий иерархическую структуру 
естественного языка.
     
     \medskip
     
     \noindent
     \textbf{Определение 2.}
     \textit{Множество правил $\langle r,\,e,\,\sim\rangle$~--- это наименьшее 
множество, удовлетворяющее сле\-ду\-ющим условиям:
     \begin{enumerate}[1.]
     \item Если $\left< r_i^j,\,e_{i^\prime}^{j^\prime}\right>$~--- это 
исходная пара фраз, то $X\rightarrow \left< r_i^j, 
e_{i^\prime}^{j^\prime}\right>$ является правилом из $\langle 
r,\,e,\,\sim\rangle$.
     \item  Если $X\rightarrow \langle\rho,\,\alpha\rangle$~--- это правило из 
$\langle r,\,e,\,\sim\rangle$, а $\left< 
r_i^j,\,e_{i^\prime}^{j^\prime},\,\sim\right>$~--- это исходная пара фраз такая, 
что $\rho =\rho_1r_i^j\rho_2$ и $\alpha=\alpha_1 
e_{i^\prime}^{j^\prime}\alpha_2$, то $X \rightarrow\left< \rho_1 
X_{\mbox{\scriptsize\fbox{\textit{k}}}}\rho_2,\,\alpha_1X_{\mbox{\scriptsize\fbox{\textit{k}}}}\alpha_2\right>$~--- это правило из 
$\langle r,\,e,\,\sim\rangle$ и $k$~--- это индекс, не используемый ни в~$\rho$, 
ни в~$\alpha$.
\end{enumerate}
     }
     
     Следующий шаг~--- формирование системы правил в нотации КТГ. 
Когнитивная трансферная грамматика~--- это унификационно-порождающая 
грамматика, имеющая иерархическую структуру и отражающая 
значительную часть языковых трансформаций, производимых при переводе с 
одного языка на другой. При этом на основе полученных авторами 
экспериментальных данных в правила КТГ включены веса различных 
вариантов их развертки. 
     
     \medskip
     
     \noindent
     \textbf{Определение 3.} \textit{Грамматикой когнитивного трансфера 
$G_{CT}$ будем называть множество
     \begin{multline*}
     G_{CT} ={}\\
     {}=\{T_{L_1},  T_{L_2}, N_{L_1}, N_{L_2}, P_{CA}, P_{CT}, 
S_{L_1}, S_{L_2}, M, D\}\,,\hspace*{-2.85pt}
%     \label{e34koz}
     \end{multline*}
где $T_{L_1}$ и $T_{L_2}$~--- множества терминальных символов 
языков~$L_1$ и~$L_2$; $N_{L_1}$ и $N_{L_2}$~--- множества 
нетерминальных символов языков~$L_1$ и~$L_2$; $P_{CA}$ и $P_{CT}$~--- 
правила анализа и синтеза на основе когнитивного трансфера; $S_{L_1}$ и
$S_{L_2}$~--- пара исходных символов языков~$L_1$ и~$L_2$, с которых 
начинается процесс анализа и выравнивания предложений; $M$~--- функция 
уста\-нов\-ле\-ния соответствия между языковыми структурами~$L_1$ и~$L_2$; 
$D$~--- функция, приписывающая значения вероятности каждому правилу из 
множеств $P_{CA}$ и~$P_{CT}$. 
}

\medskip
     
     Неоднозначность является коренным свойством естественного языка и 
вызывает основные\linebreak затруднения при создании систем машинного перево\-да. 
Неоднозначные и многозначные синтаксические структуры учитываются 
авторами в дальнейшем развитии КТГ~--- многовариантной когни\-тив\-ной 
трансферной грамматике (МКТГ)~--- и ее реализациях в виде структур 
данных, которые могут быть использованы в качестве лингвистических 
фильтров при построении статистических моделей перевода. Эти структуры 
данных назовем многовариантными когнитивными трансферными 
структурами (МКТС). 
     
     В общем виде синтаксис МКТС может быть представлен следующим 
образом:
\pagebreak

\end{multicols}

%\hrule

\noindent
     \begin{multline*}
\mbox{МКТС} 
\{\mbox{МКТС}\;\langle\mbox{идентификатор}\rangle\;\mbox{МКТС}\;\langle
\mbox{вес}\rangle\;\mbox{МКТС}\;\langle\mbox{метка}\rangle \}\rightarrow{}\\
     \langle\mbox{Входная фразовая структура}\;\&\;\mbox{набор 
атрибутов-значений}\rangle\rightarrow{}\\
     \langle\mbox{Схема трансфера, управляемого головной 
вершиной}\rangle\rightarrow{}\\
\langle\mbox{Генерируемая фразовая структура}\;\&\;\mbox{набор 
атрибутов-значений 1}\rangle\langle\mbox{вес1}\rangle\\
\langle\mbox{Генерируемая фразовая структура}\;\&\;\mbox{набор 
атрибутов-значений 2}\rangle\langle\mbox{вес2}\rangle\ldots\\ 
\ldots\langle\mbox{Генерируемая фразовая структура}\;\&\;\mbox{набор 
атрибутов-значений}\;N\rangle\langle\mbox{вес}N\rangle\,.
\end{multline*}

\hrule

\begin{multicols}{2}
     
     В новом варианте МКТГ отражено явление многозначности синтаксических 
структур и преду\-смот\-ре\-ны основные механизмы разрешения 
неоднозначности посредством включения в систему\linebreak
 правил разбора и 
перевода статистической информации о возможных контекстах языковых 
структур. Многовариантная когнитивная трансферная грамматика 
обеспечивает расширяемую платформу для разработки систем машинного 
перевода и извлечения знаний из текста. В~настоящее время основные 
правила когнитивного трансфера сформулированы также для 
     русско-французской и русско-немецкой языковых пар. На основе 
МКТГ формируется новый гибридный подход к построению моделей для 
систем машинного перевода и обработки знаний. Продолжают 
формироваться обучающие наборы данных для расширения и модификации 
правил. Для сокращения числа избыточных правил (которые неизбежно 
возникают на этапе обучения) формируются лингвистические фильтры на 
основе пространств когнитивного трансфера.
     
\section{Заключение}
     
     Актуальность проблемы создания новых гибридных методов 
представления языковых объектов обусловлена тем, что на современном 
этапе исследований встает задача оптимального сочетания сильных сторон 
двух исследовательских парадигм: логико-лингвистического моделирования, 
использующего правила, и стохастического подхода. Особое значение 
предлагаемая работа имеет для решения проблемы структурного анализа и 
компьютерного моделирования полнотекстовых научных, 
     финансово-экономических и патентных документов. 
     
     Необходимость моделирования языковых трансформаций для систем 
машинного перевода и извлечения знаний из текстов обусловлена тем, что до 
сих пор эти явления мало исследованы с точки зрения возможностей их 
компьютерной реализации и, соответственно, недостаточно учтены в 
действующих системах МП, а правила, задающие функциональную 
синонимию языковых конструкций, позволяют извлечь необходимую 
информацию из параллельных текстов и избежать формирования 
избыточных правил и <<шумов>>.
     
     При формировании процедур снятия неоднозначности были 
использованы статистические данные, полученные при изучении 
параллельных\linebreak текстов научных и патентных документов собственного 
экспериментального корпуса. Так, при ранжировании предпочтительности 
вариантов трансфера использовалась статистика соотношения личных и 
неличных форм глаголов и оборотов с ними, приоритетности употребления 
активных или пассивных конструкций, номинализаций и вербализаций в 
русском, английском, французском и немецком дискурсах.
     
     Дальнейшие исследования будут связаны с расширением числа типов 
трансформаций в линг\-ви\-сти\-че\-ских представлениях для многоязычной 
ситуации, созданием инженерно-лингвистической\linebreak
 среды исследований и 
разработок в области машинного перевода и извлечения знаний из текстов на 
разных языках. Как составная часть инженерно-лингвистической среды 
разрабатывается многоязычная лингвистическая база знаний, основанная на 
сочетании методов функционально-се\-ман\-ти\-че\-ского анализа фразовых 
структур и статистических моделях языков, включенных в базу.
     
     Полученные результаты используются также для выработки 
инновационных подходов к преподаванию курсов перевода и 
переводоведения, компьютерной лингвистики и когнитивных технологий.


{\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{99}      
      
      \bibitem{1koz}
    \Au{Brown P.\,F., Cocke~J., Della Pietra S.\,A., Della Pietra V.\,J., 
Jelinek~F., Lafferty~J.\,D., Mercer~R.\,L., Roossin~P.\,S.}
    A~statistical approach to machine translation~// Comput. Linguistics, 
1990. Vol.~16. P.~79--85.
    
    \bibitem{2koz}
    \Au{Brown P.\,F., Della Pietra S.\,A., Della Pietra V.\,J., Mercer~R.\,L.}
    The mathematics of statistical machine translation: Parameter estimation~// 
Comput. Linguistics, 1993. Vol.~19. No.\,2. P.~263--311.
    
    \bibitem{3koz}
    \Au{Rosenfeld R.}
    A maximum entropy approach to adaptive statistical language modeling~// 
Computer Speech Language, 1996. Vol.~10. P.~187--228. 
    
    \bibitem{4koz}
    \Au{Niesler T.\,R., Woodland~P.\,C.}
    Modelling word-pair relations in a category-based language model~// IEEE 
ICASSP-99, 1999. P.~795--798.
    
    \bibitem{5koz}
    \Au{Ney H., Essen U., Kneser~R.}
    On structuring probabilistic dependencies in stochastic language modeling~// 
Computer Speech Language, 1994. Vol.~8. P.~1--38.
    
    \bibitem{6koz}
    \Au{Marino J.\,B., Banchs R.\,E., Crego~J.\,M., de~Gispert~A., Lambert~P., 
Fonollosa~J.\,A. R., Costa-Jussa~M.\,R.}
    N-gram-based Machine Translation~// Comput. Linguistics, 2006. 
Vol.~32. No.\,4. P.~527--549.
    
    \bibitem{7koz}
    \Au{Gale W.\,A., Church K.\,W.}
    A program for aligning sentences in bilingual corpora~// Comput.
Linguistics, 1993. Vol.~19. P.~75--102.
    
    \bibitem{8koz}
    \Au{Chen S.\,F.}
    Aligning sentences in bilingual corpora using lexical information~// 
31st Annual Conference of the Association for Computational 
Linguistics Proceedings, 1993. P.~9--16.
    
    \bibitem{9koz}
    \Au{Masahiko H., Yamazaki~T.}
    High-performance bilingual text alignment using statistical and dictionary 
information~// ACL~34, 1996. P.~131--138.
    
    \bibitem{10koz}
    \Au{Och F.\,J., Ney H.}
    A~comparison of alignment models for statistical machine translation~// 
COLING'00: The 18th Conference (International ) on Computational Linguistics. 
Saarbrucken, Germany, 2000. P.~1086--1090.
    
    \bibitem{11koz}
    \Au{Dempster A.\,P., Laird N.\,M., Rubin~D.\,B.}
    Maximum likelihood from incomplete data via the EM algorithm~// 
    J.~Roy. Statistical Soc. Ser.~B, 1977. Vol.~39. No.\,1. P.~1--22.
    
    \bibitem{12koz}
    \Au{Vogel S., Ney~H., Tillmann~Ch.}
    HMM-based word alignment in statistical translation~// COLING'96: The 
16th Conference (International ) on Computational Linguistics Proceedings. Copenhagen, 
Denmark, 1996. P.~836--841.
    
    \bibitem{13koz}
    \Au{Smadja F., McKeown~K.\,R., Hatzivassiloglou~V.}
    Translating collocations for bilingual lexicons: A statistical approach~// 
Comput. Linguistics, 1996. Vol.~22. No.\,1. P.~1--38.
    
    \bibitem{14koz}
    \Au{Ker S.\,J., Chang J.\,S.}
    A class-based approach to word alignment~// Comput. Linguistics, 
1997. Vol.~23. No.\,2. P.~313--343.
    
    \bibitem{15koz}
    \Au{Melamed I.\,D.}
    Models of translational equivalence among words~/ Comput. 
Linguistics, 2000. Vol.~26. No.\,2. P.~221--249.
    
    \bibitem{16koz}
    \Au{Dice L.\,R.}
    Measures of the amount of ecologic association between species~// 
    J.~Ecology, 1945. Vol.~26. P.~297--302.
    
    \bibitem{17koz}
    \Au{Dagan I., Church~K.\,W., Gale~W.\,A.}
    Robust bilingual word alignment for machine aided translation~//  
Workshop on Very Large Corpora Proceedings. Columbus, Ohio, 1993. P.~1--8.
    
    \bibitem{18koz}
    \Au{Och F.\,J., Ney H.}
    A~systematic comparison of various statistical alignment models~// 
Comput. Linguistics, 2003. Vol.~29. No.\,1. P.~19--51.
    
    \bibitem{19koz}
    \Au{Och F.\,J., Ney~H.}
    The alignment template approach to statistical machine translation~// 
Comput. Linguistics, 2004. Vol.~30. P.~417--449.
    
    \bibitem{20koz}
    \Au{Marcu D., Wong~W.}
    A phrase-based, joint probability model for statistical machine translation~// 
EMNLP Proceedings. Philadelphia, PA, 2002. P.~133--139. 
      
      \bibitem{21koz}
      \Au{Och F.\,J., Ney H.}
    Discriminative training and maximum entropy models for statistical machine 
translation~// 40th Annual Meeting of the ACL Proceedings. Philadelphia, 
PA, 2002. P.~295--302.
    
    \bibitem{22koz}
    \Au{Hu R., Zong~Ch., Xu~B.}
    An approach to automatic acquisition of translation templates based on phrase 
structure extraction and alignment~// IEEE Transactions on Audio, Speech and 
Language Processing, 2006. Vol.~14. No.\,5. P.~1656--1663.
    
    \bibitem{23koz}
    \Au{Dorr B., Habash~N.}
    Interlingua approximation: A~generation-heavy approach~// AMTA-2002 
Interlingua Reliability Workshop. Tiburon, California, USA, 2002.
    
    \bibitem{24koz}
    \Au{Voss C., Dorr B.\,J.}
    Toward a lexicalized grammar for interlinguas~// Machine Translation, 1995. 
Vol.~10. No.\,1--2. P.~139--180.
    
    \bibitem{25koz}
    \Au{Marcus M.\,P., Santorini~B., Marcinkiewicz~M.\,A.}
    Building a large annotated corpus of English: The Penn Treebank~// 
Comput. Linguistics. 1993. Vol.~19. No.\,2. P.~313--330.
    
    \bibitem{26koz}
    \Au{Callison-Burch Ch., Cohn~T., Lapata~M.}
    ParaMetric: An automatic evaluation metric for paraphrasing~//  
22nd Conference (International ) on Computational Linguistics (Coling 
2008) Proceedings. Manchester, 2008. P.~97--104.
    
    \bibitem{27koz}
    \Au{Dagan I., Bar-Haim~R., Szpektor~I., Greental~I., Shnarch~E.}
    Natural language as the basis for meaning representation and inference~// 
Computational Linguistics and Intelligent Text Processing: 9th 
Conference, CICLing 2008 Proceedings. Haifa, Israel, 2008.~--- Springer, 2008. P.~151--170.
    
    \bibitem{28koz}
    \Au{Kozerenko~E.\,B.}
    Cognitive approach to language structure segmentation for machine 
translation algorithms~// Conference (International) on Machine 
Learning, Models, Technologies and Applications Proceedings. Las Vegas, USA, 2003.~--- 
CSREA Press, 2003. P.~49--55.
    

\label{end\stat}
    
    \bibitem{30koz}
    \Au{Козеренко~Е.\,Б.}
    Лингвистическое моделирование для систем машинного перевода и 
обработки знаний~// Информатика и её применения, 2007. Т.~1. Вып.~1. 
С.~54--65.

    \bibitem{29koz}
    \Au{Kozerenko~E.}
    Features and categories design for the English-Russian transfer model~// 
Advances Natural Language Processing Applications Research  
Comput. Sci., 2008. Vol.~33. P.~123--138.
    
 \end{thebibliography}
}
}

\end{multicols}
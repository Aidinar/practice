\def\stat{krivenko}

\def\tit{НЕПАРАМЕТРИЧЕСКОЕ ОЦЕНИВАНИЕ ЭЛЕМЕНТОВ БАЙЕСОВСКОГО КЛАССИФИКАТОРА}

\def\titkol{Непараметрическое оценивание элементов байесовского классификатора}

\def\autkol{М.\,П.~Кривенко}
\def\aut{М.\,П.~Кривенко$^1$}

\titel{\tit}{\aut}{\autkol}{\titkol}

%{\renewcommand{\thefootnote}{\fnsymbol{footnote}}\footnotetext[1]
%{Исследования выполнены при частичной поддержке РФФИ, гранты 08-01-00567, 08-01-91205, 09-01-12180.}}

\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Институт проблем информатики Российской академии наук, mkrivenko@ipiran.ru}


\Abst{Рассмотрена задача построения эмпирического байесовского классификатора, обеспечивающего 
распознавание текста в случае, когда отдельные знаки имеют различные размеры. Представлен 
комбинированный метод построения оценки элементов байесовского классификатора, включающий 
непараметрическую ядерную оценку и параметрическую оценку с помощью плотности нормального 
распределения. Подобная комбинированная оценка позволяет эффективно решать задачу обработки малых 
объемов обучающей выборки. Продуктивность предложенных идей иллюстрируется на примере 
распознавания реального текста.}

\KW{байесовский классификатор; комбинированная оценка многомерной плотности распределения; 
адаптивная ядерная оценка; распознавание текста}

     \vskip 18pt plus 9pt minus 6pt

      \thispagestyle{headings}

      \begin{multicols}{2}

      \label{st\stat}

\section{Введение}
      
      Рассматривается задача восстановления (распознавания) текста по изображениям строк, 
содержащих знаки из алфавита некоторого языка. Используемый при написании знаков шрифт 
содержит\linebreak
 начертания знаков разной ширины. Пред\-по\-лагается, что есть возможность 
формирования обуча\-ющей выборки (множества изображений и соответствующих кодов знаков). 
Работа является продолжением~[1].
      
      Дополнительную сложность сформулированной задаче распознавания придают следующие 
факторы:
      \begin{itemize}
\item неопределенность местоположения знака в строке;
\item значения ширины знаков используемого шрифта не одинаковы;
\item высокая степень искажения изображения.
\end{itemize}

      Рассмотрим байесовский подход, когда считаются заданными статистические свойства 
классов образов, а класс составляют изображения отдельного знака. В~качестве модели текста 
рассмотрим последовательность знаков, каждый из которых появляется независимо от других с 
вероятностью, не зависящей от его номера в последовательности. Изображение текста 
формируется в строке последовательно, знак за знаком; восстановление текста осуществляется 
также последовательно. В~силу этого основной процедурой обработки данных становится 
следующая: в определенных позициях изображения некоторой строки текста необходимо выявить 
наиболее предпочтительный знак.
      
      Предполагается, что существует набор классов~$\omega_j$, $j=1, 2, \ldots ,M$, которые 
связаны с формированием в определенных позициях изображения одного из знаков~$c(\omega_j)$. 
Вероятность появления класса~$\omega_j$ равна~$p(\omega_j)$, $j=1, 2, \ldots , M$ и  
$\sum\limits_{j=1}^M p(\omega_j)=1$. Далее рассматривается частный, но весьма 
распространенный вид функции потерь~--- единичная функция потерь. Для того чтобы 
приспособить байе\-сов\-ский подход к задаче восстановления знаков различной ширины, 
приходится ввести дополнительные предположения:
      \begin{itemize}
\item  распределения интенсивностей пикселов в области~$A_j$ и вне ее независимы;
\item распределения интенсивностей пикселов вне области~$A_j$ не зависят от 
класса~$\omega_j$;
\item  $p(\mathbf{x}(A\backslash A_j))\not= 0$.
\end{itemize}

      Здесь $A$~--- область изображения всей строки, $A_j$~--- область изображения 
знака~$c(\omega_j)$, $\mathbf{x}(A\backslash A_j)$~--- точка выборочного пространства 
интенсивностей пикселов в области~$A\backslash A_j$. Тогда можно показать~[1], что принятие 
решения основывается на величинах:
      \begin{equation}
      \fr{ p(\mathbf{x}(A_j)\vert \omega_j) p(\omega_j)}{p(\mathbf{x}(A_j))}\,,\quad j=1, 2, \ldots , 
M\,.
      \label{e1kk}
      \end{equation}
      
      Для реализации байесовского классификатора необходимо при любом  $j=1, 2, \ldots , M$ 
знать или уметь вычислять следующие величины: $p(\omega_j)$~--- априорную вероятность 
появления класса~$\omega_j$; условное распределение~$p(\mathbf{x}(A_j)\vert\omega_j)$~--- 
значение плотности распределения интенсивностей пикселов в области~$A_j$ при условии, что в 
ней изоб\-ра\-жа\-ет\-ся знак~$c(\omega_j)$; безусловное распределение~$p(\mathbf{x}(A_j))$~--- 
значение плотности распределения интенсивностей пикселов в области~$A_j$. 
      
      В~[1] рассматривался случай, когда $j$-й класс образов описывался плотностью 
многомерного нормального распределения; там показывалось, что в этом случае 
распределение~$p(\mathbf{x}(A_j))$ представляло собой смесь нормальных распределений, и 
описывались способы задания этой смеси. Основной результат указанной работы заключался в 
обосно\-ва\-нии возможности применения байесовского подхода при классификации объектов, 
имеющих различную размерность. При этом эксперименты с реальными объектами 
продемонстрировали эффективность предлагаемого подхода, но вместе с тем выявили недостатки 
<<прямолинейного>> оценивания элементов байесовского классификатора с помощью 
параметрической модели нормального распределения.
      
      Альтернативой традиционным параметрическим моделям служит непараметрический 
подход~--- эффективное средство при поиске новых решений при классификации многомерных 
данных. Здесь в первую очередь речь идет о ядерной оценке плотности распределения. 
Гистограммная оценка была исключена из рассмотрения сразу же, и причины здесь следующие:
      \begin{itemize}
\item повышенная сложность задания гисто\-грам\-мной многомерной оценки, ее 
построение требует задания не только системы и размеров ячеек (классов, категорий), но и 
определения их ориентации в пространстве;
\item выборочные свойства гистограммной оценки оказываются не наилучшими среди 
других оценок, причем она в большей степени ориентирована на оценивание равномерной 
плотности (см.\ разд.~5.4~[2]), которая при распознавании изображений явно отсутствует. 
\end{itemize}

Применение остальных воплощений непараметрического подхода (оценка по методу ближайших 
соседей, ортогональные разложения) является самостоятельной задачей и не рассматривалось в 
данной работе. 
      
      Ядерная многомерная оценка~$f^*(\mathbf{x})$ плотности распределения~$f(\mathbf{x})$  
обычно строится как обобщение соответствующей оценки одномерной плот\-ности и при заданных 
$n$-мер\-ных наблюденных значениях  $\mathbf{x}_1,\ldots , \mathbf{x}_N$ имеет следующий 
вид:
      \begin{equation}
     f^*(\mathbf{x}) =\fr{1}{N\vert \mathbf{H}\vert}\sum\limits_{i=1}^N K\left ( \mathbf{H}^{-
1}(\mathbf{x}-\mathbf{x}_i)\right )\,,\enskip \mathbf{x}\in \Re^n,
      \label{e2kk}
      \end{equation}
где \textbf{H}~--- несингулярная матрица размера $n\times n$ (обобщение одномерного параметра 
сглаживания~$h$);  $\vert \mathbf{H}\vert$~--- ее определитель; ядро~$K$~--- многомерная 
функция с ограничениями (обычно плотность некоторого распределения). Если 
$\mathbf{H}=h\mathbf{I}_n$, где $h>0$, то ядро полностью определяется параметром~$h$ 
и~(\ref{e2kk}) редуцируется к виду
\begin{equation*}
f^*(\mathbf{x}) =\fr{1}{Nh^n}\sum\limits_{i=1}^N K\left(\fr{\mathbf{x}-\mathbf{x}_i}{h}\right)\,,\quad 
\mathbf{x}\in \Re^n\,,
\end{equation*}
который и был принят за основу в данной работе.

      Непараметрическое оценивание плотности сопровождается рядом общих для многомерного 
анализа и специфических для распознавания изображений проблем:
      \begin{itemize}
\item существует множество реализаций метода построения оценки, с каждой из которых связана 
необходимость установления значений большого числа параметров;
\item многомерные данные~--- плохой объект для визуализации, поэтому весьма трудно 
представить себе реальную картину происходящего;
\item с ростом размерности данных существенно снижается качество получающейся оценки 
плотности распределения;
\item <<малый>> объем обучающей выборки (число\linebreak изображений обучающей выборки, 
относящихся к определенному знаку, может быть существенно меньше размерности векторного 
представления этого изображения) порождает\linebreak существенные теоретические проблемы (мат\-ри\-ца 
вторых моментов становится сингулярной).
\end{itemize}

      Снижение качества оценки следует из асимп\-то\-ти\-ческих свойств ядерной оценки плотности. 
В~частности, если выполняются условия регулярности для оцениваемой плотности распределения 
и ядра~$K$, то при $N\rightarrow\infty$ и $h_N\rightarrow 0$ интегральная среднеквадратичная 
ошибка ($L_2$-подход) ядерной оценки стремится к нулю со скоростью~$O(N^{-4/(n+4)})$ (см.\ 
обзор результатов в разд.~4.5.2~\cite{3kk}), т.\,е.\ скорость снижается при возрастании 
размерности~$n$. 
      
      Проблемы <<проклятия размерности>> можно пытаться решать с помощью приведения 
данных к меньшей размерности так, чтобы сохранить специфику самих данных и по возможности 
улучшить выборочные свойства оценок. Этим путем идут в методе проективного поиска 
(projection pursuit), описание которого можно найти в разд.~4.4~\cite{4kk} или в 
разд.~4.6~\cite{3kk}. В~данном случае он не подходит, так как в основном ориентирован на 
проектирование в одно- или двухмерное пространство и не решает проблемы сингулярности. По 
этой причине в данной работе предлагается комбинированный метод оценивания плотности 
распределения, когда для первых, наиболее <<существенных>> координат применяется 
непараметрическая ядерная оценка плотности, а для остальных~--- обычная параметрическая 
оценка с помощью нормального распределения.

\section{Комбинированная оценка плотностей распределения}

      Рассмотрим ситуацию, когда распределение $n$-мерной случайной величины фактически 
сосредоточено в $m$-мерном подпространстве, $m<n$. Переход к переменной в подпространстве 
сниженной размерности осуществим путем перехода к первым главным компонентам, т.\,е.\ с 
помощью линейного преобразования~$\mathbf{L}^{\mathrm{T}}$, удовлетворяющего спектральному 
разложению заданной ковариационной матрицы~\textbf{C} следующего вида: 
$\mathbf{C}=\mathbf{LDL}^{\mathrm{T}}$, где \textbf{L}~--- ортогональная матрица; \textbf{D}~--- 
диагональная матрица с неотрицательными упорядоченными по убыванию элементами на 
диагонали. 
      
      Заметим сразу же, что при таком преобразовании значение плотности остается без 
изменений. Действительно, пусть есть случайный вектор~\textbf{X}, имеющий плотность 
распределения~$\widetilde{f}(\mathbf{x})$. С~помощью ортогонального 
преобразования~\textbf{L}$^{\mathrm{T}}$ осуществляется переход к вектору 
$\mathbf{Y}=\mathbf{L}^{\mathrm{T}}\mathbf{X}$, имеющему плотность распределения~$f(\mathbf{y})$. Для 
матрицы преобразования верно следующее: $\mathbf{LL}^{\mathrm{T}}=\mathbf{I}_n$ или 
$\mathbf{L}^{\mathrm{T}}=\mathbf{L}^{-1}$. Тогда $\mathbf{x} =L\mathbf{y}$, а якобиан 
преобразования~$J$ принимает значение $J=\pm 1$. В~результате получается, что плотность 
распределения вектора~\textbf{Y} есть $f(\mathbf{y})=\vert J\vert \widetilde{f}(\mathbf{x})=\widetilde{f}(\mathbf{x})$.
      
      Оценку плотности распределения~$f^*(\mathbf{y})$ переменной~\textbf{Y} сформируем 
из базовой части~--- оценки плотности распределения первых главных компонент, и 
дополнительной части~--- параметрической оценки плотности распределения остальных главных 
компонент, для которой принята аппроксимация с помощью нормального распределения. Причин 
для выбора такой комбинации несколько:
      \begin{itemize}
\item эксперименты с реальными изображениями показывают, что в качестве распределения 
последних главных компонент приближенно можно принять нормальное распределение, для 
первых же главных компонент это не так, и для них не существует простых параметрических 
моделей;
\item распространение способа оценивания базовой части на всю исходную размерность 
приводит к неудовлетворительному качеству получающейся оценки, а при больш$\acute{\mbox{и}}$х 
значениях раз\-мер\-ности порождает и вычислительные проблемы.
\end{itemize}

      Таким образом, оценка~$f^*(\mathbf{y})$ распадается на две составляющие, которые 
считаются независимыми (некоррелированность первых главных компонент подменяется 
независимостью совокупности первых главных компонент и всех остальных компонент), и 
принимает вид:
      \begin{equation*}
      f^*(\mathbf{y}) =f^*_m\left ( y^{(1)}, \ldots , y^{(m)}\right ) \prod\limits_{j=m+1}^n 
f_1^*\left(y^{(j)}\right)\,,
      \end{equation*}
где $f_m^*\left( y^{(1)}, \ldots , y^{(m)}\right)$~--- оценка плотности первых m главных компонент, 
$f_1^*\left ( y^{(j)}\right)$~--- оценка плотности нормального одномерного распределения $j$-й 
главной компоненты.
      
      Матрицы перехода \textbf{L}$^*$ и дисперсий главных компонент~\textbf{D}$^*$ 
находятся из уравнения $\mathbf{C}^*=\mathbf{L}^*\mathbf{D}^*\mathbf{L}^{*\mathrm{T}}$, где 
оценки~$\mu^*$ и~\textbf{C}$^*$ вектора  среднего и ковариационной матрицы находятся обычным образом с 
помощью обучающей выборки. В~случае, когда объемы последней 
предполагаются меньшими, чем размерность выборочного пространства, часть 
элементов~\textbf{D}$^*$ будут нулевыми. В~связи с этим приходится вводить еще один 
параметр комбинированной оценки~--- критическое значение дис\-пер\-сии главной 
компоненты~$d_0$, $d_0>0$, ниже которого не может опускаться значение диагональных 
элементов~\textbf{D}$^*$. Этот порог является платой за малые объемы исходных данных и, как 
следствие, за незнание реальных характеристик распределений данных. Для определения 
значения~$d_0$ допустим наличие изоб\-ра\-же\-ний строк текста, для которых состав знаков и их 
расположение считаются известными. Подобную исходную информацию назовем тренировочной 
(уточняющей) выборкой. 

\section{Выбор параметров сглаживания ядерной оценки плотности 
распределения}
      
      Говоря о базовой части комбинированной оценки некоторой плотности распределения, 
будем вместо $f^*_m\left(y^{(1)}, \ldots , y^{(m)}\right)$ использовать просто запись~$f^*_m\left ( 
\widetilde{\mathrm{y}}\right)$, где $\widetilde{\mathbf{y}}\in \Re^m$. Ядерная оценка плотности 
имеет вид: 
      $$
      f^*_m\left(\widetilde{\mathbf{y}}\right) =\fr{1}{Nh^m}\sum\limits_{i=1}^N 
K\left(\fr{\widetilde{\mathbf{y}}-\widetilde{\mathbf{y}}_i}{h}\right)\,,
      $$
где $\widetilde{\mathbf{y}}_1, \ldots , \widetilde{\mathbf{y}}_N$~--- последовательность проекций 
на первые m главных компонент исходных наблюденных значений $\mathbf{x}_1, \ldots 
,\mathbf{x}_N$. 
      
      В настоящее время для выбора параметра сглаживания существует три основные группы 
методов:
      \begin{itemize}
\item грубые (\textit{rule-of-thumb}) методы нахождения наилучшего значения~$h$ при условии, 
что оцениваемая плотность распределения известна и совпадает с некоторой заданной;
\item методы перепроверки (\textit{cross-validation}), когда из выборки удаляется единственное 
наблюденное значение, скажем~$\widetilde{\mathbf{y}}_i$, вычисляется оценка плотности в 
точке~$\widetilde{\mathbf{y}}_i$ с помощью остальных $N-1$ выборочных значений, а~$h$ 
выбирается так, чтобы оптимизировать некоторый критерий, включающий все $N$~оценок 
плотности;
\item методы расширения (\textit{plug-in}), в основе которых лежит выбор~$h$ для некоторой 
пилотной плотности распределения, а затем использование получившейся оценки для оценивания 
поправочного коэффициента в окончательном выражении для~$h$. Эта процедура может быть 
итерационной.
\end{itemize}

      В данной работе строится вариант метода перепроверки в интерпретации~\cite{5kk}, но 
применительно к многомерному случаю и в форме алгоритма, который можно использовать на 
практике. В этом случае значение параметра сглаживания~$h$ ищется с помощью метода 
перепроверки как решение следующей задачи:
      \begin{equation}
      \ln L(h)\rightarrow \max\,,
      \label{e3.1kk}
      \end{equation}
где
\begin{multline*}
\ln L(h) =
\ln\prod\limits_{i=1}^N f^*_m\left ( \widetilde{\mathbf{y}}_i\right) ={}\\
{}
=\sum\limits_{i=1}^N\ln\left ( \fr{1}{(N-1)h^m}\sum\limits_{\substack{{j=1}\\ {j\not=i}}}^N K  \left(
\fr{\widetilde{\mathbf{y}}_i-\widetilde{\mathbf{y}}_j}{h}\right)\right)\,.
\end{multline*}
      
      В случае, когда ядро~--- нормальная плотность, имеем
      \begin{multline*}
      \ln L(h) =\sum\limits_{i=1}^N \ln \left (
      \vphantom{\sum\limits^N_{\substack{{j=1}\\ {j\not=i}}}} \fr{1}{(N-
1)(2\pi)^{m/2}}\,\fr{1}{h^m}\right.\times{}\\
\left.{}\times\sum\limits^N_{\substack{{j=1}\\ {j\not=i}}}\exp\left( -
\fr{(\widetilde{\mathrm{y}}_i-\widetilde{\mathrm{y}}_j)^{\mathrm{T}}(\widetilde{\mathrm{y}}_i-
\widetilde{\mathrm{y}}_j)}{2h^2}\right)\right)\,.
      \end{multline*}
Таким образом, необходимо при $h>0$   найти максимум функции 

\noindent
\begin{multline*}
\varphi(h) ={}\\
\!{}=\sum\limits_{i=1}^N\ln\left( \fr{1}{h^m}\sum\limits^N_{\substack{{j=1}\\ {j\not=i}}}\exp 
\left( -\fr{(\widetilde{\mathbf{y}}_i-\widetilde{\mathbf{y}}_j)^{\mathrm{T}}(\widetilde{\mathbf{y}}_i-
\widetilde{\mathbf{y}}_j)}{2h^2}\right)\right) ={}\\
{}=\sum\limits_{i=1}^N \ln\left(\fr{1}{h^m}
\sum\limits^N_{\substack{{j=1}\\ {j\not=i}}}
\exp\left(-
\fr{r_{ij}}{2h^2}\right)\right)\,,
\end{multline*}
где $r_{ij} =(\widetilde{\mathbf{y}}_i-\widetilde{\mathbf{y}}_j)^{\mathrm{T}}(\widetilde{\mathbf{y}}_i-
\widetilde{\mathbf{y}}_j)$.
      
      Для сужения области поиска решения задачи~(\ref{e3.1kk}) докажем
      
      \medskip
      
      \noindent
      \textbf{Утверждение.} \textit{Если $h_0$~--- точка максимума функции~$\varphi(h)$, то} 
      $$
      h_0\in \left [ \sqrt{\fr{\min\limits_{i\not=j} r_{ij}}{m}},\,\sqrt{\fr{\max\limits_{i,j} 
r_{ij}}{m}}\right]\,.
      $$
      
      \medskip
      
      \noindent
      Д\,о\,к\,а\,з\,а\,т\,е\,л\,ь\,с\,т\,в\,о\,.\ Рассмотрим функции $\psi_{ij}(h) =(1/h^m) \exp \left( -
r_{ij}/(2h^2)\right)$ и  $e_{ij}(h)\;=$\linebreak $=\;\exp \left (-r_{ij}/(2h^2)\right)$ при $r_{ij}\not=0$, $i,j=1, \ldots , 
N$. Тогда имеем следующее:
      \begin{align}
      \psi_{ij}^\prime(h) & = \fr{r_{ij}-mh^2}{h^{m+3}}\,e_{ij}(h)\,;\label{e3.2kk}\\
      \psi_{ij}^{\prime\prime}(h) & ={}\notag\\
      &\!\!\!\!\!\!\!\!\!\!{}= \fr{r_{ij}^2-(2m+3)r_{ij} h^2+m(m+1)h^4}{h^{m+6}}\,e_{ij} 
(h)\,.\label{e3.3kk}
      \end{align}
      
      Из~(\ref{e3.2kk}) следует, что уравнение $\psi_{ij}^{\prime}(h)=0$ имеет единственное 
допустимое решение $h_{ij} =\sqrt{r_{ij}/m}$, после подстановки которого в правую 
часть~(\ref{e3.3kk}) получаем: 
      $$
      \psi_{ij}^{\prime\prime}(h_{ij}) = -\fr{2r_{ij}^2}{m}\,e_{ij}(h_{ij})<0\,.
      $$
      Следовательно, точка~$h_{ij}$~--- единственная точка максимума функции~$\psi_{ij}(h)$ 
при  $h>0$.
      
      
      Если $r_{ij}\not=0$, то при $h\in (0,\,+\infty)$ каждая функция~$\psi_{ij}(h)$ достигает 
единственного максимума при $h_{ij} =\sqrt{r_{ij}/m}$. Отсюда следует, что 
функция~$\varphi(h)$ при 
$h<\sqrt{\min\limits_{i\not=j}\,r_{ij}/m}$ является возрастающей, 
а при $h>\sqrt{\max\limits_{i,j}\, r_{ij}/m}$~--- убывающей, 
т.\,е.\ искомое решение может находиться 
только на отрезке 
      \begin{equation}
      \left [ \sqrt{\fr{\min\limits_{i\not= j}\,r_{ij}}{m}},\,\sqrt{\fr{\max\limits_{i,j} 
\,r_{ij}}{m}}\right]\,,
      \label{e3.4kk}
      \end{equation}
что и требовалось доказать.
\medskip

      Кстати, введя обозначения $z=1/h^2$ и $c_{ij}\;=$\linebreak $=\;r_{ij}/2$, получаем для некоторых 
констант~$c_1$ и~$c_2$:
      \begin{multline*}
\!\!      \lim\limits_{h\rightarrow 0}\! \psi_{ij}(h) =\lim\limits_{z\rightarrow\infty}\! \psi_{ij}(z) 
=\lim\limits_{z\rightarrow \infty}\fr{z^{m/2}}{\exp\left(c_{ij} z\right)} =\ldots 
={}\\
{}=\lim\limits_{z\rightarrow\infty}
      \begin{cases}
      \fr{c_1}{\exp\left(c_{ij}z\right)}\\
      \fr{c_2}{\sqrt{z}\,\exp\left(c_{ij}z\right)}
      \end{cases}
      =0\,.
      \end{multline*}
      
      Для поиска~$h_0$ будем находить решение уравнения $\varphi^\prime(h)=0$ с помощью 
метода деления пополам, для чего необходимо получить выражение для производной:
      \begin{multline*}
      \varphi^\prime(h) =\sum\limits_{i=1}^N
      \fr{\sum\limits_{\substack{{j=1}\\ {j\not=i}}}^N \left ( r_{ij}/h^3-m/h\right)e_{ij}(h)}{ 
\sum\limits_{\substack{{j=1}\\ {j\not=i}}}^N  e_{ij}(h)} ={}\\
{}=\fr{1}{h^3}\sum\limits_{i=1}^N 
\fr{\sum\limits_{\substack{{j=1}\\ {j\not=i}}}^N \left( -mh^2+r_{ij}\right) 
e_{ij}(h)}{\sum\limits_{\substack{{j=1}\\ {j\not=i}}}^N e_{ij}(h)}\,.
      \end{multline*}
      
      После этого отрезок~(\ref{e3.4kk}) возможных решений уравнения $\varphi^\prime(h)=0$ 
можно несколько сузить. Введем для $i=1, \ldots , N$ следующие обозначения: $r_{i,\min} 
=\min\limits_{j\not=i} \,r_{ij}$; $r_{i,\max} =\max\limits_j\,r_{ij}$; 
$\overline{r}_{\min}=$\linebreak $=(1/N)\sum\limits_i r_{i,\min}$; $\overline{r}_{\max} =(1/N)\sum\limits_i 
r_{i,\max}$. Тогда
      $$
      \fr{\sum\limits_i r_{i,\min}}{h^3} -\fr{Nm}{h}\leq \varphi^\prime(h) \leq \fr{\sum\limits_i 
r_{i,\max}}{h^3}-\fr{Nm}{h}\,.
      $$

Отсюда получается, что
$$
h_0\in \left[\sqrt{\fr{\overline{r}_{\min}}{m}},\,\sqrt{\fr{\overline{r}_{\max}}{m}}\right ]\,.
$$
      
Кроме этого, можно записать выражение и для второй производной:

\noindent
      \begin{multline*}
      \varphi^{\prime\prime}(h) =-\fr{3}{h^4}\sum\limits_{i=1}^N 
\fr{\sum\limits_{\substack{{j=1}\\ {j\not=i}}}^N  r_{ij} e_{ij}(h)} {\sum\limits_{\substack{{j=1}\\ 
{j\not=i}}}^N  e_{ij}(h)}+\fr{1}{h^6}\times{}\\
      {}\times \sum\limits_{i=1}^N
      \fr{\sum\limits_{\substack{{j=1}\\ {j\not=i}}}^N r_{ij}^2 
e_{ij}(h)\cdot \sum\limits_{\substack{{j=1}\\ {j\not=i}}}^N  e_{ij}(h)-
\left(\sum\limits_{\substack{{j=1}\\ {j\not=i}}}^N  
r_{ij}e_{ij}(h)\right)^2}{\left(\sum\limits_{\substack{{j=1}\\ {j\not=i}}}^N  
e_{ij}(h)\right)^2}+{}\\
{}+\fr{Nm}{h^2}\,.
      \end{multline*}
      
      Другим, иногда более удобным для вычислений, видом второй производной является 
следующий:
      \begin{multline*}
      \varphi^{\prime\prime}(h) =\fr{1}{h^6}\times{}\\
{}     \times
      \sum\limits_{i=1}^N
      \left( \sum\limits_{j\not=i}\sum\limits_{k\not=i} \left( mh^4-3h^2r_{ij}-r_{ij}r_{ik}+r_{ij}^2\right)\right.\times\\ 
\left.{}\times e_{ij}(h) e_{ik}(h)
\vphantom{\sum\limits_{j\not=i}\sum\limits_{k\not=i}}\right)
\Bigg /\left( \sum\limits_{j\not= i}\sum\limits_{k\not= i} e_{ij}(h) e_{ik}(h)\right )\,.
      \end{multline*}
      
      Для подтверждения правильности решения~(\ref{e3.1kk}), полученного с помощью метода 
деления пополам, на практике достаточно реализовать следующее:
      \begin{itemize}
\item обеспечивать на левой границе отрезка, который делится пополам, неотрицательное 
значение~$\varphi^\prime(h)$, а на правой~--- неположительное. Или же, найдя точку, где 
$\varphi^\prime(h)\approx 0$, проверять знак~$\varphi^{\prime\prime}(h)$. Все это гарантирует, что 
полученное решение действительно служит приближением для точки максимума;
\item с помощью визуального анализа поведения функции~$\varphi(h)$, или~$\varphi^\prime(h)$, 
или~$\varphi^{\prime\prime}(h)$ удостовериться, что найденная точка экстремума является 
единственной.
\end{itemize}

      Более глубокий анализ существования и единственности точки максимума 
для~(\ref{e3.1kk}) сдерживается громоздким видом~$\varphi^\prime(h)$ 
и~$\varphi^{\prime\prime}(h)$, а также отсутствием других подходов. Следует обратить внимание, 
что во всех экспериментах, результаты которых использованы в данной работе, 
решение~(\ref{e3.1kk}) оказывалось единственным. 

\begin{table*}[b]\small
\begin{center}
\parbox{118mm}{\Caption{Параметры скорости сходимости характеристик ядерной оценки плотности распределения
\label{t1kk}}
}
\vspace*{2ex}

\tabcolsep=10pt
\begin{tabular}{|c|c|c|c|c|}
\hline
$m$ & $c_h^*$&$\gamma_h^*$&$c_J^*$&$\gamma_J^*$\\
\hline
\rowcolor[gray]{.6}1 &$c_h=0{,}75$&$\gamma_h=-0{,}2$&$1{,}03\leq c_J\leq 1{,}38$&$\gamma_J=-0{,}4$\\
\hline
1 &
\tabcolsep=0pt\begin{tabular}{c}1,11\\ (1,08, 1,13)\end{tabular}&
\tabcolsep=0pt\begin{tabular}{c}$-0{,}20$\\ ($-0{,}22$, $-0{,}19$)\end{tabular}&
\tabcolsep=0pt\begin{tabular}{c}0,89\\ (0,86, 0,92)\end{tabular}&
\tabcolsep=0pt\begin{tabular}{c}$-0{,}38$\\ ($-0{,}40$, $-0{,}37$)\end{tabular}\\
\hline
2&1,19&$-0{,}17$&1,01&$-0{,}30$\\
\hline
4 &1,24&$-0{,}13$&1,21&$-0{,}21$\\
\hline
6 &1,25&$-0{,}11$&1,31&$-0{,}15$\\
\hline
      \end{tabular}
      \end{center}
      \end{table*}
      
      Как указано в разд.~6.4~\cite{2kk}, ядерная оценка плотности распределения с параметром 
сглаживания, полученным как решение задачи~(\ref{e3.1kk}), является состоятельной оценкой, но 
задача анализа скорости сходимости и описания выборочных свойств такой оценки в реальных 
ситуациях ждет своего решения. В~связи с этим интересна любая информация о реальном 
поведении оценки при увеличивающихся значениях~$N$ и~$m$. С~этой целью для нормального 
распределения с единичной ковариационной мат\-ри\-цей была проведена серия следующих 
экспериментов: при значениях $N = 8, 16, \ldots , 1024$ находили параметр 
сглаживания в соответствии с~(\ref{e3.1kk}), а затем оценивали $L_1$~--- расстояние между 
теоретической плотностью распределения и ее ядерной оценкой, а именно величину
      $J_N =\int\left\vert f_m(\widetilde{\mathbf{y}})-
f_m^*(\widetilde{\mathbf{y}})\right\vert\,d\widetilde{\mathbf{y}}$.
Интеграл заменяли суммой, вычисляемой на отрезке $[-3,\,3]$ по 7~точкам для каждой из 
координат в $m$-мерном пространстве (всего $7^m$~точек). Оценка~$E\{J_N\}$ строилась по 
100~повторениям описанного единичного эксперимента. 

      В случае $m=1$ можно теоретически получить некоторые результаты, полезные для 
сравнения. В теореме~1 разд.~5~\cite{2kk} получены точные асимптотические выражения 
для~$E\{J_N\}$, но опереться на них при выборе параметра сглаживания~$h$ трудно. Для 
получения оптимального решения относительно~$h$ обратимся к верхней границе 
для~$E\{J_N\}$. В~частности, она не превышается при следующем выборе параметра 
сглаживания:
      \begin{equation}
      h=\left( \fr{1}{\sqrt{2\pi}}\,\fr{\sqrt{\kappa_1}}{\kappa_2}\,\fr{\nu_1}{\nu_2}\right )^{2/5} 
N^{-1/5}\equiv c_h N^{\gamma_h}\,,
      \label{e3.5kk}
      \end{equation}
где 
\begin{align*}
\gamma_h &= -\fr{1}{5}\,; & c_h &=\left ( 
\fr{1}{\sqrt{2\pi}}\,\fr{\sqrt{\kappa_1}}{\kappa_2}\,\fr{\nu_1}{\nu_2}\right )^{2/5}\,;\\
\kappa_1 &= \int K^2(u)\,du\,; & \kappa_2&=\int u^2K(u)\,du\,;\\
\nu_1 &=\int\sqrt{f_m(u)}\,du\,; & \nu_2 &=\int\left\vert f_m^{\prime\prime}(u)\right\vert\,du\,.\\
\end{align*}
При этом имеем
\begin{multline*}
C\left( \kappa_1^2\kappa_2\right)^{1/5}\left(\fr{1}{2}\,\nu_1^4\nu_2\right)^{1/5}\leq 
N^{2/5}E\{J_N\}\leq {}\\
{}\leq 5(8\pi)^{-2/5}\left( 
\kappa_1^2\kappa_2\right)^{1/5}\left(\fr{1}{2}\,\nu_1^4\nu_2\right)^{1/5}\,,
\end{multline*}
где $C$~--- универсальная константа (см.\ теорему~2 разд.~5~\cite{2kk}); далее $C\approx 1{,}03$ 
и $5(8\pi)^{-2/5}\approx 1{,}38$. Тогда 
\begin{equation}
E\{J_N\} =c_J N^{\gamma_J}\,,
\label{e3.6kk}
\end{equation}
где 
$$
\gamma_J =-\fr{2}{5}\,;
$$

\noindent
\begin{multline*}
1{,}03\left(\kappa_1^2\kappa_2\right)^{1/5}\left(\fr{1}{2}\,\nu_1^4\nu_2\right)^{1/5}\leq c_J\leq {}\\
{}\leq
1{,}38\left(\kappa_1^2\kappa_2\right)^{1/5}\left(\fr{1}{2}\,\nu_1^4\nu_2\right)^{1/5}\,.
\end{multline*}
            
В случае оценивания плотности стандартного нормального распределения и ядра~--- 
плотности опять же стандартного нормального распределения, с помощью прямых вычислений 
получаем:
      $$
      \kappa_1 =\fr{1}{2\sqrt{\pi}}\,,\enskip \kappa_2=1\,,\enskip \nu_1=(8\pi)^{1/4}\,,\enskip 
\nu_2=\sqrt{\fr{8}{\pi e}}\,.
      $$
      
      Полученные теоретические и экспериментальные результаты сведены в табл.~\ref{t1kk}. 
Дадим пояснения по ее структуре и принятым обозначениям:
      \begin{itemize}
\item знаком~$*$ помечены характеристики, оценки для которых получены методом линейной 
регрессии;
\item в строке для $m=1$, выделенной серым цветом, даны теоретические характеристики, 
полученные в соответствии с~(\ref{e3.5kk}) и~(\ref{e3.6kk});
\item в другой строке для $m=1$ в скобках даны 90-процентные доверительные интервалы выборочных 
значений.
\end{itemize}



\begin{figure*}         %fig1
\vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=120mm
\epsfbox{kri-1.eps}
}
\end{center}
\vspace*{-9pt}
\Caption{Изображение одной из строк текста
\label{f1kk}}
\end{figure*}

      Из полученных результатов можно сделать следующие выводы:
      \begin{itemize}
\item при $m=1$ наблюдается полное согласие эмпирических и теоретических зависимостей, 
правда, отказ в методе оценивания параметра сглаживания в соответствии с~(\ref{e3.1kk}) от 
информации о виде распределения приводит к снижению нижней границы для~$c_J^*$ с~1,03 
до~0,89;
\item увеличение размерности влечет существенное снижение скорости сходимости~$E\{J_N\}$ 
к~0, что может привести при больш$\acute{\mbox{и}}$х значениях размерности и ограниченных объемах данных к 
проблемам в использовании ядерных оценок плотности.
\end{itemize}

\vspace*{-12pt}

\section{Эксперименты}

      Объектом экспериментов служил отсканированный фрагмент романа Жорж Санд <<Она и 
он>>, из <<Собрания сочинений избранных иностранных писателей>>, С.-Петербург, 1897. Из-за 
старения бумаги и типографской краски изображение оказалось низкого качества, не достаточного 
для уверенного распознавания коммерческими системами (на рис.~\ref{f1kk} дан пример 
изображения одной из строк). Для исследований были взяты 80~изображений строк, которые 
содержали как знаки, для которых строилась обучающая выборка, так и 
<<неизвестные>> знаки. 
Первую совокупность знаков, которая <<известна>> классификатору, назовем алфавитом; в 
нашем случае он содержит 33~строчные буквы и пробел. 

      Все анализируемые изображения были вручную дополнены эталонными строками, 
содержащими как информацию о положении знака, так и о его имени. Первые 40~строк были 
выделены для построения базовой обучающей выборки, общая информация о ней приведена в 
табл.~2. В~ней приняты следующие обозначения: $N_{\mathrm{ch}}$~--- число изображений 
некоторого знака в обучающей выборке, $W_{\mathrm{ch}}$~--- ширина изображения некоторого знака, 
$n_{\mathrm{ch}}$~--- число пикселов в изображении некоторого знака (высота у всех знаков одинакова и 
равна 14~пикселам). Заметим, что объем обучающей выборки для практически любого знака 
меньше числа пикселов в изображении этого знака, т.\,е.\ меньше размерности признакового 
пространства.
      
      Данные о различных значениях ширины изображения для знаков обучающей выборки 
отдельно собраны в табл.~3.

\begin{center} %tabl2
\noindent
{{\tablename~2}\ \ \small{Характеристики обучающей выборки}}
\end{center}
%\vspace*{2pt}

{\small \begin{center}
\tabcolsep=15pt
\begin{tabular}{|c|c|c|c|}
\hline
Знак&$N_{\mathrm{ch}}$&$W_{\mathrm{ch}}$&$n_{\mathrm{ch}}$\\
\hline
пробел&564\hphantom{9}&3&42\\
а&153\hphantom{9}&6&84\\
б&32&6&84\\
в&82&6&84\\
г&24&5&70\\
д&50&6&84\\
е&130\hphantom{9}&5&70\\
\hspace*{-1pt}{\raisebox{-2pt}{
\epsfxsize=2.276mm
\epsfbox{izhitca.eps}
}}
&50&7&98\\
ж&22&9&126\hphantom{9}\\
з&37&5&70\\
i&19&3&42\\
и&93&7&98\\
й&14&6&84\\
к&36&6&84\\
л&83&6&84\\
м&55&7&98\\
н&118\hphantom{9}&7&98\\
о&192\hphantom{9}&6&84\\
п&42&7&98\\
р&90&6&84\\
с&88&5&70\\
т&82&6&84\\
у&46&6&84\\
х&12&6&84\\
ц&\hphantom{9}7&6&84\\
ч&32&6&84\\
ш&10&10\hphantom{9}&140\hphantom{9}\\
щ&\hphantom{9}4&10\hphantom{9}&140\hphantom{9}\\
ъ&77&8&112\hphantom{9}\\
ы&29&8&112\hphantom{9}\\
ь&43&6&84\\
ю&12&8&112\hphantom{9}\\
я&60&6&84\\
\hline
\end{tabular}
\end{center}
}
\vspace*{6pt}


%\bigskip
\addtocounter{table}{1}

%\bigskip

\begin{center} %tabl3
\noindent
{{\tablename~3}\ \ \small{Значения ширины изображения знаков}}
\end{center}
%\vspace*{2pt}

{\small \begin{center}
\tabcolsep=11.5pt
\begin{tabular}{|l|c|}
\hline
\multicolumn{1}{|c|}{Знак}&$W_{\mathrm{ch}}$\\
\hline
пробел, i&3\\
г, е, з, с&5\\
а, б, в, д, й, к, л, о, р, т, у, х, ц, ч, ь, я&6\\
\hspace*{-4pt}{\raisebox{-2pt}{
\epsfxsize=2.276mm
\epsfbox{izhitca.eps}
}}, и, м, н, п&7\\
ъ, ы, ю&8\\
ж&9\\
ш, щ&10\hphantom{9}\\
\hline
\end{tabular}
\end{center}
}
\vspace*{12pt}


%\bigskip
\addtocounter{table}{1}
      
      Кроме этого, с помощью первых 40~строк были образованы выборки для областей со 
значениями ширины: 3, 5, 6, 7, 8, 9, 10~пикселов. Их длина оказалась равной~1094.

\pagebreak


\end{multicols}

\begin{table*}\small %tabl4
\begin{center}
\Caption{Характеристики уточняющей и контрольной выборок
\label{t4kk}}
\vspace*{2ex}


\tabcolsep=5.1pt
\begin{tabular}{|c|p{50mm}|c|c|c|}
\hline
Тип выборки&\multicolumn{1}{c|}{Содержание}&Номера строк&Число знаков&Число знаков из алфавита\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{Уточняющая}}&
20~строк текста (исходные изображения и эталонные строки)&\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{41--60}}&
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{1196}}&\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{1140}}\\
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{Контрольная}}&20~строк текста (исходные изображения и эталонные строки)&
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{61--80}}&\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{1212}}&
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{1150}}\\
\hline
\end{tabular}
\end{center}
\vspace*{-6pt}
\end{table*}      
      
\begin{table*}\small %tabl5
\begin{center}
\Caption{Процедуры обработки изображений текста
\label{t5kk}}
\vspace*{2ex}

\begin{tabular}{|cp{55mm}|p{45mm}|p{45mm}|}
\hline
\multicolumn{2}{|c|}{Название процедуры обработки }&\multicolumn{1}{c|}{Входные данные}&\multicolumn{1}{c|}{Выходные данные}\\
\hline
1.&Выделение изображений строк текста (Sel\_Str)&Изображение текста&Изображения строк текста\\
\hline
2.&Формирование обучающей выборки (Create\_TrainSample)&Изображения строк текста, эталонные строки 
текста&Изображения знаков алфавита\\
\hline
3.&Оценивание характеристик классов (Est\_ClProp)&Изображения знаков алфавита&Характеристики классов, 
мат\-ри\-цы признак--объект\\
\hline
4.&Формирование выборок фрагментов (Create\_FragObs)&Изображения строк текста&Матрицы признак--объект\\
\hline
5.&Формирование эталонных строк текста (Create\_StandStr)&Изображения строк текста&Эталонные строки текста\\
\hline
6.&Оценивание порога черно-белого изоб\-ра\-же\-ния (Est\_BWT)&Изображения строк текста&Порог перехода к черно-белому изображению\\
\hline
7.&Оценивание элементов байе\-сов\-ско\-го классификатора (Est\_BayesCl)&Матрицы признак--объект&Вероятности 
классов, условные и безусловные распределения\\
\hline
8.&Подбор критических собственных значений (Est\_CrEVal)&Вероятности классов, условные и безусловные 
распределения, эталонные строки текста&Уточненные условные и безусловные распределения\\
\hline
9.&Распознавание строк текста\newline (Rec\_StrImage)&Изображения строк текста, порог перехода к черно-белому 
изображению, эталонные строки текста, вероятности классов, условные и безусловные распределения, характеристики 
классов&Строки текста, точность восстановления\\
\hline
\end{tabular}
\end{center}
\end{table*}


\begin{multicols}{2}
      
      Не все параметры алгоритмов распознавания могут быть качественно или вообще оценены 
по обучающей выборке. Например, обучающая выборка сама по себе не может дать оснований для 
выбора <<достойного>> критического значения~$d_0$ дисперсий главных компонент в 
комбинированных оценках плотностей распределения, так как она не содержит информации обо 
всем многообразии данных. Поэтому надо либо применять бутстреп-методы, либо дополнять 
технологию распознавания этапом перепроверки (уточнения, тренировки). В~связи с этим 
20~имеющихся строк пришлось выделить для перепроверки процедур восстановления текста. 
Оставшиеся же 20~строк использовались собственно для оценки качества получаемых решений. 
Общие характеристики уточняющей и контрольной выборки приведены в табл.~\ref{t4kk}.

      Исследование проводилось с помощью модулей обработки, кратко описанных в 
табл.~\ref{t5kk}.
     
      Оценивание качества восстановления текста осуществлялось следующим образом: 
распознанная последовательность знаков сравнивалась с эталонной и вычислялась точность 
восстановления. Последняя есть доля знаков в восстановленном тексте, 
<<совпавших>> со знаками эталонного текста; при этом <<совпавшими>> считаются те знаки, 
которые находятся с точностью до параметра толерантности на одинаковых позициях и совпадают 
по имени. Толерантность введена по практическим соображениям, так как при измерении 
положения знака в изображении исходного текста и восстановлении знака обязательно имеют 
место ошибки; этот параметр в данной работе принимает значение 3~пиксела. 

\begin{table*}\small %tabl6
\begin{center}
\Caption{Качество распознавания для различных значений параметров поиска знака в строке
\label{t6kk}}
\vspace*{2ex}

\tabcolsep=10pt
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{LS}}& \multicolumn{11}{c|}{RS}\\
\cline{2-12}

&0&1&2&3&4&5&6&7&8&9&10\\
\hline
0&545&821&896&888&876&857&804&711&665&645&631\\
1&627&906&911&895&876&862&805&709&668&648&632\\
2&745&925&{\bfseries\textit{928}}&899&880&863&805&703&662&648&631\\
\hline
\end{tabular}
\end{center}
\end{table*}
      
      При сравнительном анализе условий обработки данных возникает проблема учета разброса 
выборочных значений точности восстановления, так как в случае последовательного алгоритма 
распо-\linebreak\vspace*{-12pt}
\pagebreak

\begin{center} %fig2
\vspace*{3pt}
\mbox{%
\epsfxsize=78.652mm %.318mm
\epsfbox{kri-2.eps}
}
\end{center}
\vspace*{4pt}
%\begin{center}
{{\figurename~2}\ \ \small{Зависимость качества распознавания от RS при различных значениях~LS: \textit{1}~--- 
0; \textit{2}~--- 1; \textit{3}~--- 2}}
%\end{center}
\vspace*{9pt}

\bigskip
\addtocounter{figure}{1}


\noindent
знавания это бывает сделать трудно: объем исходных данных не всегда достаточен для 
формального статистического вывода, последовательность рас\-позна\-вания отдельных знаков не 
является последовательностью испытаний Бернулли. Поэтому при незначительных отличиях 
значений показателя качества будем обращать внимание на тенденции,
интерпретируя результаты 
анализа как указание на то,
что выявленная тенденция может иметь место, и поэтому представляет 
интерес. 
      
      Основным шагом алгоритма последовательного распознавания знаков в строке является 
следующий: после того как восстановлен очередной знак, от его позиции~$x_{t-1}$ в строке 
осуществляется продвижение на ширину~$w_{t-1}$ этого знака и поиск начала~$x_t$ очередного 
знака в диапазоне начальных позиций $[x_{t-1}+w_{t-1}-\mathrm{LS},\,x_{t-1}+w_{t-1}+\mathrm{RS}]$, здесь $t$~--- 
шаг последовательного просмотра строки. Таким образом, параметрами процесса 
последовательного распознавания являются следующие: $RS$~--- максимальное значение 
возможных сдвигов вправо и $LS$~--- максимальное значение возможных сдвигов влево. 
В~табл.~\ref{t6kk} для различных комбинаций указанных параметров приведены значения числа 
правильно распознанных знаков из уточняющей выборки. Следует обратить внимание на то, что 
значение $\mathrm{LS}=3$, а это минимально возможная ширина знака (см.\ табл.~3), подчас 
приводит к тому, что алгоритм возвращается к восстановлению уже распознанного знака, т.\,е.\ 
возникает зацикливание. Кроме этого, обращает на себя внимание рост качества распознавания 
при увеличении~LS, что может сопровождаться приписыванием приблизительно одним и тем же 
позициям строки как правильно, так и неправильно распознанных знаков (фактически нарушается 
принцип: при распознавании некоторого знака считается, что предшествующий знак был 
распознан правильно). По этим причинам значения для~LS, б$\acute{\mbox{о}}$льшие~2, не 
рассматривались. Распознавание проводилось с помощью метода, основанного на расстоянии типа 
хи-квадрат. В графическом виде данные табл.~\ref{t6kk} отражены на рис.~2. Для 
дальнейших экспериментов были приняты следующие значения: $\mathrm{LS}=2$ и $\mathrm{RS}=2$, 
соответствующая клетка табл.~\ref{t6kk} выделена курсивом. 

     
      Построение эффективного эмпирического байесовского классификатора связано с 
необходимостью выбора оценок следующих параметров:
      \begin{itemize}
      \item $m$ и~$d_0$ для условных распределений;
\item $m$ и  $d_0$ для безусловных распределений;
\item вероятности появления классов.
\end{itemize}

Эта задача осложняется тем, что вообще-то параметры~$m$ и~$d_0$ могут принимать для 
отдельных классов и областей различные значения. Понятно, что сделать это с помощью 
уточняющей выборки полностью не представляется возможным, поэтому в данной работе были 
реализованы следующие шаги:
\begin{itemize}
\item перебор при $m=0$ (это значение бралось одинаковым для условных и безусловных 
распределений) значений~$d_0$ (общих для всех классов и отдельно общих для всех областей) и 
оценка точности распознавания; выбор среди них той пары значений~$d_0$, которой 
соответствует наилучшая точность распознавания;
\item перебор при найденной наилучшей паре значений~$d_0$ различных комбинаций 
значений~$m$ для условного и безусловных распределений отдельно, выбор из них наилучшей 
комбинации с точки зрения точности распознавания. 
\end{itemize}


 \begin{figure*} %fig3
 \vspace*{1pt}
\begin{center} %fig3
\vspace*{12pt}
\mbox{%
\epsfxsize=163.814mm
\epsfbox{kri-3.eps}
}
\end{center}
\vspace*{-9pt}
\Caption{Зависимость качества распознавания от  $d_0$ для относительного (\textit{а})  и абсолютного 
(\textit{б}) способа задания при различных значениях~$m$: \textit{1}~--- 0; \textit{2}~--- 1; \textit{3}~--- 
2; \textit{4}~--- 3; \textit{5}~--- 4; \textit{6}~--- 5; \textit{7}~--- 6; \textit{8}~--- 7; \textit{9}~--- 8; 
\textit{10}~--- 9; \textit{11}~--- 10
\label{f3kri}}
\end{figure*}


      Перед реализацией описанных шагов были рассмотрены два приема задания~$d_0$: 
      \begin{itemize}
\item относительный, когда значение~$d_0$ определяется в терминах доли~$w$ дисперсии первой 
главной компоненты;
\item абсолютный, когда~$d_0$ принимает некоторое значение.
\end{itemize}
      
      Далее в виде графиков на рис.~3 приведены результаты оценивания точности 
распознавания изображений строк текста тренировочной выборки для различных значений~$d_0$ 
при относительном способе их задания с помощью~$w$\ \ (рис~3,\,\textit{а}) и при абсолютном способе (рис~3,\,\textit{б}) . 
Из них можно 
сделать вы-\linebreak вод о том, что абсолютный способ задания~$d_0$ имеет преимущества.
    
      Можно выделить три области значений $d_0$~--- малые, средние и большие. Для малых 
значений~$d_0$ характерно низкое качество распознавания, так как 
 оно опирается на малую по 
объему обучающую выборку (классы уплотнены). Как следствие, появление <<новых>> 
распознаваемых образов приводит к росту числа ошибок. Для больших значений~$d_0$ теряется 
индивидуальность классов (классы размыты), они начинают пересекаться, что опять же приводит 
к росту числа ошибок. Выбор средних значений~$d_0$ снижает значимость отмеченных факторов 
(либо объективно сложившейся уплотненности, либо привнесенной размытости), что приводит к 
улучшению качества распознавания.
      
      Результаты анализа точности распознавания для пар значений~$d_0$ для условных ($d_0 =$\linebreak 
$=100, 200, \ldots , 1600$) и безусловных ($d_0 =$\linebreak $= 100, 200, \ldots , 1200$) распределений говорят о 
том, что их выбор может стать источником повышения точности распознавания. В~частности, для 
дальнейших экспериментов можно предложить следующую пару значений: $d_0=100$ для 
безусловных распределений и  $d_0=300$ для условных распределений.
      
      Для выбранной пары значений~$d_0$ были опробованы различные комбинации значений 
$m\;=$\linebreak $=0, 2, \ldots , 14$ для условных и безусловных распределений. Из полученных результатов 
видно, что для условных распределений вполне можно обойтись моделью нормального 
многомерного распределения. Если для распределения образов из отдельных классов (для 
условных распределений) это достаточно естественно, то для распределения образов из областей 
(безусловных распределений) логичнее использовать смесь распределений (см.~\cite{1kk}). 
Необходимость усложнения модели для безусловных распределений подтверждается и 
экспериментами (рост показателей качества распознавания при увеличении значения~$m$) и 
должно стать предметом более тщательного исследования. По идее, качество распознавания 
должно достигать своего максимума для некоторого $m>0$, что определяется наличием двух 
противоречивых факторов, сопровождающих увеличение размерности базовой части 
комбинированной оценки:
      \begin{itemize}
      \item ростом качества аппроксимации истинных распределений данных;
\item снижением качества ядерной оценки плотности распределения.
\end{itemize}

      В итоге анализ качества распознавания для контрольной выборки проводился при 
следующих значениях параметров: для условных распределений $m=2$ и $d_0=300$, для 
безусловных распределений $m=14$ и $d_0=100$. Соответствующие результаты приведены в 
табл.~\ref{t7kk}. При реализации классифи-\linebreak\vspace*{-12pt}
\pagebreak

\end{multicols}
\begin{table}\small %tabl7
\begin{center}
\Caption{Эффективность различных методов распознавания
\label{t7kk}}
\vspace*{2ex}

\begin{tabular}{|p{86mm}|c|c|c|}
\hline
\multicolumn{1}{|c|}{Тип классификатора}&
\tabcolsep=0pt\begin{tabular}{c}Число\\ распознанных\\ знаков\end{tabular}&
\tabcolsep=0pt\begin{tabular}{c}Доля \\ распознанных\\ знаков,\\ \%\end{tabular}&
\tabcolsep=0pt\begin{tabular}{c}Время,\\  с\end{tabular}\\
\hline
Основанный на расстоянии типа сравнения классификаций&\hphantom{9}506&44&27\\
\hline
Основанный на расстоянии типа скалярного произведения&\hphantom{9}820&71&55\\
\hline
Основанный на расстоянии типа хи-квадрат&\hphantom{9}943&82&24\\
\hline
Эмпирический байесовский классификатор при равных вероятностях появления знаков из алфавита&
\multicolumn{1}{c|}{\raisebox{-6pt}[0pt][0pt]{1139}}&\multicolumn{1}{c|}{\raisebox{-6pt}[0pt][0pt]{99}}&
\multicolumn{1}{c|}{\raisebox{-6pt}[0pt][0pt]{109\hphantom{9}}}\\
\hline
Эмпирический байесовский классификатор при оценках вероятностей появления знаков из алфавита&
\multicolumn{1}{c|}{\raisebox{-6pt}[0pt][0pt]{1138}}&\multicolumn{1}{c|}{\raisebox{-6pt}[0pt][0pt]{99}}&
\multicolumn{1}{c|}{\raisebox{-6pt}[0pt][0pt]{106\hphantom{9}}}\\
      \hline
      \end{tabular}
      \end{center}
      \vspace*{-6pt}
      \end{table}
      
            \begin{figure} %fig4
      \vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=143.418mm
\epsfbox{kri-4.eps}
}
\end{center}
\vspace*{-9pt}
      \Caption{Интерактивная схема распознавания
       \label{f4kk}}
       \vspace*{-6pt}
       \end{figure}


\begin{multicols}{2}

\noindent
катора, основанного на расстоянии типа сравнения классификаций, для черно-белого порога было
 принято значение~162 (способ его нахождения дан 
в~\cite{6kk}). 
%
Полученные экспериментальные результаты говорят о том, что байесовский 
классификатор обладает явными преимуществами, при этом учет информации о распределении 
классов~--- вероятностях появления знаков~--- практически ничего не дает. Последнее, как и многие 
прежде полученные <<нечеткие>> выводы, скорее всего, объясняется большим числом 
распознанных знаков (1139 из 1150~знаков алфавита), что не позволяет проявиться 
преимуществам использования при реализации эмпирического байесовского классификатора всей 
имеющейся информации. 

\vspace*{-6pt}
 
\section{Заключение}

      Предложенная и рассмотренная в данной работе комбинированная оценка плотностей 
распределений, ее экспериментальный анализ показали высокую эффективность байесовского 
подхода при классификации объектов, имеющих различную размерность, а также 
работоспособность предложенных методов оценивания элементов байесовского эмпирического 
классификатора.
      
      При этом продемонстрирована необходимость развития технологии распознавания с 
обратной связью типа той, что представлена на рис.~\ref{f4kk}.
       
      В качестве ближайших задач можно пе\-ре\-чис\-лить:
      \begin{itemize}
\item исследование эффективности использования параметрической модели смеси распределений 
для представления базовой части оценки плотности;
\item исследование эффективности перехода от самооценки значения параметра сглаживания~$h$ 
типа~(\ref{e3.1kk}) к его оцениванию с помощью тренировочной выборки;
\item снижение временн$\acute{\mbox{о}}$й сложности алгоритмов, реализующих байесовский классификатор, с 
помощью учета особенностей комбинированной оценки плотности, наличия одинаковых 
элементов~(\ref{e1kk}) для отдельных классов и некоторой фиксированной области~$A_j$. 
\end{itemize}

{\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{9}

\bibitem{1kk}
\Au{Кривенко М.\,П.}
Распознавание элементов изображения, имеющих различные размеры~// Системы и средства 
информатики.~--- М.: ИПИ РАН, 2007. Вып.~17. С.~30--51.

\bibitem{2kk}
\Au{Деврой Л., Дьёрфи Л.}
Непараметрическое оценивание плотности: $L_1$-подход.~--- М.: Мир, 1988.  408~с.

\bibitem{3kk}
\Au{Izenman A.\,J.}
Modern multivariate statistical techniques: Regression, classification, and manifold learning.~--- Springer 
Verlag, 2008.  731~p.

\bibitem{4kk}
\Au{Simonoff J.\,S.}
Smoothing methods in statistics~// Springer series in statistics. 2nd printing, 1998.  338~p.

\bibitem{5kk}
\Au{Duin R.\,P.\,W.}
On the choice of smoothing parameters for Parzen estimators of probability density functions~// IEEE 
Transactions on Computers, 1976. Vol.~C-25. P.~1175--1179.

\label{end\stat}

\bibitem{6kk}
\Au{Кривенко М.\,П.}
Расщепление смеси вероятностных распределений на две составляющие~// Информатика и её 
применения, 2008. Т.~2. Вып.~4. С.~48--56.
 \end{thebibliography}
}
}

\end{multicols}
\def\stat{kruchin}

\def\tit{РАЗРАБОТКА ПАРАЛЛЕЛЬНЫХ ЭВРИСТИЧЕСКИХ АЛГОРИТМОВ ПОДБОРА ВЕСОВЫХ КОЭФФИЦИЕНТОВ
ИСКУССТВЕННОЙ НЕЙРОННОЙ СЕТИ}

\def\titkol{Разработка параллельных эвристических алгоритмов подбора весовых коэффициентов
ИНС}

\def\autkol{О.\,В.~Крючин}
\def\aut{О.\,В.~Крючин$^1$}

\titel{\tit}{\aut}{\autkol}{\titkol}

%{\renewcommand{\thefootnote}{\fnsymbol{footnote}}\footnotetext[1]
%{Исследования выполнены при частичной поддержке РФФИ, гранты 08-01-00567, 08-01-91205, 09-01-12180.}}

\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Тамбовский государственный университет им. Г.\,Р.~Державина, kryuchov@gmail.com}


\Abst{Описан градиентный алгоритм обучения искусственной нейронной сети (ИНС) и
основанные на нем эвристические алгоритмы QuickProp и RProp. Рассмотрена возможность
применения кластерных систем.}

\KW{искусственная нейронная сеть; эвристические алгоритмы обучения; кластерные системы}

 \vskip 18pt plus 9pt minus 6pt

 \thispagestyle{headings}

 \begin{multicols}{2}

 \label{st\stat}



\section{Введение}

 Известно, что задача обучения ИНС сводится к
минимизации функции
\begin{multline}
\varepsilon = \sum\limits_{i=1}^{N-1} \sum\limits_{j=0}^{P-1} \left ( y_{i,j}-d_{i,j}\right)={}\\
{}=
\sum\limits_{i=1}^{N-1} \sum\limits_{j=0}^{P-1} \left ( F(x,w,\mu)_j-d_{i,j}\right )\,,
\label{e1kr}
\end{multline}
где $y$, $d$~--- выходные значения ИНС и моделируемого объекта; $N$, $P$~--- число строк
и столбцов (выходных данных) в обучающей выборке; $\overline{x}$~--- вектор входных
значений; $\overline{w}$~--- вектор синоптических связей, $\mu$~--- вектор активационных
функций нейронов.

Таким образом, обучение состоит из двух частей: подбора
активационных функций (в который также следует включать подбор структуры сети) и
подбора весовых коэффициентов. В~настоящее время не существует общего алгоритма
определения структуры ИНС, подходящего для каждой рассматриваемой проблемы. Часто
такую структуру выбирают методом проб и ошибок, который зачастую отнимает у
исследователя много времени~\cite{2kr}. Поскольку для каждой структуры необходимо
подбирать значения весовых коэффициентов, то ускорение этого подбора должно ускорить
весь процесс обучения ИНС.

 На данный момент создано огромное количество алгоритмов эвристического типа,
представляющих собой в основном модификацию методов наи\-ско\-рей\-шего спуска или
сопряженных градиентов. Подобные модификации широко известных алгоритмов связаны с
внесением в них некоторых изменений, ускоряющих (по мнению авторов) процесс обучения.
Как правило, такие методы не имеют серьезного теоретического обоснования, особенно это
относится к процедуре подбора управ\-ля\-ющих параметров. Однако в таких алгоритмах\linebreak
реализуется личный опыт работы авторов с ней\-ронными сетями. К наиболее известным
эвристическим алгоритмам относятся QuickProp С.~Фальмана~\cite{8kr}, а также RProp (Resilient Propagation)\linebreak
М.~Ридмиллера и Х.~Брауна~\cite{7kr}.

 Целью данной работы является разработка параллельных версий эвристических
алгоритмов подбора весовых коэффициентов QuickProp и RProp.

\section{Существующие варианты распараллеливания}

 Традиционно параллельность нейросетевых вычислений понимается как
параллельное функ\-цио\-ни\-ро\-ва\-ние отдельных нейронов или групп нейронов. В лаборатории
искусственных нейронных\linebreak сетей ВНИИТФ предложен другой подход. Если в параллельной
вычислительной системе имеется $n$ процессоров, то обучающее множество делится на
$n$~равных частей. Процессоры взаимодействуют друг с другом по известной схеме
<<звезда>>, т.\,е.\ один из процессоров считается центральным. На очередном шаге обучения
каждый процессор вычисляет частичный градиент ошибки для своего подмножества
обучающих примеров. Затем эти частичные градиенты суммируются на центральном
процессоре. Далее центральный процессор вычисляет поправку к весам сети по формулам
того или иного оптимизирующего алгоритма, прибавляет ее и рассылает новое приближение
весов всем остальным процессорам. Центральный процессор принимает также решение о
продолжении или останове итераций, о чем он сообщает всем задействованным
процессорам.

 Именно такой вариант распараллеливания обуче\-ния feed-forward сети был реализован
в рамках комплекса нейросетевого моделирования Nimfa. Параллельные программы
написаны с использованием стандарта MPI. Реализованы параллельные варианты двух
алгоритмов оптимизации: стандартного градиентного спуска и RProp~\cite{12kr}. Для этих
методов время подсчета поправки к весам мало по сравнению со временем вычисления
суммарного градиента функции ошибки. Поэтому основным фактором, снижающим
эффективность параллельной программы, становится межпроцессорный обмен
данными~\cite{5kr}.

\section{Параллельная версия градиентного алгоритма}

 Алгоритмы QuickProp и RProp базируются на классическом градиентном алгоритме,
называемом также алгоритмом наискорейшего спуска. Поэтому для разработки
параллельных версий эвристических методов необходимо разработать параллельную версию
этого алгоритма.

 Суть метода заключается в вычислении вектора градиента и изменении весовых
коэффициентов в направлении антиградиента~\cite{9kr, 10kr}
\begin{align*}
\vec{\g} &= \fr{\partial \varepsilon}{\partial w} ={}\\
&\ \ \ {}=\left ( \fr{\partial \varepsilon (w_0)}{\partial
w_0},\,\fr{\partial \varepsilon (w_1)}{\partial w_1},\,\fr{\partial \varepsilon (w_2)}{\partial
w_2},\ldots , \fr{\partial \varepsilon (w_{l_w-1})}{\partial w_{w_{l_w-1}}}\right
)\,;\\
\g_i & = \fr{\partial \varepsilon (w_i)}{ \partial w_i} \underset{\Delta w_i\rightarrow 0}{=}
\fr{\varepsilon (w_i+\Delta w_i)-\varepsilon(w_i)}{\Delta w_i}\,.
\end{align*}

Следовательно, для вычисления $\partial \varepsilon (w_i)/\partial w_i$ необходимо определить
значение целевой функции при текущем значении весовых коэффициентов, а затем при
измененном $w_i=w_i+\Delta w_i$. После вычисления вектора градиента происходит
изменение весовых коэффициентов
\begin{equation*}
w_i = w_i+\Delta w_i = w_i-s\g_i=w_i-s\fr{\partial \varepsilon(w_i)}{\partial w_i}\,.
%\label{e4kr}
\end{equation*}

 Как можно заметить, для вычисления нового значения одного из весовых
коэффициентов используются значения остальных весовых коэффициентов, которые были
на предыдущей итерации. Исходя из этого можно сделать вывод, что элементы вектора
градиента могут быть вычислены одновременно, следовательно, этот вектор можно
разделить на $n$~частей (по числу процессоров), каж\-дая из которых вычисляется на
отдельном узле (для этого процессору необходимо лишь передать текущие значения весовых
коэффициентов). После окончания вычисления процессоры не возвращают полученные
результаты на ведущий, а изменяют значения приписанных к ним весовых коэффициентов, и
уже после этого возвращают результат (новые весовые коэффициенты). Следовательно,
каж\-дый вычислительный узел вычисляет~$l_w/n$ весовых коэффициентов.

 Таким образом, на каждой итерации происходит только две передачи данных~--- всех
весовых коэффициентов на все процессоры и части из них со всех узлов на
ведущий~\cite{3kr}.

\section{Параллельная версия алгоритма QuickProp}

 В алгоритме QuickProp изменение $i$-го весового коэффициента на $k$-й итерации
производится согласно правилу:
\begin{equation*}
\Delta w_i(k) = -s\left ( \g_i(k)+c_w w_i(k-1)\right) +q_i(k)\Delta w_i(k-1)\,,
%\label{e5kr}
\end{equation*}
где $\g_i(k)=\;\partial \varepsilon(w_i(k))/\partial
w_i(k)$~--- элемент вектора-градиента; $s$~--- коэффициент обучения; $c_w$~--- коэффициент минимизации значений
весовых коэффициентов; $q_i(k)$~--- коэффициент фактора момента.

Отличие от
классического градиентного метода заключается в наличии двух слагаемых~---
минимизатора значений весовых коэффициентов $sc_w w_i(k-1)$ и фактора момента
$q_i(k)\Delta w_i(k-1)$. Коэффициент минимизации~$c_w$ обычно принимает значение
$10^{-4}$ и служит для ослабления весовых связей (вплоть до полного разрыва), а фактор
момента необходим для адаптации алгоритма к текущим результатам обучения.
Коэффициент~$q_i$ уникален для каждого весового коэффициента и вычисляется в два
этапа. На первом определяется величина
\begin{equation}
q_i = \fr{\g_i(k)}{\g_i(k-1)-\g_i(k)}\,,
\label{e6kr}
\end{equation}
а на втором коэффициент момента принимает минимальное значение из~$q_i$
и~$q_{\max}$. В~качестве значения~$q_{\max}$ С.~Фальманом
предложено~1,75~\cite{8kr}.

 Существует также модифицированная форма алгоритма, отличающаяся уменьшением
числа управляющих параметров без потери эффектив\-ности. В~модифицированном
алгоритме формула~(\ref{e1kr}) заменяется на следующую:
%\noindent
\begin{align*}
\Delta w_i(k) &=q_i(k)\Delta w_i(k-1)& \mbox{при}\ \ \Delta w_i(k-1)&\not= 0\,;\\
\Delta w_i(k)& =s\g_i(k) & \mbox{при}\ \ \Delta w_i(k-1)&=0\,.
%\label{e7kr}
\end{align*}
То есть в случае, когда элемент вектора градиента $\g_i(k) =\partial\varepsilon(w_i(k))/\partial
w_i(k)$ принимает нулевое значение, на следующей итерации значение со\-от\-вет\-ст\-ву\-юще\-го
ему весового коэффициента вычисляется классическим градиентным методом~\cite{6kr}.

 Для распараллеливания была выбрана модифицированная версия алгоритма. Как уже
отмечалось, отличие от классического градиентного метода заключается в том, что в случае,
когда элемент вектора градиента не равен нулю, необходимо вы\-чис\-лять коэффициент
момента $q=\min (q,\,q_{\max})$, где $q$ определяется по формуле~(\ref{e6kr}).
Следовательно, кроме текущего значения элемента вектора-градиента, необходимо знать его
предыдущее значение, поэтому принцип распараллеливания, ис\-поль\-зу\-емый в классическом
градиентном методе, пригоден и здесь.

\section{Параллельная версия алгоритма RProp}

 Суть этого метода заключается в том, что игнорируются значения градиента, а
учитывается исключительно знак. Изменение весового коэффициента вычисляется по
формулам~\cite{7kr, 9kr}
\begin{align*}
\Delta w_i(k) & =-s_i(k)\theta_{\g}(i)\,;\\
s_i(k) & =
\begin{cases}
\min \left( q_a s_i(k-1),\,q_{\max}\right)\,,\\
\hspace*{20mm} \g_i(k)\g_i(k-1)>0\,;\\
\max \left( q_b s_i(k-1),\,q_{\min}\right)\,, \\
\hspace*{20mm} \g_i(k)\g_i(k-1)<0\,;\\
s_i(k-1)\,, \quad \hspace{1mm}\g_i(k) g_i(k-1)=0
\end{cases}
%\label{e9kr}
\\
\theta_{\g}(i) &=
\begin{cases}
\hphantom{-}1\,, & \g_i(k)>0\,;\\
\hphantom{-}0\,, & \g_i(k)=0\,;\\
-1\,, & \g_i(k)<0\,.
\end{cases}
%\label{e10kr}
\end{align*}
В работе~\cite{4kr} предложены следующие значения: $q_a=1{,}2$, $q_b=0{,}5$,
$q_{\min} =10^{-6}$, $q_{\max}=50$~\cite{13kr}.

 Как видно из приведенных выше формул, для вычисления нового значения весового
коэффициента необходимо знать: значения градиента на текущей и предыдущей итерации,
прошлое изменение весового коэффициента, величину коэффициента обучения (шага) на
предыдущей итерации. Отсюда можно сделать вывод, что принцип распараллеливания,
используемый для классического градиентного метода и алгоритма QuickProp, применим и к
этому методу.

\section{Теоретическая эффективность алгоритма}

 Время, необходимое для обучения на многопроцессорной машине, можно выразить
формулой
 \begin{equation*}
 \tau = \fr{t}{n}+\Psi\,,
% \label{e11kr}
 \end{equation*}
где $t$~--- время обучения на однопроцессорной машине; $n$~--- количество
вычислительных узлов.

Иными словами, существует некая величина~$\Psi$, которая представляет собой разницу
между тем вр$\acute{\mbox{е}}$менным выигрышем, который можно было бы ожидать (в~$n$~раз), и тем,
который реализуется на практике:
\begin{equation*}
\Psi = \eta+\phi+\upsilon+\gamma\,,
%\label{e12kr}
\end{equation*}
где $\eta$~--- время межпроцессорной передачи данных; $\psi$~--- время, необходимое на
подготовку данных; $\upsilon$~--- время ожидания одних вычислительных узлов другими;
$\gamma$~--- неучтенные временные затраты.

 Присутствие этих величин обусловлено тем, что параллельные алгоритмы отличаются
от последовательных. Поскольку очень редко удается пол\-ностью равномерно распределить нагрузку на\linebreak
вычислительные узлы, всегда на некоторых процессорах происходит
ожидание окончания выполнения работы какого-нибудь другого вычислительного узла.
Кроме того, средство межпроцессорной\linebreak передачи данных~--- MPI~--- пересылает данные
только определенного типа (массивы-указатели), которые отличаются от используемых в
алгоритмах\linebreak обучения высокоуровневых типов (контейнеры), и, соответственно, необходимо
конвертирование одного типа в другой, что также занимает определенное время. К~тому же
большое влияние оказывает аппаратная платформа. Средства межпроцессорной
коммуникации имеют различную скорость передачи, и поэтому выбор такого средства
(оптоволокно, витая пара, wi fi) также влияет на значение величины~$\Psi$.

 Величина~$\Psi$ показывает лишь абсолютную разницу между одно- и
многопроцессорными временными затратами, что не позволяет адекватно судить об
эффективности параллельной версии того или иного метода обучения. Поэтому необходима
новая величина, которую можно назвать коэффициентом эффективности параллельного
алгоритма
 \begin{equation*}
 \alpha = \fr{t}{n\tau}\,.
% \label{e13kr}
 \end{equation*}

\begin{figure*} %fig1
\vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=165.8756mm
\epsfbox{kru-1.eps}
}
\end{center}
\begin{minipage}[t]{81mm}
\vspace*{-15pt}
\Caption{Зависимость достигнутой погрешности от за\-тра\-чен\-ного времени
при подборе весовых коэффициентов последовательным методом
QuickProp~(\textit{1}) и параллельным, использующим
6~процессоров~(\textit{2})
\label{f1kr}}
\end{minipage}
\hfill
\begin{minipage}[t]{81mm}
%\vspace*{1pt}
\vspace*{-15pt}
\Caption{Зависимость достигнутой погрешности от за\-тра\-чен\-ного времени
при подборе весовых коэффициентов последовательным методом RProp~(\textit{1})
и параллельным, использующим 6~процессоров~(\textit{2})
\label{f2kr}}
\end{minipage}
\vspace*{-2pt}
\end{figure*}

 Эта величина показывает относительный временной выигрыш и для эффективных
алгоритмов должна быть в диапазоне от~0,7 до~1.

 Что касается сравнения эффективности описываемого способа распараллеливания с
методом, разработанным лабораторией ВНИИТФ, то можно заметить, что данный способ
требует меньшего объема межпроцессорных передач данных и в случае невысокой скорости
интерконнекта является более эффективным.

\vspace*{-5pt}

\section{Вычислительный эксперимент}

 Реализация описанных алгоритмов была выполнена в виде компьютерной программы,
написанной на языке С++. В~качестве средств межпроцессорной передачи данных была
использована библио\-те\-ка MPI. Эксперименты проводились на клас\-те\-ре Тамбовского
государственного университета им.~Г.\,Р.~Державина.

 Рисунки~\ref{f1kr} и~\ref{f2kr} показывают время, затраченное на обучение ИНС при
использовании последовательных и
параллельных алгоритмов подбора весовых коэффициентов. В~случае параллельных версий используются 6~процессоров. Как можно
видеть, в данном случае параллельные алгоритмы примерно в 5~раз быстрее
последовательных, т.\,е.\ коэффициент эффективности $\alpha\approx 0{,}83$.


\vspace*{-5pt}
\section{Заключение}

 Приведенные выше результаты показывают, что параллельные эвристические
алгоритмы весьма эффективны и могут быть использованы для подбора весовых
коэффициентов искусственных нейронных сетей.

\vspace*{-9pt}

{\small\frenchspacing
{\baselineskip=10pt
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{99}


\bibitem{2kr} %1
\Au{Арзамасцев А.\,А., Крючин~О.\,В., Азарова~П.\,А., Зенкова~Н.\,А.}
Универсальный программный комплекс для компьютерного моделирования на основе
искусственной нейронной сети с самоорганизацией структуры~// Вестн. Тамбовского
университета. Сер. Естественные и технические науки.~--- Тамбов, 2006. Т.~11. Вып.~4.
С.~564--570.

\bibitem{8kr} %2
\Au{Fahlman S.\,E.}
An empirical study of learning speed in back-propagation networks. Technical report.
CMU-CS-88-162.~--- Carnegie-Mellon University, 1988.

\bibitem{7kr} %3
\Au{Riedmiller M., Braun H.}
RProp~--- a fast adaptive learning algorithms. Technical Report.~--- Karlsruhe: University
Karlsruhe, 1992.

\bibitem{12kr} %4
\Au{Riedmiller M., Braun H.}
A direct adaptive method for faster backpropagation learning: The RProp algorithm~//
Proceedings of the IEEE International Conference on Neural Networks (ICNN~93), 1993.

\bibitem{5kr} %5
\Au{Федорова Н.\,Н., Терехов С.\,А.}
Параллельная реализация алгоритмов обучения нейронных сетей прямого
распространения с использованием стандарта MPI. Адрес в Интернете: {\sf
http://www.aconts.com/ pub/archive/ijcnn99\_p423\_rus.pdf}.

\bibitem{9kr} %6
\Au{Zadeh L.\,A.}
The concept of linguistic variable and its application to approximate reasoning. Part~1--3~//
Information Sci., 1975. P.~199--249.

\bibitem{10kr} %7
\Au{Gill P., Murray W., Wrights~M.}
Practical optimisation.~--- N.Y.: Academic Press, 1981.

\bibitem{3kr} %8
\Au{Крючин О.\,В., Арзамасцев А.\,А., Королев~А.\,Н., Горбачев~С.\,И., Семенов~Н.\,О.}
Универсальный симулятор, базирующийся на технологии искусственных нейронных
сетей, способный работать на параллельных машинах~// Вестн. Тамбовского
университета. Сер. Естественные и технические науки.~--- Тамбов, 2008. Т.~13. Вып.~5.
С.~372--375.

\bibitem{6kr} %9
\Au{Veith A.\,C., Holmes~G.\,A.}
A modified quickprop algorithm~// Neural Computation, 1991. Vol.~3. P.~310--311.

\bibitem{4kr} %10
\Au{Осовский С.}
Нейронные сети для обработки информации.~--- М.: Финансы и статистика, 2002. 344~с.

%\bibitem{11kr}
%\Au{Widrow B., Stearns S.}
%Adaptive signal processing.~--- N.Y.: Prentice Hall, 1985.

\label{end\stat}

\bibitem{13kr} %11
\Au{Riedmiller M.}
Untersuchungen zu konvergenz und generalisierungsverhalten uberwachter lernverfahren mit
dem SNNS~// Proceedings of the SNNS, 1993.

%\bibitem{1kr}
%\Au{Антонов А.\,С.}
%Введение в параллельные вычисления.~--- М.: МГУ, 2002.


 \end{thebibliography}
}
}

\end{multicols}
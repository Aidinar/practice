 \def\stat{dokukin}

\def\tit{МНОГОУРОВНЕВЫЕ МОДЕЛИ РЕШЕНИЯ МНОГОКЛАССОВЫХ ЗАДАЧ РАСПОЗНАВАНИЯ$^*$}

\def\titkol{Многоуровневые модели решения многоклассовых задач распознавания}

\def\aut{А.\,A.~Докукин$^1$, В.\,В.~Рязанов$^2$, О.\,В.~Шут$^3$}

\def\autkol{А.\,A.~Докукин, В.\,В.~Рязанов, О.\,В.~Шут}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Докукин А.\,A.}
\index{Рязанов В.\,В.}
\index{Шут О.\,В.}
\index{Dokukin A.\,A.}
\index{Ryazanov V.\,V.}
\index{Shut O.\,V.}


{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
{Работа выполнена при финансовой поддержке РФФИ (проект 15-51-04028) 
и~БРФФИ (проект Ф15РМ-037).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Федеральный исследовательский центр <<Информатика и~управ\-ле\-ние>> 
Российской академии наук, \mbox{dalex@ccas.ru}}
\footnotetext[2]{Московский физико-технический институт (государственный университет), 
\mbox{vasyarv@mail.ru}}
\footnotetext[3]{Белорусский государственный университет, \mbox{olgashut@tut.by}}

\vspace*{2pt}

\Abst{Проблема поиска набора бинарных подзадач для многоклассовых задач 
распознавания рассмотрена с~точки зрения алгебраического и~логического 
подходов к~распознаванию.
При этом теоретически исследованы границы применимости указанных подходов.
Так, рассмотрена связь корректности алгоритмов первого и~второго уровня, 
получено достаточное условие.
Кроме того, показана правомерность использования метода объектных резолюций 
для построения новых объектов на основе информации, заданной прецедентным способом.
В~качестве прикладных результатов предлагаются две модификации метода ECOC 
(error-correcting output codes~--- 
коды, исправляющие ошибки).
Первая заключается в~оптимизации набора бинарных подзадач с~учетом качества 
решающих их алгоритмов.
Вторая представляет собой развитие метода нечеткой объектной резолюции, 
где в~качестве кодового описания класса используется
 мультимножество кодов обучающих объектов.
Предложенные модификации позволяют в~различных условиях 
улучшать качество исходного метода, что продемонстрировано с~по\-мощью 
модельных и прикладных задач.}


\KW{распознавание; многоклассовая задача; ECOC; многоуровневый метод; корректность; 
алгебраический подход; логический подход; кодовое описание класса}

\DOI{10.14357/19922264170106}  

%\vspace*{-4pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}
  

\section{Введение}

В~настоящей статье рассматривается задача распознавания со~многими классами.
Будем использовать стандартную постановку задачи из~\cite{zhur1}.

\smallskip

\noindent
\textbf{Определение~1.}\
Назовем задачей распознавания~$Z$ следующую задачу.
Пусть задана обучающая выборка $\tilde{S}_t(Z)\hm=\{S_1,\ldots,S_m\}$, 
описанная векторами вещественных признаков, $S_i\hm=(a_{i1}, \ldots, a_{in})$, 
$i\hm=1,\ldots,m$.
Выборка разбита на~$l$~классов $K_1,\dots,K_l$.
Классификация объектов обучающей выборки задается 
информационными векторами ${\alpha_i}\hm=(\alpha_{i1}, \ldots, \alpha_{il})$, 
где $\alpha_{ij}$~--- значение предиката <<$S_i\hm\in K_j$>>.
Необходимо построить алгоритм~$A$, позволяющий вычислить классификацию нового 
объекта~$S$.

\smallskip

Если классы не пересекаются, то классификацию объектов можно 
задавать одним числом $\alpha_i\hm\in\{1,\ldots,l\}$, 
и~в~дальнейшем будет использоваться именно такая нотация.

Многоклассовой задачей распознавания будем называть задачу с $l\hm>2$.
Особенностью такой постановки является тот факт, что не все эффективные методы 
распознавания способны непосредственно решать многоклассовые задачи.
В~отличие от, например, метода ближайших соседей или алгоритма вычисления 
оценок~\cite{zhur1, zhur2}, такие методы, как метод опорных векторов~\cite{svm} 
или статистически взвешенные синдромы~\cite{sws}, приходится применять 
в~несколько этапов.
Сначала решается набор дихотомических подзадач, после чего их результаты 
объединяются и~интерпретируются в~терминах исходного набора классов.

Некоторые из~таких многоуровневых подходов достаточно очевидны.
Это так называемые <<один против всех>> (one-vs-all)~\cite{svm} 
и~<<каждый с~каждым>> (one-vs-one)~\cite{knerr}.
Применяются и более общие подходы.
Так, в~методе ECOC~\cite{Dietterich} используются произвольные разбиения 
множества классов на пары метаклассов.
Каждый класс при этом получает двоичный код, как и каждый распознаваемый объект.
Решение о~классификации принимается на~основании близости кодов.
Этот метод был, в~свою очередь, обобщен в~\cite{Allwein}.
Отличие заключается в~том, что при построении бинарных задач 
допускается исключение из рассмотрения исходных классов.
Таким образом, коды классов становятся троичными.
Это важное дополнение позволяет включить схему <<каждый с~каж\-дым>> в~общий метод.

В~настоящей работе предпринята попытка предложить свой метод построения 
набора бинарных подзадач и~обосновать его.
Первые главы посвяще-\linebreak\vspace*{-12pt}

\pagebreak

\noindent
ны теоретическому исследованию вопроса 
с~точки зрения алгебраического и~логического подходов к~распознаванию.
Затем предлагаются прикладные методы и~проводится тестирование.

\section{Корректность многоуровневого алгоритма} %\label{chapter:correctness}

Вопрос корректности является центральным в~алгебраической теории распознавания, 
созданной академиком Ю.\,И.~Журавлевым в~1970-х~гг.~\cite{zhur1, zhur2}.
Под корректностью понимается способность алгоритма безошибочно распознать 
заданную контрольную выборку.
Этому вопросу посвящено большое количество теоретических исследований.
Так, сам Ю.\,И.~Журавлев сформулировал теорему существования корректного 
алгоритма для задачи распознавания в~алгебраическом замыкании 
семейства алгоритмов вычисления оценок (АВО)~\cite{zhur1,zhur2} 
и~оценил его сложность, т.\,е.\ степень корректного полинома.
Эта оценка постепенно уточнялась его учениками, и~в~итоге 
точная оценка была получена А.\,Г.~Дьяконовым~\cite{djakonov}.
Таким образом, вопрос корректности в~одноуровневых схемах можно считать 
закрытым, как минимум, для семейства алгоритмов вычисления оценок.
Однако представляет интерес корректность многоуровневых схем, рассматриваемых 
в~данной работе.
Перейдем к~рассмотрению связи корректности алгоритмов на~разных уровнях 
и~для начала запишем несколько формальных определений.

\smallskip

\noindent
\textbf{Определение~2.}\
Пусть задана задача распознавания~$Z$ и контрольная выборка 
объектов $\tilde{S}_r(Z)\hm=\{S^1,\ldots,S^q\}$ с~известной классификацией 
$\alpha^t\hm\in\{1,\ldots,l\}$, т.\,е.\ выполняется предикат 
<<$S^t\hm\in K_{\alpha^t}$>>, $t\hm=1,\ldots,q$.
Будем называть алгоритм~$A$ корректным для задачи~$Z$ 
и~контрольной выборки $\tilde{S}_r(Z)$, если $A(S^t)\hm=\alpha^t$ для всех 
$t\hm=1,\ldots,q$.
Здесь $A(S^t)\hm\in\{1,\ldots,l,\Delta\}$~--- ответ алгоритма 
о~классификации объекта~$S^t$, который соответствует номеру класса или отказу 
от распознавания~---~$\Delta$.


\smallskip

\noindent
\textbf{Определение~3.}\
Пусть задана задача распознавания~$Z$ и~два непересекающихся подмножества 
множества классов $K^0\subset\{K_1,\ldots,K_l\}$, $K^1\hm\subset\{K_1,\ldots,K_l\}$, 
$K^0\cap K^1\hm=\emptyset$.
Назовем бинарной (дихотомической) подзадачей~$Z$ задачу распознавания~$Z^\prime$ 
следующего вида: $\tilde{S}_t(Z^\prime)\hm=\tilde{S}_t(Z)\cap (K^0\cup K^1)$,
 $\tilde{S}_r(Z^\prime)\hm=\tilde{S}_r(Z)\cap (K^0\cup K^1)$, классы соответствуют 
 метаклассам~$K^0$ и~$K^1$.
Будем говорить, что класс~$K_i$ активен в~бинарной подзадаче~$Z^\prime$, 
если $K_i\hm\in (K^0\cup K^1)$.
Бинарную подзадачу, в~которой все классы активны, будем называть полной.
Число активных классов будем называть рангом бинарной подзадачи~$r(Z_i)$.

\smallskip

Основным условием теоремы существования\linebreak
 корректного полинома над алгоритмами 
вы\-чис\-ления оценок~\cite{zhur1, zhur2} является попарная неизоморфность 
контрольных объектов, т.\,е.\ наличие для\linebreak любой пары контрольных объектов
такого обуча\-юще\-го, что хотя~бы по одному признаку расстояния от 
этих контрольных до него не~равны:
$\forall\ S^i,S^j\hm\in\tilde{S}_r(Z)$, $\exists\ S_k\in\tilde{S}_t(Z)$, 
$p\hm\in\{1,\ldots,n\}$, такие что $|a_{kp}\hm-a^i_p|\hm\neq|a_{kp}\hm-a^j_p|$.
Оно и попарное неравенство классов являются достаточным условием 
существования корректного алгоритма в~алгебраическом замыкании семейства 
АВО~\cite{dokukin2001}.
С~точки зрения теоремы существования рассматриваемая связь достаточно очевидна.
Несложно показать, что выполнение достаточных условий для исходной 
многоклассовой задачи не~гарантирует их выполнения для бинарных подзадач.
Подробно этот вопрос и~другие доказательства рассмотрены в~\cite{patrec}.

При этом также достаточно очевидно, что в~обратную сторону следствие выполняется.
Если набор бинарных подзадач содержит все контрольные объекты 
и~для них выполняются достаточные условия, то~для исходной многоклассовой 
задачи эти условия также выполняются и~корректный алгоритм существует.
Однако этот факт сам по~себе не~дает дополнительных конструктивных 
средств построения такого корректного алгоритма, кроме уже имеющихся 
в~теореме существования.
Исходное~же предположение состоит в~том, что двухуровневая схема позволит 
упростить такое построение.
Поэтому перейдем к~рассмотрению методов построения корректного 
многоклассового алгоритма на базе корректных двухклассовых слагаемых.

Рассмотрим следующую общую схему двухуровневого распознавания.


\smallskip

\noindent
\textbf{Определение~4.}\
Пусть задана задача распознавания~$Z$ и набор из~$W$ ее бинарных 
подзадач $Z_1,\ldots,Z_W$.
Назовем алгоритмом первого уровня алгоритмы~$A_i$, решающие соответственно 
подзадачи~$Z_i$, $i\hm=1,\ldots,W$.
Назовем алгоритмом второго уровня алгоритм~$A$, решающий задачу~$Z$ 
и~использующий для этого выходы алгоритмов первого уровня.

При этом будем называть вектор~$\gamma(K_i)$, где 
$$
\gamma(K_i)_j=
\begin{cases}
1\,,  &\ \mbox{если } K_i \in K^0_j\,;\\
-1\,, &\ \mbox{если } K_i \in K^1_j\,;\\ 
0  &\ \mbox{в~остальных~случаях}\,,
\end{cases}
$$
кодом класса~$K_i$, 
$i\hm=1,\ldots,l$, $j\hm=1,\ldots,W$.
Рангом класса $r(K_i)$ будем называть число бинарных подзадач, 
в~которых он активен, или $|\{\gamma(K_i)_j \,|\, \gamma(K_i)_j\hm\neq0$,
$j\hm=1,\ldots,W\}|$.

Аналогично определим код объекта $\gamma(S^t)$: 
$$
\gamma(S^t)_j= 
\begin{cases}
1\,,  &\ \mbox{если } K_{A_j(S^t)} \in K^0_j\,;\\
-1\,,  &\ \mbox{если } K_{A_j(S^t)} \in K^1_j\,;\\
0\,, &\ \mbox{если~произошел~отказ}\\
&\ \hspace*{17mm}\mbox{от~распознавания}\,,
\end{cases}
$$
$t\hm=1,\ldots,q$, $j\hm=1,\ldots,W$.


\smallskip

Рассмотрим многоклассовую задачу распознавания~$Z$
 и~набор бинарных подзадач $Z_1,\ldots,Z_W$.
Очевидно, если коды классов на~данном наборе попарно различаются 
и~бинарные подзадачи полны, то алгоритм, построенный по~схеме ECOC, будет корректен.

Сложность представляют неполные бинарные подзадачи, поскольку объекты 
игнорируемых классов будут получать произвольные оценки.
При этом если допустить в~наборе бинарных подзадач полные, то этот 
недостаток также легко исправить за~счет полных вспомогательных задач.

Таким образом, наибольшую сложность представляет случай, 
когда все бинарные подзадачи неполны.
К~тому же он представляет и наибольший интерес, поскольку позволяет 
упростить бинарные подзадачи за~счет сокращения количества объектов.
Перейдем к~его рассмотрению и~для опре\-де\-лен\-ности будем считать, 
что ранг всех бинарных подзадач одинаков.

\smallskip

\noindent
\textbf{Определение~5.}\
Назовем расстоянием между двумя классами $d(K_i, K_j)$ число бинарных 
подзадач, в~которых они оба активны и принадлежат различным метаклассам:
\begin{multline*}
d(K_i, K_j) = \left| \left\{ t\in\{1,\ldots,W\} \;|\; 
\gamma(K_i)_t\neq\gamma(K_j)_t,\right.\right.\\
\left.\left. \gamma(K_i)_t\neq 0,\; 
\gamma(K_j)_t\neq 0 \right\} \right|\,.
\end{multline*}


Сформулируем достаточное условие коррект\-ности алгоритма второго уровня.
Пусть задана задача распознавания~$Z$ и~набор бинарных подзадач $Z_1,\ldots,Z_W$, 
ранг всех бинарных подзадач одинаков и~равен $r\hm<l$.
Пусть также алгоритмы первого уровня $A_1,\ldots,A_W$ являются корректными 
на соответствующих бинарных подзадачах.


\smallskip
\noindent
\textbf{Теорема~1.}\
\textit{Если для любых двух классов разность их рангов меньше расстояния 
между ними, т.\,е.\ выполнено неравенство}:
\begin{equation*}
r(K_j) - r(K_i) < d(K_j,K_i),\ \forall\ i,j=1,\ldots,l,\; i\neq j\,,
\end{equation*}
\textit{то алгоритм второго уровня~$A$ является корректным}.


\smallskip

Заметим, что выполнение условия теоремы 
возможно только при положительном расстоянии между любыми двумя классами.

Обратное утверждение выполняется только для случая трех классов.
Действительно, есть следующие типы наборов бинарных подзадач, 
которые не~удовлетворяют условию утверждения: $\{\{1\}\mbox{--}\{2\}\}$, 
$\{\{1\}\mbox{--}\{2\}, \{1\}\mbox{--}\{3\}\}$ 
и~$\{\{1\}\mbox{--}\{2\}$, $\{1\}\mbox{--}\{2,3\}\}$.
Легко убедиться, что во~всех случаях алгоритм второго уровня будет некорректным.

Для случая четырех классов можно привести контрпример.
Возьмем следующие обучающие объекты: $S_1\hm=(-2, 2)\hm\in K_1$, 
$S_2\hm=(2, 2)\hm\in K_2$, $S_3\hm=(-2, -2)\hm\in K_3$ и~$S_4\hm=(2, -2)\hm\in K_4$.
Контрольные построим похожим образом: $S^1\hm=(-1, 1)\hm\in K_1$, 
$S^2=(1, 1)\hm\in K_2$, $S^3\hm=(-1, -1)\hm\in K_3$ и~$S^4\hm=(1, -1)\hm\in K_4$.
В~качестве набора бинарных подзадач возьмем схему <<каждый с~каждым>> 
и~исключим пару $\{\{1\}\mbox{--}\{4\}\}$, чтобы нарушить достаточное условие.
При этом достаточно несложно построить корректный алгоритм второго уровня.

Рассмотрим теперь случай, когда алгоритмы первого уровня некорректны.
Если ошибок незначительное количество, то можно модифицировать 
теорему~1 и~потребовать большего расстояния между классами, чтобы их исправить.
Кроме того, в~достаточном условии фигурирует верхняя оценка голосов 
за~чужие классы, что может позволить получить корректный результирующий алгоритм 
в~реальной ситуации при более равномерном распределении ошибок между классами.

Если~же ошибок значительное количество, то возникают следующие соображения.
Во-пер\-вых, теряется смысл использования теоремы~1 
и~появляется необходимость рассматривать уже не~коды классов, 
а~коды отдельных объектов, чтобы делать выводы о~корректности.
Во-вто\-рых, теряется смысл рассмотрения неполных подзадач.
Учитывая эти соображения, рассмотрим предельную, в~некотором смысле, ситуацию.


Пусть дана задача распознавания~$Z$ и~набор полных бинарных подзадач %\linebreak
 $Z_1,\ldots,Z_W$.
Пусть также дан набор алгоритмов первого уровня $A_1,\ldots,A_W$, решающих эти подзадачи.
Для простоты рас\-смот\-рим случай, когда эти алгоритмы не~дают отказов на~объектах
задачи~$Z$. %\linebreak
Построим следующую задачу~$Z^\prime$.
Число признаков~--- $W$, при этом все признаки бинарные.
Обуча\-ющую выборку составляют коды классов.
Объектами контрольной выборки являются коды объектов исходной задачи~$\tilde{S}_r(Z)$, 
полученные алгоритмами первого уровня.
При этом в~рамках одного класса повторяющиеся объекты исключаются.
Тогда справедлива следующая тео\-рема.

\smallskip

\noindent
\textbf{Теорема~2.}\
\textit{Пусть в~обучающей выборке~$\tilde{S}_t(Z^\prime)$ попарно различны классы,
тогда для существования алгоритма~$A^\prime$, корректного для задачи~$Z^\prime$, 
необходимо и~достаточно попарное различие объектов контрольной 
выборки}~$\tilde{S}_r(Z^\prime)$.


\smallskip

Таким образом, ошибки в результирующем алгоритме возникают, 
если невозможно предложить такую бинарную подзадачу, в~которой два 
объекта из~разных классов относились~бы к~разным метаклассам.
Объекты, неправильно классифициру\-емые по~этой причине, логично считать 
выбросами и~исключать из~рассмотрения.

Очевидно, такой подход обладает всеми недостатками теоремы существования.
Хотя решение и~строится конструктивно, оно громоздко, 
а~получаемый алгоритм склонен к~переобучению.
Прикладной метод двухуровневого распознавания, основанный на~оптимизации 
набора бинарных подзадач, будет предложен в~разд.~4 и~испытан на~практике.

\section{Метод резолюций}

В~задачах распознавания образов часто используются два способа представления информации:
 логический, представляющий собой описание объектов с~использованием логических 
 формул или правил,
 и~прецедентный, заключающийся в~непосредственном перечислении объектов и~классов, 
 которым принадлежат эти объекты.
Первый из~них\linebreak
 применяется в~продукционных экспертных системах~\cite{Giarratano}, 
второй характерен для большинства задач распознавания с~обучением.
Для решения \mbox{задач}, информация в~которых представлена логическим или прецедентным 
способом, соответственно используются метод резолюций и многочисленные алгоритмы 
распознавания, примером которых может служить семейство алгоритмов, 
описанное в~\cite{krasn1998}.
Существуют задачи, в~которых используются оба способа одновременно,~--- например, 
это задача медицинской диагностики~\cite{ablam2011}.
В~данной работе предлагается использовать аналогичный подход для построения 
многоуровневых схем распознавания.
При этом на~первом уровне могут использоваться любые алгоритмы, 
а~объединение их результатов будет производиться в~рамках логического подхода.

Переформулируем задачу распознавания образов~$Z$ в~общей постановке~\cite{ablam2011} 
с~использованием принятых в~логическом подходе обозначений.

На множестве объектов~$X$ произвольной природы заданы 
подмножества $ X_{1},\ldots,X_{l} $, на\-зы\-ва\-емые классами.
Задана также начальная информация~$I_{0}$ о~классах $X _{1},\ldots,X_{l} $.
Требуется указать алгоритм~$A$, определенный на~всем множестве~$X$, 
вычисля\-ющий на~основании информации~$I_{0}$ для произвольного объекта $ x \hm\in X $ 
результат, который может быть интерпретирован в~терминах принадлежности~$x$ 
к~классам~$X _{1},\ldots,X_{l} $.

Введем систему предикатов, характеризующую принадлежность произвольного 
объекта $x \hm\in X $ классам $X _{1},\ldots,X_{l} $:
$$
P_{i}(x)=\begin{cases}
1\,, &\ x \in X_{i}\,; \\
0\,, &\ x \notin X_{i}\,,
\end{cases}\qquad i=1,\ldots,l \,. 
$$

Информацию $I_{0}$ представим в~виде:
$$
I_{0}=\left\lbrace (x,P(x))|x \in X, P(x)=(P_{1}(x),\ldots,P_{l}(x)) 
\right\rbrace \,, 
$$
где $ P(x) $ называется информационным вектором, который 
сопоставляется объектам $x \hm\in X$~\cite{zhur1}.
Для каждого такого объекта, входящего в~описание~$I_{0}$, 
информационный вектор считается известным.
В~распознавании образов множество таких объектов называется выборкой.
Обычно ее принято разделять на~две части~\cite{zhur1}.
Первая часть называется обучающей выборкой 
и~используется для определения параметров или настройки процесса 
обучения алгоритмов распознавания.
Вторая часть называется контрольной выборкой 
и~используется для оценки качества работы алгоритмов.
Обозначим эти части через~$X^{0}$ и~$X^{q}$ соответственно.
К~введенным выборкам чаще всего предъявляется требование 
$X^{0} \cap X^{q}\hm= \emptyset$.

Будем говорить, что любой алгоритм~$A$, реша\-ющий задачу~$Z$, 
строит классификационный вектор $ P^{A}(x)\hm=(P^{A}_{1}(x),\ldots,P^{A}_{l}(x)) $,
 где $ P^{A}_{i}(x)\hm\in \left\lbrace 0,1\right\rbrace $.
Если $ P^{A}_{i}(x)\hm=1 $, то результат алгоритма интерпретируется как 
$x \hm\in X_{i} $; если $ P^{A}_{i}(x)\hm=0 $, то $x \notin X_{i} $.

Для оценки качества работы алгоритма~$A$ вводится функционал качества $\Phi_{A}(X)$, 
значения которого легко интерпретировались бы в~терминах совпадения или близости 
$P_{i}(x)$ и~$P^{A}_{i}(x)$.
В~общем случае чем ближе значение $\Phi_{A}(X^{q})$ к~$1$, тем 
меньше ошибок допускает алгоритм~$A$.
Поэтому из~нескольких алгоритмов предпочтительным считается тот, который имеет 
наибольшее значение функционала~$\Phi_{A}$.
В~предельном случае, если $\Phi_{A}(X^{q})\hm=1$, то алгоритм~$A$ решает задачу~$Z$ 
безошибочно.
Такие алгоритмы называются корректными в~алгебраической теории распознавания 
(см.\ определение~2).

Введем следующие определения и обозначения.
Пусть $S\hm=\left\lbrace s_{1},\ldots,s_{n} \right\rbrace $~--- 
множество всех признаков в~предметной области задачи~$Z$, где $n\hm<\infty $;
 $D_{j}$~--- множество значений признака~$s_{j}\hm\in S$.
Не нарушая общности, можно считать, что $D_{j}\hm=\left\lbrace 0,1,\ldots,|D_{j}|-1 
\right\rbrace $.
Обозначим
$$
 D=\left\lbrace 0,1,\ldots,\max\limits_{j} \left\lbrace |D_{j}|-1\right\rbrace 
 \right\rbrace =\left\lbrace 0,1,\ldots,k-1 \right\rbrace \,. 
 $$

В~дальнейшем предполагается, что все признаки принимают значения 
из~множества~$D$, где $k\hm\neq 1 $.
Объектом назовем отображение

\pagebreak

\noindent
$$
p\left(s_{1},\ldots,s_{n}\right)=\left(D^{p}_{1},\ldots,D^{p}_{n}\right)\,, 
$$
где $D^{p}_{j}\subset D $~--- множество значений признака $s_{j}\hm\in S $ объекта~$p$, 
причем $ D^{p}_{j}\hm\neq\emptyset $.
Объекты называются равными, если $ \forall\ j\, D^{p}_{j}\hm = D^{q}_{j} $.

Если $ D^{p}_{j}\hm=\emptyset$, то считается, что~$p$ не обладает признаком~$s_{j}$ 
и~потому не~рассматривается в~рамках задачи~$Z$.
В~общем случае для произвольного признака существует $ |\rho(D)|\hm=2^{k} $ 
возможных комбинаций его допустимых значений, где $\rho(D)$~--- множество 
всех подмножеств~$D$.
Поэтому будем считать, что $X\hm=(\rho(D))^{n} $.

Для удобства рассуждений назовем множество объектов набором.

В~\cite{patrec} показано существование кодировки, использование которой 
правомерно для описания как нормализованных, так и~ненормализованных объектов,
откуда следует эквивалентность прецедентного 
и~логического способов представления информации в~задаче~$Z$.

Опишем применение метода резолюций для решения задачи~$Z$.
Рассмотрим метод резолюций, исходными данными для которого являются 
не логические формулы, а~объекты из~$X$.
Этот модифицированный метод назовем методом объектных резолюций.

Объект~$r$ называется объектной резольвентой, построенной по~объектам~$p$ 
и~$q$, если значения признаков~$r$ удовлетворяют следующему условию:
$$
 D_{j}^{r}= \begin{cases}
D_{j}^{p} \bigcup D_{j}^{q}\,, & j=h\,; \\
D_{j}^{p} \bigcap D_{j}^{q}\,, & j \neq h\,,
\end{cases}
$$
где $h$~--- номер произвольного признака~$ s_{h}\hm\in S $.

Операцию построения объектной резольвенты обозначим $r\hm=Or_{h}(p,q) $.

Также в~\cite{patrec} показана правомерность использования метода объектных 
резолюций для построения новых объектов на~основе информации, заданной 
прецедентным способом.

Рассмотрим алгоритм использования метода объектных резолюций для решения задачи~$Z$.
Зафиксируем номер~$i$ класса~$X_{i}$ и~определим, принадлежит ли объект~$x$ 
этому классу.
Пусть $ X_{i}^{0}\hm=X^{0} \bigcap X_{i} $.

Алгоритм объектных резолюций~$A_{1} $:
\begin{description}
\item[Шаг~1.] Введем множество $Y_{i}\hm=X_{i}^{0} $.

\item[Шаг~2.] Если $ x \hm\in Y_{i} $, то переходим к~шагу~6, иначе~--- к~шагу~3.

\item[Шаг~3.] Выбираем из~$Y_{i}$ нерассмотренную тройку $(p,q,h) $, где~$p$ и~$q$~--- 
объекты; $h$~--- номер признака.
Если все такие тройки уже рассматривались, то переходим к~шагу~6.

\item[Шаг~4.] Вычисляем $r\hm=Or_{h}(p,q) $. Если $\exists\ j \ D_{j}^{r}\hm=\emptyset$, 
возвращаемся к~шагу~3.

\item[Шаг~5.] Если $r \notin Y_{i} $, то $ Y_{i} := Y_{i} \bigcup \left\lbrace r 
\right\rbrace$. Возвращаемся к~шагу~2.

\item[Шаг~6.] Алгоритм завершает работу.
\end{description}

Алгоритм~$ A_{1} $ можно применять как для прямого, так и для обратного вывода.
В~последнем случае на~шаге~1 вводится множество $ Y_{i}\hm=X_{i}^{0} \bigcup 
\left\lbrace x \right\rbrace$, а~в~качестве объекта~$x$ рассматривается несуществующий 
объект~$o$: $\exists\ j \ D_{j}^{o}\hm=\emptyset$.
Обратный вывод может использоваться, например, в~случае, когда число классов~$l$ 
рав\-но~2.
В~зависимости от~типа вывода результат работы алгоритма~$A_{1}$ можно 
интерпретировать следующим образом.
\begin{enumerate}[1.]
\item Прямой вывод: если алгоритм закончил работу из-за получения объекта~$x$, 
это значит, что набор $\mathrm{Norm}(X_{i}^{0})$ содержит объект~$x$.
Поэтому  $x \hm\in X_{i}$.

\item Обратный вывод: если алгоритм закончил работу из-за 
получения объекта~$o$, это значит, что $ \mathrm{Norm}(Y_{i})\hm=X $ , т.\,е.~$Y_{i}$ 
потенциально содержит все объекты из~$X$.
Поэтому $x \hm\notin X_{i}$.
\end{enumerate}

Если ни~один из~этих результатов не~получен, то с~по\-мощью данного 
алгоритма нельзя сделать никаких выводов о~принадлежности~$x$ классу~$X_{i}$.

Приведем результаты алгоритма~$A_{1}$ к~численному виду.
По~результатам работы алгоритма построим классификационный вектор
$$
A_{1}(x)=\left(P_{1}^{A_{1}}(x),\ldots,P_{l}^{A_{1}}(x)\right) \,,
$$
где 
$$
P_{i}^{A_{1}}(x) = \begin{cases}
1\,, &\ x \in X_{i}\,; \\
0\,, &\ x \notin X_{i}\,.
\end{cases} 
$$

Решение о~принадлежности~$x$ классу~$X_{i}$ принимается алгоритмом~$A_{1}$.

Опишем применение метода резолюций на~втором уровне многоуровневой схемы.
Для этого также построим новую задачу с~$W$ бинарными признаками, где 
объектами являются коды исходных объектов, полученные алгоритмами первого уровня.
Только, в~отличие от~разд.~2, не~будем исключать повторы~--- это позволит 
в~дальнейшем оценить степень вхождения объекта в~класс с~по\-мощью 
метода нечеткой резолюции,
 т.\,е.\ метода резолюций для решения задачи~$Z$ 
 в~случае, когда информация~$I_{0}$ задана с~помощью функций нечеткой логики.

Пусть $E $~--- произвольное множество.
Введем характеристическую функцию~$\mu_{E}(x) $,
 значения которой описывают степень принадлежности элемента~$x$ множеству~$E$: 
 $\mu_{E}(x) \hm\in [0,1]$.

Пусть $ E_{1}$ и~$ E_{2} $~--- нечеткие подмножества~$E$.
Рассмотрим следующие операции нечеткой логики~\cite{kofman}:
\begin{enumerate}[(1)]
\item дополнение:
$$
\mu_{\overline{E_{1}}} (x)= 1 - \mu_{E_{1}}(x)\,;
$$

\item пересечение:
$$
 \mu_{E_{1} \bigcap E_{2}}(x) = \min\{{\mu_{E_{1}}(x), \mu_{E_{2}}(x)}\}\,;
 $$ 
    
\item объединение:
$$ \mu_{E_{1} \bigcup E_{2}}(x) = \max\{{\mu_{E_{1}}(x), \mu_{E_{2}}(x)}\}\,.
$$
\end{enumerate}

Припишем каждому набору $V \subset X$ характеристическую функцию~$\mu_{V}(p)$, 
значение которой описывает степень принадлежности объекта~$p$ набору~$V$.
Приведем пример такой функции.
Пусть\linebreak
 объекты могут повторяться в~обучающей выборке~$X^{0}$.
Обозначим через~$N^{p}$ общее количество вхождений объекта~$p$ в~$X^{0}$, 
а~через $ N_{i}^{p} $~--- число вхождений~$p$ в~$X_{i}^{0} $.
Таким образом, $N^{p} \hm= \sum\nolimits_{i=1}^{l} N_{i}^{p}$.
Определим $\mu_{X_{i}}(p)$ следующим образом:
$$
\mu_{X_{i}}(p) = \fr {N_{i}^{p}} {N^{p}} \,.
$$
Тогда $\mu_{X_{i}}(p)\hm \in [0,1] $, что и~требуется.

Поскольку алгебра нормализованных объектов~$G^{\mathrm{norm}}$ 
изоморфна алгебре логических функций~$L_{k}$, то операции нечеткой 
логики справедливы и~для наборов:
\begin{align*}
\mu_{\overline{V}} (p) &= 1 - \mu_{V}(p) \,; \\
\mu_{V \bigcap W}(p) &= \min\{{\mu_{V}(x), \mu_{W}(p)}\}  \,; \\
\mu_{V \bigcup W}(p) &= \max\{{\mu_{V}(x), \mu_{W}(p)}\}  \,.
\end{align*}
Здесь $ V \subset X $ и~$W \subset X $~--- произвольные наборы.

Для нечеткой логики также существуют аналоги метода резолюций.
Один из~примеров такого аналога приведен в~\cite{lee}.
Рассмотрим обобщение метода объектных резолюций для случая 
нечеткого описания объектов.


\smallskip

\noindent
\textbf{Теорема~3.}\
\textit{Пусть заданы наборы $x_{1}$ и~$x_{2}$.
Построим $ r\hm=Or_{h}(x_1,x_2)$.
Тогда существует такое значение} $t \hm\in [0,1]$, 
\textit{что $ \mu_{x_{1}}(p)\hm > t$, $\mu_{x_{2}}(p) \hm> t$ и $\mu_{r}(p) \hm> t$}.


\smallskip

Несложно видеть, что утверждение данной тео\-ре\-мы можно обобщить для 
произвольного числа объектов.
Таким образом, для любых набора объектов~$V$ 
и~класса~$X_{i}$ существует такое $t \hm\in [0,1]$, 
которое можно принять в~качестве порогового значения, интерпретируемого 
в~терминах принадлежности заданному классу:
$$
 \forall\ p \in V (p \in X_{i} \Leftrightarrow \mu_{X_{i}}(p) > t) \,.
 $$

В~\cite{lee} показано, что при выполнении условий $\mu_{E}(x_{1}) \hm> 0{,}5$ 
и~$ \mu_{E}(x_{2})\hm > 0{,}5 $ справедливо следу\-ющее утверждение:
$$
\mu_{E}(r) > \mu_{E}\left(x_{1} \wedge x_{2}\right) \,, 
$$
а~следовательно, $\mu_{E}(r)\hm > 0{,}5$.
Поэтому пороговое значение $t\hm=0{,}5$ 
особенно удобно для применения на~практике.
%, так как в~этом случае 
%не~требуется знать значения $\mu_{z_{1}}(p)$ и~$\mu_{z_{2}}(p)$.

Опишем алгоритм использования нечеткого метода объектных резолюций для решения 
задачи~$Z$.
Пусть задано пороговое значение $t \hm\in [0,1]$.

\smallskip

Алгоритм~$ A_{1}^{f} $:
\begin{description}
\item[Шаг~1.] Применим алгоритм~$A_{1} $ к~$ X $.
Пусть $Y_{i} $~--- набор объектов, которые по~результатам выполнения алгоритма~$A_{1}$ 
считаются принадлежащими классу~$X_{i}$, $i\hm=1,\ldots,l $, т.\,е.\
 $\forall\ p \hm\in Y_{i}$, $\mu_{X_{i}}(p)\hm>t$.

\item[Шаг~2.] Для каждого объекта
 $ p\hm \in X \backslash \left(\bigcup\limits_{i=1}^{l} Y_{i}\right)$
выполним шаги~3--4.

\item[Шаг~3.] Для каждого класса~$X_{i}$ вычислим $\mu_{X_{i}}(p)$.

\item[Шаг~4.] Пусть $\{\mu_{Y_{v}}(p) \} \hm= \max\limits_{i} \{ \mu_{Y_{i}}(p) \}$,
 $w\hm = \max\limits_{v} \{v\}$.
Если $\mu_{X_{w}}(p) \hm> t$, добавим объект~$p$ в~$Y_{w}$:
$$ 
Y_{w} = Y_{w} \bigcup p\,. 
$$

\item[Шаг~5.] Алгоритм завершает работу.

\end{description}

Результаты алгоритма~$ A_{1}^{f}$ интерпретируются следующим образом: если 
по окончании работы алгоритма $p \hm\in Y_{i}$, то $p \hm\in X_{i}$.

Таким образом, алгоритм~$ A_{1}^{f}$ относит объект~$p$ к~классу, 
в~котором функция принадлежности этого объекта принимает максимальное значение, 
а~если таких классов несколько, выбирает среди них класс с~максимальным номером.

Покажем, что алгоритм~$ A_{1}^{f} $ работает не хуже алгоритма~$A_{1}$.
Определим $ P_{i}^{A_{1}^{f}}(p) $ следующим образом:
$$
 P_{i}^{A_{1}^{f}}(p) = \begin{cases}
1\,, &\ p \in X_{i}\,; \\
0\,, &\ p \notin X_{i}\,.
\end{cases} 
$$

В~определении $ P_{i}^{A_{1}^{f}}(p)$ решение о~принадлеж\-ности~$p$ 
классу~$X_{i}$ принимается алгоритмом~$A_{1}^{f}$.


\smallskip

\noindent
\textbf{Теорема~4.}\
$\Phi_{A_{1}^{f}}(X^{q}) \hm\geqslant \Phi_{A_{1}}(X^{q})$.

\smallskip


Метод нечеткой объектной резолюции с~некоторыми модификациями будет 
использован в~сле\-ду\-ющем разделе для объединения результатов алгоритмов первого уровня.
Описанием класса при этом будет служить мультимножество кодов 
объектов обучающей выборки, полученное набором алгоритмов, 
настроенных на~выбранном наборе бинарных подзадач.

\section{Прикладная реализация}

Перейдем к~рассмотрению прикладного метода построения наборов бинарных подзадач.
Будем учитывать, во-первых, полученные выше теоретические результаты.
Так, потребуем выполнения достаточного условия теоремы~1.
Для полностью определенных задач это будет означать различимость кодов классов.

Построим работу метода следующим образом.
Сначала получим некоторый набор бинарных подзадач.
Единственным условием на~этом этапе является различимость классов.
Затем найдем веса этих наборов, исходя из~оптимальности рас\-сто\-яний между 
кодами классов.
На~практике веса дихотомий часто обнуляются, что позволяет сократить исходный набор.

Исходный набор бинарных подзадач строится несколькими способами.
Самый очевидный из~них~--- конструировать случайные дихотомии, т.\,е.\
 полагать вектор $\beta \hm=(\beta_{1} ,\beta_{2},\ldots,\beta_{N})
 \hm\in \{ 0,1\}$ случайным.
Качество полученных дихотомий, или вероятность правильной классификации на~два 
класса, будет различаться.
При этом в~итоговом наборе выгодно иметь алгоритмы лучшего качества, 
совершающие меньшее число ошибок.

Для построения таких подзадач, называемых в~дальнейшем <<оптимальными>>, 
будем использовать метод наискорейшего спуска.
В~качестве начального приближения будет использоваться случайная дихотомия 
$\beta \hm=(\beta _{1} ,\beta _{2} ,\ldots,\beta _{N} )\in \{ 0,1\}$.\linebreak
Далее находится и изменяется компонента век\-тора~$\beta _{i}$ дихотомии, 
дающая максимальное увеличение критерия качества.
После единичной итерации вектор изменяется следующим образом:\linebreak 
$\beta \hm=(\beta_{1},\beta_{2},\ldots,\beta_{i-1}, 1-\beta_{i},
\beta_{i+1},\ldots,\beta_{N})$~--- и~процесс повторяется.
Если требуемой компоненты~$\beta_{i}$ не~существует, то процесс оптимизации 
заверша\-ется.

Исходный набор бинарных подзадач составляется из~оптимальных дихотомий.
В~ряде случаев алгоритм оптимизации оказывается не~в~состоянии обеспечить 
разделимость классов.
Тогда кодовая матрица пополняется случайными.

Предлагается использовать два подхода к обработке первичного набора
 бинарных подзадач.
В~рамках первого этот набор еще раз оптимизируется с~целью обеспечения 
максимального расстояния между кодами классов.
Для этого вводятся веса дихотомий и рассматривается следующая задача оптимизации.
Пусть ${\left\| \alpha _{ij} \right\|_{l \times W}}$~--- кодовая матрица, 
полученная путем поиска дихотомий, где $l$~--- число классов исходной выборки, 
а~$W$~--- число дихотомий:

\noindent
\begin{align*}
\sum\limits_{j = 1}^W {\left| \alpha _{\nu j} - \alpha _{\mu j} \right|} {x_j} &\ge y 
\enskip
 \forall\ \nu,\mu;\,\nu  > \mu;\enskip \nu,\,\mu  = 1,\ldots,l\,; \\
\sum\limits_{j = 1}^W {{x_j}} & = W\,; \\
y &\rightarrow  \max\,.
\end{align*}
Кроме отбора существенных бинарных подзадач, т.\,е.\
 подзадач, у~которых $x_i\hm>0$, найденные веса будем использовать непосредственно 
 при расчете функции близости кода объекта к~коду класса:
\begin{equation*}
d(S^t,K_j) = \sum\limits_{j = 1}^W {\left| {{\alpha _{i j}} - 
{\beta _{j}}} \right|} {x_j}\,,
\end{equation*}
где $\beta$~--- код объекта~$S^t$, $\gamma(S^t)\hm=\beta_j$.
Этот метод будем называть методом оптимизации дихотомий с~весами (ОДВ).

Второй подход основан на~методе НОР (нечеткой объектной резолюции).
Вместо кода класса\linebreak введем его кодовое описание, вычисленное как 
мультимножество кодов обучающих объектов, полученных тем~же набором алгоритмов.
Пусть класс~$K_j$ описан набором пар $\{\gamma_{ji}, \nu_{ji}\}$, $i\hm=1,\ldots,W_j$,
 где $\gamma_{ji}\hm=\gamma(S)$; $S\hm\in K_j\cap \tilde{S}_t(Z)$~--- коды 
 объектов класса~$K_j$ обучающей выборки;
 $\nu_{ji}$~--- час\-то\-та кода~$\gamma_{ji}$ в~описании класса~$K_j$: 
 $$
 \nu_{ji}=\fr{{|\{S|S\hm\in K_j\cap \tilde{S}_t(Z),
 \gamma(S)=\gamma_{ji}\}|}}{{|K_j\cap \tilde{S}_t(Z)|}}\,;
 $$
 $W_j$~--- число различных кодов в~описании класса~$K_j$.
Тогда оценку произвольного объекта~$S$ за~класс~$K_j$ будем вычислять по~формуле:
$$
\Gamma_j(S) = \sum\limits_{i=1}^{W_j}\nu_{ji}\fr{1}
{\left(1+d(\gamma(S),\gamma_{ji})\right)^2}\,,
$$
где $d(\gamma_1, \gamma_2)$ обозначает расстояние Хэмминга между 
кодами~$\gamma_1$ и~$\gamma_2$.



Этот подход будем называть методом кодовых описаний классов, или КОК.

Эксперименты для оценки эффективности предлагаемых подходов проводились
  с~модельной задачей model2 (12~классов) и двумя реальными задачами:
  предсказание года (10~классов, year)
  и~распознавание букв (26~классов, letter recognition, UCI Machine Learning 
  Repository~\cite{uci}).
При этом для каждой задачи генерировались случайные наборы бинарных подзадач 
заданных мощностей, которые затем использовались для обучения 
и~проверки на~независимой выборке предлагаемых подходов: ОДВ и~КОК.
В~качестве ориентира используется алгоритм, основанный на~поиске ближайшего
 кода\linebreak\vspace*{-12pt}
 
 \pagebreak
 
{ \small \begin{center}  %fig2
\begin{tabular}{|c|c|c|c|c|}
\multicolumn{5}{c}{Результаты экспериментов}\\
\multicolumn{5}{c}{\ }\\[-6pt]
  \hline
    Задача & \tabcolsep=0pt
    \begin{tabular}{c}Число\\ подзадач\end{tabular} & ECOC & КОК & ОДВ \\
  \hline
 & 20 & 68,0 & 68,9 & 66,4 \\
    & 40 & 69,1 & 69,9 & 69,7 \\
    model2  & 60 & 70,2 & 70,1 & 69,7 \\
    & 80 & 70,5 & 70,6 & 70,9 \\
    & 100\hphantom{9} & 70,8 & 70,7 & 71,0 \\
  \hline
 & 20 & 85,0 & 85,3 & 84,2 \\
    & 40 & 86,2 & 86,2 & 85,5 \\
  letter    & 60 & 86,5 & 86,5 & 86,3 \\
    & 80 & 86,7 & 86,8 & 86,7 \\
    & 100\hphantom{9} & 87,0 & 87,0 & 86,8 \\
  \hline
 \multicolumn{1}{|c|}{\raisebox{-18pt}[0pt][0pt]{ year }}  & 20 & 41,8 & 40,4 & 44,8 \\
    & 40 & 40,1 & 40,4 & 40,5 \\
    & 60 & 39,7 & 47,4 & 40,3 \\
    & 80 & 42,1 & 43,6 & 44,5 \\
  \hline
\end{tabular}
\end{center}}

\vspace*{6pt}


 
 \noindent
  класса, т.\,е.\ реализация метода ECOC, выполненная авторами работы.
Для каждой задачи и каждой мощности генерировались по~три случайных набора, 
после чего результаты усреднялись.
Бинарные подзадачи решались с~помощью метода опорных векторов из~пакета 
scikit-learn~\cite{scikit}.

Модельная задача заслуживает отдельного упоминания.
На двумерной плоскости генерировалось~20~выборок из~нормального распределения 
с~центрами, расположенными в~пять столбцов и~четыре строки.
После чего некоторые пары и~тройки получившихся скоплений точек объединялись 
в~один класс. Всего таких классов было~12.

Результаты экспериментов показаны в~таблице.
Задачи model2 и~letter демонстрируют похожую тенденцию~--- при небольшом 
количестве закономерностей метод ОДВ обычно отстает, но с~его ростом 
отставание уменьшается и~даже сменяется опережением.
Такое поведение метода достаточно ожидаемо, так как 
чем больше исходного материала для оптимизации, тем проще отобрать хорошие подзадачи.
В~этих экспериментах отбиралось примерно две трети подзадач.


Вместе с~тем при небольшом числе исходных случайных подзадач метод КОК 
демонстрирует преимущество перед простым методом.
Затем,\linebreak с~рос\-том их числа, преимущество ослабевает.
Исключение составила задача~{year}, в~которой качество 
менялось непредсказуемо.
Причем даже в~рамках одного метода и~одного набора подзадач 
дисперсия качества распознавания была очень высока.
С~другой стороны, это позволило получить наибольший абсолютный 
выигрыш от~применения пред\-ла\-га\-емых подходов.
Такое поведение алгоритмов на~данной задаче требует дополнительного 
исследования и~объяснения.

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{zhur1}
    \Au{Журавлёв~Ю.\,И.}
{Корректные алгебры над множеством некорректных (эвристических) алгоритмов~I}~//
    Кибернетика,  1977. №\,4. С.~14--21.
\bibitem{zhur2}
    \Au{Журавлёв~Ю.\,И.}
{Корректные алгебры над множеством некорректных (эвристических) алгоритмов~II}~//
    Кибернетика,  1977. №\,6. С.~21--27.
\bibitem{svm}
\Au{Cortes~C., Vapnik~V.}
{Support-vector networks}~//
    Mach. Learn.,  1995. Vol.~20. No.\,3. P.~273--297.
\bibitem{sws}
    \Au{Кузнецов~В.\,А., Сенько~О.\,В., Кузнецова~А.\,В., Семенова~Л.\,П., 
    Алещенко~А.\,В., Гладышева~Т.\,Б., Ившина~А.\,В.} 
    Распознавание нечетких сис\-тем по методу статистически взвешенных синдромов 
    и~его применение для иммуногематологической характеристики нормы 
    и~хронической патологии~// Хим. физика, 1996.  Т.~15. №\,1.  С.~81--100.


\bibitem{knerr}
    \Au{Knerr~S., Personnaz~L., Dreyfus~G.}
{ Single-layer learning revisited: A~stepwise procedure for building and training neural network}~//
    Neurocomputing: Algorithms, architectures and applications~/
    Eds. F.\,F.~Soulie, J.~Herault.~--- NATO ASI subser. F.~---  Berlin--Heidelberg:
    Springer-Verlag, 1990. Vol.\,68. 
    P.~41--50.
\bibitem{Dietterich}
 \Au{Dietterich~T.\,G., Bakiri~G.}
{Solving multiclass learning problems via error-correcting output codes}~//
    J.~Artif. Intell. Res., 1995. No.\,2. P.~263--286.
\bibitem{Allwein}
  \Au{Allwein~E., Shapire~R., Singer~Y.}
{Reducing multi-class to binary: A unifying approach for margin classifiers}~//
    J.~Mach. Learn. Res., 2000. Vol.~1. No.\,1. P.~113--141.
\bibitem{djakonov}
    \Au{Дьяконов~А.\,Г.}
    {Алгебра над алгоритмами вычисления оценок: минимальная степень корректного алгоритма}~//
    Ж.~вычисл. матем. и матем. физ.,  2005. Т.~45. №\,6. С.~1134--1145.
\bibitem{dokukin2001}
    \Au{Докукин~А.\,А.}
{О~построении в~алгебраическом замыкании одного алгоритма распознавания}~//
    Ж.~вычисл. матем. и матем. физ., 2001. Т.~41. №\,12. С.~1811--1815.
\bibitem{patrec}
    \Au{Dokukin~A., Ryazanov~V., Shut~O.}
{Multilevel models for solution of multiclass recognition problems}~//
    Pattern Recognition Image Anal., 2016. Vol.~26. No.\,3. P.~461--473.

\bibitem{Giarratano} %11
    \Au{Джарратано~Дж., Райли~Г.}
Экспертные системы: принципы разработки и программирование~/
Пер. с~англ.~--- М.: Вильямс, 2007. 
1152~с. (\Au{Giarratano~J.\,C., Riley~G.\,D.} {Expert systems: 
Principles and programming}.~---  Boston, MA, USA: PWS Publ. Co., 2004. 856~p.)
\bibitem{krasn1998}
    \Au{Краснопрошин~В.\,В., Образцов~В.\,А.}
{Распознавание с~обучением как задача выбора}~//
    Цифровая обработка изображений, 1998. С.~80--94.
\bibitem{ablam2011}
    \Au{Абламейко~С.\,В., Краснопрошин~В.\,В., Образцов~В.\,А.}
{Модели и технологии распознавания образов с~приложением в~интеллектуальном анализе данных}~//
    Вестник БГУ. Сер.~1: Физика. Математика. Информатика, 2011. №\,3. С.~62--72.
\bibitem{kofman}
    \Au{Кофман~А.}
Введение в~теорию нечетких множеств~/
Пер. с~англ.~--- М.: Радио и связь, 1982. 432~с.
(\Au{Kaufman~A.} {Introduction to the theory of fuzzy subsets.}~---
New York, NY, USA: Academic Press, 1975.
432~p.)
\bibitem{lee}
    \Au{Lee~R.\,C.\,T.}
{Fuzzy logic and the resolution principle}~//
    J.~ACM, 1972. Vol.~19. No.\,1. P.~109--119.

%\bibitem{berger}
% \Au{Berger~A.}
%    {Error-correcting output coding for text classification}~//
%    IJCAI'99: Workshop on Machine Learning for Information Filtering Proceedings, 1999. 8~p.


\bibitem{uci}
    \Au{Lichman~M.}
    {{UCI} machine learning repository}.~--- Irvine, CA, USA: University of 
    California, School of Information and Computer Science, 2013. 
    {\sf http://archive. ics.uci.edu/ml}.
\bibitem{scikit}
    \Au{Pedregosa~F., Varoquaux~G., Gramfort~A., Michel~V., Thirion~B., Grisel~O., 
    Blondel~M., Prettenhofer~P.,
         Weiss~R., Dubourg~V., Vanderplas~J., Passos~A., Cournapeau~D., Brucher~M., 
         Perrot~M., Duchesnay~E.}
{Scikit-learn: Machine learning in {P}ython}~//
{J.~Mach. Learn. Res.}, 2011. Vol.~12. P.~2825--2830.


 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 2.08.16}}

\vspace*{8pt}

%\newpage

%\vspace*{-24pt}

\hrule

\vspace*{2pt}

\hrule

%\vspace*{8pt}


\def\tit{MULTILEVEL MODELS FOR~PATTERN RECOGNITION TASKS WITH~MULTIPLE CLASSES}

\def\titkol{Multilevel models for~pattern recognition tasks with~multiple classes}

\def\aut{A.\,A.~Dokukin$^1$, V.\,V.~Ryazanov$^2$, and~O.\,V.~Shut$^3$}

\def\autkol{A.\,A.~Dokukin, V.\,V.~Ryazanov, and~O.\,V.~Shut}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-9pt}

\noindent
$^1$Federal Research Center 
``Computer Science and Control'' of Russian Academy of Sciences,
 40~Vavilov Str.,\linebreak
 $\hphantom{^1}$Moscow, 119333, Russian Federation
 
 \noindent
 $^2$Moscow Institute of Physics and Technology, 9~Institutskiy Per., 
Dolgoprudny, Moscow Region 141700, Russian\linebreak
 $\hphantom{^1}$Federation


 \noindent
$^3$Belarusian State University, 4~Nezavisimosti Av., 
Minsk 220030, Republic of Belarus



\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2017\ \ \ volume~11\ \ \ issue\ 1}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2017\ \ \ volume~11\ \ \ issue\ 1
\hfill \textbf{\thepage}}}

\vspace*{3pt}



\Abste{The problem of choosing binary subtasks for recognition tasks with multiple 
classes is considered from the points of view of the algebraic and logical approaches 
to recognition. The limits of their applicability were studied theoretically. The 
sufficient condition of correctness of algorithms is stated as a~result of research 
of dependency between the first and the second level algorithms. Additionally, 
the paper proves that the object resolution method is applicable to constructing 
new objects using the precedent information. As an applied result, two modifications 
of the ECOC (error-correcting output codes)
method are proposed. The first one is based on optimization of the binary 
subtasks set. The second one develops ideas of the fuzzy object resolution method 
with classes described by multisets of codes of their precedents. The proposed 
modifications make it possible to increase the initial method's quality in 
various situations, which is demonstrated by the example of model and 
real-world tasks.}

\KWE{classification; multiclass task; ECOC; multilevel method; correctness; algebraic
approach; logical approach; code class description}



\DOI{10.14357/19922264170106}  

\vspace*{-9pt}

\Ack
\noindent
The work is supported by the Russian Foundation for Basic Research 
(grant No.\,15-51-04028) and BRFBR (grant No.\,F15PM-037).



%\vspace*{3pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}

\bibitem{1-dok-1}
\Aue{Zhuravlev, Yu.\,I.} 1977. Korrektnye algebry nad mno\-zhe\-st\-vom nekorrektnykh 
(evristicheskikh) algoritmov~I 
[Correct algebras over sets of incorrect (heuristic) algorithms~I]. 
\textit{Kibernetika} [Cybernetics] 4:14--21.
\bibitem{2-dok-1}
\Aue{Zhuravlev, Yu.\,I.} 1977. Korrektnye algebry nad mno\-zhe\-st\-vom nekorrektnykh 
(evristicheskikh) algoritmov~II 
[Correct algebras over sets of incorrect (heuristic) algorithms~II].
\textit{Kibernetika} [Cybernetics] 6:21--27.
\bibitem{3-dok-1}
\Aue{Cortes, C., and V.~Vapnik}. 1995. Support-vector networks. 
\textit{Mach. Learn.} 3(20):273--297.
\bibitem{4-dok-1}
\Aue{Kuznetsov, V.\,A., O.\,V.~Senko, A.\,V.~Kuznetsova, L.\,P.~Semenova,
A.\,V.~Aleshchenko, T.\,B.~Gladysheva, and
A.\,V.~Ivshina.} 1996. 
Raspoznavanie nechetkikh sistem po metodu statisticheski vzveshennykh sindromov 
i~ego primenenie dlya immunogematologicheskoy kha\-rak\-te\-ri\-sti\-ki normy i~khronicheskoy 
patologii
[Recognition of fuzzy systems by method of statistically weighed syndromes 
and its using for immunological and hematological norm and chronic pathology].
\textit{Khim. Fiz.} 15(1):81--100.
\bibitem{5-dok-1}
\Aue{Knerr, S., L.~Personnaz, and G.~Dreyfus}. 1990. 
Single-layer learning revisited: A~stepwise procedure for building and training 
neural network. 
\textit{Neurocomputing: Algorithms, architectures and applications}. Eds. F.\,F.~Soulie and 
J.~Herault.
NATO ASI subser. F. Berlin--Heidelberg:
    Springer-Verlag. 68:41--50.

\bibitem{6-dok-1}
\Aue{Dietterich, T.\,G., and G.~Bakiri}. 1995. Solving multiclass 
learning problems via error-correcting output codes. 
\textit{J.~Artif. Intell. Res.} 2:263--286.
\bibitem{7-dok-1}
\Aue{Allwein, E., R.~Shapire, and Y.~Singer}. 2000. Reducing multi-class to binary: 
A~unifying approach for margin classifiers. 
\textit{J.~Mach. Learn. Res.} 1(1):113--141.
\bibitem{8-dok-1}
\Aue{D'yakonov, A.\,G.} 2005. Algebra over estimation algorithms: 
The minimal degree of correct algorithms.
\textit{Comp. Math. Math. Phys.} 45(6):1095--1106.
\bibitem{9-dok-1}
\Aue{Dokukin, A.\,A.} 2001. The construction of a recognition algorithm in 
the algebraic closure. 
\textit{Comp. Math. Math. Phys.} 41(12):1907--1911.
\bibitem{10-dok-1}
\Aue{Dokukin, A., V.~Ryazanov, and O.~Shut}. 2016. Multilevel
 models for solution of multiclass recognition problems. 
 \textit{Pattern Recognition Image Anal.} 26(3):461--473.
\bibitem{11-dok-1}
\Aue{Giarratano, J.\,C., and G.\,D.~Riley}. 2004. \textit{Expert systems: 
Principles and programming}.  Boston, MA: PWS Publ. Co. 856~p.
\bibitem{12-dok-1}
\Aue{Krasnoproshin, V.\,V., and V.\,A.~Obraztsov}. 1998. 
Ras\-po\-zna\-va\-nie s~obucheniem kak zadacha vybora
[Supervised recognition as selection problem]. 
\textit{Tsifrovaya obrabotka izobrazheniy} [Digital image processing]. 
Minsk: ITK. 80--94.
\bibitem{13-dok-1}
\Aue{Ablamejko, S.\,V., V.\,V.~Krasnoproshin, and V.\,A.~Ob\-raz\-tsov}. 
2011. Modeli i~tehnologii ras\-po\-zna\-va\-niya obrazov s~prilozheniem v~intellektual'nom
analize dannykh [\mbox{Models} and technologies of pattern recognition with 
application to data mining]. 
\textit{Vestnik BGU. Ser.~1, Fizika, Ma\-te\-ma\-ti\-ka, Informatika}
[BSU Herald. Ser.~1, Physics, Mathematics, Informatics] 3:62--72.
\bibitem{14-dok-1}
\Aue{Kaufman, A.} 1975. \textit{Introduction to the theory of fuzzy subsets.}
New York, NY: Academic Press.
432~p.
\bibitem{15-dok-1}
\Aue{Lee, R.\,C.\,T.} 1972. Fuzzy logic and the resolution principle. 
\textit{J.~ACM} 19(1):109--119.
%\bibitem{16-dok-1}
%\Aue{Berger, A.} 1999. Error-correcting output coding for text classification. 
%\textit{IJCAI: Workshop on Machine Learning For Information Filtering Proceedings}. 
%Stockholm. 8~p. 
\bibitem{17-dok-1}
\Aue{Lichman, M.} 2013. \textit{UCI machine learning repository}. 
Irvine, CA: University of California, School of Information and Computer Science.
Available at: {\sf http://archive.ics.uci.edu/ml}
(accessed March~3, 2017). 

\bibitem{18-dok-1}
\Aue{Pedregosa, F., G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel, 
M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos, 
D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay}. 2011. Scikit-learn: 
Machine learning in Python. \textit{J.~Mach. Learn. Res.} 12:2825--2830.
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-3pt}

\hfill{\small\textit{Received August 2, 2016}}

\Contr

\noindent
\textbf{Dokukin Alexander A.}\ (b.\ 1980)~--- 
PhD in physics and mathematics, senior scientist, Federal Research Center 
``Computer Science and Control'' of the Russian Academy of Sciences,
 40~Vavilov Str., Moscow, 119333, Russian Federation; \mbox{dalex@ccas.ru} 
 
 \vspace*{3pt}

\noindent
\textbf{Ryazanov Vasiliy V.}\ (b.\ 1991)~--- 
PhD student, Moscow Institute of Physics and Technology, 9~Institutskiy Per., 
Dolgoprudny, Moscow Region 141700, Russian Federation; \mbox{vasyarv@mail.ru}

\vspace*{3pt}


\noindent
\textbf{Shut Olga V.}\ (b.\ 1987)~--- 
assistant professor, Belarusian State University, 4~Nezavisimosti Av., 
Minsk 220030, Republic of Belarus; \mbox{olgashut@tut.by}


\label{end\stat}


\renewcommand{\bibname}{\protect\rm Литература} 
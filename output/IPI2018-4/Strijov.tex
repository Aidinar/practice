\newcommand{\bs}{{\boldsymbol{\sigma}}}
\newcommand{\bb}{{\mathbf{b}}}


%\newcommand{\ba}{{\mathbf{a}}}

\newcommand{\bx}{{\mathbf{x}}}
\newcommand{\bbf}{{\mathbf{f}}}
\newcommand{\by}{{\mathbf{y}}}
\newcommand{\bw}{{\mathbf{w}}}
\newcommand{\bW}{{\mathbf{W}}}
\newcommand{\bWs}{{\mathbf{W}_{\mathbf{s}}}}
\newcommand{\bU}{{\mathbf{U}}}
\newcommand{\bV}{{\mathbf{V}}}
\newcommand{\bh}{{\mathbf{h}}}
\newcommand{\bu}{{\mathbf{u}}}
\newcommand{\bbW}{{\mathbf{b}_{\mathbf{W}}}}
\newcommand{\bbU}{{\mathbf{b}_{\mathbf{U}}}}
\newcommand{\bbV}{{\mathbf{b}_{\mathbf{V}}}}

%\newcommand{\al}{{\alpha}}
%\newcommand{\bal}{\boldsymbol{\alpha}}
%\newcommand{\bbt}{\boldsymbol{\beta}}
\newcommand{\bApr}{\mathbf{A}^{-1}_{\mathrm{pr}}}
\newcommand{\bAps}{\mathbf{A}^{-1}_{\mathrm{ps}}}
\newcommand{\mupr}{\boldsymbol{\mu}}
\newcommand{\mups}{\mathbf{m}}
\newcommand{\DKL}{D_{\mathrm{KL}}}


\def\stat{strijov}

\def\tit{ВЫБОР ОПТИМАЛЬНОЙ МОДЕЛИ РЕКУРРЕНТНОЙ СЕТИ В~ЗАДАЧАХ ПОИСКА 
ПАРАФРАЗА$^*$}

\def\titkol{Выбор оптимальной модели рекуррентной сети в~задачах поиска 
парафраза}

\def\aut{А.\,Н.~Смердов$^1$, О.\,Ю.~Бахтеев$^2$, В.\,В.~Стрижов$^3$}

\def\autkol{А.\,Н.~Смердов, О.\,Ю.~Бахтеев, В.\,В.~Стрижов}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Смердов А.\,Н.}
\index{Бахтеев О.\,Ю.}
\index{Стрижов В.\,В.}
\index{Smerdov A.\,N.}
\index{Bakhteev O.\,Y.}
\index{Strijov V.\,V.}




{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
{Работа выполнена при  финансовой поддержке РФФИ (проект 16-07-01160) 
и~Правительства Российской Федерации (соглашение №\,05.Y09.21.0018).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Московский физико-технический институт, 
\mbox{anton.smerdov1@gmail.com}}
\footnotetext[2]{Московский физико-технический 
институт, \mbox{bakhteev@phystech.edu}}
\footnotetext[3]{Московский физико-технический 
институт; Вычислительный центр 
им.\ А.\,А.~Дородницына Федерального исследовательского центра <<Информатика 
и~управ\-ле\-ние>> Российской академии наук, \mbox{strijov@ccas.ru}}

%\vspace*{8pt}



\Abst{Рассматривается задача выбора оптимальной 
рекуррентной нейронной сети. В~качестве критерия оптимальности используется 
нижняя оценка правдоподобия модели. Исследование сконцентрировано на применении 
вариационного подхода к~аппроксимации апостериорного распределения параметров 
модели. Частным случаем аппроксимации выступает нормальное распределение 
параметров с~различными видами матрицы ковариаций. Для увеличения правдоподобия 
модели предлагается метод удаления параметров с~наибольшей плотностью 
вероятности в~нуле. В~качестве иллюстративного примера рассматривается задача 
многоклассовой классификации на выборке пар схожих и~несхожих предложений 
SemEval~2015.}
    
\KW{глубокое обучение; выбор оптимальной модели; 
рекуррентная нейросеть; разреживание нейросети; вариационный вывод}

\DOI{10.14357/19922264180409}
  
%\vspace*{4pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}

\section{Введение}

Целью работы является выбор оптимальной нейросетевой модели  из класса 
рекуррентных нейронных сетей. Рекуррентной нейросетью называется нейросеть со 
связью между нейронами одного слоя. В~качестве критерия оптимальности 
используется нижняя оценка правдоподобия модели.

Число параметров в~моделях глубокого обучения может достигать 
миллионов~\cite{DeepGoogle}. Большое число параметров влечет сложность 
оптимизации параметров и~переобучение моделей~\cite{Bishop}. Предлагается 
уменьшить число параметров рекуррентной сети. Это обеспечит б$\acute{\mbox{о}}$льшую 
устойчивость модели и~снизит время оптимизации ее параметров. Для решения 
поставленной задачи используются как байесовские методы~\cite{Strijov_1}, так 
и~методы прореживания переусложненной нейросети, наращивания простой нейросети 
и~их комбинации~\cite{Strijov_2}.

Для построения модели рекуррентной сети рассматривается модель 
из~\cite{Sanborn}, решающая задачу определения сходства предложений.
Модель при\-ни\-мает на вход векторизованные представления слов. Векторизация 
выполняется с~по\-мощью алгорит-\linebreak ма GloVe (Global Vectors for word representation), осно\-ван\-но\-го на факторизации матрицы 
слов-кон\-текс\-тов и~использовании весовой функции для уменьшения значимости редких 
слов~\cite{GloVe}. Альтернативой этому алгоритму выступает линейная модель 
Word2vec, комбинирующая в~себе Continuous Bag-of-Words, skip-gram и~negative 
sampling~\cite{word2vec}. Несмотря на разные подходы к~проблеме, GloVe 
и~Word2vec оптимизируют схожие функционалы~\cite{Glo2vec}. Упрощенной линейной 
моделью Word2vec, предназначенной для классификации документов, является 
fastText~--- метод, работающий на символьных $n$-грам\-мах~\cite{fastText}.

%При оптимизации параметров рекуррентных сетей возникает проблема затухания 
%градиентов, что существенно замедляет сходимость модели~\cite{vanishing_grad}. 
%Кроме того, при анализе временных зависимостей рекуррентные нейросети склонны 
%придавать большее значение поздним сигналам, теряя часть информации о~начале 
%временного ряда. Для решения этих проблем могут быть использованы LSTM и~GRU, 
%использующие внутреннюю память для контроллируемого хранения и~модификации 
%полученной информации\cite{GRU, Socher_2}. Наиболее существенным отличием GRU от 
%LSTM является отсутствие output gate и~меньшее число параметров.

В работе предлагается подход, основанный на получении вариационной нижней оценки 
правдоподобия модели. Подобная задача решалась в~\cite{Graves} аппроксимацией 
апостериорного распределения нормальным, получением аналитических формул для 
нижней границы правдоподобия модели и~удалением параметров с~наибольшей 
плотностью вероятности в~нуле. Описанный ниже подход продолжает это 
исследование. Априорное и~апостериорное распределения параметров 
аппроксимируются нормальным со скалярным, диагональным и~блочным видами матрицы 
ковариаций. После оптимизации гиперпараметров выполняется прореживание сети.

Предлагаемый подход сравнивается с~методом удаления параметров Optimal Brain 
Damage, базирующимся на анализе функции ошибки~\cite{OBD}. Его обобщенной 
версией выступает алгоритм Optimal Brain Surgeon~\cite{OBS}, не предполагающий 
диагонального вида гессиана функции ошибки.

Вычислительный эксперимент проводится на выборке размеченных пар предложений 
\mbox{SemEval} 2015. Для каждой пары предложений из корпуса дана экспертная оценка их 
семантической бли\-зости. Требуется построить модель, оценивающую семантическую 
близость двух предложений. Проблема рассматривается как задача многоклассовой 
классификации аналогично~\cite{Sanborn}. Критерием качества служит F1-ме\-ра, 
учитывающая как полноту, так и~точность предсказаний.
В~качестве базовой модели рассматривается пара соединенных рекуррентных сетей 
с~общим вектором параметров и~softmax-клас\-си\-фи\-ка\-то\-ром на выходе.

\section{Задача выбора оптимальной нейросетевой модели}

Для построения выборки используем набор пар предложений SemEval 
2015~\cite{SemEval2015}.
%\footnote{http://alt.qcri.org/semeval2015/task2/index.php?id=data-and-tools} 
%с~%экспертной разметкой семантической близости.
Каждому слову сопоставим вектор размерности~$n$.
Обозначим через~$l$ число слов в~самом длинном предложении. Предложения длины, 
меньшей~$l$, дополним нулевыми векторами.
Построим выборку
$$ 
\mathfrak{D} = \{(\bx_i,y_i)\},\enskip i = 1,\dots,N\,,
$$
где $\bx_i = [\bx_i^1,\bx_i^2]$~--- пары последовательностей векторов слов, 
соответствующих $i$-й паре предложений, $\bx_i^1, \bx_i^2 \hm\in \mathbb{R}^{n\times l}$;
$y_i \hm\in Y \hm= \{0,\dots,5\}$~--- экспертная оценка семантической близости.

Требуется построить модель $f(\mathbf{w}): \mathbb{R}^{n\times l} 
\hm\times \mathbb{R}^{n\times l} \hm\to 
Y$, сопоставляющую паре предложений $\bx_i^1$ и~$\bx_i^2$ класс семантической 
близости, где $\mathbf{w} \hm\in \mathbb{W}\subseteq\mathbb{R}^s$~--- пространство параметров модели.
%$f: \mathbb{R}^{n\times l} \times \mathbb{R}^{n\times l} \times \mathbb{W} \to Y$
Искомая модель выбирается из множества $\mathcal{F}$ рекуррентных нейронных сетей 
с~функцией активации $\tanh$. Модель
\[
f(\mathbf{w}): \mathbb{R}^{n\times l} \times \mathbb{R}^{n\times l} \to Y
\]
принадлежит искомому классу моделей~$\mathcal{F}$, если существуют такие матрицы 
перехода $\bW\hm\in \mathbb{R}^{n\times m}$, 
$\bU\hm\in \mathbb{R}^{n\times n}$, $\bV\hm\in \mathbb{R}^{(|Y| 
\times 2n)}$ и~вектор смещения $\bb \hm\in \mathbb{R}^{n}$, что для $j$-х элементов 
$\bx_{ij}^1, \bx_{ij}^2 \hm\in \mathbb{R}^m$ последовательностей $\bx_i^1$ и~$\bx_i^2$ 
определены векторы скрытого слоя $\bh_{ij}^1, \bh_{ij}^2 \hm\in \mathbb{R}^{n}$:
\begin{align*}
\bh_{ij}^1 &= \tanh\left(\bW\cdot \bx_{ij}^1 + \bU\cdot \bh_{i,j-1}^1 + \bb\right)\,;\\
\bh_{ij}^2 &= \tanh\left(\bW\cdot \bx_{ij}^2 + \bU\cdot \bh_{i,j-1}^2 + \bb\right)\,.
\end{align*}
Для определения класса семантической близости используются последние значения 
скрытого слоя $\bh_{il}^1$ и~$\bh_{il}^2$, сконкатенированные в~один вектор. 
После $l$-й итерации
пару предложений будем относить к~классу с~наибольшим значением, полученным 
после $l$-й итерации, $j\hm=1,\dots,l$:
\begin{equation*}
y = \argmax_{k\in Y}
\left(\bV%\cdot
\begin{bmatrix}
\bh_{il}^1\\
\bh_{il}^2
\end{bmatrix}\right)_k\,,
%\label{eq:classify}
\end{equation*}
где $(\cdot)_k$~--- $k$-я компонента вектора. Для каждой модели 
и~соответствующего ей вектора па\-ра\-мет\-ров $\mathbf{w} \hm\in \mathbb{W}$ определим логарифмическую 
функцию правдоподобия выборки~$L_\mathfrak{D}(\mathfrak{D},\bbf,\mathbf{w})$:
\begin{multline}
L_\mathfrak{D}(\mathfrak{D}, \bbf,\mathbf{w}) = 
\log p(\by|\bx,\bbf,\mathbf{w}) = \log p(\mathfrak{D}|\bbf,\mathbf{w}) = {}\\
{}=
\sum\limits_{(\bx_i,y_i)\in\mathfrak{D}} \log p(y_i|\bx_i,\bbf,\mathbf{w}) \,,
\label{L^N}
\end{multline}
где $p(\by|\bx,\bbf,\mathbf{w})$~--- апостериорная вероятность вектора~$\by$ при 
заданных $\bx$, $\bbf$ и~$\mathbf{w}$.
Здесь и~далее используется обозначение~$p(\bx|\by)\hm = p(\mathfrak{D})$.

Оптимальная модель $\bbf$ находится максимизацией логарифма ее правдоподобия:
\begin{multline}
L_\bbf(\mathfrak{D},\bbf) = L_\bbf(\mathfrak{D}|\bbf) = \log p(\by|\bx,\bbf) ={}\\
{}= \log 
p(\mathfrak{D}|\bbf) = \log \int\limits_{\mathbf{w} \in \mathbb{W}}
p(\mathfrak{D}|\mathbf{w},\bbf)p(\mathbf{w}|\bbf)\,d\mathbf{w}\,. 
\label{truth}
\end{multline}
Апостериорное распределение параметров модели находится из уравнения:
\begin{equation}
p(\mathbf{w}|\mathfrak{D},\bbf) = \fr{p(\mathfrak{D}|\mathbf{w},\bbf)p(\mathbf{w}|\bbf)}{p(\mathfrak{D}|\bbf)}\,.
\label{param}
\end{equation}
Приблизим интеграл~\eqref{truth} вариационной нижней оценкой. Воспользуемся 
оценкой~\cite[разд.~10.2--10.4]{Bishop}, полученной из неравенства Йенсена:
\begin{multline*}
L_\bbf(\mathfrak{D},\bbf) = \log\int\limits_{\mathbf{w} \in \mathbb{W}}
p(\mathfrak{D}|\mathbf{w})p(\mathbf{w}|\bbf)\,d\mathbf{w} 
={}\\
{}=\int\limits_{\mathbf{w} \in 
\mathbb{W}}p(\mathbf{w}|\mathfrak{D},\bbf)\log\fr{p(\mathbf{w}|\mathfrak{D},\bbf)}
{p(\mathfrak{D}|\bbf,\mathbf{w})}\,d\mathbf{w} + {}\\
{}+
\DKL\left(p(\mathfrak{D}|\mathbf{w})||p(\mathfrak{D}|\bbf)\right),
\end{multline*}
где~$\DKL\bigl(q(\mathbf{w})||p(\mathbf{w})\bigr)$~--- расстояние Куль\-ба\-ка--Лейб\-ле\-ра между 
$q(\mathbf{w})$ и~$p(\mathbf{w})$,
\begin{equation*}
 \DKL\left(q(\mathbf{w})||p(\mathbf{w})\right) = - 
\int\limits_{\mathbf{w}}q(\mathbf{w})\log\fr{q(\mathbf{w})}{p(\mathbf{w})}\,d\mathbf{w}\,.
\end{equation*}
Учитывая неотрицательность расстояния Куль\-ба\-ка--Лейб\-ле\-ра, получаем:
\begin{equation}
L_\bbf(\mathfrak{D},\bbf) \geq \int\limits_{\mathbf{w} \in 
\mathbb{W}}p(\mathbf{w}|\mathfrak{D},\bbf)\log
\fr{p(\mathbf{w}|\mathfrak{D},\bbf)}{p(\mathfrak{D}|\bbf,\mathbf{w})}\,d\mathbf{w}\,.
\label{est_1}
\end{equation}
Упростим интеграл в~левой части~\eqref{est_1}:
\begin{multline}
\int\limits_{\mathbf{w} \in 
\mathbb{W}}p(\mathbf{w}|\mathfrak{D},\bbf)\log\fr{p(\mathbf{w}|\mathfrak{D},\bbf)}{p(\mathfrak{D}|\bbf,\mathbf{w})}\,d\mathbf{w} ={} \\
{}= -\DKL\bigl(p(\mathbf{w}|\mathfrak{D},\bbf)||p(\mathbf{w}|\bbf)\bigr) + {}\\
{}+\int\limits_{\mathbf{w} \in 
\mathbb{W}}p(\mathbf{w}|\mathfrak{D},\bbf)\log 
p(\mathbf{w}|\mathfrak{D},\bbf)\,d\mathbf{w}\,.
\label{est_2}
\end{multline}
Обозначим сумму в~левой части~\eqref{est_2} через $-L(\mathfrak{D},\bbf,\mathbf{w})$:
\begin{multline}
L(\mathfrak{D},\bbf,\mathbf{w}) = 
\underbrace{\DKL\left(p(\mathbf{w}|\mathfrak{D},\bbf)||p(\mathbf{w}|\bbf)\right)}_{L_\mathbf{w}(\mathfrak{D},
\bbf,\bw)} - {}\\
{}-\underbrace{\int\limits_{\mathbf{w} \in 
\mathbb{W}}p(\mathbf{w}|\mathfrak{D},\bbf)\log p(\mathfrak{D}|\bbf,\mathbf{w})\,
d\mathbf{w}}_{L_E(\mathfrak{D},\bbf)}\,.
\label{est_3}
\end{multline}
Первое слагаемое формулы~\eqref{est_3} интерпретируется как минимальная длина 
описания распределения $p(\mathbf{w}|\mathfrak{D},\bbf)$ с~помощью $p(\mathbf{w}|\bbf)$. Эту 
величину назовем сложностью модели $L_\mathbf{w}(\mathfrak{D},\bbf,\mathbf{w})$:
\begin{equation*}
L_\mathbf{w}(\mathfrak{D},\bbf,\mathbf{w}) = \DKL
\left(p(\mathbf{w}|\mathfrak{D},\bbf)||p(\mathbf{w}|\bbf)\right)\,. 
%\label{complexity}
\end{equation*}
Второе слагаемое формулы~\eqref{est_3} есть минус мат\-ожи\-да\-ние правдоподобия 
выборки $L_\mathfrak{D}$~\eqref{L^N}, и~оно тем меньше, чем выше правдоподобие выборки, 
поэтому интерпретируется как функционал ошибки~$L_E (\mathfrak{D},\bbf)$ в~ходе 
вычислительного эксперимента:
\begin{equation*}
L_E (\mathfrak{D},\bbf) = 
\mathsf{E}_{\mathbf{w}\sim p(\mathbf{w}|\mathfrak{D},\bbf)}L_\mathfrak{D}(\by,\mathfrak{D}, \bbf,\mathbf{w}).
\end{equation*}
Запишем суммарную функцию потерь $L(\mathfrak{D},\bbf,\mathbf{w})$ как сумму функционала 
сложности модели $L_\mathbf{w}(\mathfrak{D},\bbf,\mathbf{w})$ и~функционала ошибки $L_E (\mathfrak{D},\bbf)$:
\begin{equation}
L(\mathfrak{D},\bbf,\mathbf{w}) = L_\mathbf{w}(\mathfrak{D},\bbf,\mathbf{w}) + L_E (\mathfrak{D},\bbf)\,. 
\label{loss}
\end{equation}
Искомая модель минимизирует суммарный функционал потерь
\begin{equation*}
\bbf = \argmin\limits_{\bbf \in \mathcal{F}}L(\mathfrak{D},\bbf,\mathbf{w})\,.
\end{equation*}

\section{Предлагаемое решение оптимизационной задачи}

Так как апостериорное распределение $p(\mathbf{w}|\mathfrak{D},\bbf)$~\eqref{param} невозможно 
получить аналитически, минимизация функционала потерь 
$L(\mathfrak{D},\bbf,\mathbf{w})$~\eqref{loss} затруднена. Для решения этой проблемы применим вариационный подход. 
Он заключается в~аппроксимации неизвестного распределения распределением из 
известного класса.
В качестве приближения $p(\mathbf{w}|\mathfrak{D},\bbf)$ выберем нормальное распределение:
$$
p(\mathbf{w}|\mathfrak{D},\bbf) \sim \mathcal{N}(\mups,\bAps)\,,
$$
где $\mups$ и~$\bAps$~--- вектор средних и~матрица ковариации этого распределения.
Априорное распределение $p(\mathbf{w}|\bbf)$ вектора параметров~$\mathbf{w}$ будем считать 
нормальным с~параметрами~$\mupr$ и~$\bApr$:
$$
p(\mathbf{w}|\bbf) \sim \mathcal{N}(\mupr,\bApr)\,,
$$
где $\mupr$~--- вектор средних; $\bApr$~--- матрица ковариаций.
Расстояние Кульбака--Лейблера между нормальными распределениями 
$\mathcal{N}(\mupr,\bApr)$ и~$\mathcal{N}(\mups,\bAps)$ вычисляется по формуле:
\begin{multline*}
\DKL\left(\mathcal{N}(\mupr,\bApr)||\mathcal{N}(\mups,\bAps)\right) 
= {}\\
{}=\fr{1}{2} \left(\log 
\fr{|\bAps|}{|\bApr|}-d+\mathrm{tr}\left(\mathbf{A}_2\bApr\right) + {}\right.\\
\left.{}+(\mupr-
\mups)^{\mathsf{T}}\mathbf{A}_2(\mupr-\mups) \right). 
%\label{DKL_norm}
\end{multline*}

Рассмотрим частные случаи вида матриц ковариаций $\bApr$ и~$\bAps$. Так как 
априори нет предпочтений при выборе параметров, то априорное распределение для 
всех параметров считаем одинаковым, т.\,е.\ вектор средних $\mupr \hm= \mu 
\boldsymbol{1}$, матрица ковариаций скалярна: $\bApr\hm = \sigma\mathbf{I}$.
После получения информации о выборке получаем апостериорный вектор средних 
$\mups$.


Алгоритм решения оптимизационной задачи заключается в~выполнении градиентного 
шага при заданном априорном распределении, вычислении апостериорного 
распределения и~аппроксимации нового априорного распределения полученным 
апостериорным.
Рассмотрим различные виды апостериорной матрицы ковариаций~$\bAps$.
\begin{enumerate}
    \item Матрица ковариаций скалярна: $\bAps \hm= \alpha \mathbf{I}$.
    В~этом случае 
    \begin{multline*}
    \DKL\left(\mathcal{N}(\mupr,\bApr)||\mathcal{N}(\mups,\bAps)\right) ={}\\
    {}= 
\sum\limits_{i=1}^W\left(\log\fr{\sigma}{\alpha} + \fr{(\mu-m_i)^2 + \alpha^2 
+ \sigma^2}{2\sigma^2}\right).
\end{multline*}
    По значениям параметров $\alpha$ и~$\mathbf{m}$ апостериорного распределения  
вычислим  параметры априорного. Число элементов вектора~$\mathbf{m}$ обозначим~$W$. Из 
условия 
$$
\fr{\partial}{\partial\mu}\,\DKL \hm= \sum\limits_{i=1}^W
\fr{\mu\hm-
m_i}{\sigma^2}=0
$$ 
получаем выражения для~$\mu$ на следующей итерации $\hat{\mu} 
\hm= (1/W)\sum\nolimits_{i=1}^W m_i$.
    Аналогично 
   \begin{multline*}
    \fr{\partial}{\partial\sigma^2}\,\DKL = \sum\limits_{i=1}^W 
\fr{1}{{2\sigma^2}}-\fr{(\mu-m_i)^2 + \alpha^2}{{2\sigma^4}}=0 \Rightarrow {}\\
{}\Rightarrow
\hat{\sigma}^2 = \fr{1}{W}\sum\limits_{i=1}^W (\mu\hm-m_i)^2 \hm+ \alpha^2\,.
\end{multline*}



    \item Матрица ковариаций диагональна: $\bAps \hm= \mathrm{diag}(\bs^2)$. 
    В~этом случае 
\begin{multline*}
    \DKL\left(\mathcal{N}(\mupr,\bApr)||\mathcal{N}(\mups,\bAps)\right) = {}\\
    {}=
\sum\limits_{i=1}^W\left(\log\fr{\sigma}{\sigma_i} + \fr{(\mu-m_i)^2 + \sigma_i^2 
+ \sigma^2}{2\sigma^2}\right).
\end{multline*}
    Значения параметров априорного распределения для следующей итерации 
вычисляются следующим образом:
\begin{multline*}
\mbox{из} \ \fr{\partial}{\partial\mu}\,\DKL = \sum\limits_{i=1}^W\fr{\mu-
m_i}{\sigma^2}=0 \\
 \mbox{получаем} \ \hat{\mu} = \fr{1}{W}\sum\limits_{i=1}^W 
m_i\,;
\end{multline*}

\vspace*{-12pt}


\noindent
\begin{multline*}
\mbox{из} \ \fr{\partial}{\partial\sigma^2}\,\DKL = \sum\limits_{i=1}^W 
\fr{1}{2\sigma^2}-\fr{(\mu-m_i)^2 + \sigma_i^2}{2\sigma^4}=0 \\
 \mbox{получаем} 
\ \hat{\sigma}^2 = \fr{1}{W}\sum\limits_{i=1}^W (\mu-m_i)^2 + \sigma_i^2.
\end{multline*}
\end{enumerate}

Оптимизация параметров сводится к~следующему алгоритму.
\begin{description}
\item[\,] 
Инициализировать~$\bs\hm = \textbf{1}$, $\mathbf{m} \hm= \textbf{0}$, $\mu \hm= 
0$, $\sigma^2 \hm= 1$.
\item[\,] {\textbf{Повторять}}:
\begin{description}
\item[\,] Сделать градиентный шаг $\bs:=\bs-\eta\nabla\bs$, $\mathbf{m}:=\mathbf{m}\hm-
\eta\nabla\mathbf{m}$, $\mathbf{w} := \mathbf{w} \hm- \eta \nabla \mathbf{w}$.


\item[\,] Обновить параметры априорного распределения $\mu:= \hat{\mu}$,  
$\sigma^2:=\hat{\sigma}^2$.
\end{description}
\item[\,] {\textbf{Пока}} значение~$L$ не стабилизируется.
\end{description}



Результаты вычислительного эксперимента представлены в~таблице.

%\begin{table*}
{\small %tabl1
    \begin{center}
\vspace*{6pt}
%    \label{my-label}
    \begin{tabular}{|l|c|c|}
    \multicolumn{3}{c}{Результаты вычислительного эксперимента}\\
    \multicolumn{3}{c}{\ }\\[-6pt]
        \hline %\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{
       \multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{Метод}}          & \multicolumn{2}{c|}{F1-мера}\\
        \cline{2-3}
& Валидация &  Эксперимент \\ 
\hline
        Logistic Regression    & 0,286                 & 0,286            \\ 
%\hline
        SVC                    & 0,290                 & 0,290            \\ 
%\hline
        DecisionTreeClassifier & 0,316                 & 0,316            \\ 
%\hline
        KNeighborsClassifier   & 0,322                 & 0,322            \\ 
%\hline
        RNN                    & 0,393                 & 0,362            \\ 
%\hline
        RNN+variational, I, I  &  ---                     & 0,311            
\\ 
%\hline
        RNN+variational, D, I  &  ---                     & 0,330           \\ 
\hline
    \end{tabular}
        \vspace*{3pt}
    \end{center}
}
%\end{table*}

%\nolinenumbers






\section{Удаление параметров из сети}

\vspace*{-14pt}

Введем множество индексов активных па\-ра\-мет\-ров модели $\mathcal{A} \hm= \{i | w_i 
\neq 0\} $. Для увеличения правдоподобия модели предлагается уменьшить ее 
сложность, т.\,е.\ уменьшить число параметров~$|\mathcal{A}|$. Для удаления 
выберем параметры, имеющие наибольшую плотность апостериорной вероятности~$\rho$ 
в~нуле.
Если апостериорная матрица ковариаций скалярна, то

\vspace*{-12pt}

\noindent
\begin{equation*}
\rho_i = \exp\left(-\fr{\mu_i^2}{2\sigma^2}\right).
\end{equation*}

\vspace*{-12pt}

\noindent
Чем больше $\rho$, тем меньше $|{\mu_i}/{\sigma}|$, поэтому удаляются 
параметры со значением $|{\mu_i}/{\sigma}|\hm < \lambda$, где~$\lambda$~--- 
пороговое значение. Варьируя пороговое значение~$\lambda$, выбираем оптимальное 
число неудаленных параметров.
Для диагонального вида матрицы ковариаций критерий удаления параметров 
записывается как $|{\mu_i}/{\sigma_i}| \hm< \lambda$.

\vspace*{-20pt}

\section{Вычислительный эксперимент}

\vspace*{-12pt}

Цель эксперимента~--- проверка работоспособности предложенного алгоритма 
и~сравнение результатов с~ранее полученными. В качестве данных использовалась 
выборка SemEval~2015, состоящая из~8331~пары схожих и~несхожих предложений. 
Слова преобразовывались в~векторы размерности~50 при помощи алгоритма 
GloVe~\cite{GloveURL}.
Для базовых алгоритмов тренировочная, валидационная и~тес\-то\-вая выборки составили 
70\%, 15\% и~15\% соответственно.
Для рекуррентной нейронной сети, полученной вариационным методом, валидационная 
выборка отсутствовала, а тренировочная и~тес\-то\-вая выборки составили 85\% и~15\% 
соответственно.
Критерием качества была выбрана F1-ме\-ра.
В~качестве базовых алгоритмов использовались линейная регрессия, метод ближайших 
соседей, ре\-ша\-ющее дерево и~модификация метода опорных векторов SVC
(support vector clustering). Базовые 
алгоритмы взяты из биб\-лио\-те\-ки sklearn.
Дополнительно были построены рекуррентная нейросеть с~одним скрытым 
слоем~\cite{Sanborn} и~нейросеть с~одним скрытым слоем и~вариационной 
оптимизацией параметров~\cite{Graves, code}.
{\looseness=1

}





На рис.~1,\,\textit{а} представлены зависимости оценки 
правдоподобия $L$ \eqref{loss} от параметра~$\lambda$.
Для обоих случаев существует оптимальное значение~$\lambda$, минимизирующее~$L$; 
модели с~таким параметром будут оптимальными. На 
рис.~1,\,\textit{б} и~1,\,\textit{в} 
отображены зависимости качества модели от~$\lambda$ и~доли выброшенных 
параметров. Видно, что даже при удалении большинства параметров из сети качество 
предсказаний\linebreak\vspace*{-12pt}

\pagebreak

\end{multicols}



\begin{figure}[h] %fig1
\vspace*{1pt}
 \begin{center}
 \mbox{%
 \epsfxsize=163mm 
 \epsfbox{str-1.eps}
 }
 \end{center}
\vspace*{-9pt}
    \Caption{Зависимость нижней оценки правдоподобия модели 
    от $\lambda$~(\textit{а})
    и~F1-ме\-ры от~$\lambda$~(\textit{б})
    и~доли выброшенных параметров~(\textit{в})
     для скалярной (левый столбец) 
и~диагональной (правый столбец) матриц:
\textit{1}~--- первоначальная оценка;
\textit{2}~--- полученная оценка}
    \label{results}
    \vspace*{4pt}
\end{figure}

%\pagebreak

\begin{multicols}{2}




\noindent
 меняется несущественно, что говорит о~слишком большом числе 
параметров исходной модели.



Из рис.~2 видно, что при малых~$\lambda$ из сети с~диагональной 
апостериорной матрицей ковариаций удаляется больше весов, а при больших 
$\lambda$~--- меньше, что говорит о~лучшем отборе параметров такой моделью.


\section{Заключение}

\vspace*{-26pt}

С помощью вариационного байесовского подхода был построен набор моделей 
глубинного обуче\-ния с~оптимальной нижней оценкой правдоподобия, отличающихся 
различными предположениями о~виде априорного и~апостериорного\linebreak\vspace*{-12pt}

{ \begin{center}  %fig2
 \vspace*{1pt}
  \mbox{%
 \epsfxsize=78.898mm 
 \epsfbox{str-2.eps}
 }


\end{center}


\noindent
{{\figurename~2}\ \ \small{Доля неудаленных параметров сети в~зависимости от порогового 
значения~$\lambda$ для скалярного~($I$) и~диагонального~($D$) вида апостериорной 
матрицы ковариаций}}
}

\vspace*{9pt}

\noindent
 распреде\-ле\-ния 
параметров. Из случайности распределения параметров был получен критерий их 
удаления, что позволило увеличить нижнюю оценку правдоподобия моделей. 
Результаты полученных нейросетей в~вычислительном эксперименте оказались близки к~результатам других алгоритмов.



{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
    \bibitem{DeepGoogle}
    \Au{Sutskever~I., Vinyals~O,  Le~Q.\,V}. Sequence to sequence 
learning with neural networks~// Adv. Neur. In., 2014. Vol. 27. P.~3104--3112. {\sf
https://papers.nips.cc/\linebreak paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf}.
    
    \bibitem{Bishop} %2
    \Au{Bishop~C.\,M}. Pattern recognition and machine learning.~--- 
Springer, 2006. 758~p.
    
    \bibitem{Strijov_1} %3
    \Au{Kuznetsov~M.\,P., Tokmakova~A.\,A., Strijov~V.\,V.} Analytic and 
stochastic methods of structure parameter estimation~// Informatica,~2016. 
Vol.~27. No.\,3. P.~607--624.
    
    \bibitem{Strijov_2} %4
    \Au{Попова~М.\,С., Стрижов~В.\,В.} Выбор оптимальной модели 
классификации физической активности по измерениям акселерометра~// Информатика 
и~её применения, 2015. Т.~9. Вып.~1. С.~76--86.
%   Informatics and Applications 2015, Volume 9, Issue 1, pp 76-86
    
    \bibitem{Sanborn} %5
    \Au{Sanborn~A., Skryzalin~J.}
    Deep learning for semantic similarity~// Deep learning for natural 
language processing.~--- Stanford, CA, USA: Stanford University, 2015. 
Vol.~CS224d. P.~1--7. {\sf 
https://cs224d.stanford.\linebreak edu/reports/SanbornAdrian.pdf}.

    \bibitem{GloVe} %6
    \Au{Pennington~J., Socher~R.,  Manning~C.\,D.} GloVe: Global vectors 
for word representation~// Conference on Empiricial Methods in Natural 
Language Processing Proceedings,~2014. Vol.~12.  P.~1532--1543. {\sf
https://nlp.\linebreak stanford.edu/pubs/glove.pdf}.
        
    \bibitem{word2vec} %7
    \Au{Rong~X.} Word2vec parameter learning explained~//
    \mbox{arXiv}:1411.2738, 2014. 21~p.
%   {\ttfamily 
%{\sf https://pdfs.semanticscholar.org/51b5/207c269a62f33442f7f25bf7f541f3ee53d8.pdf}.
    
    \bibitem{Glo2vec}
    \Au{Shi~T.,  Liu~Z.} Linking GloVe with word2vec~// \mbox{arXiv}:\linebreak 
    1411.5595, 2014. 5~p.
    
    \bibitem{fastText}
    \Au{Zolotov~V., Kung~D.}  Analysis and optimization of fastText linear 
text classifier~// arXiv:1702.05531, 2017. 9~p.
%    https://pdfs.semanticscholar.org/9d69/93f60539d30ee325138b3465aa020fa3bcb4.pdf
    
    \bibitem{Graves}
    \Au{Graves~A.}
    Practical variational inference for neural networks~// Adv. Neur. 
In., 2011. Vol.~24. P.~2348--2356. {\sf
http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf}.
    
    \bibitem{OBD}
    \Au{Le Cun~Y., Denker~J.\,S., Solla~S.\,A.} Optimal brain damage~// 
Adv. Neur. In., 1989. Vol.~2. P.~598--605. 
{\sf https://papers.nips.cc/paper/250-optimal-brain-\linebreak damage.pdf}.
    
    \bibitem{OBS}
    \Au{Hassibi~B., Stork~D.\,G., Wolff~G.\,J.} Optimal brain surgeon and 
general network pruning~// Neural Comput., 1992. Vol.~4. P.~1--8.

    \bibitem{SemEval2015}
    Выборка пар предложений различной степени похожести. {\sf  
http://alt.qcri.org/semeval2015/task2/index. php?id=data-and-tools}.

\bibitem{GloveURL}
    Библиотека GloVe. Python.
    {\sf https://github.com/\linebreak stanfordnlp/GloVe}.

    \bibitem{code}
    \Au{Смердов~А.\,Н.} Код вычислительного эксперимента. {\sf 
    https://sourceforge.net/p/mlalgorithms/code/HEAD/\linebreak tree/Group474/Smerdov2017Paraphrase/code}.
    
    
    
     \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 08.05.18}}

\vspace*{6pt}

%\pagebreak

%\newpage

%\vspace*{-28pt}

\hrule

\vspace*{2pt}

\hrule

\vspace*{-2pt}

\def\tit{OPTIMAL RECURRENT NEURAL NETWORK MODEL IN~PARAPHRASE DETECTION}

\def\titkol{Optimal recurrent neural network model in~paraphrase detection}

\def\aut{A.\,N.~Smerdov$^1$,  O.\,Y.~Bakhteev$^1$, and~V.\,V.~Strijov$^{1,2}$}

\def\autkol{A.\,N.~Smerdov,  O.\,Y.~Bakhteev, and~V.\,V.~Strijov}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-15pt}


\noindent
$^1$Moscow Institute of Physics and Technology, 
9~Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian\linebreak
$\hphantom{^1}$Federation

\noindent
$^2$A.\,A.~Dorodnicyn Computing Center, 
Federal Research Center ``Computer Science and Control'' of the Russian\linebreak
$\hphantom{^1}$Academy 
of Sciences, 40~Vavilov Str., Moscow 119333, Russian Federation


\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2018\ \ \ volume~12\ \ \ issue\ 4}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2018\ \ \ volume~12\ \ \ issue\ 4
\hfill \textbf{\thepage}}}

\vspace*{3pt}
    



\Abste{This paper addresses the problem of optimal recurrent neural network selection. 
It asserts the neural network evidence lower bound as the optimal criterion for 
selection. It investigates variational inference methods to approximate the 
posterior distribution of the network parameters. As a~particular case, the normal 
distribution\linebreak\vspace*{-12pt}}

\Abstend{of the parameters with different types of the covariance matrix is 
investigated. The authors propose a~method of pruning parameters with the highest 
probability density in zero to increase the model marginal likelihood. 
As an illustrative example, a~computational experiment of multiclass classification 
on the SemEval~2015 dataset was carried out.}

\KWE{deep learning; recurrent neural network; neural network pruning; variational 
approach}

\DOI{10.14357/19922264180409}

\vspace*{-22pt}

\Ack

\vspace*{-2pt}

\noindent
This research was supported by the Russian Foundation for Basic Research, 
project 16-07-01160,  and by Government of the Russian 
Federation, agreement 05.Y09.21.0018.


\vspace*{-3pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
 
    \bibitem{DeepGoogle-1}
\Aue{Sutskever,~I., O.~Vinyals, and Q.\,V.~Le.} 
2014. Sequence to sequence learning with neural networks. 
\textit{Adv. Neur. In.} 27:3104--3112. 
Available at: {\sf 
https://papers.nips.cc/\linebreak paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf} (accessed December~29, 2017).

\bibitem{Bishop-1}
\Aue{Bishop,~C.\,M.} 2006.
\textit{Pattern recognition and machine learning}. Springer. 758~p.

\bibitem{Strijov_1-1}
\Aue{Kuznetsov,~M.\,P., A.\,A. Tokmakova, and V.\,V.~Strijov.} 2016. 
Analytic and stochastic methods of structure parameter estimation. 
\textit{Informatica} 27(3):607--624.
    
\bibitem{Strijov_2-1}
Popova,\,M.\,S., and V.\,V. Strijov. 2015. 
Vybor optimal'noy modeli klassifikatsii fizicheskoy aktivnosti po izmereniyam
akselerometra
[Selection of optimal physical activity classification model using measurements 
of accelerometer]. \textit{Informatika i~ee Primeneniya~--- Inform. Appl}. 9(1):76--86.
    
\bibitem{Sanborn-1}
\Aue{Sanborn,\,A., and J.~Skryzalin.} 2015.
Deep learning for semantic similarity. 
\textit{Deep learning for natural language processing}. 
Stanford, CA: Stanford University. CS224d:1--7. Available at: 
{\sf https://cs224d.\linebreak stanford.edu/reports/SanbornAdrian.pdf} (accessed December~29, 2017).
    
\bibitem{GloVe-1}
\Aue{Pennington, J., R.~Socher, and C.\,D.~Manning.}
2014. GloVe: Global vectors for word representation. 
\textit{Conference on Empiricial Methods in Natural Language Processing Proceedings}. 
12:1532--1543.   {\sf https://nlp.stanford.\linebreak edu/pubs/glove.pdf}
(accessed December~29, 2017).

\bibitem{word2vec-1}
\Aue{Rong,~X.} 2014. Word2vec parameter learning explained. 
{Arxiv}. Available at: {\sf 
https://arxiv.org/abs/1411.2738} (accessed December~29, 2017).
%   {\ttfamily https://pdfs.semanticscholar.org/51b5/207c269a62f33442f7f25bf7f541f3ee53d8.pdf}

\bibitem{Glo2vec-1}
\Aue{Shi,~T., and  Z.~Liu.} 2014. Linking GloVe with word2vec. 
{Arxiv}. Available at: {\sf 
http://arxiv.org/abs/1411.5595} (accessed December~29, 2017).

\bibitem{fastText-1}
\Aue{Zolotov,~V., and D.~Kung.} 2017. Analysis and optimization of fastText 
linear text classifier. {Arxiv}. 
Available at: {\sf 
https://arxiv.org/ftp/arxiv/papers/1702/1702.05531.\linebreak pdf} (accessed December~29, 2017).
%   https://pdfs.semanticscholar.org/9d69/93f60539d30ee325138b3465aa020fa3bcb4.pdf

\bibitem{Graves-1}
\Aue{Graves,~A.} 2011. Practical variational inference for neural networks.
 \textit{Adv. Neur. In.} 24:2348--2356. 
 Available at: {\sf  
 http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf} (accessed December 29, 2017).

\bibitem{OBD-1}
\Aue{Le Cun,~Y., J.\,S.~Denker., and S.\,A.~Solla.} 1989. Optimal brain damage. 
\textit{Adv. Neur. In.} 
2:598--605. Available at: 
{\sf https://papers.nips.cc/paper/250-optimal-brain-damage.pdf}
(accessed December~29, 2017).

\bibitem{OBS-1}
\Aue{Hassibi,~B., D.\,G. Stork, and G.\,J.~Wolff.} 
1992. Optimal brain surgeon and general network pruning. 
\textit{Neural Comput.} 4:1--8.

\bibitem{SemEval2015-1}
Dataset of sentences with different types of similarity.
 Available at: {\sf  http://alt.qcri.org/semeval2015/task2/\linebreak index.php?id=data-and-tools} 
 (accessed December~29, 2017).



\bibitem{GloveURL-1}
GloVe Python library. Available at: 
{\sf https://github.com/\linebreak stanfordnlp/GloVe} (accessed December~29, 2017).

\bibitem{code-1}
\Aue{Smerdov, A.\,N.} Computational experiment code. Available at: {\sf 
https://sourceforge.net/p/mlalgorithms/code/\linebreak  HEAD/tree/Group474/
Smerdov2017Paraphrase/code/} (accessed December 29, 2017).
%   \bibitem{sklearn}
%   http://scikit-learn.org/stable/
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-16pt}

\hfill{\small\textit{Received May 5, 2018}}

%\pagebreak

\vspace*{-18pt}


\Contr

\noindent
\textbf{Smerdov Anton N.} (b.\ 1995)~--- student, 
Moscow Institute of Physics and Technology, 
9~Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian Federation;  
\mbox{anton.smerdov1@gmail.com}

\vspace*{6pt}


\noindent
\textbf{Bakhteev Oleg Y.} (b.\ 1991)~--- 
graduate student, Moscow Institute of Physics 
and Technology, 9~Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian Federation; 
\mbox{bakhteev@phystech.edu}

\vspace*{6pt}

\noindent
\textbf{Strijov Vadim V.} (b.\ 1967)~--- 
Doctor of Science in physics and mathematics, professor,
Department of Intelligent Systems,
Moscow Institute of Physics 
and Technology, 9~Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian Federation; 
leading scientist, A.\,A.~Dorodnicyn Computing Center, 
Federal Research Center ``Computer Science and Control'' of the Russian Academy 
of Sciences, 40~Vavilov Str., Moscow 119333, Russian Federation; 
\mbox{strijov@ccas.ru}
\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература}       
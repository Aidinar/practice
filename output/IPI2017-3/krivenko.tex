\def\stat{krivenko}

\def\tit{ОБУЧАЕМАЯ КЛАССИФИКАЦИЯ НЕПОЛНЫХ КЛИНИЧЕСКИХ ДАННЫХ}

\def\titkol{Обучаемая классификация неполных клинических данных}

\def\aut{М.\,П.~Кривенко$^1$}

\def\autkol{М.\,П.~Кривенко}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Кривенко М.\,П.}
\index{Krivenko M.\,P.}


%{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
%{Исследование выполнено при финансовой поддержке Российского научного фонда (проект 16-11-10227).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Институт проблем информатики Федерального исследовательского центра <<Информатика 
и~управление>> Российской академии наук, \mbox{mkrivenko@ipiran.ru}}

\vspace*{-6pt}


\Abst{Рассматриваются вопросы эффективности методов классификации неполных 
клинических данных. Обучение байесовского классификатора проводится методом 
максимального правдоподобия (МП) для модели смеси нормальных распределений. Строгий 
вывод формул, обеспечивающих реализацию шагов EM (expectation-maximization)
алгоритма, позволил корректно 
применять итерационный процесс получения оценок параметров смеси. Для неполных 
данных предлагаются приемы выбора начальных значений и~коррекции вырождающихся 
ковариационных матриц элементов смеси. Экспериментальная часть работы заключалась 
в~анализе зависимости качества классификации от степени пропуска отдельных значений, 
для этого использовались данные о ферментах, полученные для пациентов с~заболеваниями 
печени. Обработка реальных данных продемонстрировала практически идентичные ошибки 
классификации при применении простых и~сложных методов обработки пропусков в~случае 
невысокой степени случайного пропуска отдельных значений.}

\KW{пропущенные данные; EM-алгоритм; смеси нормальных распределений}

\DOI{10.14357/19922264170303} 


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}

\section{Введение. Общие принципы обработки неполных 
данных}

     В клинических исследованиях отсутствующие данные~--- это данные, 
которые планировались к~фиксации, но оказались неотраженными в~базе 
данных. Понимание причин отсутствия данных важно для правильной 
обработки оставшихся данных. Если значения отсутствуют совершенно 
случайно, выборка данных, скорее всего, останется репрезентативной для 
популяции. Но если значения отсутствуют систематически, анализ может стать 
предвзятым. Из-за этих проблем необходимо планировать исследования так, 
чтобы минимизировать появление отсутствующих значений либо 
внимательным образом разбираться с~причинами появления отсутствующих 
наблюдений. Поскольку нет уверенности, что принятые предположения 
о~пропусках верны, а сами данные недоступны, необходим дополнительный 
анализ чувствительности процедур анализа результатов клинических 
исследований для оценки достоверности проведенных исследований.
     
     Независимо от того, насколько хорошо спроектированы и~проведены 
испытания, появление отдельных недостающих данных является ожи\-да\-емым. 
Подобное явление может быть совершенно не связано с~состоянием здоровья 
пациента и~характером лечения: в~частности, данные могут быть неполными  
из-за проблем планирования и~реализации отдельных исследований (например, 
для них не созданы условия проведения), из-за человеческой ошибки при 
записи данных. С~другой стороны, данные могут отсутствовать по причинам, 
связан\-ным со здоровьем субъекта и~экспериментальным лечением, которому он 
подвергается (например, субъекты могут отказаться от определенных 
клинических исследований из-за состояния своего здоровья или в~силу  
ка\-ких-ли\-бо предубеждений). Помимо недостающих данных из-за 
пропущенных посещений пропуски могут возникать просто из-за способа 
измерения или характера заболевания (например, данные будут отсутствовать, 
когда нет смысла в~их получении). 
     
     Далее рассматриваются способы обработки неполных данных 
результатов клинических обследований; при этом не затрагивается специфика 
задач клинических испытаний (оценивание эф\-фек\-тив\-ности и~безопасности 
лекарственного препарата или метода лечения и~диагностики). 
     
     В~методологии отсутствующих данных используются два термина: 
недостающее значение и~механизм отсутствия. Механизм отсутствия указывает 
на распределение вероятностей двоичного события отсутствия информации. 
     
     Классификации механизмов пропуска данных, введенные в~[1--3], 
представляют собой формальную структуру, описывающую вероятностные 
характеристики данных и~пропусков их значений; она позволяет получить 
представление о~том, как механизм отсутствия может влиять на выводы 
о~клиническом результате. Различают полностью случайный пропуск (Missing 
Completely at Random, MCAR), случайный пропуск (Missing at Random, MAR) 
и~механизм с~отсутствием слу\-чай\-ности в~пропусках (Missing Not at Random, 
MNAR). Когда данные пропущены по ор\-га\-ни\-за\-ци\-он\-но-ад\-ми\-ни\-стра\-тив\-ным 
причинам, механизм отсутствия может быть MCAR, потому что природа 
отсутствия не имеет ничего общего с~моделью получения результатов. Может 
оказаться востребованной и~модель MAR. Важно только отметить, что MAR не 
является неотъемлемой характеристикой самих данных или механизма 
пропусков, а~тесно связан с~моделью анализа: если включить в~используемую 
модель все факторы, от которых зависят пропуски в~этой модели, то будем 
работать в~рамках MAR; в~противном случае анализ не будет соответствовать 
предположениям MAR.   Самым жестким и~наименее 
реалистичным является MCAR, в~то время как MNAR является наименее ограничительным. Тем 
не менее наличие очень разнообразных предположений, возможных в~рамках 
MNAR, может рассматриваться как проблема (согласно~[4]); в~силу этого 
трудно предварительно определить один окончательный вариант анализа 
MNAR. Пе\-ре\-чис\-лен\-ные типы пропусков можно формально определить, 
используя обозначения, аналогичные обозначениям~[5]. 
{\looseness=1

}
     
     При некоторых предположениях пропуски могут быть признаны 
игнорируемыми. Пропуск классифицируется как игнорируемый, если можно 
\mbox{найти} действительную оценку результата без учета механизма отсутствия. 
В~своей первой работе, посвященной проблеме недостающих данных, 
автор~\cite{1-kri} показал, что при использовании байесовских процедур или 
правдоподобия для оценки любого параметра~$\Theta$, связанного 
с~клиническим исходом, недостающие данные игнорируются, когда механизм 
отсутствия является MAR, а~$\Theta$ является <<отличным от>> параметра 
механизма пропуска (кавычки используются, потому что условие отличимости 
является весьма специфичным).
     
     С учетом специфики данной статьи (обработка матрицы признак--объ\-ект, построение классификатора при наличии обучающей выборки) могут 
быть выделены следующие группы методов анализа неполных данных:
     \begin{itemize}
\item отбрасывание (игнорирование) пропусков с~последующим применением 
обычных процедур;
\item заполнение пропусков (вменение, приписывание недостающих значений) с~последующим применением обычных процедур;
\item обработка наблюденных и~пропущенных данных в~совокупности с~необходимостью разработки оригинальных процедур.
\end{itemize}

     При отбрасывании пропусков, т.\,е.\ обработке комплектных наблюдений 
(complete-case method в~англоязычной литературе), используются только 
объекты с~полными данными. Очевидным преимуществом такого анализа 
является простота реализации. Кроме того, он дает достоверные результаты 
в~случае MCAR. Тем не менее у~подхода, исклю\-ча\-юще\-го пациентов 
с~неполными данными, существует ряд недостатков: для относительно 
небольшого объема данных большой размерности процесс отбрасывания может 
привести к~тому, что нечего будет обрабатывать; 
игнорирование части данных влечет снижение эффективности по\-лу\-ча\-ющих\-ся 
решений; если механизм не является MCAR, то анализ может повлечь 
необъективное сравнение лечений (см. примеры в~[6]).
     
     Проблему пропущенных данных может решить, как кажется на первый 
взгляд, метод обработки доступных наблюдений (available-case method), когда 
за счет декомпозиции размерности обрабатываемых данных становится больше 
комплектных значений (например, если при нахождении ковариационной 
матрицы отдельно проводить оценивание для пар признаков). Но здесь при 
<<сборке>> итоговых характеристик из полученных фрагментов могут 
возникнуть свои трудности: перестанут выполняться требуемые свойства. 
В~качестве решения можно предложить корректировку получаемых оценок, 
только она носит индивидуальный характер для каждой решаемой задачи. 
     
     Следующий класс методов обработки неполных данных включает 
процедуры заполнения (imputing). Заполнение~--- это любой метод, при 
котором пропущенные значения в~наборе данных заполняются 
правдоподобными оценками. Цель любого метода заполнения заключается 
в~создании полного набора данных, который затем может быть 
проанализирован с~использованием стандартных статистических методов.
     
     Заполнение может осуществляться на основании частных или условных 
характеристик совокуп\-ности признаков. Наиболее простой, широко 
ис-\linebreak пользуемой и~постоянно критикуемой является\linebreak
 подстановка в~качестве 
пропущенных данных оценки безусловного среднего (или, например, медианы), 
полученной по доступным данным. Более перспективным выглядит заполнение 
пропусков\linebreak
 с~по\-мощью распределений, условных по отношению к~признакам, 
содержащим наблюденные значения. Это могут быть в~случае некоторого 
объекта как простейшие характеристики распределения (среднее или опять же 
медиана), так и~смоделированные значения из условного распределения. 
     
     Существуют и~другие методы единичного заполнения, в~частности метод 
hot-deck (обзор дан в~[7]). С ним связаны не всегда реализованные надежды на 
методы заполнения (в~част\-ности, см.\ работу~[8] с~многообещающим 
названием).
     
     В последнее время демонстрирует повышенное к~себе внимание 
в~литературе так называемое множественное заполнение. Его идея в~том, 
чтобы использовать для замещения пропусков более одного значения. 
В~результате его применения возникают два или более полных набора данных. 
В~за\-ви\-си\-мости от характера задачи результаты обработки этих наборов данных 
либо усредняются, либо из них выбираются экстремальные. Каждый из 
упомянутых методов дает достоверные оценки, когда данные являются MAR.
     
     Обработка наблюденных и~пропущенных данных в~совокупности 
основывается на построении модели данных об исследуемых характеристиках 
и~особенностях порождения пропусков. Выводы в~этом случае получают с~по\-мощью 
функции правдоподобия при условии, что действует игно\-ри\-ру\-ющий 
механизм пропуска. В~случае неигнорируемого отсева значений данный метод 
может приводить к~необъективным результатам (см., например,~[9]). 
     
     Понятно, что существует еще одна категория, которая определяется как 
<<другие>> методы анализа неполных данных, с~их примерами можно 
познакомиться в~\cite{6-kri}. Они обычно ориентированы на специфические 
задачи и~не работают в~задаче обуча\-емой классификации данных с~пропусками.

\vspace*{-6pt}

\section{Метод максимального правдоподобия}

\vspace*{-2pt}

     Рассмотрим задачу обучаемой классификации на основе модели смеси 
нормальных многомерных распределений в~условиях, когда отдельные 
значения признаков могут отсутствовать. Последнее имеет место как для 
обучающей выборки, так и~при классификации нового объекта. Принимая во 
внимание, что для структуры классов и~для каж\-до\-го класса принята 
определенная вероятностная модель, можно строить байесовский 
классификатор. Для этого в~первую очередь необходимо найти оценки 
параметров смеси, описывающей отдельный класс.
     
     Оценивание по методу МП для полных 
и~неполных данных принципиально не отличается, но проблемы реализации 
все-та\-ки возникают. Фундаментальную роль в~их решении играют базовая 
работа~[10] и~ее развернутое изложение в~\cite{3-kri}. 
 Во-пер\-вых, для эффективного применения метода МП требуется лишь MAR, 
а~не более жесткое условие MCAR. Во-вто\-рых, оценка МП неизвестных 
параметров модели должна находиться путем максимизации 
     $L(\Theta\vert \mathbf{x}_0)$, причем предполагается, что имеется 
возможность максимизировать 
     $L(\Theta\vert \mathbf{x}_0, \mathbf{x}_m)$, в~частности с~помощью 
EM-ал\-го\-рит\-ма; здесь~$\Theta$~---  совокупность параметров, 
опи\-сы\-ва\-ющих модель данных; $\mathbf{x}_0$ и~$\mathbf{x}_m$~--- 
наблюденные и~пропущенные значения соответственно. И наконец, в-треть\-их, 
наиболее важным моментом становится нахождение математического 
ожидания правдоподобия по распределению пропущенных данных.
     
     Смесь нормальных распределений в~качестве распределения данных 
вносит существенные трудности как в~вывод базовых соотношений, так 
и~в~реализацию итерационных шагов EM-ал\-го\-рит\-ма. Воспользуемся 
стандартными обозначениями: если плотность $d$-мер\-но\-го нормального 
распределения обозначить как 
     $\varphi(\mathbf{u},\vartheta)$, где 
$\vartheta\hm=(\boldsymbol{\mu},\boldsymbol{\Sigma})$, то плотность смеси 
нормальных распределений есть 

\noindent
     $$
     f(\mathbf{u},\Theta)= \sum\limits^K_{k=1} 
\pi_k \varphi (\mathbf{u},\vartheta_k)\,,$$ 
где $\Theta\hm= (\pi_1,\ldots,\pi_K, 
\vartheta_1,\ldots ,\vartheta_K)$.
     
     Исходные данные представляют собой последовательность объектов 
     $\{\mathbf{x}_i,\ i\hm=1,\ldots, N\}$, причем для каждого~$i$ определен 
набор индексов~$O_i$, указывающих на наблюденные признаки. Дополняет 
каждый~$O_i$ набор~$M_i$, соответствующий пропущенным данным. Эти 
наборы будут использоваться в~качестве верхних индексов у~обобщенных 
параметров и~матриц (векторов), служить для отсылки к~элементам 
соответствующих строк и~столбцов (строк). 
     
     Если предполагается наличие пропущенных данных, то логарифм 
правдоподобия принимает вид 

\noindent
     $$
     \ln L(\Theta)\hm= \sum\limits^N_{i=1} \ln\left(\sum\nolimits^K_{k=1} \pi_K 
\varphi\left(\mathbf{x}_i^{O_i},\vartheta_k^{O_i}\right)\right)\,,
$$
 где  
$\varphi(\mathbf{x}_i^{O_i},\vartheta_k^{O_i})$~--- маргинальная плотность 
нормального распределения для наблюденных значений~$\mathbf{x}_i^{O_i}$.
     
     Вектор значений признаков~$\mathbf{x}_i$ для $i$-го объекта запишем 
в~форме  $(\mathbf{x}_{O,i}, \mathbf{x}_{m,i})$, где~$\mathbf{x}_{O,i}$ 
и~$\mathbf{x}_{m,i}$ соответственно обозначают наблюденные 
и~пропущенные значения признаков. Это формальная запись, не означающая 
переупорядочения в~соответствии со структурой пропусков. При подборе 
модели смеси распределений в~присутствии пропущенных значений признаков 
возникают два типа пропусков: один, концептуальный (привнесенный), 
связанный с~ненаблюдаемым индика-\linebreak\vspace*{-12pt}

\pagebreak

\noindent
тором~$z_{ik}$ принадлежности 
некоторого объекта~$\mathbf{x}_i$ к~элементу смеси с~номером~$k$, и~другой, 
ненамеренный (не\-предусмот\-рен\-ный), ему соответствует 
обозначение~$\mathbf{x}_{m,i}$. 
     
     EM-алгоритм на ($t\hm+1$)-м шаге итерации требует вычисления 
     $$
     Q\left(\Theta\vert \Theta^{(t)}\right)= {\sf E}\left\{ L(\Theta\vert 
\mathbf{x}_0,\Theta^{(t)}\right\}\,.
$$

Для нахождения~$Q(\Theta\vert \Theta^{(t)})$ 
понадобится

\noindent
     \begin{multline*}
     \hat{z}_{ik}=\hat{z}_{ik}^{(t)} ={\sf E}\left\{ z_{ik}\vert 
\mathbf{x}_{O,i};\Theta^{(t)}\right\}={}\\[-2pt]
{}=
     \fr{\pi_k \varphi_k\left(\mathbf{x}_{O,i};\vartheta_k^{(t)}\right)} 
{\sum\nolimits^K_{k=1} \pi_k\varphi_k\left(\mathbf{x}_{O,i}; \vartheta_k^{(t)}\right)}\,.
\end{multline*}
     
Для достаточных статистик каждого элемента смеси сначала получаем:

\noindent
     \begin{multline*}
     {\sf E}\left\{ z_{ik}x_{ij} \vert \mathbf{x}_{O,i};\vartheta_k^{(t)}\right\}={}\\
     {}= \begin{cases}
     \hat{z}_{ik}x_{ij}\,, &\ \mbox{если\ $x_{ij}$\ наблюдено;}\\
     \hat{z}_{ik} {\sf E}\left\{ x_{ij}\vert \mathbf{x}_{O,i};\vartheta_k^{(t)}\right\}\,, 
&\ \mbox{если\ $x_{ij}$\ пропущено.}
     \end{cases}
     \end{multline*}
     
Далее подобным же образом можно выписать выражения для
${\sf E}\left\{ z_{ik} x_{ij}^2\vert \mathbf{x}_{O,i};\vartheta_k^{(t)}\right\}$ 
и~${\sf E}\left\{ z_{ik}x_{ij}x_{ij^\prime}\vert 
\mathbf{x}_{O,i};\vartheta_k^{(t)}\right\}$, $i,j\hm=1,\ldots , N$, $k\hm=1,\ldots , 
K$, $j\not= j^\prime$. Заметим, если одно значение некоторого признака 
пропущено, то оно просто заменяется на $E\left\{ 
x_{ij}\vert\mathbf{x}_{O,i};\vartheta_k^{(t)}\right\}$. Далее для теперь уже 
полных данных M-шаг EM-ал\-го\-рит\-ма приводит к~оценкам параметров 
смеси общего вида. Для того чтобы сделать конкретными полученные 
представления, необходимо воспользоваться видом условного нормального 
распределения.
     
     Завершим рассуждения итоговыми представлениями. Пусть для каждого 
элемента смеси заданы~$\pi_k$, $\boldsymbol{\mu}_k$ и~$\boldsymbol{\Sigma}_k$ 
(ссылка на шаг итерации опущена). Тогда для $i\hm=1,\ldots ,N$ 
и~$k\hm=1,\ldots , K$ результаты очередного E-ша\-га следующие:

\noindent
    \begin{align*}
     \hat{z}_{ik} &= \fr{\pi_k \varphi_k 
\left(\mathbf{x}_i^{O_i},\vartheta_k^{O_i}\right)}{\sum\nolimits^K_{k=1} \pi_k 
\varphi_k\left(\mathbf{x}_i^{O_i},\vartheta_k^{O_i}\right)}\,;
\\
     \tilde{\boldsymbol{\mu}}_{ik}^{M_i}& = {\sf E}\left\{ \mathbf{x}_i^{M_i} \vert 
\mathbf{x}_i^{O_i}\right\} ={}\\
&\hspace*{-5mm}{}={\boldsymbol{\mu}}_k^{M_i} 
+{\boldsymbol{\Sigma}}_k^{M_iO_i}\left({\boldsymbol{\Sigma}}_k^{O_iO_i}
\right)^{-1}\left( \mathbf{x}_i^{O_i}-{\boldsymbol{\mu}}_k^{O_i}\right)\,;\\ 
     \tilde{\mathbf{x}}_{ik}&=\begin{pmatrix}
     \mathbf{x}_i^{O_i}\\
     \tilde{\mu}_{ik}^{M_i}
     \end{pmatrix}\,;
     \\
     \tilde{{\boldsymbol{\Sigma}}}_{ik}^{M_iM_i} &= 
{\boldsymbol{\Sigma}}_k^{M_iM_i}- {\boldsymbol{\Sigma}}_k^{M_iO_i}\left( 
     {\boldsymbol{\Sigma}}_k^{O_iO_i}\right)^{-1} 
{\boldsymbol{\Sigma}}_k^{O_iM_i}\,;\\
     \tilde{{\boldsymbol{\Sigma}}}_{ik}&=\begin{pmatrix}
     \mathbf{0}^{O_iO_i} & \mathbf{0}^{O_iM_i}\\
     \mathbf{0}^{M_iO_i} & \tilde{{\boldsymbol{\Sigma}}}_{ik}^{M_iM_i}
     \end{pmatrix}\,.
    \end{align*}
     
     Далее M-шаг приводит к~новым значениям оценок~$\hat{\pi}_k$, 
$\hat{\boldsymbol{\mu}}_k$ и~$\hat{\boldsymbol{\Sigma}}_k$:
     $$
     \hat{\pi}_k=\fr{1}{N}\sum\limits_{i=1}^N \hat{z}_{ik}\,;
     $$
     $$
     \hat{\boldsymbol{\mu}}_k= \fr{\sum\nolimits_{i=1}^N 
\hat{z}_{ik}\tilde{\mathbf{x}}_{ik}}{\sum\nolimits^N_{i=1} \hat{z}_{ik}}\,;
     $$
     
\vspace*{-12pt}

\noindent
\begin{multline*}
     \hat{\boldsymbol{\Sigma}}_k = \fr{\sum\nolimits^N_{i=1}\hat{z}_{ik}[ 
\tilde{\mathbf{x}}_{ik} -\hat{\boldsymbol{\mu}}_k) (\tilde{\mathbf{x}}_{ik} - 
\hat{\boldsymbol{\mu}}_k)^{\mathrm{T}}+\tilde{\boldsymbol{\Sigma}}_{ik}]} 
{\sum\nolimits^N_{i=1}\hat{z}_{ik}} ={}\\
\hspace*{-2pt}{}=
     \fr{\sum\nolimits^N_{i=1}\! \hat{z}_{ik} (\tilde{\mathbf{x}}_{ik} - 
\hat{\boldsymbol{\mu}}_k) (\tilde{\mathbf{x}}_{ik}-
\hat{\boldsymbol{\mu}}_k)^{\mathrm{T}}} 
{\sum\nolimits^N_{i=1}\!\hat{z}_{ik}}+
     \fr{\sum\nolimits^N_{i=1} \hat{z}_{ik}\tilde{\boldsymbol{\Sigma}}_{ik}} 
{\sum\nolimits^N_{i=1}\hat{z}_{ik}}.\hspace*{-1.6729pt}
    \end{multline*}
     
     Остановимся на формулах байесовской классификации (в~случае 
единичной матрицы потерь) наблюдений с~пропущенными значениями 
отдельных признаков. Пусть $s$-й класс описывается смесью нормальных 
распределений 
     $$
     f_s(\mathbf{u})= \sum\limits_{k=1}^K 
\pi_{sk}\varphi(\mathbf{u},\vartheta_{sk})\,,\enskip 
s=1,\ldots ,S\,,
$$
 а вероятность его 
появления равна~$q_s$. Тогда предпочтение для некоторого наблюденного 
вектора~$\mathbf{y}_0$ будет отдаваться тому классу, для которого~$q_s 
f_s(\mathbf{y}_0)$ достигает по~$s$ своего максимума. 

\vspace*{-6pt}

\section{Эксперименты}

\vspace*{-2pt}
     
     Экспериментальная часть работы заключалась в~анализе зависимости 
качества классификации от степени пропуска отдельных значений. Для этого 
использовались результаты реальных обследований, приведенных 
в~\cite[разд.~5]{12-kri}. В~этой работе применялась дискриминация на основе 
нормального распределения данных о ферментах, полученных 
для~218~пациентов с~заболеваниями печени. Рассматривались четыре 
заболевания: острый вирусный гепатит (далее для краткости~D$_1$, 
57~пациентов); хронический персистирующий гепатит (D$_2$, 44~пациента); 
агрессивный хронический гепатит (D$_3$, 40~пациентов); постнекротический 
цирроз (D$_4$, 77~пациентов). Диагноз острого вирусного гепатита 
основывался на классических кли\-ни\-ко-био\-хи\-ми\-че\-ских признаках. Все 
остальные пациенты были диагностированы на основе результатов 
лапароскопии и~биопсии. Лабораторные исследования, на основе результатов 
которых должна\linebreak\vspace*{-12pt}

\pagebreak

\noindent
 проводиться классификация, заключались в~измерениях для 
четырех ферментов печени: аспартатаминотрансфераза (AST), 
аланинаминотрансфераза (ALT), глутаматдегидрогеназа (GLDH) 
и~орнитинкарбонилтрансфераза (OCT). 
     
     Для представления элементов обучающей выборки для каждого из 
классов D$_1$--D$_4$ использовалась смесь нормальных распределений из пяти 
элементов. Значение $K\hm=5$ было выбрано как компромисс после 
применения информационных критериев AIC (Akaike Information Criterion), 
BIC (Bayesian Information Criterion),  
AWE (Approximate Weight of Evidence)~\cite[разд.~2.3.3]{13-kri}, 
которые показали достаточно четко, что $K\hm>1$
  (модель нормального распределения не подходит), и~весьма расплывчато, что 
     $K\hm\in [2,10]$. Возникновение пропусков моделировалось с~по\-мощью 
индикаторной переменной~$r_{ij}\hm=1$ с~вероятностью~$1-p_m$ 
и~$r_{ij}\hm=0$ с~вероятностью~$p_m$.
     
     Для каждого выбранного значения~$p_m$ генерировалась индикаторная 
матрица, затем на ее основе с~помощью одного из методов обработки данных 
с~пропусками строились оценки смесей, описывающих классы D$_1$--D$_4$, 
и~тем самым формировался эмпирический байесовский классификатор; далее 
методом перепроверки оценивалась вероятность~$p_c^*$ правильного решения 
подобного классификатора. Подобная процедура повторялась~$N_{\mathrm{exp}}$ раз, 
и~оценивались выборочные характеристики для~$p_c^*$: среднее, стандартное 
отклонение, минимальное и~максимальное значения. В~ходе экспериментов 
при больших значениях~$p_m$ возникла проб\-ле\-ма с~появлением 
<<пустых>> (с~пол\-ностью пропущенными значениями) столбцов и~строк 
матрицы приз\-нак--объ\-ект для исходных данных. В~рамках данного 
исследования она решалась путем отбрасывания подобных столбцов (для 
некоторого объекта нет ни одного наблюденного признака) и~строк (для 
некоторого признака и~хотя бы одного класса полностью отсутствуют 
наблюдения). Это привело к~фактическому снижению размерности 
признакового пространства~$d$ и~объема обучающей выборки~$N$ 
и~необходимости в~отказе от классификации в~случае полного вырождения 
признакового пространства и~снижения объема части обучающей выборки для 
некоторого класса ниже критического (в~данной работе это 
     $d\hm+1$). 
     
     Результаты моделирования при $N_{\mathrm{exp}}\hm=100$ для методов 
комплектных данных (CC), заполнения частным средним (PM), заполнения 
условным средним (CM), МП (ML) приведены на 
рисунке. Для метода CC проявляется эффект отказа от классификации: если 
при $p_m\hm=20\%$ относительная  частота 
 составляла~0\%, то при 
$p_m\hm=30\%$~--- 4\%, при $p_m\hm=40\%$~---  64\%, при  
$p_m\hm=50\%$~--- 100\%. 


 { \begin{center}  %fig1
 \vspace*{-4pt}
 \mbox{%
\epsfxsize=77.656mm
\epsfbox{kri-1.eps}
}


\end{center}


\noindent
{\small{Зависимость оценки вероятности правильной классификации~$p_c^*$ от вероятности 
пропуска~$p_m$ для методов CC~(\textit{1}), PM~(\textit{2}), CM~(\textit{3}) 
и~ML~(\textit{4})}}

}

\vspace*{12pt}

\addtocounter{figure}{1}






     
     Из рисунка можно сделать выводы:
     \begin{itemize}
     \item в~диапазоне $p_m\hm=0$\%--20\% результативность всех методов 
практически одинакова;
\item в~диапазоне всех представленных значений~$p_m$ методы PM и~CM 
фактически совпадают;
\item метод ML обладает очевидными преимуще\-ст\-вами.
\end{itemize}

     Если к~полученным результатам добавить информацию о временн$\acute{\mbox{о}}$й 
сложности анализируемых методов, то можно заключить, что при малых 
значениях вероятности пропуска 
     ($p_m\hm=0\%\mbox{--}20\%$) следует пользоваться простыми методами 
комплектных данных и~заполнения частным средним, при больших значениях 
вероятности пропуска предпочтение следует отдать методу МП. 
     
     Целесообразность усложнения модели данных за счет перехода от просто 
нормального распределения к~смеси таковых можно проиллюстрировать 
сравнением поведения эффективности методов PM и~ML при $K\hm=5$ 
и~1, что дает приблизительно 10\%-ное снижение значений~$p_c^*$ 
в~последнем случае.

\section{Заключение}

     В ходе исследований пришлось заново про\-вес\-ти строгий вывод формул, 
обеспечивающих реа\-ли\-за\-цию шагов EM-ал\-го\-рит\-ма. 
%
Дело в~том, что %\linebreak 
в~большин\-ст\-ве доступных работ~[13--16] лишь декла\-ри\-ро\-ва\-лись базовые 
соотношения, иногда с~неточностями. Самое важное, встречались 
со\-мни\-тель\-ные переходы при получении ре\-зуль\-та\-тов (на\-при\-мер, не было ясно, 
является ли фактическое запол\-не\-ние пропущенных значений на %\linebreak 
\mbox{M-ша}\-ге 
следствием удобства или формальным ре\-зуль\-татом, следующим из общих 
оптимизационных принципов), вообще не упоминались условия и~приемы 
регуляризации оценок вторых моментов. 
     
     Полученные результаты позволили корректно применять итерационный 
процесс получения оценок параметров смеси. В~качестве начального шага 
итерационного EM-ал\-го\-рит\-ма было предложено использовать метод 
заполнения пропущенных данных частными средними и~случайный перебор 
матриц апостериорных вероятностей. 
     
     Экспериментальная часть работы заключалась в~анализе зависимости 
качества классификации от степени пропуска отдельных значений на примере 
данных о~ферментах, полученных для пациентов с~заболеваниями печени. 
Обработка реальных данных продемонстрировала практически идентичные 
ошибки классификации при применении простых и~сложных методов 
обработки пропусков в~случае невысокой степени случайного пропуска 
отдельных значений. Рассмотренный метод заполнения пропусков условными 
средними является вариацией на тему использования метода hot-deck, его 
очевидная <<непрактичность>> (времена обработки по методам CC, PM, CM, 
ML относятся как $1:1:92:2$) при фактически той же эффективности по 
сравнению с~более простыми методами позволяют усомниться в~
выводах~\cite{8-kri}. 
     
     Метод МП совместно со смесью нормальных 
распределений в~качестве модели данных обладает безусловными 
преимуществами, хотя и~оказывается достаточно сложным при реализации. 
Последнее стимулирует исследования по анализу и~разработке 
соответствующих алгоритмов обработки данных, в~частности 
с~использованием идеи sweep-опе\-ра\-то\-ра~\cite{3-kri} или 
приемов древовидной организации вычислений, упомянутых в~\cite{16-kri}.
     
{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{1-kri}
\Au{Rubin D.\,B.} Inference and missing data~// Biometrika, 1976. Vol.~63.  
P.~581--592. 
\bibitem{2-kri}
\Au{Rubin D.\,B.} Multiple imputation for nonresponse in surveys.~--- New York, 
NY, USA: John Wiley\,\&\,Sons, 1987. 256~p.
\bibitem{3-kri}
\Au{Little R.\,J.\,A., Rubin~D.\,B.} Statistical analysis with missing data.~--- 2nd 
ed.~--- New York, NY, USA: John Wiley\,\&\,Sons, 2002. 408~p.
\bibitem{4-kri}
\Au{Mallinckrodt C.\,H., Lane~ P.\,W., Schnell~D., Peng~Y., Mancuso~J.} 
Recommendation for the primary analysis of continuous endpoints in longitudinal 
clinical trials~// Drug Inf.~J., 2008. Vol.~42. P.~303--319.
\bibitem{5-kri}
\Au{Molenberghs G., Kenward~M.\,G.} Missing data in clinical studies.~--- West 
Sussex: John Wiley\,\&\,Sons, 2007. 526~p.
\bibitem{6-kri}
\Au{Myers W.\,R.} Handling missing data in clinical trials: An overview~// Drug 
Inf.~J., 2000. Vol.~34. P.~525--533.
\bibitem{7-kri}
\Au{Andridge R.\,R., Little~R.\,J.\,A.} A~review of hot deck imputation for survey 
non-response~// Int. Stat. Rev., 2010. Vol.~78. No.\,1. P.~40--64.
\bibitem{8-kri}
\Au{Myers T.\,A.} Goodbye, listwise deletion: Presenting hot deck imputation as an 
easy and effective tool for handling missing data~// Commun. Meth.  
Measures, 2011. Vol.~5. No.\,4. P.~297--310.
\bibitem{9-kri}
\Au{Little R.\,J.\,A.} Modeling the drop-out mechanism in repeated-measures 
studies~// J.~Am. Stat. Assoc., 1995. Vol.~90. No.\,431.  
P.~1112--1121.
\bibitem{10-kri}
\Au{Dempster A.\,P., Laird~N.\,M., Rubin~D.\,B.} Maximum likelihood from 
incomplete data via EM algorithm~// J.~Roy. Stat. Soc.~B Met., 
1977. Vol.~39. No.\,1. P.~1--38.
%\bibitem{11-kri}
%\Au{Литтл Р.\,Д.\,А., Рубин~Д.\,Б.} Статистический анализ данных 
%с~пропусками.~--- М.: Финансы и~статистика, 1991. 336~с.
\bibitem{12-kri}
\Au{Alber A.} Multivariate interpretation of clinical laboratory data.~--- New York, 
NY, USA: CRC Press, 1987. 386~p.
\bibitem{13-kri}
\Au{Кривенко М.\,П.} Статистические методы представления и~статистической 
предварительной обработки рефeренсных значений.~--- М.: ФИЦ ИУ РАН, 
2016. 160~c.
\bibitem{14-kri}
\Au{Ghahramani Z., Jordan~M.\,I.} Learning from incomplete data.~--- 
MIT AI, 1994.  A.I.\ 
Memo No.\,1509. C.B.C.L. Paper No.\,108. {\sf 
https://dspace.mit.edu/\linebreak handle/1721.1/7202}.
\bibitem{15-kri}
\Au{Hunt L., Jorgensen~M.} Mixture model clustering for mixed data with missing 
information~// Comput. Stat. Data An., 2003. Vol.~41. P.~429--440.
\bibitem{16-kri}
\Au{Delalleau O., Courville~A., Bengio~Y.} Efficient EM training of Gaussian 
mixtures with missing data. arXiv.org, 2012. {\sf https://arxiv.org/abs/1209.0521}.
\bibitem{17-kri}
\Au{Eirola E., Lendasse~A., Vandewalle~V., Biernacki~C.} Mixture of Gaussians for 
distance estimation with missing data~// Neurocomputing, 2014. Vol.~131.  
P.~32--42.
 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-3pt}

\hfill{\small\textit{Поступила в~редакцию 14.06.17}}

%\vspace*{8pt}

\newpage

\vspace*{-28pt}

%\hrule

%\vspace*{2pt}

%\hrule

%\vspace*{8pt}


\def\tit{SUPERVISED LEARNING CLASSIFICATION OF~INCOMPLETE CLINICAL DATA}

\def\titkol{Supervised learning classification of~incomplete clinical data}

\def\aut{M.\,P.~Krivenko}

\def\autkol{M.\,P.~Krivenko}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-9pt}


\noindent
Institute of Informatics Problems, Federal Research Center ``Computer Science and Control'' of the 
Russian Academy of Sciences, 44-2~Vavilov Str., Moscow 119333, Russian Federation



\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2017\ \ \ volume~11\ \ \ issue\ 3}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2017\ \ \ volume~11\ \ \ issue\ 3
\hfill \textbf{\thepage}}}

\vspace*{3pt}

 
\Abste{The article examines the effectiveness of classification methods for incomplete clinical 
data. Training Bayesian classifier is carried out by the maximum likelihood method for the model 
of a mixture of normal distributions. Rigorous derivation of formulas ensuring the realization of the 
steps of the EM algorithm allowed correctly applying the iterative process of obtaining estimates of 
the parameters of the mixture. For incomplete data, methods for selecting initial values and 
correcting degenerate covariance matrices for the elements of the mixture are proposed. The 
experimental part of the work consisted in analyzing the dependence of the quality of classification 
on the number of missing individual values, using data on enzymes obtained for patients with liver 
diseases. The real data treatment has demonstrated almost identical classification errors when 
applying simple and complex methods of processing of missing values in the case of low number of 
randomly missing individual values.} 

\KWE{missing data; EM algorithm; mixtures of normal distributions}



\DOI{10.14357/19922264170303} 

%\vspace*{-18pt}

%\Ack
%\noindent



%\vspace*{3pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{1-kri-1}
\Aue{Rubin, D.\,B.} 1976. Inference and missing data. \textit{Biometrika}  
63:581--592. 
\bibitem{2-kri-1}
\Aue{Rubin, D.\,B.} 1987. \textit{Multiple imputation for nonresponse in surveys}. 
New York, NY: John Wiley\,\&\,Sons. 256~p.
\bibitem{3-kri-1}
\Aue{Little, R.\,J.\,A, and D.\,B.~Rubin.} 2002. \textit{Statistical analysis with 
missing data}. 2nd ed. New York, NY: John Wiley\,\&\,Sons. 408~p.
\bibitem{4-kri-1}
\Aue{Mallinckrodt, C.\,H., P.\,W.~Lane, D.~Schnell, Y.~Peng, and J.~Mancuso}. 
2008. Recommendation for the primary analysis of continuous endpoints in 
longitudinal clinical trials. \textit{Drug Inf.~J.} 42:303--319.
\bibitem{5-kri-1}
\Aue{Molenberghs, G., and M.\,G.~Kenward.} 2007. \textit{Missing data in clinical 
studies}. West Sussex: John Wiley\,\&\,Sons. 526~p.
\bibitem{6-kri-1}
\Aue{Myers, W.\,R.} 2000. Handling missing data in clinical trials: An overview. 
\textit{Drug Inf.~J.} 34:525--533.
\bibitem{7-kri-1}
\Aue{Andridge, R.\,R., and R.\,J.\,A.~Little.} 2010. A~review of hot deck imputation 
for survey non-response. \textit{Int. Stat. Rev.} 78(1):40--64.
\bibitem{8-kri-1}
\Aue{Myers, T.\,A.} 2011. Goodbye, listwise deletion: presenting hot deck 
imputation as an easy and effective tool for handling missing data. 
\textit{Commun. Meth. Measures} 5(4):297--310.
\bibitem{9-kri-1}
\Aue{Little, R.\,J.\,A.} 1995. Modeling the drop-out mechanism in  
repeated-measures studies. \textit{J.~Am. Stat. Assoc.} 
90(431):1112--1121.
\bibitem{10-kri-1}
\Aue{Dempster, A.\,P., N.\,M.~Laird, and D.\,B.~Rubin.} 1977. Maximum 
likelihood from incomplete data via EM algorithm. \textit{J.~Roy. Stat. Soc.~B 
Met.} 39(1):1--38.
%\bibitem{11-kri-1}
%\Aue{Little, R.\,D.\,A., and D.\,B.~Rubin.} 1991. \textit{Statisticheskiy analiz 
%dannykh s~propuskami} [Statistical analysis with missing data]. Moscow: Finansy 
%i~statistika. 336~p.
\bibitem{12-kri-1}
\Aue{Alber, A.} 1987. \textit{Multivariate interpretation of clinical laboratory data}. 
New York, NY: CRC Press. 386~p.
\bibitem{13-kri-1}
\Aue{Krivenko, M.\,P.} 2016. \textit{Statisticheskie metody predstavleniya 
i~statisticheskoy predvaritel'noy obrabotki referensnykh znacheniy} [Statistical 
methods for representation and pretreatment of reference values]. Moscow: FRC CSC RAS. 
160~p.
\bibitem{14-kri-1}
\Aue{Ghahramani, Z., and M.\,I.~Jordan.} 1995. Learning from incomplete data. 
MIT AI. A.I. Memo No.\,1509. C.B.C.L. Paper No.\,108. Available 
at: {\sf https:// dspace.mit.edu/handle/1721.1/7202} (accessed June~14, 2017).
\bibitem{15-kri-1}
\Aue{Hunt, L., and M.~Jorgensen.} 2003. Mixture model clustering for mixed data 
with missing information. \textit{Comput. Stat. Data An.} 41:429--440.
\bibitem{16-kri-1}
\Aue{Delalleau, O., A.~Courville, and Y.~Bengio.} 2012. Efficient EM training of 
Gaussian mixtures with missing data. Available at: {\sf 
https://arxiv.org/abs/1209.0521} (accessed June~14, 2017).
\bibitem{17-kri-1}
\Aue{Eirola, E., A.~Lendasse, V.~Vandewalle, and C.~Biernacki}. 2014. Mixture 
of Gaussians for distance estimation with missing data. \textit{Neurocomputing} 
131:32--42.
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Received June 14, 2017}}

\vspace*{-18pt}

\Contrl

\noindent
\textbf{Krivenko Michail P.} (b.\ 1946)~--- Doctor of Science in technology, 
professor, leading scientist, Institute of Informatics Problems, Federal Research 
Center ``Computer Science and Control'' of the Russian Academy of Sciences,  
44-2~Vavilov Str., Moscow 119333, Russian Federation; 
\mbox{mkrivenko@ipiran.ru}


\label{end\stat}


\renewcommand{\bibname}{\protect\rm Литература} 
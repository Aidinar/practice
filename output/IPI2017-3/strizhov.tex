\newcommand{\TSF}{{\sf T}}

\def\stat{strijov}

\def\tit{ПОВЫШЕНИЕ КАЧЕСТВА КЛАССИФИКАЦИИ В~ЗАДАЧЕ~ОБНАРУЖЕНИЯ ВНУТРЕННЕГО ПЛАГИАТА$^*$}

\def\titkol{Повышение качества классификации в~задаче обнаружения внутреннего плагиата}

\def\aut{И.\,О.~Молибог$^1$, А.\,П.~Мотренко$^2$, В.\,В.~Стрижов$^3$}

\def\autkol{И.\,О.~Молибог, А.\,П.~Мотренко, В.\,В.~Стрижов}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Молибог И.\,О.}
\index{Мотренко А.\,П.}
\index{Стрижов В.\,В.}
\index{Molybog I.\,O.}
\index{Motrenko A.\,P.}
\index{Strijov V.\,V.}


{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
{Работа выполнена при финансовой поддержке РФФИ (проект 16-07-01155).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Центр энергетических систем, Сколковский институт науки и~технологий; 
Московский фи\-зи\-ко-тех\-ни\-че\-ский институт,  \mbox{i.molybog@skoltech.ru}}
\footnotetext[2]{Московский физико-технический институт, \mbox{anastasiya.motrenko@phystech.edu}}
\footnotetext[3]{Вычислительный центр им.\ А.\,А.~Дородницына Федерального исследовательского 
центра <<Информатика и~управление>> Российской академии наук, \mbox{strijov@phystech.edu}}

%\vspace*{-18pt}


\Abst{Исследуется задача классификации объектов в~многомерных пространствах. 
Для снижения размерности задачи предлагается модификация алгоритма t-SNE
 (\textit{англ}.\ t-distributed Stochastic 
Neighbor Embedding), 
в~которой при обучении используется информация о~разметке, не возникает необходимости 
заново обучать алгоритм при добавлении новых данных, а также предусмотрена 
параллельная реализация. Предлагаемый алгоритм решает задачу внутреннего плагиата, 
в~которой признаками являются частотные словесные профили сегментов текста. 
Показано, что качество классификации после применения алгоритма выше, чем 
без него или с~другими алгоритмами.}

\KW{анализ данных; снижение размерности; нелинейные методы снижения размерности; 
обучение многообразий; обнаружение внутреннего плагиата}

\DOI{10.14357/19922264170307} 

\vspace*{6pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}

\section{Введение}

В работе рассматривается задача классификации объектов в~пространствах большой 
раз\-мер\-ности, признаковое описание которых имеет в~себе скрытые функциональные 
зависимости.
Предполагается, что объекты содержатся вблизи многообразия много меньшей размерности, 
чем размерность исходного пространства. Назовем это предположение гипотезой 
многообразия~\cite{fefferman2016testing}. Данные ряда практических задач, включая 
задачи анализа генома, анализа текста и~распознавания изображений, не противоречат 
этой гипотезе~\cite{maaten2008visualizing}. В~\cite{narayanan2010sample} было 
дано ее формальное определение и~перечислены идеи методов, которыми ее можно 
проверить. Практической задачей, рассматриваемой в~данной работе, является задача 
обнаружения внутреннего плагиата~\cite{zu2006intrinsic, kuznetsov2016methods}.

Задача обнаружения внутреннего плагиата состоит в~поиске заимствованных частей 
документа без использования внешних источников. При решении задачи исследуемый 
текст некоторым образом разбивается на сегменты. Каждому сегменту соответствует 
его вектор признаков. Сегмент считается минимальной единицей заимствования. Он считается 
либо полностью заимствованным, либо полностью оригинальным. Тогда задача обнаружения 
внутреннего плагиата является задачей классификации, где объектами являются векторы 
признаков сегментов, а~классами~--- метки заимствования или оригинальности.

Способы разбиения на сегменты, как и~способы вычисления вектора признаков, являются 
предметом отдельного исследования. 
Подходы~\cite{zu2006intrinsic,muhr2010external,stamatatos2009intrinsic,kestemont2011intrinsic} 
продемонстрировали 
на конкурсе PAN-2011~\cite{potthast2011overview} наилучшее качество решения задачи 
обнаружения внутреннего плагиата. Они включают разбиение документа на абзацы, 
предложения, блоки слов или символов. В~них используются признаки, основанные на 
частотных профилях сегментов. Такие признаки имеют размерность, пропорциональную 
числу слов в~документе, сильно разрежены и~не всегда информативны.

В данной работе предполагается, что объекты с~таким признаковым описанием
 подчиняются гипотезе многообразия. Это означает, что метрически близкие объекты 
 могут быть геодезически далекими, и~дает возможность применить методы снижения 
 размерности для улучшения качества классификации.

В задаче понижения размерности требуется построить гладкое отображение 
множества~$\mathbf{X}$ в~пространстве исходных данных в~некоторое 
мно\-же\-ст\-во~$\mathbf{Z}$ в~пространстве меньшей размерности.\linebreak Будем называть 
элементы~$\mathbf{Z}$ образами элементов~$\mathbf{X}$. Пространство образов 
будем называть результирующим.
В конкретных алгоритмах на это отображение накладывают необходимые ограничения, 
исходя из специфики задачи~\cite{fodor2002survey}. Приведем некоторые из них.

Для снижения размерности широко применяются линейные методы, основанные на 
анализе дисперсии: латентно-семантический 
анализ~\cite{brooke2012paragraph, brooke2012unsupervised}, анализ главных 
компонент~\cite{gorban2008principal}. Одна\-ко они могут не сохранять кластерную 
структуру исходных данных и~потому не применимы для решения задач вложений из 
нелинейных мно\-го\-об\-разий.
{ %\looseness=1

}

Для выполнения вложений из нелинейных многообразий были разработаны алгоритмы, 
использующие изометрические отображения. Алгоритмы ISOMAP
(Isometric Mapping)~\cite{tenenbaum2000global} 
и~Laplacian Eigenmap~\cite{belkin2001laplacian} приближают геодезическое расстояние 
с~по\-мощью графа~$k$ ближайших соседей. Алгоритмы Local Linear 
Embedding (LLE)~\cite{roweis2000nonlinear} и~Hessian-based LLE~\cite{donoho2003hessian} основаны на предположении, что 
многообразие аппроксимируется ку\-соч\-но-ли\-ней\-ной функцией. Для каждого 
объекта исходного пространства строится его линейное приближенное описание 
через соседние объекты, после чего по этим описаниям строятся образы в~ре\-зуль\-ти\-ру\-ющем 
пространстве. Метод~\cite{donoho2003hessian} использует для описания объектов 
специальную квадратичную форму, что гарантирует асимптотическую оптимальность 
метода даже в~случае невыпуклых множеств.

Алгоритм Local Tangent Space Alignment Algorithm~\cite{zhang2004principal} 
также использует ку\-соч\-но-ли\-ней\-ную аппроксимацию. Многообразие приближается 
гиперплоскостью в~окрестности каждой точки, после чего полученные приближения 
сглаживаются между собой. При помощи Semidefinite 
Embedding~\cite{weinberger2006unsupervised} можно получить вложение, в~котором 
сохранены точные расстояния между ближайшими объектами. Для этого метод 
максимизирует след матрицы Грама для образов при ограничениях, накладываемых 
отношением соседства объектов исходного пространства~и их~матрицей Грама.

Все перечисленные методы нацелены на наиболее точное сохранение расстояний между 
объектами при снижении размерности. Это может при\-вес\-ти к~неустойчивости решения, 
связанной с~тем, что изменения расстояния между далекими и~близкими объектами 
штрафуются одинаково. Кроме того, они не приспособлены для решения задачи 
классификации, поскольку не учитывают разметку при выполнении вложения, хотя 
существуют их модификации, обладающие этим свойством. В~\cite{chen2010distance} 
метод аппроксимации расстояний, используемый в~ISOMAP, модифицирован в~методе 
оптимизации целевого функционала. Полученный метод получил название TRIMAP. 
В~нем при обучении используется разметка обучающей выборки.

В данной работе применяется метод t-NSE~\cite{maaten2008visualizing}. Выгодной особенностью метода t-SNE 
является склонность к~локализации изолированных плотных пространственных структур 
произвольной геометрии. Под изолированной плотной структурой подразумевается 
множество точек, имеющих близких соседей из той же структуры, но сравнительно 
удаленных от всех точек не из нее. Такой эффект достигается тем, что близким 
и~далеким объектам назначаются разные приоритеты.

Недостатком метода t-SNE в~отношении задачи классификации является то, что в~нем 
не преду\-смот\-ре\-но функции вложения объектов, не участвовавших в~построении уже 
существующего вложения. В~работе~\cite{van2009learning} описана параметрическая 
модификация t-SNE, которая частично избавлена от этой особенности, однако в~данной 
работе она не использовалась.

Дополнительным ограничением применимости метода t-SNE является высокая по 
сравнению с~другими методами вложений вычислительная сложность. 
Хотя в~\cite{van2014accelerating} предлагаются два способа вычисления градиента, 
при использовании которых сложность непараметрического t-SNE 
со\-став\-ля\-ет~$O(k  m \log(m))$, где~$m$ ~--- размер выборки, а~$k$ ~--- 
размерность результирующего пространства, этого ускорения недостаточно для 
обеспечения комфортной работы даже с~выборками длиной порядка~$10^3$.

Основным вкладом данной статьи в~теорию распознавания образов является предложенная 
модификация метода t-SNE, позволяющая строить классификаторы в~результирующем 
пространстве. Преиму\-ществом предлагаемого метода является то, что он расширяет 
границы применимости оригинального метода t-SNE. Разработанная модификация 
предусматривает вложение тестовых данных без повторного вложения обучающих, 
а~также может учитывать разметку обучающих данных и~имеет параллельную реализацию.

\section{Постановка задачи}

Обозначим~$\mathbb X \subset \mathbb R^n$ множество всех возможных 
векторов~$\mathbf x$ признаков изучаемых объектов.
Предполагается, что объекты~$\mathbb X$ подчиняются
\textit{гипотезе многообразия}: найдется гладкое 
отображение~$\mathbf f: \mathbb {R}^{d} \hm\to \mathbb {R}^{n}$ такое, что
$$
\mbox{для~} \mathbf x~ \in \mathbf{X}~\mbox{существует~} \mathbf z^* \in 
\mathbb{R}^{d}: \mathbf x = \mathbf{f}(\mathbf z^*) + \boldsymbol{\varepsilon}\,,
$$
где~$\boldsymbol{\varepsilon}$~--- случайный вектор с~нулевым 
математическим ожиданием и~конечной матрицей корреляций. Будем называть~$d$ 
эффективной размерностью исходного пространства~$\mathbb X$. 
Она определяется природой признакового пространства. Поскольку~$d$ 
заранее не известно, введем понятие результирующего пространства~$\mathbb {R}^{k}$, в~котором выполняется поиск решения. В~общем случае~$k \hm\ne d$. Процесс поиска 
образов объектов выборки в~результирующем пространстве назовем вложением в~него.

Рассмотрим выборку из~$m$ объектов, заданную матрицей
\begin{equation}
\label{X}\mathbf X = 
\left[\mathbf x_{1} \cdots \mathbf x_{m}\right]^{\TSF}\,, 
\quad \mathbf x_i \in \mathbb X\,, \quad i = 1, \dots, m\,.
\end{equation}
Пусть~$p_{ij} = P(\mathbf x_i, \mathbf x_j)$ и~$q_{ij} 
\hm= Q(\mathbf z_i, \mathbf z_j)$~--- расстояния между объектами 
в~$\mathbb R^n$ и~$\mathbb {R}^{k}$ соответственно:
\begin{multline*}
p_{ij} = \fr{p_{j|i} + p_{i|j}}{2m}, \\
p_{i|j} = \fr
{\exp\left(-
{||\mathbf x_i - \mathbf x_j||^2}/
\left({2\sigma_i^2}\right)\right)}
{\sum\nolimits_{k\ne i}\hspace*{-2pt}\exp\left(-{||\mathbf x_i - \mathbf x_k||^2}/
\left({2\sigma_i^2}\right)\right)}\,;
\end{multline*}

\vspace*{-12pt}

\noindent
\begin{multline*}
q_{ij} = \fr
{\left(1 + ||\mathbf z_i - \mathbf z_j||^2\right)^{-1}}
{\sum\nolimits_{k\ne i}
\left(1 + ||\mathbf z_i - \mathbf z_k||^2\right)^{-1}}\,,\quad q_{ii} = 0\,, 
\\
i, j \in\{1,\dots,m\}\,.
\end{multline*}

Параметр~$\sigma_i$ в~условном распределении~$p_{ij}$ задан для 
каждого~$i$ и~зависит от расположения~$\mathbf{x}_i$ относительно других 
объектов в~исходном пространстве. Eсли он расположен в~области высокой 
концентрации исходных данных, то коэффициент~$\sigma_i$ имеет 
меньшие значения, чем если бы концентрация была низкой.

Расположение 
\begin{equation}
\label{Z} 
\mathbf Z = \left[\mathbf z_{1}\cdots\mathbf z_{m}\right]^{\TSF} \subset 
\mathbb {R}^{k}
\end{equation} 
как образов~$\mathbf{X}$ в~результирующем пространстве~$\mathbb R^k$ 
находится путем минимизации дивергенции Куль\-ба\-ка--Лейб\-ле\-ра:
\begin{equation}
\label{argmin}
{\mathbf Z_{\min} = 
\operatornamewithlimits{argmin}\limits_{\mathbf Z \in \mathbb {R}^{m\times k}}
C(\mathbf X, \mathbf Z)}\,,
\end{equation}
где
\begin{equation}
\label{KL}
C(\mathbf X, \mathbf Z) = \text{KL}(P||Q) = 
\sum\limits_{i \neq j} p_{ij} \log \fr{p_{ij}}{q_{ij}}\,.
\end{equation}

Заметим, что минимизация происходит только по координатам 
объектов~$\mathbf z_{1}, \dots ,\mathbf z_{m}$ как по переменным, 
а~координаты~$\mathbf x_{1}, \dots , \mathbf x_{m}$ считаются известными константами.

Задача решается градиентными методами~\cite{maaten2008visualizing}. 
Для инициализации начальных точек~$\mathbf{Z}^{(0)} \hm= [\mathbf z_1^{(0)} \cdots 
\mathbf z_m^{(0)}]^{\TSF}$ градиентного спуска в~стандартной реализации было
 предложено~\cite{maaten2008visualizing} два метода: инициализировать 
 случайными точками либо использовать для задания начальной инициализации 
 метод Principal Components Analysis. От качества начальной 
 инициализации, в~случае с~невыпуклой задачей оптимизации, зависят не 
 только ско\-рость сходимости к~оптимуму, но и~локальный минимум, к~которому 
 будет сходиться градиентный метод.

\section{Предлагаемая модификация t-SNE}

Рассмотрим задачу классификации с~обуча\-ющей выборкой~$\mathbf{X}$~(\ref{X}) и~тестовой 
выборкой из~$m'$ объектов~$\mathbf{X}'\hm= [\mathbf x_{m+1}\cdots\mathbf x_{m+m'}]^{\TSF} 
\hm\subset \mathbb X$. Соответственно, метки классов~$y_i \hm\in \{0,1\}$, 
$i \hm= 1, \ldots, m,$ известны, а~$\hat y_i$, $i\hm = m\hm+1, \ldots, m\hm+m',$ 
необходимо оценить. Так как на этапе обучения данные~$\mathbf{X}'$ могут быть
 недоступны, метод непараметрического t-SNE не применим для снижения размерности 
 в~задачах классификации. Назовем это проблемой непросмотренных объектов 
 (out-of-sample problem). Для ее решения предлагается минимизировать~(\ref{KL}) 
 независимо по различным подмножествам объектов.
 


Для повышения качества классификатора в~результирующем пространстве 
предлагается перед вложением обучающей выборки добавить в~ней метки 
классов в~качестве признаков и~улучшить таким образом начальное приближение 
градиентного метода. Идея такого подхода заключается в~том, что, поскольку t-SNE 
сохраняет только локальную структуру схожести между объектами, после проведения 
процедуры понижения размерности классифицируемые объекты отображаются в~клас\-те\-ры, 
предварительно разнесенные с~учетом меток. При этом используется предположение, 
что объекты из~$\mathbf{X}'$ больше схожи с~объектами~$\mathbf{X}$ того же 
класса, чем с~объектами противоположного. Таким образом удается увеличить 
расстояние между образами классифицируемых объектов из различных классов, 
что упрощает их классификацию. Ниже % рис. \ref{schema}
показаны основные 
отображения оригинального непараметрического t-SNE
\begin{align*}
\mathbf{X}\in\mathbb{R}^{m \times n} &\longrightarrow   
\mathbf{Z}\in\mathbb{R}^{m \times k}\,; \\
\mathbf{X^\prime}\in\mathbb{R}^{m^\prime \times n} &\longrightarrow
\mathbf{Z^\prime}\in\mathbb{R}^{m^\prime \times k}
%\label{tSNE_original}
\end{align*}
и предложенной модификации


\noindent
\begin{center}
\mbox{%
\epsfxsize=74.974mm
\epsfbox{str-0.eps}
}
\end{center}
%{\small\begin{equation*}
%\xymatrix{
%\mathbf{X}|\mu\mathbf{y}\in\mathbb{R}^{m \times (n+1)}
% \ar@<1ex>[rr]^{\text{Начальная партия}} %{\text{Start batch}}
%\ar[rr]_{\text{Дополнительные партии}}%{\text{Supplimentary batches}}
%\ar[dr] &   &\mathbf{Z}\in\mathbb{R}^{m \times k} \ar[dl]\\
%\mathbf{X^\prime}\in\mathbb{R}^{m^\prime \times n} \ar[r] & \mathbf{Z^\prime}^{(0)}\in\mathbb{R}^{m^\prime \times k} \ar[r] & %\ar[dr] &
%\mathbf{Z^\prime}\in\mathbb{R}^{m^\prime \times k}\\
%%\mathbf{X^\prime}\in\mathbb{R}^{m^\prime \times n} \ar[ur]   &    & \mathbf{Z^\prime}\in\mathbb{R}^{m^\prime \times k}   \\
%}
%\label{tSNE_modified}
%\end{equation*}}

\vspace*{-12pt}

\paragraph*{Использование исходной разметки выборки при вложении 
для обучения классификатора.}
Для учета\linebreak\vspace*{-12pt}

\pagebreak

\noindent
 разметки обучающей выборки признаковая матрица~$\mathbf{X}$ 
расширяется дополнительным столбцом признаков

\noindent
$$
\tilde{\mathbf X} = \left (
\begin{array}{c|c}
\mathbf X & \mu \mathbf y
\end{array}\right ),
$$

\vspace*{-2pt}

\noindent
где~$\mu$~--- вес меток как признаков.
В~модифицированном алгоритме на основе расширенной матрицы~$\tilde X$ выполняется 
поиск образов~$\mathbf{Z}$~(\ref{KL}), на которых обучается классификатор.
Таким образом, при построении вложения обучающей выборки решается задача

\noindent
$$
\mathbf Z_{\min} = \argmin\limits_{\mathbf Z \in \mathbb {R}^{m\times k}}
C\left (\mathbf{\left (
\begin{array}{c|c}
\mathbf X & \mu \mathbf y
\end{array}\right )}, \mathbf Z\right )\,.
$$

\vspace*{-12pt}

\paragraph*{Вложение новых объектов в~пространство со сниженной размерностью 
для классификации.}
Обозначим через~$\mathbf{Z}' \hm= [\mathbf z_{m+1}\cdots \mathbf z_{m+m'}]^{\TSF}$ 
образы~$\mathbf{X}'$ в~результирующем пространстве. Аналогично~(\ref{argmin}) 
сформулируем задачу поиска~$\mathbf{Z}'$ в~виде~$m'$ задач~$k$-мер\-ной минимизации, 
которые могут быть решены независимо:

\noindent
\begin{multline*}
\mathbf z_i^{\min} = \argmin\limits_{\mathbf z_i \in \mathbb {R}^{m'}}C
\left (\left [
\fr{\mathbf X}{\mathbf{x}_i^{\TSF} }\right ],\,
\left [ \fr{\mathbf {Z}}{\mathbf {z}_i^{\TSF}}\right ]
\right )\,,\\
i = m+1, \ldots, m+m'\,,
\end{multline*}
где матрицы $\left [
\fr{\mathbf X}{\mathbf x_i^{\TSF}}\right ]$ и~$\left [ \fr{\mathbf{\mathbf Z}}{\mathbf{z}_i^{\TSF} }\right ]$
получены из~$\mathbf{X}$ и~$\mathbf{Z}$ добавлением строк~$\mathbf x_i^{\TSF}$ 
и~$\mathbf z_i^{\TSF}$ соответственно. При использовании такого подхода предполагается, 
что обучающая выборка~$\mathbf{X}$~(\ref{X}) достаточно репрезентативна.

Для инициализации образов~$\mathbf z_{i'}$ классифицируемых объектов предлагается 
использовать метод взвешенного среднего по образам соседей: 
\begin{multline*}
\mathbf z^{(0)}_{i'}
 = \sum\limits_{i = 1}^m\mathbf z_i w_{ii'}\,,\enskip \sum\limits_{i=1}^mw_{ii'} 
= 1\,,\\
i' = m+1, \ldots, m+m'\,,
\end{multline*}
где~$w_{ii'}$~--- веса образов объектов~$\mathbf x_i$, $i\hm = 1, \ldots, m$.
В~работе рассмотрены два способа задания весов: %\linebreak\vspace*{-12pt} 


\end{multicols}

 \begin{algorithm*} %[!htbp]
\caption{Вложение выборки с~известным вектором ответов классификации~$\mathbf y$} 
\label{start}
    \SetAlgoLined
    \setcounter{AlgoLine}{0}
    \KwData{$\mathbf{X}, \mathbf y, \mu, S_s, S_b$}
    \KwResult{$\mathbf{Z}$}

    $\tilde{\mathbf X} = (\mathbf X | \mu \mathbf y)$

    Инициализировать~$\mathbf{Z}$~(\ref{Z}) случайно или при помощи PCA($\tilde{\mathbf X}$). Положить инициализацию начальной точкой градиентного метода:~$\mathbf{Z}^{(0)}$.

    \eIf{$m$ > $S_s$}
    {
        Разбить~$\mathbf{Z}$ на партии: начальная партия~$\mathbf{Z}_0$ размером~$S_s$ и~$B = \left \lceil ({m - S_s})/{S_b} \right \rceil$ дополнительных партий~$\mathbf{Z}_1, \dots, \mathbf{Z}_B$ размером не больше чем~$S_b$ каждая.
        Оптимизировать~(\ref{KL}) по~$\mathbf{Z}_0$, зафиксировав координаты остальных объектов из~$\mathbf{Z}$, известные из предыдущего шага.
        \For{$\mathbf{Z}_i$ $\in$ $\{\mathbf Z_1, \dots \mathbf Z_B\}$}
        {
            Оптимизировать~(\ref{KL}) по $\mathbf{Z}_i$, зафиксировав координаты остальных объектов из~$\mathbf{Z}$, известные из предыдущего шага.
        }
    }
    {
        Оптимизировать~(\ref{KL})
    }
\end{algorithm*}
\begin{algorithm*} %[!htbp]
    \SetAlgoLined
    \setcounter{AlgoLine}{0}
    \KwData{$\left [
\begin{array}{c}
\mathbf X \\
\hline
\mathbf X' \\
\end{array}\right ], \mathbf Z$}
    \KwResult{$\mathbf{Z}'$}
    Инициализировать~$\mathbf{Z}$~(\ref{Z}) случайно, при помощи PCA($\tilde{\mathbf X}$), либо используя~(\ref{knnstud2}) или~(\ref{knnstud}) для расчета~$\mathbf W$ и~считать~$\mathbf{Z}'^{(0)} = \mathbf Z^{\TSF} \mathbf W$.
    
    \For {$i \in \{m+1, \dots, m+m'\}$}
    {
        Оптимизировать~(\ref{KL}) по~$\mathbf z_{i}$, зафиксировав координаты остальных объектов из $\left [\begin{array}{c}
\mathbf Z \\
\hline
\mathbf Z' \\
\end{array}\right ]$, известные из предыдущего шага.
}
\caption{Вложение выборки без известного вектора ответов классификации} \label{batch}
\end{algorithm*}

\begin{figure*}[b] %fig1
    \vspace*{4pt}
\begin{center}
\mbox{%
\epsfxsize=161.897mm
\epsfbox{str-1.eps}
}
\end{center}
\vspace*{-9pt}
\Caption{Зависимость~$F_1$ от~$k$ при различных эффективных размерностях 
выборки~$d\hm=4$~(\textit{a}), 8~(\textit{б}); 12~(\textit{в})  
и~16~(\textit{г}) при использовании t-SNE (\textit{1}~--- Student; \textit{2}~--- Softmax; \textit{3}~--- Random; 
\textit{4}~--- PCA)
и~других методов снижения
размерности (\textit{5}~--- LLE; \textit{6}~--- PCA; \textit{7}~--- ISOMAP),
а~также без применения снижения размерности~(\textit{8})}\label{dim_z_inform}
\end{figure*}

\begin{multicols}{2}

\noindent
\begin{equation}
\label{knnstud}
w_{ii'}^{\mathrm{softmax}} = \fr{\exp(-\|\mathbf x_i - \mathbf x_{i'} \| )}
{\sum\nolimits_{k = 1}^m\exp(-\|\mathbf x_k - \mathbf x_{i'} \| )}
\end{equation}
%\noindent
или

\noindent
\begin{equation}
\label{knnstud2}
 w_{ii'}^{\mathrm{stud}}= \fr{(1 + \|\mathbf x_k - \mathbf x_{i'}\|^2)^{-1}}
{\sum\nolimits_{k = 1}^m (1 + \|\mathbf x_k - \mathbf x_{i'}\|^2)^{-1}}\,.
\end{equation}

Для ускорения процедуры вложения при работе с~большими данными предлагается 
процедура поэтапного вложения объектов блоками, размер которых~--- $S_s$ для 
первого по очереди и~$S_b$ для всех остальных~--- много меньше размера~$m$ всей 
вы\-борки.

Псевдокод предложенного метода приведен в~алгоритмах~1 и~2.

\vspace*{-5pt}

\section{Вычислительный эксперимент}

\vspace*{-2pt}

Вычислительный эксперимент состоит из двух частей: исследование 
разработанного алгоритма на синтетических данных и~применение разработанного 
алгоритма для решения задачи внутреннего плагиата.

Для инициализации вложения тестовых данных использовались четыре различных подхода: 
случайный~--- инициализация случайным образом; PCA (Principal Component Analysis)~--- 
инициализация образами при 
снижении раз\-мер\-ности методом главных компонент; Softmax и~Student~--- задаваемые 
по формулам~(\ref{knnstud}) и~(\ref{knnstud2}). 

На рис.~1--5 представлены 
результаты экспериментов, проведенных при использовании всех этих способов 
инициализации.

Все инициализированные таким образом объекты далее преобразуются, 
минимизируя~(\ref{KL}) при фиксированных образах объектов обучающей выборки. 
После этого происходит классификация полученных образов.



Методы инициализации~(\ref{knnstud}) и~(\ref{knnstud2}) были предложены так, 
чтобы инициализированные данные обладали свойством сохранения локальной структуры 
исходной выборки. Предполагалось, что это\linebreak\vspace*{-12pt}

\pagebreak

\end{multicols}

\begin{figure*} %fig2
\vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=161.897mm
\epsfbox{str-2.eps}
}
\end{center}
\vspace*{-11pt}
\Caption{Зависимость $F_1$ от~$\mu$ при различных размерностях 
выборки $n\hm=6$~(\textit{a}); 60~(\textit{б}); 300~(\textit{в})
и~600~(\textit{г}) при использовании t-SNE 
(\textit{1}~--- Student; \textit{2}~--- KNN ($k$ nearest neighbor); \textit{3}~--- Random; 
\textit{4}~--- PCA)
и~других методов снижения
размерности (\textit{5}~--- LLE; \textit{6}~--- PCA; \textit{7}~--- ISOMAP),
а~также без применения снижения размерности~(\textit{8})} 
%\vspace*{3pt}
\label{weight_x}
\end{figure*}

\begin{multicols}{2}

\noindent
 улучшит сходимость градиентного метода, 
используемого для минимизации~(\ref{KL}), по сравнению с~инициализациями PCA и~random.

\vspace*{-2pt}

\subsection{Исследование свойств алгоритма на~синтетических данных}

\vspace*{-1pt}

В данном подразделе для эмпирического исследования свойств предлагаемого 
алгоритма использовались синтетические выборки~$\mathbf{X} \hm= 
[\mathbf x_1 \cdots \mathbf x_m]^{\TSF}$. Для любого вектора~$\mathbf x_i$ компоненты 
были сгенерированы как стандартные нормальные распределения на гранях гиперкуба. 
При этом эффективная размерность выборки составляла~$d$, а~оставшиеся 
признаки были шумовыми. Далее выборка сворачивалась в~спираль по одной из 
размерностей. Это делалось для того, чтобы реализовать предположение 
о~существовании многообразия меньшей размерности, в~котором содержится выборка. 
Генерировалось одинаковое количество объектов разных классов, 
а~на обучение и~контроль выборка разбивалась в~соотношении~$1:4$.

В этом подразделе описывается исследование качества классификации с~применением 
предлагаемого алгоритма в~зависимости от основных его па\-ра\-мет\-ров и~специфики выборки. 
Для сравнения предлагаемого алгоритма и~его исследования рассматривается классификация 
в~комбинации с~другими методами снижения размерности: 
PCA~\cite{kim2007distance}, LLE~\cite{roweis2000nonlinear}, 
ISOMAP~\cite{tenenbaum2000global}, а~также без применения снижения 
размерности. Для построения классификатора использовался метод логистической 
регрессии на основе Stochastic Gradient Descent~\cite{bottou2012stochastic}.

\begin{figure*} %fig3
\vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=164.997mm
\epsfbox{str-3.eps}
}
\end{center}
\vspace*{-9pt}
\Caption{Зависимость~$F_1$ от~${S_s}/{m}$ при различных размерах выборки
$m=1000$~(\textit{а}); 2000~(\textit{б}); 3000~(\textit{в})
и~4000~(\textit{г}) при использовании t-SNE (\textit{1}~--- Student; \textit{2}~--- KKN;
\textit{3}~--- Random) и~других методов снижения размерности
(\textit{4}~--- LLE; \textit{5}~--- PCA; \textit{6}~--- ISOMAP),
а~также без применения снижения размерности~(\textit{7})} 
\label{Ss_n}
\end{figure*}
\begin{figure*} %fig4
\vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=162.776mm
\epsfbox{str-4.eps}
}
\end{center}
\vspace*{-9pt}
\Caption{Зависимость~$F_1$ от~$S_b$ при различных размерах выборки
$m\hm=500$~(\textit{а}) и~1000~(\textit{б})
при использовании t-SNE (\textit{1}~--- Student; \textit{2}~--- KKN;
\textit{3}~--- Random) и~других методов снижения размерности
(\textit{4}~--- LLE; \textit{5}~--- PCA; \textit{6}~--- ISOMAP),
а~также без применения снижения размерности~(\textit{7})} 
\label{Sb_n}
\end{figure*}


На рис.~\ref{dim_z_inform} изображена зависимость меры качества~$F_1$ от 
размерности вложения~$k$ при различных значениях эффективной размерности~$d$. 
%Пунктиром отмечено стандартное отклонение, срезанное по уровню единицы. 
На графиках видно, что качество значительно ухудшается при увеличении~$k$ 
независимо от соотношения~$k$ и~$d$. Эксперимент проведен при постоянных~$m \hm= 500$,
$n \hm= 20$, $S_b \hm= 100$, $S_s \hm= 400$ и~$\mu \hm= 150$.


\begin{figure*} %fig5
\vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=155.148mm
\epsfbox{str-6.eps}
}
\end{center}
\vspace*{-9pt}
\Caption{Демонстрация вложения, выполненного предлагаемым методом 
((\textit{а})~Student; (\textit{б})~Softmax; (\textit{в})~PCA; (\textit{г})~Random)
 при $\mu \hm= 0$ (левый столбец) и~10 (правый столбец):
 \textit{1}~--- обучающая выборка; \textit{2}~--- тестовая
 выборка; \textit{3}~--- плагиат} 
\label{mu0}
\end{figure*}

На рис.~\ref{weight_x} изображена зависимость~$F_1$ от веса~$\mu$ меток класса~$y$ 
в~стартовой выборке при различных значениях размерности выборки~$n$. 
Из них можно сделать вывод, что качество классификации повышается с~ростом~$\mu$, 
при этом скорость роста падает с~ростом~$n$. Также видно, что разработанный метод 
при достаточно больших значениях~$\mu$ показывает в~среднем лучшие результаты 
среди всех рас\-смот\-рен\-ных методов снижения размерности, а~также превосходит 
по качеству классификацию в~исходном пространстве. Эксперимент проведен 
при постоянных~$m \hm= 500$,
$k \hm= 3$, $S_b \hm= 100$ и~$S_s \hm= 400$. 
В~этом эксперименте все исходные признаки были информативными.


Для исследования зависимости качества классификации от величины 
отношения размера стартовой части к~размеру выборки~$S_s/m$ был поставлен 
эксперимент, где при постоянных $n \hm= 6$, $k \hm= d \hm= 3$ 
и~$\mu \hm= 150$ исследовалась зависимость меры качест\-ва~$F_1$ от размера выборки~$m$ 
и~размера начального вложения~$S_s$. При этом размер дополнительно вкладываемых 
блоков~$S_b$ принимался заведомо б$\acute{\mbox{о}}$льшим размера выборки~$m$, 
так что дополняющая часть не разбивалась на блоки. На рис.~\ref{Ss_n} выведены 
результаты. Можно видеть, что зависимость от этих параметров незначительна. 
При этом скорость работы алгоритма увеличивается при наличии разбиений на 
стартовую и~дополняющую части. Таким образом, показано, что предложенная 
модификация алгоритма позволяет значительно ускорить его работу без 
существенного снижения качества.



На графиках рис.~\ref{Ss_n} также видно, что методы инициализации 
с~по\-мощью~(\ref{knnstud2}) и~случайной инициализации дают лучшие результаты, 
в~то время как метод инициализации PCA показал результаты порядка~0,5, 
по причине чего было принято решение не выносить его на рисунок.



Целью эксперимента, результаты которого приведены на рис.~\ref{Sb_n}, было 
исследование зависимости значения функции качества классификации от~$S_b$. 
Он был проведен при постоянных~$n \hm= 6$, $k \hm= d \hm= 3$, $\mu\hm = 150$
и~$S_s \hm= 200$. 
В~результате было обнаружено, что предлагаемый метод устойчив относительно 
параметра~$S_b$.

\subsection{Задача обнаружения внутреннего плагиата}

Целью данной части эксперимента был анализ предложенного метода 
снижения размерности в~применении к~реальным данным задачи внутреннего плагиата. 
Рассматривается набор документов. Каждый документ рассматривается как 
последовательность сегментов~$s_i$, каждый из которых описывается вектором 
признаков~$\mathbf x$. В~данной работе в~качестве сегментов рассматриваются 
предложения. Каждому~$s_i$ поставлена в~соответствие метка класса 
$ y_i \hm\in \{0, 1\}$: $y_i \hm= 1$, если~$s_i$~--- заимствованный сегмент, 
иначе~$y_i \hm= 0$. Задача распознавания внутреннего плагиата ставится 
как задача восстановления меток~$y_i$ по документу.

\vspace*{-7pt}

\paragraph*{Иллюстрация вложения реальных данных.}

Для демонстрации работы алгоритма на реальных данных из 
предоставленного корпуса~\cite{Pan2011Collection}~\verb"part1" выделен один 
из документов. Выделенные из него объекты были разделены на обучающую и~тестовую 
выборки. Каждой из них соответствуют непрерывные части текста. Это разделение 
необходимо для демонстрации работы предложенной модификации и~не учитывается 
при применении оригинального t-SNE. На рис.~6 приведен результат 
применения оригинального непараметрического метода t-SNE к~объектам, выделенным 
из выбранного документа. На нем видно, что объекты, соответствующие заимствованным 
частям текста, имеют очаги концентрации в~исходном пространстве, что 
свидетельствует об информативности выбранных признаков.
Рисунки~5 и~6 имеют безразмерные оси, полученные в~результате нелинейных отображений.
Физического смысла эти оси не несут.

 { \begin{center}  %fig6
 \vspace*{7pt}
 \mbox{%
\epsfxsize=73.201mm
\epsfbox{str-5.eps}
}


\end{center}


\noindent
{{\figurename~6}\ \ \small{Визуализация документа с~использованием оригинального алгоритма t-SNE:
\textit{1}~--- обучающая выборка; \textit{2}~--- тестовая выборка; 
\textit{3}~--- плагиат}}

}

\vspace*{10pt}






На рис.~5
представлены результаты вложения данных выбранного документа с~использованием 
предложенного алгоритма при различных методах начальной инициализации и~при 
различных значениях веса~$\mu$. При выполнении вложений были зафиксированы 
параметры~$S_s \hm= 500$ и~$S_b \hm= 200$. 
В~эксперименте данные из выбранного документа были разделены на обучающую и~тестовую 
выборки. В~тестовую часть попали образы предложений, которые образовывали в~исходном 
тексте непрерывную цепочку. Обучающая часть вкладывалась с~учетом ее 
разметки, а~тестовая~--- без учета.






\vspace*{-7pt}

\paragraph*{Результаты.}
Из полученных графиков можно сделать вывод, что предложенная модификация 
при больших значениях веса~$\mu$ принимает на себя часть ответственности 
за классификацию. Она склонна разделять и~кластеризовать тестовую выборку 
по целевому признаку. Таким образом, исходя из описанных выше свойств t-SNE, 
любой построенный в~результирующем пространстве классификатор получает 
свойство классификатора ближайших соседей с~адаптивной константой, 
подстраиваемой под локальную геометрию выборки.
Следует отметить также, что при больших значениях~$\mu$ минимизация целевой 
функции~(\ref{KL}) требует больше шагов градиентного алгоритма. Таким образом, 
этот параметр следует выбирать с~оглядкой на время работы программы. 
Авторы рекомендуют значение порядка характерной величины координат 
векторов обучающей выборки.

\vspace*{-8pt}

\section{Заключение}

\vspace*{-2pt}

В работе была предложена модификация непараметрического метода 
снижения размерности t-SNE, состоящая в~воплощении возможности\linebreak 
выполнения вложения поэтапно, решении проб-\linebreak лемы непросмотренных объектов и~внедрении 
возмож\-ности учета разметки при выполнении\linebreak вложения для классификации. Был проведен 
вычислительный эксперимент на синтетических данных, показывающий эффективность 
предложенного метода в~применении к~задаче классификации.
 Была определена зависимость 
качества класси-\linebreak фикации с~применением описанного метода от его па\-ра\-мет\-ров,
экспериментально обосновано использование поэтапного обучающего вложения. 
Полученные зна\-че\-ния качества сравнивались с~результатами классификации с~применением 
других методов снижения размерности, а~также без их применения.

Была показана устойчивость алгоритма к~введенным параметрам размера 
начальной части~$S_s$ и~максимального размера блоков~$S_b$, что облегчает его
 использование на практике. Также явно продемонстрирована зависимость свойств 
 метода от параметра веса разметки выборки~$\mu$.

Проанализировано признаковое пространство задачи внутреннего плагиата. 
Проиллюстрированы свойства предложенного алгоритма относительно данных задачи 
внутреннего плагиата. Продемонстрирована эффективность предложенных методов 
инициализации при вложении образов объектов, которые не были использованы 
при выполнении начального вложения.

\vspace*{-8pt}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
 
 \vspace*{-2pt}
 
\bibitem{fefferman2016testing}
\Au{Fefferman~C., Mitter~S., Narayanan~H.} 
Testing the manifold hypothesis~// J.~Am. Math. Soc., 2016. Vol.~29. No.\,4 
P.~983--1049.

\bibitem{maaten2008visualizing} %2
\Au{Van der Maaten~L., Hinton~G.} 
Visualizing data using \mbox{t-SNE}~// J.~Mach. Learn. Res., 2008. Vol.~9. 
P.~2579--2605.

\bibitem{narayanan2010sample} %3
\Au{Narayanan~H., Mitter~S.} 
Sample complexity of testing the manifold hypothesis~// Advances in neural 
information 
processing systems~/ Eds. J.\,D.~Lafferty, C.\,K.\,I.~Williams, J.~Shawe-Taylor, \textit{et al}.~---
Curran Associates, Inc., 2010. Vol.~23. P.~1786--1794.

\bibitem{zu2006intrinsic} %4
 \Au{Zu Eissen~S.\,M., Stein~B.} 
 Intrinsic plagiarism detection~// European Conference on Information Retrieval.~--- 
Springer,  2006. P.~565--569.

\bibitem{kuznetsov2016methods} %5
 \Au{Kuznetsov~M.\,P., Motrenko~A.\,P., Kuznetsova~M.\,V., Strijov~V.\,V.} 
 Methods for intrinsic plagiarism detection and author diarization~// 
 Working Notes  of CLEF~/ Eds.\ K.~Balog, L.~Cappellato, N.~Ferro, C.~Macdonald.~---
 $\acute{\mbox{E}}$vora, Portugal:
 CEUR-WS, 2016. Vol.~1609. P.~912--919.
 
 \bibitem{stamatatos2009intrinsic} %6
\Au{Stamatatos~E.} Intrinsic plagiarism detection using character n-gram profiles~// 
SEPLN Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse, 
2009. P.~38--46.

\bibitem{muhr2010external} %7
  \Au{Muhr~M., Kern~R., Zechner~M., Granitzer~M.} 
  External and intrinsic plagiarism detection using a~cross-lingual retrieval 
  and segmentation system~// Working Notes for CLEF Conference~/ Eds. M.~Braschler, 
  D.~Harman, E.~Pianta, N.~Ferro.~--- Padua, Italy: CEUR-WS, 2010. 
  Vol.~1176. 
  {\sf http://ceur-ws.org/Vol-1176/CLEF2010wn-PAN-MuhrEt2010.pdf}. 
%  (accessed September~15, 2017).



\bibitem{kestemont2011intrinsic} %8
  \Au{Kestemont~M., Luyckx~K., Daelemans~W.} 
  Intrinsic plagiarism detection using character trigram distance scores~// 
 Working Notes for CLEF Conference~/ Eds. V.~Petras, P.~Forner, P.~Clough, N.~Ferro.~--- 
 Amsterdam, The Netherlands: CEUR-WS, 2011. Vol.~1177. 
 {\sf http://ceur-ws.org/Vol-1177/CLEF2011wn-PAN-KestemontEt2011.pdf}. 
% (accessed September~15, 2017).


\bibitem{potthast2011overview} %9
\Au{Potthast~M., Eiselt~A., Cede$\tilde{\mbox{n}}$o~L.\,A., Stein~B., Rosso~P.}  
Overview of the 3rd international competition on plagiarism detection~// 
Working Notes for CLEF Conference~/ Eds. V.~Petras, P.~Forner, P.~Clough, N.~Ferro.~--- 
Amsterdam, The Netherlands: CEUR-WS, 2011. Vol.~1177. 
{\sf http://ceur-ws.org/Vol-1177/CLEF2011wn-PAN-PotthastEt2011a.pdf}. 



\bibitem{fodor2002survey} %10
 \Au{Fodor~I.\,K.} {A~survey of dimension reduction techniques}. 
 Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, 
 2002. Technical Report. P.~1--18.

\bibitem{brooke2012paragraph} %11
  \Au{Brooke~J., Hirst~G.} Paragraph clustering for intrinsic plagiarism 
  detection using a stylistic vector-space model with extrinsic features~// 
  Working Notes for CLEF Conference~/ Eds. P.~Forner, J.~Karlgren, C.~Womser-Hacker, 
  N.~Ferro.~--- Rome, Italy: CEUR-WS, 2012. Vol.~1178. 
  {\sf http://ceur-ws.org/Vol-1178/CLEF2012wn-PAN-BrookeEt2012.pdf}. 
  %(accessed September~15, 2017).


\bibitem{brooke2012unsupervised} %12
  \Au{Brooke~J., Hammond~A., Hirst~G.} 
  Unsupervised stylistic segmentation of poetry with change curves and extrinsic 
  features~// 1st NAACL-HLT Workshop on Computational Linguistics for Literature
  Proceedings, 2012. Stroudsburg, PA, USA: Association for Computational Linguistics.
  P.~26--35.

\bibitem{gorban2008principal} %13
  \Au{Gorban~A.\,N., K$\acute{\mbox{e}}$gl~B., Wunsch~D.\,C., %Zinovyev~A.\,Y.,
  \textit{et al.}} Principal manifolds for data visualization and dimension 
  reduction.~--- Springer, 2008. 58~p.

\bibitem{tenenbaum2000global} %14
  \Au{Tenenbaum~J.\,B., De Silva~V., Langford~J.\,C.} 
  A~global geometric framework for nonlinear dimensionality reduction~// 
  Science, 2000. Vol.~290. Iss.~5500. P.~2319--2323.

\bibitem{belkin2001laplacian} %15
  \Au{Belkin~M., Niyogi~P.} 
  Laplacian eigenmaps and spectral techniques for embedding and clustering~// 
  Advances in neural information processing systems~/
  Eds.\ T.\,G.~Dietterich, S.~Becker, Z.~Ghahramani.~---
  NIPS Foundation, Inc., 2001. Vol.~14. P.~585--591.

\bibitem{roweis2000nonlinear} %16
  \Au{Roweis~S.\,T., Saul~L.\,K.} 
  Nonlinear dimensionality reduction by locally linear embedding~// 
  Science, 2000. Vol.~290. Iss.~5500. P.~2323--2326.

\bibitem{donoho2003hessian} %17
  \Au{Donoho~D.\,L., Grimes~C.} 
  Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data~// 
  P. Natl. Acad. Sci. USA, 2003. Vol.~100. No.\,10. P.~5591--5596.

\bibitem{zhang2004principal} %18
  \Au{Zhang~Z., Zha~H.} Principal manifolds and nonlinear dimensionality reduction
   via tangent space alignment~// J.~Shanghai University (English Edition), 2004. 
   Vol.~8. No.\,4. P.~406--424.

\bibitem{weinberger2006unsupervised} %19
  \Au{Weinberger~K.\,Q., Saul~L.\,K.} 
  Unsupervised learning of image manifolds by semidefinite programming~// 
  Int. J.~Comput. Vision, 2006. Vol.~70. No.\,1. P.~77--90.

\bibitem{chen2010distance} %20
  \Au{Chen~C., Zhang~J., Fleischer~R.} 
  Distance approximating dimension reduction of Riemannian manifolds~// 
  IEEE T. Syst. Man Cy.~B, 2010. Vol.~40. No.\,1. 
  P.~208--217.

\bibitem{van2009learning} %21
\Au{Van der Maaten~L.} 
Learning a parametric embedding by preserving local structure~// 
RBM, 2009. Vol.~500. P.~26.

\bibitem{van2014accelerating} %22
  \Au{Van der Maaten~L.} Accelerating t-SNE using tree-based algorithms~// 
  J.~Mach. Learn. Res., 2014. Vol.~15. No.\,1. P.~3221--3245.

\bibitem{kim2007distance} %23
  \Au{Kim~H., Park~H., Zha~H.} 
  Distance preserving dimension reduction for manifold learning~// 
  SIAM  Conference (International) on Data Mining Proceedings, 2007. P.~527--532.

\bibitem{bottou2012stochastic} %24
  \Au{Bottou~L.} 
  Stochastic gradient descent tricks~// Neural networks: Tricks of the trade~/ 
Eds. G.~Montavon, G.\,B.~Orr, K.-R.~Muller.~--- 
Lecture notes in computer science ser.~--- 2nd ed.~--- Berlin--Heidelberg: 
Springer, 2012. Vol.~7700. P.~421--436.

 \bibitem{Pan2011Collection} %25
  \Au{Potthast~M., Stein~B., Barr$\acute{\mbox{o}}$n-Cede$\tilde{\mbox{n}}$o~A.,  
  Rosso~P.} 
  An evaluation framework for plagiarism detection~// 
  23rd  Conference (International) on Computational Linguistics Posters, 
  2010. P.~997--1005.
  \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-3pt}

\hfill{\small\textit{Поступила в~редакцию 20.02.17}}

\vspace*{8pt}

%\newpage

%\vspace*{-24pt}

\hrule

\vspace*{2pt}

\hrule

%\vspace*{8pt}


\def\tit{IMPROVING CLASSIFICATION QUALITY FOR~THE~TASK OF~FINDING INTRINSIC PLAGIARISM}

\def\titkol{Improving classification quality for~the~task of~finding intrinsic plagiarism}

\def\aut{I.\,O.~Molybog$^{1,2}$, A.\,P.~Motrenko$^2$, and~V.\,V.~Strijov$^3$}

\def\autkol{I.\,O.~Molybog, A.\,P.~Motrenko, and~V.\,V.~Strijov}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-9pt}


\noindent
$^1$Center for Energy Systems,
Skolkovo Institute of Science and Technology, 
Skolkovo Innovation Center, 3~Nobel\linebreak
$\hphantom{^1}$Str., Moscow 143026, 
Russian Federation

\noindent
$^2$Moscow Institute of Physics and Technology, 
9~Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian\linebreak
$\hphantom{^1}$Federation 

\noindent
$^3$A.\,A.~Dorodnicyn 
Computing Center, Federal Research Center ``Computer Science and Control'' 
of the Russian\linebreak
$\hphantom{^1}$Academy of Sciences, 40~Vavilov Str., Moscow 119333, 
Russian Federation



\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2017\ \ \ volume~11\ \ \ issue\ 3}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2017\ \ \ volume~11\ \ \ issue\ 3
\hfill \textbf{\thepage}}}

\vspace*{3pt}

\Abste{The paper addresses the classification problem in multidimensional spaces.
The authors propose a~supervised modification of the t-distributed Stochastic
Neighbor Embedding Algorithm. Additional features of the proposed modification
are that, unlike the original algorithm, it does not require retraining
if new data are added to the training set and can be easily parallelized. The
novel method was applied to detect intrinsic plagiarism in a collection of
documents. The authors also tested the performance of their algorithm using
synthetic data and showed that the quality of classification is higher with the
algorithm than without or with other algorithms for dimension reduction.}

\KWE{data analysis; dimension reduction; nonlinear dimension reduction; manifold learning; intrinsic plagiarism detection}


\DOI{10.14357/19922264170307} 

%\vspace*{-18pt}

\Ack
\noindent
This publication is funded by the Russian Foundation for Basic Research
(project No.\,16-07-01155).



%\vspace*{3pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {\baselineskip=11.3pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}


\bibitem{fefferman2016testing-1} %1
\Aue{Fefferman,~C., S.~Mitter, and H.~Narayanan.} 
2016. Testing the manifold hypothesis. \textit{J.~Am. Math. Soc.} 
29(4):983--1049. 

\bibitem{maaten2008visualizing-1} %2
\Aue{Van der Maaten, L., and G.~Hinton.} 
2008. Visualizing data using t-SNE. \textit{J.~Mach. Learn. Res.} 
9(Nov):2579--2605.

\bibitem{narayanan2010sample-1} %3
\Aue{Narayanan, H., and S.~Mitter.} 
2010. Sample complexity of testing the manifold hypothesis. 
\textit{Advances in neural 
information 
processing systems}. Eds. J.\,D.~Lafferty, C.\,K.\,I.~Williams, J.~Shawe-Taylor, 
\textit{et al}. Curran Associates, Inc. 23:1786--1794.

\bibitem{zu2006intrinsic-1} %4
 \Aue{Zu Eissen, S.~M., and B.~Stein.} 
 2006. Intrinsic plagiarism detection. \textit{European Conference on 
 Information Retrieval.} Springer. 565--569.

\bibitem{kuznetsov2016methods-1} %5
  \Aue{Kuznetsov, M.\,P., A.\,P.~Motrenko, M.\,V.~Kuznetsova, and V.\,V.~Strijov}. 
  2016. Methods for intrinsic plagiarism detection and author diarization. 
\textit{Working Notes  of CLEF}. Eds.\ K.~Balog, L.~Cappellato, N.~Ferro,
and C.~Macdonald.  $\acute{\mbox{E}}$vora, Portugal:
 CEUR-WS. 1609:912--919.
 
 \bibitem{stamatatos2009intrinsic-1} %6
 \Aue{Stamatatos, E.} 
 2009. Intrinsic plagiarism detection using character n-gram profiles. 
 \textit{SEPLN Workshop on Uncovering Plagiarism, Authorship, and Social 
 Software Misuse.} 38--46.


\bibitem{muhr2010external-1} %7
  \Aue{Muhr, M., R. Kern, M.~Zechner, and M.~Granitzer.} 
  2010. External and intrinsic plagiarism detection using a~cross-lingual retrieval
   and segmentation system. \textit{Working Notes for CLEF Conference}.
   Eds. M.~Braschler, 
  D.~Harman, E.~Pianta, and N.~Ferro. Padua, Italy: CEUR-WS. 
  Vol.~1176. Available at: 
  {\sf http://ceur-ws.org/Vol-1176/CLEF2010wn-PAN-MuhrEt2010.pdf} 
  (accessed September~15, 2017).


\bibitem{kestemont2011intrinsic-1} %8
  \Aue{Kestemont, M., K.~Luyckx, and W.~Daelemans.} 
  2011. Intrinsic plagiarism detection using character trigram distance scores. 
  \textit{Working Notes for CLEF Conference}. 
  Eds. V.~Petras, P.~Forner, P.~Clough, and N.~Ferro.
 Amsterdam, The Netherlands: CEUR-WS. Vol.~1177. 
Available at:  {\sf http://ceur-ws.org/Vol-1177/CLEF2011wn-PAN-\linebreak KestemontEt2011.pdf} 
 (accessed September~15, 2017).

\bibitem{potthast2011overview-1} %9
\Aue{Potthast, M., A.~Eiselt, L.\,A.~Cede$\tilde{\mbox{o}}$o, B.~Stein, and P.~Rosso.}
2011. Overview of the 3rd international competition on plagiarism detection. 
\textit{Working Notes for CLEF Conference}. Eds. V.~Petras, P.~Forner, P.~Clough, 
and N.~Ferro. Amsterdam, The Netherlands: CEUR-WS. Vol.~1177. 
Available at: {\sf http://ceur-ws.org/Vol-1177/CLEF2011wn-PAN-\linebreak PotthastEt2011a.pdf}
(accessed September~15, 2017).


\bibitem{fodor2002survey-1} %10
\Aue{Fodor, I.\,K.} 2002. A~survey of dimension reduction techniques. 
{Center for Applied Scientific Computing, Lawrence Livermore National 
Laboratory}. Technical Report. 1--18.

\bibitem{brooke2012paragraph-1} %11
  \Aue{Brooke, J., and G.~Hirst.} 2012. 
  Paragraph clustering for intrinsic plagiarism detection using a stylistic 
  vector-space model with extrinsic features. \textit{Working Notes for CLEF Conference}.
  Eds. P.~Forner, J.~Karlgren, C.~Womser-Hacker, 
  and N.~Ferro. Rome, Italy: CEUR-WS. Vol.~1178. 
Available at:   {\sf http://ceur-ws.org/Vol-1178/CLEF2012wn-PAN-BrookeEt2012.pdf}
(accessed September~15, 2017).

\bibitem{brooke2012unsupervised-1} %12
  \Aue{Brooke, J., A.~Hammond, and G.~Hirst.} 2012. Unsupervised stylistic 
  segmentation of poetry with change curves and extrinsic features. 
  \textit{1st NAACL-HLT Workshop on Computational Linguistics for Literature
  Proceedings}.  Stroudsburg, PA: Association for Computational Linguistics. 26--35.

\bibitem{gorban2008principal-1}
\Aue{Gorban, A.\,N., B.~K$\acute{\mbox{e}}$gl, D.\,C.~Wunsch, %A.\,Y.~Zinovyev, 
\textit{et al}.} 2008. \textit{Principal manifolds for data visualization 
and dimension reduction.} Springer. 58~p.

\bibitem{tenenbaum2000global-1} %14
  \Aue{Tenenbaum, J.\,B., V.~De Silva, and J.\,C.~Langford.} 2000. 
  A~global geometric framework for nonlinear dimensionality reduction. 
  \textit{Science} 290(5500):2319--2323.

\bibitem{belkin2001laplacian-1} %15
  \Aue{Belkin, M., and P.~Niyogi.} 
  2001. Laplacian eigenmaps and spectral techniques for embedding and clustering. 
  \textit{Advances in neural information processing systems}.
  Eds.\ T.\,G.~Dietterich, S.~Becker, and Z.~Ghahramani.
  NIPS Foundation, Inc. 14:585--591.

\bibitem{roweis2000nonlinear-1} %16
\Aue{Roweis, S.\,T., and L.\,K.~Saul.} 2000. 
Nonlinear dimensionality reduction by locally linear embedding. 
\textit{Science} 290(5500):2323--2326.

\bibitem{donoho2003hessian-1} %17
\Aue{Donoho, D.\,L., and C.~Grimes.} 2003. 
Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data. 
\textit{P. Natl. Acad. Sci. USA} 100(10):5591--5596.

\bibitem{zhang2004principal-1}
  \Aue{Zhang, Z., and H.~Zha.} 
  2004. Principal manifolds and nonlinear dimensionality reduction via tangent space
   alignment. \textit{J.~Shanghai University (English Edition)} 
8(4):406--424.

\bibitem{weinberger2006unsupervised-1}
  \Aue{Weinberger, K.\,Q., and L.\,K.~Saul.} 
  2006. Unsupervised learning of image manifolds by semidefinite programming. 
  \textit{Int. J.~Comput. Vision} 70(1):77--90.

\bibitem{chen2010distance-1}
 \Aue{Chen, C., J.~Zhang, and R.~Fleischer}. 2010. 
 Distance approximating dimension reduction of Riemannian manifolds. 
 \textit{IEEE T. Syst. Man Cy.~B} 
 40(1):208--217.

\bibitem{van2009learning-1} %21
\Aue{Van der Maaten, L.} 2009. Learning a parametric embedding by preserving 
local structure. \textit{RBM} 500:26.

\bibitem{van2014accelerating-1}
  \Aue{Van der Maaten, L.} 2014. 
  Accelerating t-SNE using tree-based algorithms. 
  \textit{J.~Mach. Learn. Res.} 15(1):3221--3245.

\bibitem{kim2007distance-1} %23
\Aue{Kim, H., H.~Park, and H.~Zha.} 
2007. Distance preserving dimension reduction for manifold learning. 
\textit{SIAM  Conference (International) on Data Mining Proceedings.} 527--532.
{\looseness=1

}

\bibitem{bottou2012stochastic-1} %24
\Aue{Bottou, L.} 2012. Stochastic gradient descent tricks. 
\textit{Neural networks: Tricks of the trade}. 
Eds. G.~Montavon, G.\,B.~Orr, and K.-R.~Muller. 
Lecture notes in computer science ser. 2nd ed. Berlin--Heidelberg: 
Springer. 7700:421--436.

 \bibitem{Pan2011Collection-1}
 \Aue{Potthast, M., B.~Stein, A.~Barr$\acute{\mbox{o}}$n-Cede$\tilde{\mbox{n}}$o, 
 and P.~Rosso.} 2010. An evaluation framework for plagiarism detection. 
 \textit{23rd Conference (International) on Computational Linguistics 
 Posters.} 997--1005.
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-3pt}

\hfill{\small\textit{Received February 20, 2017}}

\Contr

\noindent
\textbf{Molybog Igor O.} (b.\ 1995)~--- 
apprentice researcher, Skolkovo Institute of Science and Technology, 
Center for Energy Systems, Skolkovo Innovation Center, 3~Nobel Str., Moscow 143026, 
Russian Federation; student, Moscow Institute of Physics and Technology, 
9~Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian Federation; 
\mbox{i.molybog@skoltech.ru}

\vspace*{5pt} 

\noindent
\textbf{Motrenko Anastasia P.} (b.\ 1992)~--- 
PhD student, Moscow Institute of Physics and Technology, 
9~Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian Federation; 
\mbox{anastasiya.motrenko@phystech.edu}

\vspace*{5pt}

\noindent
\textbf{Strijov Vadim V.} (b.\ 1967)~--- 
Doctor of Science in physics and mathematics, leading scientist, A.\,A.~Dorodnicyn 
Computing Centre, Federal Research Center ``Computer Science and Control'' 
of the Russian Academy of Sciences, 40~Vavilov Str., Moscow 119333, 
Russian Federation; \mbox{strijov@ccas.ru}


\label{end\stat}


\renewcommand{\bibname}{\protect\rm Литература} 
\def\stat{chehovich}

\def\tit{МЕТОДЫ ОБНАРУЖЕНИЯ ПЕРЕВОДНЫХ ЗАИМСТВОВАНИЙ В~БОЛЬШИХ ТЕКСТОВЫХ 
КОЛЛЕКЦИЯХ$^*$}

\def\titkol{Методы обнаружения переводных заимствований в~больших текстовых 
коллекциях}

\def\aut{Р.\,В.~Кузнецова$^1$, О.\,Ю.~Бахтеев$^2$, Ю.\,В.~Чехович$^3$}

\def\autkol{Р.\,В.~Кузнецова, О.\,Ю.~Бахтеев, Ю.\,В.~Чехович}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Кузнецова Р.\,В.}
\index{Бахтеев О.\,Ю.}
\index{Чехович Ю.\,В.}
\index{Kuznetsova R.\,V.}
\index{Bakhteev O.\,Yu.}
\index{Chekhovich Yu.\,V.}

{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
{Работа выполнена при поддержке РФФИ (проект 18-07-01441) 
и~Фонда содействия развитию малых форм предприятий в~на\-уч\-но-тех\-ни\-че\-ской сфере 
(проект~44116).}}

\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Московский физико-технический институт, 
\mbox{rita.kuznetsova@phystech.edu}}
\footnotetext[2]{Компания 
Антиплагиат; 
Московский фи\-зи\-ко-тех\-ни\-че\-ский институт, \mbox{bakhteev@ap-team.ru}}
\footnotetext[3]{Вычислительный центр им.\ А.\,А.~Дородницына Федерального 
исследовательского центра <<Информатика и~управ\-ле\-ние>> Российской академии наук, 
\mbox{chehovich@ap-team.ru}}


\vspace*{-8pt}


\Abst{Рассматривается задача обнаружения переводных заимствований. 
Для решения предлагается использовать моноязыковой подход~--- свести задачу 
обнаружения заимствований к~одному языку, используя машинный перевод. В~связи со 
спецификой рассматриваемой задачи предлагаемый алгоритм обнаружения должен быть 
устойчив к~неоднозначностям перевода. Предлагается декомпозировать задачу на 
несколько этапов.
Сначала отбираются до\-ку\-мен\-ты-кан\-ди\-да\-ты,  устойчивость к~неоднозначности перевода 
достигается за счет замены слов на метки кластеров, полученных с~по\-мощью 
дистрибутивной модели. Затем происходит сравнение найденных кандидатов 
и~рассматриваемого документа, для этого используется отображение текстовых 
фрагментов документов в~векторное пространство высокой размерности. 
Вычислительный эксперимент проводится для языковой пары 
<<рус\-ский--анг\-лий\-ский>> на двух выборках~--- синтетическом корпусе и~на статьях из 
журналов, входящих в~Российский индекс научного цитирования (РИНЦ).}


\KW{автоматическая обработка текстов; машинный перевод; 
глубокое обучение; переводные заимствования; обнаружение переводных 
заимствований; дистрибутивная семантика}

\DOI{10.14357/19922264210105}

%\vspace*{-2pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}

\section{Введение}

\vspace*{-2pt}

Проблема некорректных текстовых заимствований актуальна для сферы образования 
и~научных исследований~\cite{plag_cheh}. По материалам исследования~\cite{hist}, 
проведенного в~2013~г., более 1500~диссертаций по историческим наукам, 
защищенных в~России после 2000~г., содержат значительные заимствования из 
других диссертаций.

Для задачи обнаружения заимствований в~рамках одного языка высокую полноту 
поиска показывают промышленные инструменты~\cite{plag_cheh}, работа которых 
основана на представлении документов в~виде набора перекрывающих друг друга 
пословных $n$-грамм (шинглов)~\cite{shingles1}. Такой подход позволяет 
эффективно проводить поиск точных текстовых заимствований, но не позволяет 
обнаруживать заимствования с~большой долей перефразированного текста или со 
вставками  текста, переведенного с~другого языка.

Существуют несколько подходов, опи\-сы\-ва\-ющих проб\-ле\-му поиска переводных 
заимствований для некоторых пар языков~\cite{clkga,clfreshonto}, например для 
пары ис\-пан\-ский--анг\-лий\-ский. Настоящая работа посвящена обнаружению переводных 
заимствований для пары языков рус\-ский--анг\-лий\-ский. Данная пара нечасто 
встречается в~литературе и~не является родственной. Выбор пары языков рус\-ский--анг\-лий\-ский 
обуслов\-лен преобладанием англоязычных пуб\-ли\-ка\-ций в~интернете 
и~лучшим знанием этого языка по сравнению с~другими. Аналогично 
работам~\cite{framework1,framework2} в~данной статье предлагается описание 
алгоритма полного цик\-ла поиска заимствований~--- сначала ведется поиск 
до\-ку\-мен\-тов-кан\-ди\-да\-тов по внешней коллекции, затем происходит их детальное 
сравнение с~проверяемым документом. Предлагается алгоритм, основанный на 
моноязыковом анализе документов, схожем с~проведенным 
в~работах~\cite{mono,fruct}~--- про\-ве\-ря\-емый документ переводится на английский 
язык с~использованием системы машинного перевода с~дальнейшим сравнением 
текс\-то\-вых фрагментов внут\-ри документов.

В ряде работ, посвященных поиску переводных заимствований, используются 
дополнительные ресурсы, такие как тезаурусы и~онтологии.
В~работах~\cite{clkga,clfreshonto} авторы предлагают использовать базы знаний 
для извлечения информации о~близости между текстами. В работе~\cite{clfreshonto} 
предлагается алгоритм, основанный на комбинации нейронных сетей и~графов знаний. 
Основной недостаток этого подхода~--- ресурсоемкость: использование 
мультиязычных онтологий и~баз знаний требует больших вычислительных мощностей 
для построения семантических графов для каждого текстового фрагмента, а также 
сравнения полученных семантических графов.

В данной работе предлагается декомпозиция алгоритма обнаружения переводных 
заимствований для поиска по большим текстовым коллекциям.
Общая схема алгоритма включает следующие шаги.
\begin{enumerate}
\item \textit{Машинный перевод}~--- перевод проверяемого документа на английский 
язык. Для этого используется система статистического машинного 
перевода~\cite{moses}.
\item \textit{Поиск до\-ку\-мен\-тов-кан\-ди\-да\-тов}~--- для проверяемого документа 
находятся наиболее релевантные до\-ку\-мен\-ты-кан\-ди\-да\-ты, для этого используется 
модификация алгоритма шинглов.
\item \textit{Сравнение документов}~--- текст разбивается на фрагменты, строится 
отображение каждой фразы в~векторное пространство. Для каждого вектора 
проверяемого документа находятся ближайшие векторы из до\-ку\-мен\-тов-кан\-ди\-да\-тов, 
после чего проводится классификация пар данных векторов на схожие и~несхожие 
пары текстовых фрагментов.
\end{enumerate}

Так как в~предлагаемом алгоритме используется моноязыковой анализ заимствований, 
то задача близка к~задаче обнаружения перефразированного текс\-та.
Ряд подходов~\cite{Socher1,wieting,Iyyer,vbta} к~решению этой задачи используют 
векторные пред\-став\-ле\-ния фраз, полученные с~по\-мощью нейронных сетей глубокого 
обуче\-ния.
В работе~\cite{Iyyer} предлагается  нейронный мешок слов (\textit{англ}.\ Neural Bag-of-Words) 
и~глубокие усред\-ня\-ющие сети (\textit{англ}.\ Deep averaging networks).
В~данной \mbox{статье} предлагается использовать выходы нейронной сети как векторные 
пред\-став\-ле\-ния текс\-то\-вых фрагментов для дальнейшего при\-бли\-жен\-но\-го алгоритма 
поиска бли\-жай\-ше\-го соседа~\cite{ann}.

В работе исследуются свойства предлагаемого метода обнаружения переводных 
заимствований. Проводится анализ моделей глубокого обучения, используемых на 
этапе сравнения документов, а~также составной оптимизируемой функции. Проверка 
качества предложенного метода проводится как на синтетической выборке, так и~на 
статьях из журналов, входящих в~РИНЦ. 
Проводится анализ ошибок. Предложенный метод поиска заимствований сравнивается 
с~базовым алгоритмом поиска заимствований, основанным на использовании машинного 
перевода и~алгоритме шинглов.

\vspace*{-6pt}

\section{Постановка задачи}

Пусть заданы коллекции документов на английском языке 

\noindent
$$
D_e = \{d_e^j\}_{j=1}^N
$$ 
и~русском языке 
$$
D_r = \{d_r^i\}_{i=1}^M.
$$
 Документы на русском и~английском 
языке представимы в~виде конкатенации текстовых фрагментов:
$$
d_e^j = \left[s_{e_1}^j \sqcup \cdots \sqcup s_{e_h}^j\right];\enskip
d_r^i \hm= \left[s_{r_1}^i 
\sqcup \cdots \sqcup s_{r_k}^i\right].
$$

Пусть задана выборка  
$$
\mathcal{D}=\left\{(d_e^l, d_r^l), \mathrm{RL}^l\right\}_{l=1}^L,
$$
 где каждой 
паре документов $(d_e^l, d_r^l)_{d_e^l \in D_e, d_r^l \in D_r}$ сопоставлен 
список пар фрагментов 
$$
\mathrm{RL}= \left[(s_{e_1}^l, s_{r_1}^l), \ldots, (s_{e_{k(l)}}^l, 
s_{r_{k(l)}}^l)\right].
$$
 Для каждой пары $(s_{e_{\tilde{k}}}^l, s_{r_{\tilde{k}}}^l)$ 
известно, что фрагмент~$s_{r_{\tilde{k}}}^l$ является переводом фрагмента~$s_{e_{\tilde{k}}}^l$.


Модель $f$ задается как последовательное выполнение функций {filter} 
и~comparison, где
\begin{align*}
\mbox{filter:}&\ \ (d_r^i, D_e)_{d_r^i \in D_r} \rightarrow D_e^{\mathrm{retrieved}_i} \subset 
D_e,\\
\mbox{comparison:}&\ \  (d_r^i, D_e^{\mathrm{retrieved}_i})_{d_r^i \in D_r} \rightarrow \mathrm{RL}^{i}.
\end{align*}
Здесь $\mathrm{RL}^{i}$~--- список пар фрагментов. Функция {filter} отвечает за 
сужение числа документов коллекции, сравниваемых с~проверяемым документом, 
и~позволяет проводить дальнейшее более детальное сравнение {comparison} 
с~использованием ресурсоемких вычислительных алгоритмов, основанных на моделях 
глубокого обучения.


Качество модели $f$ оценивается с~помощью функций Precision и~Recall:
\begin{align*}
\mbox{Precision}& = \fr{|(\mathop{\cup}\nolimits_{l=1}^L \mathrm{RL}^l) \cap (\mathop{\cup}\nolimits_{i=1}^M 
\mathrm{RL}^{i})|}{|\mathop{\cup}\nolimits_{i=1}^M \mathrm{RL}^i|},\\
\mbox{Recall} &= \fr{|(\mathop{\cup}\nolimits_{l=1}^L \mathrm{RL}^l) \cap (\mathop{\cup}\nolimits_{i=1}^M 
\mathrm{RL}^{i})|}{|\mathop{\cup}\nolimits_{l=1}^L \mathrm{RL}^{l}|}.
\end{align*}

Требуется найти функцию $f$, мак\-си\-ми\-зи\-ру\-ющую~F1, среднее гармоническое 
показателей Precision и~Recall:
\begin{align*}
\hat{f}& = \argmax\limits_{f \in \mathcal{F}}\text{F1}(f, \mathcal{D}), \\[3pt]
 \text{F1} &= \fr{2  \mbox{Precision} \cdot \mbox{Recall} 
}{\mbox{Precision}+\mbox{Recall}},
%\label{opt_frag}
\end{align*}
где $\mathcal{F}$~--- заданное семейство моделей.



\section{Поиск документов-кандидатов}

Одним из алгоритмов поиска до\-ку\-мен\-тов-кан\-ди\-да\-тов в~задачах обнаружения дословных 
заимствований и~поиска \textit{поч\-ти-дуб\-ли\-ка\-тов} текста\linebreak служит алгоритм, 
основанный на построении инвертированного индекса, в~котором каждый документ 
коллекции представляется набором \textit{шинглов}~\cite{shingles1}, т.\,е.\ 
набором перекрывающихся $n$-грамм. Про\-ве\-ря\-емый документ также разбивается на 
шинглы, после чего проводится поиск документов по инвертированному индексу 
с~наибольшим совпадением шинглов. В~данной работе предлагается обобщение алгоритма 
шинглов, позволяющее улучшить качество поиска кандидатов в~случае обнаружения 
переводных заимствований.

Предлагается функция filter следующего вида:
\begin{multline*}
\mbox{filter}\left(d_r^i, D_e\right) ={}\\
{}=\! \!\!\!\!\!\argmax\limits_{D_e^{'} \subset D_e, |D_e^{'}|=k}\sum\limits_{d_e^j \in 
D_e^{'}}\sum\limits_{h \in \mathcal{H}(d_r^i)}\!\!\!\! \mathbf{I}\left[h \in 
\mathcal{H}(d_e^j)\right]\! \Big / \!
\left(|d_e^{j^{'}} \in D_e\!:\right.\\
\left. h \in 
\mathcal{H}(d_e^{j^{'}})|^{\alpha} + \mbox{const}\right)\,.
\end{multline*}
Здесь $\mathcal{H}$~--- множество $n$-грамм документа, упорядоченная 
последовательность $n$ меток кластеров, где процедура формирования кластеров 
описана ниже; $\alpha \hm\in \mathbb{R}$; $k$~--- оптимизируемый гиперпараметр.


Для уменьшения влияния не\-од\-но\-знач\-ности перевода на поиск до\-ку\-мен\-тов-кан\-ди\-да\-тов 
предлагается заменять слова на соответствующие им метки кластеров:
$$
\left\{x_1, \ldots, x_n\right\} \rightarrow \left\{ \mbox{class}\left(x_1\right), \ldots, 
\mbox{class}\left(x_n\right)\right\} = h\,,
$$
где $x_1, \ldots, x_n$~--- слова. Кластеры предварительно выделены из текстового 
корпуса и~содержат семантически близкие слова.
Для уменьшения неоднозначности перевода перед разбиением на $n$-граммы 
предлагается удалять из текста стоп-сло\-ва и~проводить лемматизацию. Для учета 
возможных перестановок слов, возникающих после перевода текста, слова внутри 
каждой $n$-грам\-мы сортируются в~лексикографическом порядке.

В данной работе для получения кластеров используется модель векторного 
представления слов, основанная на дистрибутивной гипотезе. Кластеризация 
проводится с~использованием косинусной функции расстояния
\begin{equation}
\label{eq:cos}
    \cos \left(\mathbf{c}_1, \mathbf{c}_2\right) = \fr{\langle\mathbf{c}_1, 
\mathbf{c}_2\rangle}{{||\mathbf{c}_1||_2||\mathbf{c}_2||_2}},
\end{equation}
где $\mathbf{c}_1$ и~$\mathbf{c}_2$~--- векторы из одного векторного пространства.


Ниже приведены примеры полученных кластеров:
\begin{itemize}
\item $[$beer, beers, brewing, ale, brew, brewery, pint, stout, guinness, 
ipa, brewed, lager, ales, brews, pints, cask$]$;
\item $[$brilliant, excellent, exceptional, finest, outstanding, super, 
terrific$]$.
\end{itemize}


\vspace*{-6pt}

\section{Сравнение документов}

Для сравнения найденных документов-кан\-ди\-да\-тов $D_e^{\mathrm{retrieved}_i}$ 
и~проверяемого документа~$(d_r^i)$\linebreak используется модель векторного пред\-став\-ле\-ния 
фразы~--- текс\-ты разбиваются на фрагменты и~сравниваются соответствующие им 
векторы. Ниже пред\-став\-ле\-ны детали алгоритма сравнения, а~также анализ 
пред\-ла\-га\-емой оптимизационной задачи.

\vspace*{-6pt}

\subsection{Модель векторного представления фразы}

Рассмотрим подробнее этап построения отображения фрагмента в~вектор.
Пусть каждому слову документа на языке коллекции поставлен в~соответствие вектор 
$\mathbf{v} \hm\in \mathbb{R}^u$ размерности~$u$. Для прос\-то\-ты будем полагать, что 
все фрагменты на языке коллекции имеют ограниченную длину $n_{\mathrm{col}}$.
Тогда моделью векторизации фрагмента будем называть отображение
$$
    \mathbf{h}: \mathbb{W} \times \mathbb{R}^{u \times n_{\mathrm{col}}} \to 
\mathbb{R}^u\,,
$$
где $\mathbb{W}$~--- пространство параметров модели. Объекты из множества 
$\mathbb{R}^{u \times n_{\mathrm{col}}}$ являются последовательной конкатенацией 
векторов векторных представлений слов для фрагментов выборки:
$$
\mathbf{x} \in [\mathbf{v}_1, \dots, \mathbf{v}_{n_{\mathrm{col}}}]^{\mathrm{T}}\,,  
\mathbf{x} \in \mathbb{R}^{u \times n_{\mathrm{col}}}.
$$
Для работы с~фрагментами длиной меньше $n_{\mathrm{col}}$ определим некоторый 
вектор, обозначающий пус\-тое слово.


Модель оптимизируется в~режиме частичного обучения с~учителем. В~качестве 
оптимизируемой функции используется составная функция ошибки, представляющая 
собой сумму ошибки реконструкции и~ошибки отступа:
\begin{equation}
\label{eq:alpha}
\alpha E_{\mathrm{rec}}\left(\mathbf{X}_{\mathrm{rec}}, \mathbf{w}\right) + (1 - 
\alpha)E_{\mathrm{me}}(\mathbf{X}_{\mathrm{me}},\mathbf{w}) \to \min_{\mathbf{w} \in 
\mathbb{W}},
\end{equation}
где $E_{\mathrm{rec}}$~--- ошибка реконструкции; $E_{\mathrm{me}}$ --- ошибка отступа; 
$\mathbf{X}_{\mathrm{rec}}$ и~$\mathbf{X}_{\mathrm{me}}$~--- обучающие выборки;  
$\mathbf{w}$~--- параметры модели; $\alpha$~--- настраиваемый гиперпараметр.
Рассмотрим подробнее каждое слагаемое функции ошибки.

Первое слагаемое функции ошибки соответствует модели автокодировщика.
Пусть задана выбор\-ка $\mathbf{X}_{\mathrm{rec}} \subset \mathbb{R}^{u \times 
n_{\mathrm{col}}}$.  Модель~$\mathbf{h}$ выступает в~качестве функции кодирования 
информации о выборке~$\mathbf{X}_{\mathrm{rec}}$. Пусть также задана вспомогательная 
функция декодирования~$\mathbf{g}$, восстанавливающая исходное векторное 
представление~$\mathbf{x}$ по выходам модели~$\mathbf{h}$:
$$
   \mathbf{r}(\mathbf{x}, \mathbf{w}) = \mathbf{g}(\cdot, \mathbf{w}) \circ 
\mathbf{h} (\mathbf{x}, \mathbf{w}) \approx  \mathbf{x},  \mathbf{x} \in 
\mathbb{R}^{u \times n_{\mathrm{col}}}.
$$
Минимизируемая ошибка реконструкции выглядит следующим образом:
\begin{equation}
\label{eq:rec}
E_{\mathrm{rec}}(\mathbf{X}_{\mathrm{rec}}, \mathbf{w}) = 
\fr{1}{|\mathbf{X}_{\mathrm{rec}}|}\sum\limits_{\mathbf{x} \in \mathbf{X}_{\mathrm{rec}}} 
\parallel\mathbf{x}  - \mathbf{r}(\mathbf{x}, \mathbf{w}) \parallel^2_2.
\end{equation}


Выбор ошибки реконструкции в~качестве оптимизируемой функции можно обосновать, 
используя результаты статьи~\cite{ae}. Будем пользоваться результатами, 
доказанными в~работе~\cite{ae}, где было показано, что автокодировщики 
с~регуляризацией специального вида позволяют оценить распределение~$p(\mathbf{X})$ 
объектов, принадлежащих генеральной совокупности.

\smallskip

\noindent
\textbf{Теорема~1}\ 
\cite{ae}. %\label{manifold}
\textit{Пусть $p$~--- дифференцируемая плотность вероятности и~$\forall\, 
\mathbf{x}_i \hm\in \mathbb{R}^{u \times n_{\mathrm{col}}}$ $p(\mathbf{x}_i)\hm\neq 0$.  
Пусть $\mathcal{L}_{\sigma^2}$~--- функция потерь вида}
\begin{multline*}
\mathcal{L}_{\sigma^2} ={}\\
{}=\!\!\!\int\limits_{\mathbb{R}^{u \times n_{\mathrm{col}}}} \!\!\!\!\!\!
p(\mathbf{x}) \left[
\parallel \mathbf{x} - \mathbf{r}(\mathbf{x}, \mathbf{w}) 
\parallel^2_2 + \sigma^2  \left\| \fr{\partial \mathbf{r}(\mathbf{x}, 
\mathbf{w})}{\partial \mathbf{x}} \right\|_{F}^2\right] d\mathbf{x}\,,\hspace*{-7.12155pt}
\end{multline*}
\textit{где $\mathbf{r}$ дважды дифференцируема}; $0 \hm\leqslant \sigma \hm\in \mathbb{R}$. 
\textit{Пусть $\hat{\mathbf{w}}$~--- оптимум функции~$\mathbf{r}$ по параметрам моделей 
кодирования и~декодирования, доставляющий минимум~$\mathcal{L}_{\sigma^2}$. 
Тогда}
$$
\hat{\mathbf{r}}_{\sigma^2}\left(\mathbf{x}, \mathbf{w}\right) = \mathbf{x} + \sigma^2 
\fr{\partial \log p(\mathbf{x})}{\partial \mathbf{x}} + o\left(\sigma^2\right), \quad 
\sigma^2 \rightarrow 0\,.
$$


Используя результаты теоремы~1, можно сделать следующее утверждение.

\smallskip

\noindent
\textbf{Теорема~2.}\ 
\textit{Плотность вероятности представима в~виде}:
$$
\fr{\hat{\mathbf{r}}_{\sigma^2}(\mathbf{x}, \mathbf{w}) - 
\mathbf{x}}{\sigma^2} \approx -\fr{\partial}{\partial 
\mathbf{x}}E(\mathbf{x}),
$$ 
\textit{где} 
$(\mathbf{x}) = ({1}/{Z})\exp(-
E(\mathbf{x}))$, $Z$~--- \textit{нормировочная константа}.

\smallskip

\noindent
Д\,о\,к\,а\,з\,а\,т\,е\,л\,ь\,с\,т\,в\,о\,.
\begin{align*}
{\mathbf{r}}_{\sigma^2}\left(\mathbf{x}, \hat{\mathbf{w}}\right) &= \mathbf{x} + \sigma^2 
\fr{\partial}{\partial \mathbf{x}}\log p(\mathbf{x}) + o\left(\sigma^2\right);
\\
\fr{{\mathbf{r}}_{\sigma^2}(\mathbf{x}, \hat{\mathbf{w}}) - 
\mathbf{x}}{\sigma^2} &= \fr{\partial}{\partial \mathbf{x}}\log p(\mathbf{x}) + o(1);
\\
\fr{{\mathbf{r}}_{\sigma^2}(\mathbf{x}, \hat{\mathbf{w}}) - 
\mathbf{x}}{\sigma^2} &\approx \fr{\partial}{\partial \mathbf{x}}\log 
p(\mathbf{x}).
\end{align*}
Представляя $\log p(\mathbf{x})$ в~форме $-E(\mathbf{x}) \hm- \log Z$, получим 
искомое выражение.

\smallskip

Таким образом, при устремлении регуляризатора~$\sigma$ к~нулю получается  
\textit{языковая модель}, т.\,е.\ распределение вероятностей на множестве~$\mathbf{X}$~--- 
множестве текстовых последовательностей.

Второе слагаемое составной функции ошибки~--- ошибка отступа~\cite{wieting}. Для 
оптимизации этой функции ошибки используется выборка $\mathbf{X}_{\mathrm{me}}\hm = 
\{ (\mathbf{x}_i, \mathbf{x}_j)\}$, состоящая из пар объектов:
$$
    \mathbf{X}_{\mathrm{me}} = \left[\mathbf{X}_{\mathrm{me}}^A; \mathbf{X}_{\mathrm{me}}^B\right]  
\subset \mathbb{R}^{u \times n_{\mathrm{col}}}  \times  \mathbb{R}^{u \times 
n_{\mathrm{col}}};
$$

\vspace*{-14pt}

\noindent
\begin{multline}
\label{eq:me}
E_{\mathrm{me}} = \fr{1}{|\mathbf{X}_{\mathrm{me}}|}\left( \sum\limits_{(\mathbf{x}_i, 
\mathbf{x}_j) \in \mathbf{X}_{\mathrm{me}}}\!\!\!\!\max \left(0, \delta - c_{-}\right) +{}\right.\\
\left.{}+ \max\left(0, \delta 
- c_{+}\right) 
\vphantom{\sum\limits_{(\mathbf{x}_i, 
\mathbf{x}_j) \in \mathbf{X}_{\mathrm{me}}}}
\right),
\end{multline}

\vspace*{-6pt}


\noindent
где 

\vspace*{-6pt}

\noindent
\begin{multline*}
c_{-} = \cos\left(\mathbf{h}\left(\mathbf{x}_i, \mathbf{w}\right), 
\mathbf{h}\left(\mathbf{x}_j, \mathbf{w}\right)\right) -{}\\
{}- \cos\left(\mathbf{h}\left(\mathbf{x}_i, 
\mathbf{w}\right), \mathbf{h}\left(\mathbf{x}_{i^{'}}, \mathbf{w}\right)\right);
\end{multline*}

\vspace*{-12pt}

\noindent
\begin{multline*}
c_{+} = \cos\left(\mathbf{h}\left(\mathbf{x}_i, \mathbf{w}\right), 
\mathbf{h}\left(\mathbf{x}_j, \mathbf{w}\right)\right) - {}\\
{}-\cos\left(\mathbf{h}\left(\mathbf{x}_j, 
\mathbf{w}\right), \mathbf{h}\left(\mathbf{x}_{j'}, \mathbf{w}\right)\right);
\end{multline*}

\vspace*{-2pt}

\noindent
$\delta$~--- отступ; $\cos$~--- функция расстояния~\eqref{eq:cos}, 

\noindent
\begin{align*}
\mathbf{x}_{i^{'}}&=\argmax\limits_{\mathbf{x}_{i^{'}} \in \mathbf{X}^B, 
\mathbf{x}_{i^{'}} \neq \mathbf{x}_{j}}\cos\left(\mathbf{x}_i, 
\mathbf{x}_{i^{'}}\right);
\\
\mathbf{x}_{j^{'}}&=\argmax\limits_{\mathbf{x}_{j^{'}} \in \mathbf{X}^A, 
\mathbf{x}_{j^{'}} \neq \mathbf{x}_{i}}\cos\left(\mathbf{x}_i, 
\mathbf{x}_{i^{'}}\right).
\end{align*}

Следующая теорема объясняет поведение данного слагаемого при проводимой 
оптимизации параметров~$\mathbf{w}$ модели~$\mathbf{h}$.

\smallskip

\noindent
\textbf{Теорема~3.}\ 
\textit{Пусть выполнены следующие условия}.
\begin{enumerate}
\item \textit{Задан гиперпараметр} $\delta \hm\in (0, 2).$
\item \textit{Мощность выборки} $|\mathbf{X}_{\mathrm{me}}|$ \textit{ограничена следующей величиной}:
\begin{multline}
    |\mathbf{X}_{\mathrm{me}}|(|\mathbf{X}_{\mathrm{me}}|-1)  \leq{}\\
    \hspace*{-3pt}{}\leq \sqrt{\pi} 
\fr{\Gamma((u-1)/2)}{\Gamma(u/2)} \left( \int\limits_{0}^{\arccos(1 - \delta)}\!\! \!\!\!\!\!\!
\sin^{u-2} x\, dx \right )^{\!-1}\!\!.\!\!
\label{eq:sphere_code}
\end{multline}
\item \textit{Подвыборки $\mathbf{X}^A_{\mathrm{me}}$ и~$\mathbf{X}^B_{\mathrm{me}}$ содержат все 
элементы в~единственном числе, ни один элемент не встречается в~обеих выборках}.
\end{enumerate}
\textit{Тогда существует непрерывное отображение~$\hat{\mathbf{h}}$ из множества 
векторных представлений слов} $\mathbb{R}^{u \times n_{\mathrm{col}}}$
\textit{в~векторное 
пространство~$\mathbb{R}^{u}$, доставляющее глобальный минимум функции} $E_{\mathrm{me}} \hm= 
0$.

\pagebreak

\noindent
Д\,о\,к\,а\,з\,а\,т\,е\,л\,ь\,с\,т\,в\,о\,.\ \
Построим отображение~$\hat{\mathbf{h}}$ явно.
Положим для каждой пары $(\mathbf{x}_1, \mathbf{x}_2)$: 
$\hat{\mathbf{h}}(\mathbf{x}_1) \hm= \hat{\mathbf{h}}(\mathbf{x}_2).$

Тогда функция $E_{\mathrm{me}}$ выглядит следующим образом с~точностью до множителя:
\begin{multline*}
\!E_{\mathrm{me}}= \!\!\!\!\!\sum\limits_{(\mathbf{x}_i, \mathbf{x}_j) \in \mathbf{X}_{\mathrm{me}}}
\!\!\!\!\!\!\!\!\max\left(0, \delta - 1 
+ \cos\left(\hat{\mathbf{h}}(\mathbf{x}_i), 
\hat{\mathbf{h}}\left(
\mathbf{x}_{i^{'}}\right)\!\right)\!\right) +{}\\
{}+ \max\left(0, \delta - 1  + 
\cos\left(\hat{\mathbf{h}}(\mathbf{x}_j), 
\hat{\mathbf{h}}\left(\mathbf{x}_{j^{'}}\right)\!\right)\!\right).
\end{multline*}

Область значений функции ограничена снизу нулем, который достигается при 
выполнении условий:
$$
   1 - \delta \geq  \cos \left(\hat{\mathbf{h}}(\mathbf{x}), 
\hat{\mathbf{h}}\left(\mathbf{x}'\right)\right)
$$
 для любой пары $\mathbf{x} \hm\in 
\mathbf{X}^A_{\mathrm{me}}$, $\mathbf{x}' \hm\in \mathbf{X}^B_{\mathrm{me}}$,  
$(\mathbf{x}, \mathbf{x}') \notin \mathbf{X}_{\mathrm{me}}$, $(\mathbf{x}', \mathbf{x}) \hm\notin 
\mathbf{X}_{\mathrm{me}}.$
Число пар, описанных выше, в~множестве~$\mathbf{X}_{\mathrm{me}}$ при выполнении 
третьего условия теоремы равно $|\mathbf{X}_{\mathrm{me}}|(|\mathbf{X}_{\mathrm{me}}|\hm-
1)$. Назначим значение отображения~$\hat{\mathbf{h}}$ для каждой такой пары так, 
чтобы $\cos(\hat{\mathbf{h}}(\mathbf{x}), \hat{\mathbf{h}}(\mathbf{x}'))  
\hm\leq 1 \hm- \delta$.

Существование такого отображения следует из задачи о нахождении сферического 
кода максимального размера для сферы в~пространстве размерности~$u$ и~углом 
$\arccos(1 \hm- \delta).$
В~работе~\cite{sphere_code} представлена нижняя оценка для размерности выборки, 
удовлетворяющей заданным условиям. Оценка соответствует правой части 
неравенства~\eqref{eq:sphere_code}. Так как выборка $\mathbf{X}_{\mathrm{me}}$  
конечна, то для построения непрерывной функции, заданной условиями, описанными 
выше, можно использовать интерполяционные полиномы, что и~требовалось доказать.

\smallskip


Заметим, что предложенное в~теореме отображение является непрерывным, поэтому 
для приближения данного отображения можно использовать нейросетевые модели. По 
теореме Цыбенко отображения из класса нейросетевых моделей будут приближать 
непрерывные модели сколь угодно хорошо~\cite{cybenko}.

Таким образом, составная оптимизируемая функция~\eqref{eq:alpha} позволяет 
получить модель, которая, с~одной стороны, обладает обобщающими свойствами, за 
которые отвечает языковая модель~\eqref{eq:rec}, с~другой стороны, эффективно 
разделяет схожие и~несхожие фразы из обучающей выборки~\eqref{eq:me}. 
Гиперпараметр~$\alpha$ отвечает за вклад каждого из оптимизируемых слагаемых в~данную функцию.

\subsection{Классификатор}

Для каждого вектора фразы~$\mathbf{h}(\mathbf{x}_{r_{a}}^i)$ из про\-ве\-ря\-емо\-го 
документа~$d_r^i$ находится~$v$~ближайших векторов по косинусной функции 
расстояния~\eqref{eq:cos} для фрагментов из до\-ку\-мен\-тов-кан\-ди\-да\-тов 
$D_e^{\mathrm{retrieved}_i}$, используя метод приближенного поиска ближайшего соседа.
Основная цель данной процедуры~---~сократить число пар фрагментов для 
классификации для снижения ресурсоемкости этапа сравнения документа.

Для векторного представления пары фрагментов $(\mathbf{h}(\mathbf{x}_{e_{b}}^j), 
\mathbf{h}(\mathbf{x}_{r_{a}}^i))$ рассматривается следующее решающее правило:
\begin{multline}
\label{eq:t1t2}
f_{\mathrm{frag}}\left(\left(\mathbf{h}\left(\mathbf{x}_{e_{b}}^j\right), \mathbf{h}(\mathbf{x}_{r_{a}}^i)\right)\right) ={}\\
{}=
\begin{cases}
1, &\mbox{ если } \cos\left(\mathbf{h}(\mathbf{x}_{e_{b}}^j), 
\mathbf{h}\left(\mathbf{x}_{r_{a}}^i\right)\right)>t_1\\
& \hspace*{5mm}\mbox{ и~} 
p\left(\mathbf{h}\left(\mathbf{x}_{e_{b}}^j\right), \mathbf{h}\left(\mathbf{x}_{r_{a}}^i\right)\right)>t_2;\\
0 &\mbox{ иначе},
\end{cases}
\end{multline}
где $p$~--- вероятность классификатора; $t_1$~--- порог косинусной  функции 
расстояния~\eqref{eq:cos}; $t_2$~--- минимальный порог вероятности 
классификатора. 

В качестве признаков используется конкате\-на\-ция разницы по модулю 
и~покомпонентное\linebreak произведение компонент вектора 
$[|\mathbf{h}(\mathbf{x}_{e_{b}}^j)\hm- 
\mathbf{h}(\mathbf{x}_{r_{a}}^i)|,\mathbf{h}(\mathbf{x}_{e_{b}}^j) \odot 
\mathbf{h}(\mathbf{x}_{r_{a}}^i)]$. В~качестве классификатора выступает модель 
случайного леса.


%\subsection{Анализ предложенного метода}

%Приведем анализ сложности предложенного метода.

%\begin{theorem}
%Пусть заданы:
%\begin{enumerate}
%\item Количество документов в~коллекции $M$.
%\item Сложность поиска одного шингла по коллекции $T_\text{search}(M)$.
%\item Сложность перевода текста длиной $n$ символов $T_\text{tr}(n)$.
%\item Среднее число документов, находящихся в~коллекции по одному шинглу 
%$\alpha$.
%\item Сложность векторизации текста $T_\text{vectorize}(n)$, являющаяся 
%монотонно-возрастающей по $n$ функцией.
%\item Сложность поиска $v$ ближайших соседей по коллекции из $m$ векторов 
%$T_\text{KNN}(m, v)$, являющаяся монотонно возрастаюшей по $m$ функцией.
%\item Максимальный размер текста в~коллекции $n_\text{col}^\text{max}$ в~словах.
%\end{enumerate}


%Тогда средняя сложность проверки документа длиной $n_\text{susp}$ слов 
%оценивается как:
%\begin{equation}
%\label{eq:complexity}
%    T_\text{tr}(n_\text{susp}) + O(n_\text{susp})T_\text{search}(M) + O(\alpha 
%n_\text{susp} \log (\alpha n_\text{susp})) +
%\end{equation}
%\[
%+ T_\text{vectorize}(n_\text{susp}) + 
%KT_\text{vectorize}(n_\text{col}^\text{max}) + 
%n_\text{susp}T_\text{KNN}(O(Kn_\text{susp}), v) + O(vn_\text{susp}).
%\]
%\end{theorem}
%\begin{proof}

%Сложность перевода документа равняется $T_\text{tr}(n_\text{susp})$ 
%в~%соответствии с~заданными обозначениями.
%Поиск документов кандидатов производится по шинглам. Количество шинглов в~
%документе зависит линейно от количества слов, поэтому оценивается как 
%$O(n_\text{susp})$.
%Поиск документов по каждому шинглу осуществляется за $T_\text{search}(M)$, 
%поэтому суммарный поиск всех документов занимает 
%$O(n_\text{susp})T_\text{search}(M)$.
%Количество унникальных документов, найденных по всем шинглам документа можно 
%оценить сверху как $O(n_\text{susp})\alpha$.

%Для дальнейшей обработки документа требуется построить список из $K$ документов-
%кандидатов, сложность построения такого списка равна $O(\alpha n_\text{susp} 
%\log (\alpha n_\text{susp}))$.

%Сложность векторизации проверяемого текста и~$K$ текстов документов-кандидатов 
%можно оценить сверху как $T_\text{vectorize}(n_\text{susp}) + 
%KT_\text{vectorize}(n_\text{col}^\text{max})$ в~силу монотонности функции 
%$T_\text{vectorize}(n)$.
%Полагая, что число предложений в~тексте линейно зависит от длины текста, оценим 
%сложность поиска ближайших соседей как $n_\text{susp} 
%T_\text{KNN}(O(Kn_\text{col}^\text{max}), v)$.

%На итоговом этапе проверки документа для каждой из найденных пар векторов-
%соседей производится классификации. Всего таких пар $O(vn_\text{susp})$,
%Таким образом, средняя сложность проверки документа соответствует 
%формуле~\eqref{eq:complexity}.
%\end{proof}

%Как видно из приведенной теоремы, сложность итогового алгоритма зависит от 
%сложности операций перевода, поиска шинглов, векторизации и~поиска ближайших 
%соседей.
%В качестве следствия теоремы приведем оценку сложности для простого случая.
%\begin{theorem}
%Пусть в~качестве системы машинного перевода используется статистичесекий 
%машинный перевод. Пусть также поиск по шинглам производится с~использованием 
%бинарного дерева поиска~\cite{cormen}, векторизация имеет линейную сложность, а 
%для поиска ближайших соседей используется алгоритм $k-d$-tree~\cite{kd}.
%Тогда сложность:
%\begin{equation}
%\label{eq:complexity}
%    O(n_\text{susp} \log M) + O(\alpha n_\text{susp} \log (\alpha 
%n_\text{susp})) + K O(n_\text{col}^\text{max}) + O(n_\text{susp}) 
%O(Kn_\text{col}^\text{max}) \log (Kn_\text{col}^\text{max})) + 
%O(vn_\text{susp})ю
%\end{equation}
%\end{theorem}
%\begin{proof}
%Сложность статистического перевода линейна по количесту слов, поэтому 
%$T_\text{tr}(n_\text{susp})  = O(n_\text{susp})$.
%Полагая коллекцию фиксированной, сложность поиска в~бинарном дереве можно 
%оценить как $O(\log M)$, тогда сложность поиска шинглов по коллекции равна 
%$O(n_\text{susp}\log M)$.
%Сложность векторизации проверяемого документа и~документов-кандидатов составляет 
%$O(n_\text{susp}) + K O(n_\text{col}^\text{max})$.
%Сложность построения индекса и~поиска составляет в~среднем $O(n_\text{susp}) 
%O(Kn_\text{col}^\text{max}) \log (Kn_\text{col}^\text{max}))$.
%Таким образом, средняя сложность проверки документа соответствует 
%формуле~\eqref{eq:complexity2}.
%\end{proof}

%Как видно из следствия, сложность итогового алгоритма субквадратична по 
%количеству слов в~проверяемом документе и~сублинейная по размеру коллекции 
%текстов. TODO. Заметим, что алгоритм $kd$-tree был приведен в~следствии в~
%качестве примера: для векторных пространств высокой размерности его 
%использование нецелесообразно, вместо него применяются методы со схожими 
%сложностными показателями.

\vspace*{-6pt}

\section{Вычислительный эксперимент}

Для анализа качества предложенного алгоритма был проведен ряд вычислительных 
экспериментов как на синтетической выборке~\cite{dataset}, так и~на реальных 
коллекциях документов.
В~данном разделе приводятся детали порождения синтетических выборок 
и~эксперименты, проведенные на них.

\vspace*{-6pt}

\subsection{Синтетическая коллекция переводных заимствований}

Для порождения переводных заимствований были использованы документы из 
английской и~русской версии сайта Wikipedia.
В качестве коллекции документов~$D_e$ были использованы 100~тыс.\ статей из 
английской версии Wikipedia.
В~качестве коллекции проверяемых документов~$D_r$ использовалась случайная 
подвыборка документов из русской версии Wikipedia. Для порождения заимствований 
для каждого документа $d_r^i \hm\in D_r$ применялся следующий алгоритм.
\begin{enumerate}
\item Выбрать документы-кан\-ди\-да\-ты~$\{d_e^j\}$ из коллекции~$D_e$. Для уменьшения 
разброса лексики\linebreak в~до\-ку\-мен\-тах-кан\-ди\-да\-тах и~проверяемом до\-ку\-менте выбор 
до\-ку\-мен\-тов-кан\-ди\-да\-тов проводился  из подвыборки~500~наиболее релевантных 
документов для проверяемого документа $d_r^i$. Для определения релевантности 
использовалась tf\;$\cdot$\;idf-ме\-ра. Чис\-ло до\-ку\-мен\-тов-кан\-ди\-да\-тов 
выбиралось случайно от~1 до~10.
\item Выбрать предложения из до\-ку\-мен\-тов-кан\-ди\-да\-тов~$\{d_e^j\}$ случайным образом и~перевести их на русский язык.
\item Заменить случайные предложения из проверяемого документа~$d_r^i$ на 
переведенные предложения из до\-ку\-мен\-тов-кан\-ди\-да\-тов. Доля замененных 
предложений из про\-ве\-ря\-емо\-го документа $d_r^i$ выбиралась случайно от~20\% до~80\%.
\end{enumerate}

\vspace*{-6pt}

\subsection{Оптимизация параметров рассматриваемых моделей}

В качестве модели векторного представления слов использовалась библиотека 
\texttt{fastText}~\cite{ft}, оптимизация параметров которой проводилась на 
английской версии Wikipedia. Размерность векторного пространства для векторного 
представления слов и~фрагментов была установлена как~100. Для оптимизации модели 
векторного представления текстовых фрагментов использовался алгоритм AdaDelta 
с~параметрами $\varepsilon\hm=10^{-6}$, $\mu\hm=0{,}95$ и~L2-ре\-гу\-ля\-ри\-за\-ция 
$\lambda_2\hm=10^{-6}$. Для итоговой  функции потерь~\eqref{eq:alpha} были установлены следующие 
значения гиперпараметров: $\delta\hm=0{,}3$; $\alpha\hm=0{,}1$. Пороги 
классификатора~\eqref{eq:t1t2} были подобраны на основе процедуры 
кросс-ва\-ли\-да\-ции: $t_1\hm=0{,}6$; $t_2\hm=0{,}5$. Для построения кластеров была использована 
агломеративная кластеризация на векторах слов. В~качестве меры близости слов 
рассматривалась косинусная  функция расстояния~\eqref{eq:cos} между 
соответствующими векторными представлениями. Итоговая модель содержала 30~тыс.\ 
кластеров для 777 тыс. слов. В качестве моделей кодирования~$\mathbf{h}$ 
и~декодирования~${\mathbf{g}}$ использовалась рекуррентная модель GRU
(gated recurrent unit)~\cite{gru}.
В~качестве системы машинного перевода использовался Moses~\cite{moses}, модель 
которого была была обучена на 18,5~млн параллельных предложений из корпусов 
Opus~\cite{opus}.
В~качестве выборки для минимизации ошибки реконструкции $E_{\mathrm{rec}}$~\eqref{eq:rec} 
использовались 10~млн предложений из английской версии Wikipedia.
Второе слагаемое функции потерь~\eqref{eq:me} использует информацию о похожих 
предложениях $\mathbf{X}_{\mathrm{me}}\hm = \{(\mathbf{x}_i, \mathbf{x}_j)\}$. 
В~качестве выборки таких предложений использовались пары параллельных предложений 
из корпуса OpenSubtitles~\cite{opus}. 

\vspace*{-6pt}

\subsection{Детали вычислительного эксперимента}

Было проведено три эксперимента на синтетических данных.
\begin{enumerate}
\item Поиск кандидатов. В~данном эксперименте анализировалось  качество 
полученной модели кластеров слов. В~качестве базового эксперимента для сравнения 
рассматривался алгоритм, основанный на шинглах без приведения слов к~меткам 
кластеров.
\item Сравнение фрагментов текста. В~данном эксперименте рассматривался случай, 
когда отбор кандидатов был проведен полностью корректно: Recall$@10\hm=1{,}0.$ 
В~качестве базового алгоритма также выступал алгоритм, основанный на шинглах: 
проверяемый документ~$d_r^i$ переводился на английский язык. После этого 
полученный текст проходил лемматизацию и~разбивался на множество перекрывающихся 
4-грамм.
Для учета возможных перестановок слов при переводе слова внутри каждой 4-грам\-мы сортировались. 
Результатом сравнения двух документов выступало множество 
совпавших отсортированных 4-грамм.

\item Эксперимент, оценивающий качество всего алгоритма (поиск кандидатов 
и~сравнение фрагментов текста). Данный эксперимент позволял оценить качество 
представленного алгоритма в~целом.
\end{enumerate}

Результаты эксперимента по поиску кандидатов представлены в~табл.~1.
Представленный алгоритм, основанный на построении кластеров, дает лучшее 
качество, чем базовый алгоритм, основанный на шинглах.



Результаты экспериментов по сравнению фрагментов текста представлены 
в~табл.~2. Пред\-став\-лен\-ный алгоритм показывает точ\-ность, 
сравнимую с~точностью базового алгоритма, и~полноту, значительно превосходящую 
полноту базового алгоритма. Точ\-ность базового алгоритма объясняется тем, что 
данный алгоритм учитывает схожесть только поч\-ти-дуб\-ли\-ка\-тов текста.




В третьем эксперименте, учитывавшем качество представленного алгоритма в~целом, 
были получены следующие показатели: $\mbox{Precision}\hm=0{,}83$; 
$\mbox{Recall}\hm=0{,}79$; $\mbox{F1}\hm=0{,}80$.

%\begin{table*}\small %tabl1
\begin{center}
\noindent
\parbox{138pt}{{{\tablename~1}\ \ \small{
Результаты эксперимента по поиску кандидатов
}}}

\vspace*{6pt}

%\label{table:stage2}
%\vspace*{2ex}

{\small \begin{tabular}{|l|c|}
\hline 
\multicolumn{1}{|c|}{\bf Алгоритм} & \bf  Recall@$10$  \\ 
\hline
Базовый & 0,93\\
%\hline
Представленный&  0,95 \\
\hline
\end{tabular}
}
\end{center}
%\end{table*}
%\begin{table*}\small %tabl2
\begin{center}
\noindent
\parbox{202pt}{{{\tablename~2}\ \ \small{
Результаты экспериментов по поиску схожих фрагментов текста
}}
}


\vspace*{6pt}
%\label{table:stage34}
%\vspace*{2ex}

{\small 
\tabcolsep=7pt
\begin{tabular}{|l|c|c|c|}
\hline 
\multicolumn{1}{|c|}{\bf Алгоритм} &  \bf Precision & \bf Recall  &  $\mathbf{F1}$ \\\hline
Базовый &  0,99 & 0,15 & 0,26 \\
%\hline
Представленный &  0,93 & 0,80 & 0,85 \\
\hline
\end{tabular}
}
\end{center}
%\end{table*}

\section{Результаты экспериментов на~реальной коллекции научных документов}

Для апробации представленного алгоритма был проведен эксперимент по поиску 
переводных заимствований на коллекции документов из электронной биб\-лио\-те\-ки 
{\sf eLibrary.ru}. Данная биб\-лио\-те\-ка содержит научные документы, входящие в~РИНЦ. 
Данный ресурс также содержит дополнительные\linebreak 
метаданные для каждого документа: заголовок,\linebreak авторов документа, язык документа 
и~принадлежность к~тематике, соответствующей Государственному рубрикатору 
на\-уч\-но-тех\-ни\-че\-ской информации (ГРНТИ).
Для апробации алгоритма в~качестве проверяемых документов~$D_r$ были 
подготовлены 2,5~млн документов на русском языке.

В качестве коллекции документов~$D_e$ использовались документы из английской 
версии Wikipedia, документы на английском языке из {\sf eLibrary.ru} и~\mbox{статьи} ресурса 
arXiv.org. Суммарное число полученных документов составило 7,6~млн.

В силу большого числа проверяемых документов~$D_r$ для дальнейшего анализа 
рассматривались документы, содержащие значительное число найден\-ных 
заимствований.
Была получена 21~тыс.\ документов со значительным числом заимствований. Из них 
были проанализированы 7,6~тыс.\ документов, выбранных случайно. Основной \mbox{целью} 
эксперимента было обнаружение переводных заимствований, когда заимствование 
произошло из англоязычного документа в~русскоязычный документ. В~то же время при 
анализе полученных результатов был выявлен ряд других срабатываний 
представленного алгоритма, которые были в~дальнейшем разделены на несколько 
типов:
\begin{itemize}
\item переводные заимствования~--- документ содержит заимствования, 
переведенные с~английского языка, выданные за оригинальный текст;
\item другие заимствования~--- заимствования из русскоязычных ресурсов или 
заимствования, направление которых нельзя определить по датам документов;
\item двуязычные статьи~--- работы одного и~того же автора на двух языках;
\item самоцитирование~--- цитирование автором его англоязычной работы;
\item цитирование законов~--- использование формулировок нормативных актов;
\item ошибочные срабатывания~--- лож\-но-по\-ло\-жи\-тель\-ные срабатывания 
представленного алгоритма;
\end{itemize}

%\setcounter{table}{2}
%\begin{table*}\small %tabl3
\begin{center}
\noindent
\parbox{202pt}{{{\tablename~3}\ \ \small{
езультаты экспериментов для коллекции документов {\sf eLibrary.ru}
}}
}


\vspace*{6pt}

{\small 
\tabcolsep=10pt
\begin{tabular}{|l|c|}
  \hline
 \multicolumn{1}{|c|}{\bf Тип} & \bf Количество  \\
  \hline
Переводные заимствования & 921 \\ 
%\hline
Другие заимствования & 2548\hphantom{9} \\ 
%\hline
Двуязычные статьи & 788 \\ 
%\hline
Самоцитирование & 669 \\ 
%\hline
Цитирование законов & 1567\hphantom{9} \\ 
%\hline
Ошибочные срабатывания & 507 \\ 
%\hline
Другое & 698 \\ 
\hline 
%\hline
Всего & 7689\hphantom{9} \\ 
\hline
\end{tabular}
}
\end{center}
%\end{table*}

%\vspace*{6pt}


\begin{itemize}
\item другое --- срабатывания, которые сложно отнести к~ка\-кой-ли\-бо категории из-за 
нехватки метаданных или плохого качества текстов.
\end{itemize}

Результаты экспериментов представлены в~табл.~3. Заметим, что были 
проанализированы только~36\% всех срабатываний алгоритма, поэтому можно 
предварительно оценить число документов с~переводными заимствованиями по всей 
коллекции в~2,5~тыс., что составляет~0,1\% всех документов. 
Заметим, что результаты были получены в~автоматическом режиме и~требуют 
дальнейшей экспертной верификации.




Распределение доли заимствований в~проанализированных документах 
представлено на рис.~1. Средняя доля заимствований со\-став\-ля\-ет~20\%.

Для анализа научных тематик, в~которых переводные заимствования происходят 
наиболее час\-то, были проанализированы документы, отнесенные к~типу 
\textit{переводные заимствования}.
Около~70\% проанализированных документов были классифицированы по~10~научным 
рубрикам. Наибольшая часть
 документов оказалась распределена между руб\-ри\-ка\-ми 
<<Экономика. Народное хозяйство. Экономические науки>> и~<<Право. Юридические 
науки>>. За-\linebreak\vspace*{-12pt}

{ \begin{center}  %fig1
 \vspace*{9pt}
    \mbox{%
\epsfxsize=79mm
\epsfbox{che-2.eps}
}

\end{center}

\noindent
{{\figurename~1}\ \ \small{
Гистограмма распределения доли заимствования в~тексте
}}}

\pagebreak

\end{multicols}

\setcounter{figure}{1}
\begin{figure*} 
 \vspace*{1pt}
\begin{center}  %fig2
   \mbox{%
\epsfxsize=142.211mm
\epsfbox{che-1.eps}
}

\end{center}
\vspace*{-9pt}
\Caption{Распределение заимствований по рубрикам ГРНТИ для типов 
\textit{переводные заимствования}~(\textit{а}) и~\textit{двуязычные статьи}~(\textit{б})
}
\vspace*{-3pt}

\end{figure*}

\begin{multicols}{2}

\noindent
метим, что распределение по рубрикам заимствований, отнесенных к~типу 
\textit{двуязычные статьи},
 значительно отличается от данного распределения.
Диаграммы десяти наиболее представительных руб\-рик для данных типов срабатываний 
показаны на рис.~2.




\paragraph*{Анализ ложно-от\-ри\-ца\-тель\-ных срабатываний.}
Для анализа лож\-но-от\-ри\-ца\-тель\-ных срабатываний представленного алгоритма была 
проанализирована полнота нахождения двуязычных документов. Оценка полноты была 
проведена с~помощью метаданных, полученных из {\sf eLibrary.ru}. Анализ срабатываний 
алгоритма показал, что только~85$\%$ документов были найдены алгоритмом 
корректно.  Заметим, что представленная оценка полноты является грубой, так как 
учитывает только полные переводы текстов.





Основная причина лож\-но-от\-ри\-ца\-тель\-ных срабатываний~--- низкое качество машинного 
перевода. Другой проб\-ле\-мой, значительно повлиявшей на качество нахождения 
двуязычных статей, является используемый алгоритм поиска кандидатов, позволяющий 
находить только близкие по структуре заимствования. Кроме того, значительная 
часть проанализированных документов имела некорректную кодировку, что также 
повлияло на полноту поиска документов.

\vspace*{-12pt}

\paragraph*{Анализ лож\-но-по\-ло\-жи\-тель\-ных срабатываний.}
Для анализа лож\-но-по\-ло\-жи\-тель\-ных срабатываний были проанализированы вручную 90 
документов, отнесенных к~типу \textit{ошибочные срабатывания}.
\mbox{Основная} проблема лож\-но-по\-ло\-жи\-тель\-ных срабатываний со\-сто\-яла в~некорректном 
векторном представлении предложений, содержащих именованные сущ\-ности, не 
встре\-ча\-емые в~обуча\-ющей \mbox{выборке}, а~также содержащих слова, незнакомые модели 
машинного перевода. Также было замечено, что алгоритм сравнения документов час\-то 
находил общие фразы вида <<Работа посвящена сле\-ду\-ющей проб\-ле\-ме$\ldots$>> и~т.\,п. 
Несмотря на корректность данных срабатываний, общие фразы представленного вида 
встречаются в~большом числе документов и~потому не долж\-ны рассматриваться как 
переводные заимствования.
Общий процент документов с~ложно-положительными срабатываниями составил 7\%.

\section{Заключение}

В работе предложен алгоритм обнаружения переводных заимствований. Предложена 
декомпозиция алгоритма обнаружения переводных заимствований, позволяющая 
проводить эффективный поиск заимствований на больших текстовых коллекциях. 
Проведен анализ предложенного метода обнаружения заимствований, а также 
составной функции ошибки, используемой для оптимизации модели глубокого 
обучения.  Для анализа качества представленного алгоритма были проведены 
эксперименты на синтетических данных для пары языков рус\-ский--анг\-лий\-ский. 
Качество алгоритма было также продемонстрировано на коллекции русскоязычных 
документов, входящих в~РИНЦ.
В~дальнейшем планируется развитие предложенного алгоритма: использование модели 
векторного представления предложений для задачи поиска кандидатов и~улучшение 
качества отображения, ставящего в~соответствие фразе вектор.\\

\bigskip

Авторы выражают свою благодарность Г.\,О.~Еременко, ООО <<Научная электронная 
библиотека>>, за предоставленные материалы.

{\small\frenchspacing
{%\baselineskip=10.8pt
%\addcontentsline{toc}{section}{References}
\begin{thebibliography}{99}

\bibitem{plag_cheh}
\Au{Никитов А.\,В., Орчаков~О.\,А., Чехович~Ю.\,В.} Плагиат в~работах 
студентов и~аспирантов: проблема и~методы противодействия~// Университетское 
управление: практика и~анализ, 2012. Т.~5. С.~61--68.

\bibitem{hist} %2
\Au{Khritankov A., Botov~P., Surovenko~N., Tsarkov~S., Viuchnov~D., 
Chekhovich~Y.} Discovering text reuse in large collections of documents: A~study 
of theses in history sciences~//  Artificial Intelligence and Natural Language 
\& Information Extraction, Social Media and Web Search FRUCT Conference.~--- 
IEEE, 2015. P.~26--32.


\bibitem{shingles1}
\Au{Зеленков И.\,В., Сегалович~И.\,В.} Сравнительный анализ методов 
определения нечетких дубликатов для Web-до\-ку\-мен\-тов~// Электронные библиотеки: 
перспективные методы и~технологии, электронные коллекции: Тр. 9-й 
Всеросс. научн. конф. RCDL.~--- Пе\-ре\-славль-За\-лес\-ский: 
Университет г.~Переславля, 2007. С.~166--174.



\bibitem{clkga} %4
\Au{Franco-Salvador~M., Gupta~P., Rosso~P.} Cross-language plagiarism 
detection using a multilingual semantic network~//  European Conference on 
Information Retrieval~/
Eds. P.~Serdyukov, P.~Braslavski, S.\,O.~Kuznetsov, \textit{et al}.~---
Lecture notes in computer science ser.~--- Berlin--Heidelberg: Springer,  2013. Vol.~7814. P.~710--713.

\bibitem{clfreshonto}
\Au{Franco-Salvador M., Gupta~P., Rosso~P.,  Banchs~R.} Cross-language 
plagiarism detection over continuous-space-and knowledge graph-based 
representations of language~// Knowl.-Based Syst., 2016. Vol.~111. P.~87--99.


\bibitem{framework1} %6
\Au{Grman J.,  Ravas~R.} Improved implementation for finding text 
similarities in large collections of data~// Notebook papers of CLEF 2011 Labs and Workshops~/ 
Eds. V.~Petras, P.~Forner, P.\,D.~Clough.~--- Amsterdam, The Netherlands, 
2011. Vol.~1177. 6~p. {\sf http://ceur-ws.org/Vol-1177/CLEF2011wn-PAN-GrmanEt2011.pdf}.

\bibitem{framework2} %7
\Au{Grozea C.,  Popescu~M.} The encoplot similarity measure for automatic 
detection of plagiarism~// Notebook papers of CLEF 2011 Labs and Workshops~/ Eds. V.~Petras, P.~Forner, P.\,D.~Clough.~--- Amsterdam, The Netherlands, 
2011. Vol.~1177. {\sf http://ceur-ws.org/Vol-1177/CLEF2011wn-PAN-GrozeaEt2011.pdf}. 

\bibitem{mono} %8
\Au{Muhr M., Kern~R., Zechner~M., Granitzer~M.} External and intrinsic 
plagiarism detection using a cross-lingual retrieval and segmentation system~// 
Notebook papers of CLEF 2010 Labs and Workshops~/
Eds. M.~Braschler, D.~Harman, E.~Pianta.~---
Padua, Italy, 2010. Vol.~1176.
{\sf http://ceur-ws.org/Vol-1176/CLEF2010wn-PAN-MuhrEt2010.pdf}.

\bibitem{fruct}
\Au{Bakhteev O., Kuznetsova~R., Romanov~A., Khritankov~A.} A~monolingual 
approach to detection of text reuse in Russian--English collection~// Artificial 
Intelligence and Natural Language \& Information Extraction, Social Media and 
Web Search FRUCT Conference.~--- IEEE, 2015. P.~3--10.

\bibitem{moses} %10
\Au{Koehn P., Hoang Hien, Birch~A., \textit{et al.}} Moses: Open source toolkit for statistical machine 
translation~//  45th Annual Meeting of the Association for 
Computational Linguistics Companion Volume Proceedings of the Demo and Poster 
Sessions Proceedings.~--- ACL, 2007. P.~177--180.


\bibitem{Socher1}
\Au{Tai K., Socher~R., Manning~C.}  Improved semantic representations from 
tree-structured long short-term memory networks~// 53rd 
Annual Meeting of the Association for Computational Linguistics and the 7th 
 Joint Conference (International) on Natural Language Processing Proceedings.~--- ACL, 2015. 
Vol.~1. P.~1556--1566.


\bibitem{wieting}
\Au{Wieting J.,  Bansal~M., Gimpel~K.,  Livescu~K.} Towards universal 
paraphrastic sentence embeddings~// arXiv.org, 2015. arXiv:1511.08198 [cs.CL].



\bibitem{Iyyer}
\Au{Iyyer M., Manjunatha~V.,  Boyd-Graber~J. Daume~H.} Deep unordered 
composition rivals syntactic methods for text classification~// 
53rd Annual Meeting of the Association for Computational Linguistics and the 
7th  Joint Conference (International) on Natural Language Processing Proceedings.~--- ACL, 
2015. Vol.~1. P.~1681--1691.

\bibitem{vbta} %14
\Au{Kuznetsova~R., Bakhteev~O., Ogaltsov~A.} Variational learning across 
domains with triplet information~// 3rd Workshop on Bayesian Deep Learning.~--- Montreal, Canada. 
{\sf http://bayesiandeeplearning.org/2018/papers/65.pdf}.


\bibitem{ann}
\Au{Wang J.,  Shen~H.,  Song~J., Ji~J.} Hashing for similarity search: 
A~survey~// arXiv.org, 2014. 29~p. \mbox{arXiv}:1408.2927 [cs.DS].


\bibitem{ae}
\Au{Alain G., Bengio~Y.} What regularized auto-encoders learn from the data-generating 
distribution~// J.~Mach. Learn. Res., 2014. 
Vol.~15. No.\,1. P.~3563--3593.


\bibitem{sphere_code}
\Au{Jenssen M., Joos~F., Perkins~W.} On kissing numbers and spherical codes 
in high dimensions~// Adv.  Math., 2018. Vol.~335. P.~307--321.


\bibitem{cybenko}
\Au{Cybenko G.} Approximation by superpositions of a~sigmoidal function~// 
Math. Control Signal., 1989. Vol.~2. No.\,4. P.~303--314.

\bibitem{dataset}
Синтетическая выборка для задачи обнаружения переводных заимствований. 
{\sf https://tiny.cc/cl\_ru\_en}.


\bibitem{ft}
\Au{Bojanowski P., Grave~E., Joulin~A., Mikolov~T.} Enriching word vectors 
with subword information~// Transactions Association for Computational 
Linguistics, 2017.  Vol.~5. P.~135--146.


\bibitem{gru}
\Au{Chung J.,  Gulcehre~C.,  Cho~K.,  Bengio~Y.} Empirical evaluation of 
gated recurrent neural networks on sequence modeling~// arXiv.org, 2014. 9~p.
arXiv:1412.3555 [cs.NE].

\bibitem{opus} %22
\Au{Tiedemann~J.} News from OPUS~--- a~collection of multilingual parallel 
corpora with tools and interfaces~// Advances in natural language 
processing.~--- Amsterdam/Philadelphia: John Benjamins, 2009. Vol.~5. P.~237--248.
\end{thebibliography}

}
}

\end{multicols}

\vspace*{-3pt}

\hfill{\small\textit{Поступила в~редакцию 19.03.2020}}

\vspace*{8pt}

%\pagebreak

%\newpage

%\vspace*{-28pt}

\hrule

\vspace*{2pt}

\hrule

%\vspace*{-2pt}

\def\tit{METHODS OF CROSS-LINGUAL TEXT REUSE DETECTION IN~LARGE TEXTUAL COLLECTIONS}

\def\titkol{Methods of cross-lingual text reuse detection in~large textual collections}

\def\aut{R.\,V.~Kuznetsova$^1$, O.\,Yu.~Bakhteev$^{1,2}$, and~Yu.\,V.~Chekhovich$^3$}

\def\autkol{R.\,V.~Kuznetsova, O.\,Yu.~Bakhteev, and~Yu.\,V.~Chekhovich}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-11pt}


\noindent
$^1$Moscow Institute of Physics and Technology, 9~Institutskiy Per., 
Dolgoprudny, Moscow Region 141700, Russian\linebreak
$\hphantom{^1}$Federation

\noindent
$^2$Antiplagiat Co., 42-1 Bolshoy Blvd., Moscow 121205, Russian Federation

\noindent
$^3$A.\,A.~Dorodnicyn Computing Center, Federal Research Center ``Computer Science and Control''
 of the Russian\linebreak
 $\hphantom{^1}$Academy of Sciences, 40~Vavilov Str., Moscow 119333, Russian Federation
 
 
\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2021\ \ \ volume~15\ \ \ issue\ 1}
}%
\def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2021\ \ \ volume~15\ \ \ issue\ 1
\hfill \textbf{\thepage}}}

\vspace*{3pt}




\Abste{The paper investigates the cross-lingual text reuse detection problem. 
The paper proposes a~monolingual approach to this problem: to translate 
the suspicious document into the language of the collection for the further monolingual analysis. 
One of the major requirements for the proposed method is robustness to the machine translation ambiguity. 
The further document analysis is divided into two steps. 
At the first step, the authors retrieve documents-candidates which are likely to be the source 
of the text reuse. For the robustness, the authors propose to retrieve the documents 
using word clusters that are constructed using distributional semantics. 
At the second\linebreak\vspace*{-12pt}}

\Abstend{step, the authors compare the suspicious document with candidates 
using sentence embeddings that are obtained by deep learning neural networks.
 The experiment was conducted for the ``English--Russian'' language pair both on the synthetic
  data and on the articles included in the Russian Science Citation Index.}
  
  \KWE{natural language processing; machine translation; deep learning; cross-lingual text 
  reuse detection; distributional semantics}




\DOI{10.14357/19922264210105}

%\vspace*{-15pt}

\Ack
\noindent
This research was supported by RFBR (project 18-07-01441) and Foundation for Assitance to Small Innovative
Enterprises in Science and Technology (project 44116).

\vspace*{12pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{1-ce}
\Aue{Nikitov, A.\,V., O.\,A.~Orchakov, and Y.\,V.~Chekhovich.}
 2012. Plagiat v~rabotakh studentov i~aspirantov: problema i~metody protivodeystviya 
 [Plagiarism in papers of students and graduate students: The problem and methods of counteraction].
 \textit{Universitetskoe upravlenie: praktika i~analiz}
  [University Management: Practice and Analysis] 5:61--68.
\bibitem{2-ce}
\Aue{Khritankov, A.\,S., P.\,V.~Botov, N.\,S.~Surovenko, S.\,V.~Tsarkov, D.\,V.~Viuchnov, and 
Y.\,V.~Chekhovich.}
 2015. Discovering text reuse in large collections of documents: 
 A~study of theses in history sciences. 
 \textit{Artificial Intelligence and Natural Language and Information Extraction, 
 Social Media and Web Search FRUCT Conference Proceedings}. IEEE. 26--32.
\bibitem{3-ce}
\Aue{Zelenkov, I.\,V., and I.\,V.~Segalovich.} 
2007. Sravnitel'nyy analiz metodov opredeleniya nechetkikh dublikatov dlya 
Web-dokumentov [Comparative analysis of methods for determining fuzzy duplicates for Web-documents]. 
\textit{Tr. 9-y Vseross. nauchn. konf. ``Elektronnye biblioteki: perspektivnye metody i~tekhnologii, 
elektronnye kollektsii''} [9th All-Russian Scientific Conference ``Digital libraries: 
Advanced Methods and Technologies, Electronic Collections'' Proceedings]. Pereslavl-Zalessky:
Pereslavl-Zalessky University. 166--174.
\bibitem{4-ce}
\Aue{Franco-Salvador, M., P.~Gupta, and P.~Rosso.}
 2013. Cross-language plagiarism detection using a~multilingual semantic network. 
 \textit{European Conference on Information Retrieval}. 
 Eds. P.~Serdyukov, P.~Braslavski, S.\,O.~Kuznetsov, \textit{et al}. Lecture notes in computer science ser. 
 Berlin--Heidelberg: Springer. 7814:710--713.
\bibitem{5-ce}
\Aue{Franco-Salvador, M., P.~Gupta., P.~Rosso, and R.\,E.~Banchs.}
 2016. Cross-language plagiarism detection over continuous-space-and knowledge graph-based 
 representations of language. \textit{Knowl.-Based Syst.} 111:87--99.
\bibitem{6-ce}
\Aue{Grman, J., and R.~Ravas.}
 2011. Improved implementation for finding text similarities in large collections of data.
 \textit{Notebook papers of CLEF 2011 Labs and Workshops}.  Eds. V.~Petras, P.~Forner, and P.\,D.~Clough. 1177. 6~p.
 {\sf http://ceur-ws.org/Vol-1177/CLEF2011wn-PAN-GrmanEt2011.pdf} (accessed January~18, 2021).
\bibitem{7-ce}
\Aue{Grozea, C., and M.~Popescu.} 2011. 
The encoplot similarity measure for automatic detection of plagiarism. 
\textit{Notebook papers of CLEF 2011 Labs and Workshops}.  Eds. V.~Petras, P.~Forner, and P.\,D.~Clough.
Amsterdam, The Netherlands. 1177. Available at: 
{\sf http://ceur-ws.org/\linebreak  Vol-1177/CLEF2011wn-PAN-GrozeaEt2011.pdf} (accessed January~18, 2021).
\bibitem{8-ce}
\Aue{Muhr, M., R.~Kern, M.~Zechner, and M.~Granitzer.}
 2010. External and intrinsic plagiarism detection using 
 a~cross-lingual retrieval and segmentation system. 
 \textit{Notebook paper of CLEF 2010 Labs and Workshops}.
Eds. M.~Braschler, D.~Harman, and  E.~Pianta. Padua, Italy. 1176. 
 Available at: {\sf http://ceur-ws.org/Vol-1176/CLEF2010wn-PAN-MuhrEt2010.pdf} (accessed January~18, 2021).
\bibitem{9-ce}
\Aue{Bakhteev, O., R.~Kuznetsova, A.~Romanov, and A.~Khritankov.}
 2015. A~monolingual approach to detection of text reuse in Russian--English collection. 
 \textit{Artificial Intelligence and Natural Language and Information Extraction, 
 Social Media and Web Search FRUCT Conference Proceedings}. IEEE. 3--10.
\bibitem{10-ce}
\Aue{Koehn, P.,  Hien Hoang, A.~Birch, \textit{et al.}} 2007. Moses: 
Open source toolkit for statistical machine translation. 
\textit{45th Annual Meeting of the Association for Computational Linguistics 
Companion Volume Proceedings of the Demo and Poster Sessions Proceedings}. ACL. 177--180.
\bibitem{11-ce}
\Aue{Tai, K.\,S., R.~Socher, and C.\,D.~Manning.}
 2015. Improved semantic representations from tree-structured long short-term memory networks. 
 \textit{53rd Annual Meeting of the Association for Computational Linguistics and the 
 7th  Joint Conference (International) on Natural Language Processing Proceedings}. ACL. 1:1556--1566.
\bibitem{12-ce}
\Aue{Wieting, J., M.~Bansal, K.~Gimpel, and K.~Livescu.} 2015.
 Towards universal paraphrastic sentence embeddings. 19~p. 
 Available at: {\sf https://arxiv.org/abs/1511.08198} (accessed January~18, 2021).
\bibitem{13-ce}
\Aue{Iyyer, M., V.~Manjunatha, J.~Boyd-Graber, and H.~Daum$\acute{\mbox{e}}$.} 
2015. Deep unordered composition rivals syntactic methods for text classification. 
\textit{53rd Annual Meeting of the Association for Computational Linguistics and the
 7th  Joint Conference (International) on Natural Language Processing Proceedings}. ACL. 1:1681--1691.
\bibitem{14-ce}
\Aue{Kuznetsova, R., O.~Bakhteev, and A.~Ogaltsov.}
 2018. Variational learning across domains with triplet information. 
 \textit{3rd Workshop on Bayesian Deep Learning Proceedings}. 
 Available at: {\sf http://bayesiandeeplearning.org/2018/\linebreak papers/65.pdf} (accessed January~18, 2021).
\bibitem{15-ce}
\Aue{Wang, J., H.\,T.~Shen, J.~Song, and J.~Ji.}
 2014. Hashing for similarity search: A~survey. 29~p. Available at: 
 {\sf https:// arxiv.org/abs/1408.2927} (accessed January~18, 2021).
\bibitem{16-ce}
\Aue{Alain, G., and Y.~Bengio.} 
2014. What regularized auto-encoders learn from the data-generating distribution. 
\textit{J.~Mach. Learn. Res.} 15(1):3563--3593.
\bibitem{17-ce}
\Aue{Jenssen, M., F.~Joos, and W.~Perkins.} 2018. 
On kissing numbers and spherical codes in high dimensions. \textit{Adv. Math.} 335:307--321.
\bibitem{18-ce}
\Aue{Cybenko, G.}
 1989. Approximation by superpositions of a~sigmoidal function. 
 \textit{Math. Control Signal.} 2(4):303--314.
\bibitem{19-ce}
Sinteticheskaya vyborka dlya zadachi obnaruzheniya perevodnykh zaimstvovaniy
 [Synthetic dataset for the cross-lingual text reuse detection problem]. 
 Available at: {\sf https://tiny.cc/cl\_ru\_en} (accessed January~18, 2021).
\bibitem{20-ce}
\Aue{Bojanowski, P., E.~Grave, A.~Joulin, and T.~Mikolov.}
 2017. Enriching word vectors with subword information. 
 \textit{Transactions Association for Computational Linguistics} 5:135--146.
\bibitem{21-ce}
\Aue{Chung, J., C.~Gulcehre, K.~Cho, and Y.~Bengio.}
 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. 9~p. 
 Available at: {\sf https://\linebreak arxiv.org/abs/1412.3555} (accessed January~18, 2021).
\bibitem{22-ce}
\Aue{Tiedemann, J.}
 2009. News from OPUS~-- a~collection of multilingual parallel corpora with tools and interfaces. 
 \textit{Advances in natural language 
processing}. Amsterdam/Philadelphia: John Benjamins. 5:237--248.
 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-3pt}

  \hfill{\small\textit{Received March~19, 2020}}


%\pagebreak

%\vspace*{-8pt}     

\Contr

\noindent
\textbf{Kuznetsova Rita V.} (b.\ 1990)~--- 
PhD student, Moscow Institute of Physics and Technology, 
9~Institutskiy Per., Dolgoprudny, Moscow Region 141701, Russian Federation; 
\mbox{rita.kuznetsova@phystech.edu}

\vspace*{3pt}

\noindent
\textbf{Bakhteev Oleg Yu.} (b.\ 1993)~--- assistant professor, Moscow Institute of Physics and Technology, 
9~Institutskiy Per., Dolgoprudny, Moscow Region 141701, Russian Federation; Head of Research Department, 
Antiplagiat Co., 42-1~Bolshoy Blvd., Moscow 121205, Russian Federation;
\mbox{bakhteev@ap-team.ru}

\vspace*{3pt}

\noindent
\textbf{Chekhovich Yury V.} (b.\ 1976)~--- 
Candidate of Science (PhD) in physics and mathematics, 
Head of Department, A.\,A.~Dorodnicyn Computing Center, 
Federal Research Center ``Computer Science and Control'' of the Russian Academy of Sciences, 
40~Vavilov Str., Moscow 119333, Russian Federation; \mbox{chehovich@ap-team.ru}

\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература}

\def\stat{andreev}

\def\tit{ПОДХОД К АВТОМАТИЗИРОВАННОМУ КОНТРОЛЮ РАБОТЫ СИСТЕМЫ 
ИЗВЛЕЧЕНИЯ ДАННЫХ С~ВЕБ-САЙТОВ$^*$}

\def\titkol{Подход к автоматизированному контролю работы системы 
извлечения данных с~веб-сайтов}

\def\autkol{А.\,М.~Андреев, Д.\,В.~Березкин, И.\,А.~Козлов, 
К.\,В.~Симаков}

\def\aut{А.\,М.~Андреев$^1$, Д.\,В.~Березкин$^2$, И.\,А.~Козлов$^3$, 
К.\,В.~Симаков$^4$}

\titel{\tit}{\aut}{\autkol}{\titkol}

{\renewcommand{\thefootnote}{\fnsymbol{footnote}}\footnotetext[1] {Статья рекомендована 
к публикации в журнале Программным комитетом конференции <<Электронные 
библиотеки: перспективные методы и технологии, электронные коллекции>> (RCDL-2012).}}

\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Московский государственный технический университет им.\ Н.\,Э.~Баумана, arkandreev@gmail.com}
\footnotetext[2]{Московский государственный технический университет им.\ Н.\,Э.~Баумана, dmitryb2007@yandex.ru}
\footnotetext[3]{Московский государственный технический университет им.\ Н.\,Э.~Баумана, kozlovilya89@gmail.com}
\footnotetext[4]{Московский государственный технический университет им.\ Н.\,Э.~Баумана, skv@ixlab.ru}


\Abst{Системы извлечения данных с веб-сайтов используют информацию о разметке 
HTML-стра\-ниц. Для обеспечения бесперебойной работы таких систем необходимо 
решить проблему своевременного обнаружения изменений структуры веб-сай\-тов. 
В~статье предложен подход к решению этой проблемы, предполагающий наличие двух 
этапов детектирования изменений верстки: оперативного и отложенного. В~основе 
первого из них лежит кластеризация, при этом HTML-до\-ку\-мент рассматривается как 
вектор некоторых характеристик. Второй этап основан на сравнении распределений этих 
характеристик для эталонного и тестового наборов документов. Проведена 
экспериментальная оценка предложенного подхода, демонстрирующая его практическую 
применимость.}

\KW{сбор текстовой информации; парсинг веб-сайтов; кластеризация; статистический 
анализ HTML-верстки} 

\vskip 14pt plus 9pt minus 6pt

      \thispagestyle{headings}

      \begin{multicols}{2}

            \label{st\stat}
  

\section{Введение}
  
    При разработке промышленных систем интеллектуальной обработки текстов 
класса Text Mining приходится сталкиваться с задачами сбора текстовой 
информации из открытых ин\-тер\-нет-ис\-точ\-ни\-ков, ее унификации и 
накопления. Методы автоматической обработки текстов (кластеризация, 
полнотекстовый поиск, выявление скрытых зависимостей) могут эффективно 
использоваться лишь при наличии актуальной, регулярно пополняющейся базы 
документов.


  
    В данной статье рассматривается решение задачи качественного сбора 
информации с новостных веб-сай\-тов. Эта информация включает в себя текст 
новости, а также сопутствующие метаданные: название, дату публикации, 
автора новости и~др. Под качественным сбором в первую очередь 
подразумевается очистка текста новости от окружающей его служебной 
информации: меню сайта, рекламных баннеров, блоков социальных сетей, 
комментариев пользователей и~т.\,д. 
  
    Основное внимание в данной работе уделено проблеме своевременного 
обнаружения изменения структуры опрашиваемых веб-сай\-тов. Пред\-ла\-га\-емый 
подход может быть использован как для обработки новостных сайтов, так и для 
сбора сообщений из электронных библиотек, блогов, форумов и социальных сетей.

\section{Постановка задачи}

\vspace*{-3pt}
  
\subsection{Функционирование системы сбора}

\vspace*{-3pt}
  
    Существует множество подходов к организации сбора открытых текстовых 
материалов с веб-сай\-тов. Как правило, система сбора использует информацию 
об HTML-раз\-мет\-ке целевых страниц для поиска в них нужной 
информации~[1]. Эта информация используется правилами распознавания, 
записываемыми на принятом в системе формальном языке. 

Распространение 
получили как ручной способ описания правил, когда правила распознавания 
формирует программист~[2], так и автоматизированный способ, когда правила 
формируются автоматически на основе обучающей выборки, подготовленной 
оператором~[3--5]. Имея набор правил, сис\-те\-ма сбора выполняет 
периодический опрос веб-сай\-тов в поисках новых материалов. 
  
    В данной работе рассматривается система, выполняющая сбор информации 
на основе правил, заданных вручную программистом. Основные 
функциональные элементы системы сбора представлены на рис.~1.
\begin{figure*} %fig1
\vspace*{9pt}
 \begin{center}
 \mbox{%
 \epsfxsize=164.84mm
 \epsfbox{and-1.eps}
 }
 \end{center}
 \vspace*{-6pt}
  \Caption{Функционирование системы сбора}
  \end{figure*}
  
    В рамках системы осуществляется периодический опрос открытых 
  ин\-тер\-нет-ис\-точ\-ни\-ков и получение новых текстовых материалов. 
Система сбора выполняет чтение RSS (Rich Site Summary)  (или HTML) ленты сайта, откуда 
извлекаются метаданные о каждом документе: название, аннотация, время 
публикации и URL текста. Далее по полученному URL (Uniform Research Locator)
осуществляется чтение 
страницы с текстом документа, выполняется построение объектной мо\-де\-ли (DOM~---
Document Object Model) этой 
страницы, откуда и выполняется извлечение чистого текста на основе 
имеющихся XPath-пра\-вил. Результат сбора представляет собой чистый текст 
документа и XML-файл с метаданными. Далее эта информация заносится в базу 
данных, где осуществляется ее накопление и аналитическая обработка. Кроме 
этого, система выполняет постоянную регистрацию и накопление 
статистической информации о состоянии и структуре опрашиваемого 
  веб-сайта.

\subsection{Задача обнаружения сбоев}
  
    Все методы сбора информации с веб-сай\-тов, использующих особенности 
разметки страниц, объединяет то, что при изменении верстки сайта воз\-никает 
необходимость перенастраивать правила распознавания. При выполнении 
круглосуточного опроса целевых сайтов своевременность обнаружения 
изменения верстки является весьма актуальной задачей, поскольку система 
сбора фактически перестает работать до тех пор, пока оператор не 
откорректирует набор правил распознавания.
  
    В простейшем случае при существенном изменении структуры сайта 
система сбора станет выдавать в качестве результата пустые текстовые 
документы. Однако существуют достаточно сложные ситуации, когда при 
изменении верстки система сбора начинает извлекать тексты не полностью 
либо фрагменты из других участков сайта, например комментарии 
пользователей. Именно выявлению таких нетривиальных ситуаций посвящена 
данная статья.

\subsection{Существующие подходы к~решению задачи}
  
    В работах, посвященных теме выявления сбоев систем извлечения 
данных~[6--8], представлено несколько подходов к решению вышеуказанной 
задачи. Большинство из них основано на оценке статистических характеристик 
документов, извлекаемых системой. При этом оценке может подвергаться как 
отдельно взятый документ~\cite{6-and} (в этом случае вычисляется вероятность 
его корректности, которая затем сравнивается с задаваемым пользователем 
пороговым значением), так и их набор~\cite{8-and} (оценке подвергается 
схожесть законов распределения случайных величин, соответствующих 
характеристикам документов из обучающей и тестовой выборок. Для сравнения 
используется критерий согласия Пирсона~\cite{9-and}).

\begin{figure*}[b] %fig2
\vspace*{9pt}
 \begin{center}
 \mbox{%
 \epsfxsize=112.962mm
 \epsfbox{and-2.eps}
 }
 \end{center}
 \vspace*{-6pt}
\Caption{Предложенный подход}
\end{figure*}
  
  В~\cite{8-and} также представлен подход, основанный на использовании 
методов machine learning для обучения системы обнаружения сбоев на наборах 
корректных документов для последующего определения правильности ее 
работы на новых данных. В~качест\-ве таких методов используется, в частности, 
одноклассовая классификация (выявление аномалий)~\cite{10-and}.
  
  Кроме того, анализ и выявление изменений в процессе сбора информации 
осуществляется на основе статистических данных и логов, накопленных 
системой сбора, а потому рассматриваемая задача может быть отнесена к 
направлению Process Mining~\cite{11-and}. Эта дисциплина, находящаяся на 
стыке Data Mining и моделирования процессов, предлагает ряд подходов к 
анализу процессов на основе знаний, извлеченных из логов 
  событий~\cite{12-and}.

\section{Принцип обнаружения сбоев}
  
    Для распознавания сбоев, связанных с изменением верстки, в систему сбора 
встраивается подсистема, осуществляющая контроль корректности 
поступающих документов и выявляющая сбои в их верстке. Возможны два 
следующих подхода к обнаружению сбоев.
  \begin{enumerate}[1.]
\item \textbf{Анализ одной загруженной веб-стра\-ни\-цы.} Суть данного подхода 
заключается в использовании классификатора, который определяет 
принадлежность веб-стра\-ни\-цы к классу корректных или некорректных 
страниц. В~своей работе классификатор использует набор выделяемых из 
веб-стра\-ни\-цы признаков. Обучение классификатора осуществляется на 
предопределенных наборах документов обоих классов. Преимуществом 
такого подхода является высокая скорость реакции детектора на сбой: 
<<плохой>> документ будет выявлен непосредственно после его 
поступления. Однако этот метод имеет и серьезный недостаток. Документы, 
подвергающиеся анализу, могут сильно отличаться друг от друга. Так, 
иногда на вход детектора поступают <<хорошие>>, но нетипичные для 
данного источника веб-стра\-ни\-цы. Если подобных документов не было в 
обучающей выборке классификатора, они не могут быть корректно 
распознаны, и в результате происходит ложное срабатывание. При 
накапливании корректных документов и увеличении обучающей выборки 
частота возникновения таких ошибок постепенно уменьшается, но они 
продолжают периодически возникать. 
\item \textbf{Анализ контрольной серии из нескольких последних загруженных 
веб-стра\-ниц.} Данный подход позволяет избавиться от ложных 
срабатываний. Даже если в контрольную серию попало несколько 
подозрительных документов, то усредненные характеристики этой 
коллекции останутся близкими к характеристикам эталонной обучающей 
выборки. Если же сомнительные документы будут поступать от источника 
регулярно, то через некоторое время, когда в контрольной серии их будет 
накоплено достаточное количество, они будут составлять значительную 
долю анализируемого набора. В~результате характеристики контрольной 
серии изменятся и можно будет обнаружить сбой. Такой подход к фиксации 
сбоев более надежен. Причем качество проверки будет возрастать с 
увеличением числа документов в контрольной серии. Но это приведет к 
возникновению значительной задержки между моментом, в который 
произошел сбой, и временем его обнаружения. 
\end{enumerate}

    Предложенный в данной работе метод сочетает преимущества двух 
вышеописанных подходов (рис.~2): быструю реакцию на сбой и высокое 
качество проверки. 



  Данный метод положен в основу подсистемы контроля корректности 
загружаемых документов. Подсистема представляет собой двухступенчатый 
детектор сбоев. Один из его компонентов~--- <<оперативный детектор>>~--- 
проверяет документы непосредственно в момент их поступления и делает 
предварительный вывод о вероятности сбоя. Если вероятность высока, 
выполняется проверка <<отложенным детектором>>, уточняющая этот 
результат.

\section{Предложенные модели документов}
  
    В основе системы обнаружения сбоев лежит модель анализируемых данных. 
Два основных компонента системы работают с разными входными данными и 
анализируют различные характеристики, поэтому для каждого из них 
предложена своя модель: модель документа, подвергающаяся обработке 
<<оперативным детектором>>, и модель набора документов, анализируемая 
<<отложенным детектором>>.

\subsection{Модель документа}
  
    Под моделью документа понимается совокупность его характеристик, 
учитываемых <<оперативным детектором>> при его обработке.   

При создании 
детектора для системы сбора выбор па\-ра\-мет\-ров производился с учетом 
некоторых особенностей функционирования системы. Текст на целевых 
  веб-стра\-ни\-цах обычно разбит на параграфы (HTML-эле\-мент $\langle 
p\rangle$). Также внутри текстовых параграфов могут встречаться стилевые 
элементы разметки. С~учетом этих факторов для \mbox{оценки} корректности 
документов были выбраны следующие характеристики:
  \begin{itemize}
\item объем веб-страницы, содержащей статью ($P$);
\item суммарный размер параграфов документа ($S$). Учитывается только 
текст, без HTML-эле\-мен\-тов;
\item число параграфов в статье ($N$);
\item дисперсия размера параграфа в рамках документа ($V$);
\item количество HTML-эле\-мен\-тов различных типов, включенных в текст 
документа. Для сокращения типов HTML-элементов они были 
сгруппированы по нескольким категориям. Были выделены классы наиболее 
часто встречающихся элементов: <<Гиперссылки ($H$)>> (в этот класс 
попал элемент href), <<Текстовые блоки ($B$)>> (br, div, span), 
<<Форматирование текста ($S$)>> ($i$, $b$, $u$, em, strong), 
<<Изображения ($I$)>> (img). Остальные теги попали в класс <<Прочее 
($O$)>>. Для каждой категории был введен параметр (соответственно 
$T_H$, $T_B$, $T_S$, $T_I$ и $T_O$), значение которого равно числу 
элементов соответствующего класса, включенных в текст документа.
\end{itemize}

    Таким образом, каждый документ характеризуется рядом параметров (в 
данном случае~--- де\-вятью), поэтому с точки зрения детектора документ 
представлен девятимерным случайным вектором, элементами которого 
являются значения пе\-ре\-чис\-лен\-ных характеристик:
  \begin{equation}
  X=(P,S,N,V,T_H,T_B,T_S,T_I,T_O)\,.
  \label{e1-and}
  \end{equation}

\subsection{Модель набора документов}
  
  Для описания модели набора из нескольких документов заметим следующее. 
Группы характеристик $(P, S, N, V)$ и $(T_H, T_B, T_S, T_I, T_O)$ имеют 
разную природу. Характеристики первой группы описывают свойства текста 
документа, тогда как характеристики второй группы отражают свойства его 
разметки. Для описания свойств набора из нескольких документов будем 
рассматривать эти группы характеристик отдельно.
  
  Случайные величины группы $(P, S, N, V)$ имеют разнородные области 
значений. Так, величина $N$ обычно принимает значения в диапазоне от~1 
до~100, величина~$V$ непрерывна, а значения дискретной величины~$P$ 
могут достигать 10$^5$. В~связи с этим для последующего анализа удобно все 
величины при\-вес\-ти к дискретному виду, а области их значений отоб\-ра\-зить на 
множество фиксированной мощности. Для этого необходимо разбить область 
значений каж\-дой величины группы $(P, S, N, V)$ на фиксированное количество 
интервалов равной длины. Число таких интервалов~$m$ выбирается в 
зависимости от объема выборки. Одним из наиболее распространенных 
способов определения оптимального числа интервалов является формула 
Стерджесса $m\hm=1\hm+\log_2n$, где $n$~--- количество документов в 
наборе~\cite{13-and}.
  
  Для снижения вычислительной сложности алгоритмов, использующих 
предлагаемую модель, в контексте набора из нескольких документов будем 
рассматривать величины $(P, S, N, V)$ независимо друг от друга. Поэтому с 
точки зрения величин $(P, S, N, V)$ модель для набора документов будет 
представлять собой следующие четыре статистических ряда:
  \begin{equation}
  \left.
  \begin{array}{rl}
  P^n & =(P_1, \ldots , P_m)\,;\quad S^n=(S_1, \ldots , S_m)\,;\\[9pt]
  N^n & =(N_1, \ldots , N_m)\,; \quad V^n=(V_1, \ldots , V_m)\,,
  \end{array}
  \right\}
  \label{e2-and}
  \end{equation}
  где $P_i$, $S_i$, $N_i$ и $V_i$~--- частота попадания в $i$-й интервал 
значения величин~$P$, $S$, $N$ и~$V$ соответственно на выборке из $n$ 
документов.
  
    Для учета в модели~(\ref{e2-and}) величин ($T_H$, $T_B$, $T_S$, $T_I$, $T_O$) 
рассмотрим другой подход к представлению информации о HTML-эле\-мен\-тах. 
В~$i$-м документе выборки встречается определенное количество тегов 
каждой из выделенных ранее пяти категорий $H$, $B$, $S$, $I$ и~$O$. 
Обозначим эти количества $T_H^i$, $T_B^i$, $T_S^i$, $T_I^i$, $T_O^i$ соответственно. Просуммируем их по всем 
документам выборки и получим следующие значения: 
  \begin{gather*}
  T_H=\sum\limits_{i=1}^n T_H^i\,;\enskip  T_B=\sum\limits_{i=1}^n T_B^i\,;\enskip 
T_S=\sum\limits_{i=1}^n T_S^i\,;\\
T_I=\sum\limits_{i=1}^n T_I^i\,; \enskip
T_O=\sum\limits_{i=1}^n T_O^i\,,
\end{gather*}
которые образуют пятиэлементный 
статистический ряд $T^n\hm=(T_H, T_B, T_S, T_I, T_O)$. Этот ряд будем 
рассматривать в качестве модели набора документов с точки зрения частоты 
встречаемости в нем тегов из пяти выделенных категорий.
  
    Таким образом, модель набора документов представляет собой совокупность 
следующих пяти статистических рядов:
  \begin{equation}
  \left.
  \begin{array}{c}
  P^n=(P_1, \ldots , P_m)\,;\enskip S^n=(S_1, \ldots , S_m)\,;\\[9pt]
  N^n=(N_1, \ldots , N_m)\,;\enskip V^n=(V_1, \ldots ,V_m)\,;\\[9pt]
  T^{n} =\left(T_{H}, T_{B}, T_{S}, 
T_{I}, T_{O}\right)\,.
  \end{array}
  \right\}
  \label{e3-and}
  \end{equation}

\section{Оперативный детектор}

\subsection{Принцип работы оперативного детектора}
  
    Быстродействующий компонент детек\-ти\-ру\-ющей системы представляет 
собой бинарный классификатор, который на основании значений параметров 
документа делает вывод о его корректности или некорректности. При выборе 
метода классификации нужно учитывать, что при обучении оперативного 
детектора в большинстве случаев количество <<хороших>> документов 
намного больше числа <<плохих>>. В~некоторых случаях в обучающей 
выборке может вообще не содержаться некорректных документов. Поэтому 
было решено проводить обучение классификатора на позитивных примерах, но 
при этом его работа была организована следующим образом: в режиме 
проверки документов детектор должен считать корректными лишь статьи, 
похожие на элементы обучающей выборки. Определим эту схожесть в 
терминах выбранной модели документа.

\begin{center}  %fig3
\vspace*{-3pt}
\mbox{%
 \epsfxsize=78.877mm
 \epsfbox{and-3.eps}
 }
 \vspace*{6pt}
{{\figurename~3}\ \ \small{Распределение значений параметров~$N$ и~$P$}}
 \end{center}

%\pagebreak

\vspace*{6pt}

\addtocounter{figure}{1}

 
    Каждый документ представлен девятимерным вектором. Для примера 
рассмотрим двумерную проекцию множества таких векторов, 
соответствующего набору новостей с сайта {\sf kp.ru}, на плоскость, задаваемую 
параметрами~$N$ (количество параграфов в статье) и~$P$ (объем 
  веб-стра\-ни\-цы, содержащей статью) (рис.~3).
  

  
    Точки не распределены в пространстве равномерно, они сгруппированы в 
некоторых областях. Новый документ, поступающий на проверку, можно 
считать корректным, если соответствующая ему точка попадает в одну из таких 
областей. Если же точка находится в отдалении от этих зон (как, например, три 
точки в правой верхней части рисунка, помеченные крестиком), то 
соответствующая статья является подозрительной.
  
    Таким образом, обучение оперативного детектора сводится к выделению 
таких областей, а классификация статей на корректные и некорректные~--- к 
определению, попадает ли документ в одну из выделенных областей.
  
     Рисунок~3 демонстрирует применение предложенного подхода для 
определения корректности объектов с двумерными векторами характеристик, 
но аналогичным образом может осуществляться классификация и в случае 
большей размерности векторов. Однако с ростом размерности для 
формирования плотных областей требуется существенно увеличивать 
обучающую выборку. Учитывая предполагаемые объемы наборов документов 
(десятки тысяч), при использовании девяти характеристик добиться высокой 
плотности при сохранении небольшого количества выделяемых зон 
невозможно.
  
    Таким образом, описанная в~(\ref{e1-and}) модель документа в виде 
  9-мер\-но\-го вектора оказывается неудобной для непосредственного 
использования оперативным детектором, поэтому в нее были внесены 
изменения. Заменим девятимерный вектор~$X$ на набор векторов меньшей 
размерности ($Y_1, Y_2, \ldots , Y_k$), каждый из которых содержит некоторое 
подмножество элементов~$X$. Будем выбирать этот набор векторов исходя из 
следующих соображений:
  \begin{enumerate}[(1)]
\item нужно по возможности использовать векторы наименьшей 
размерности (двумерные) для получения максимальной плотности 
кластеров;
\item нужно избегать использования векторов, которые могут оказаться 
бесполезными для некоторых источников.
\end{enumerate}

    Второй пункт относится прежде всего к характеристикам, отражающим 
количество HTML-эле\-мен\-тов, включенных в текст документа. Сайты 
обычно применяют для оформления текста лишь небольшой набор тегов, при 
этом некоторые группы HTML-эле\-мен\-тов могут не использоваться вовсе. 
Поэтому только некоторые из параметров ($T_H$, $T_B$, $T_S$, $T_I$, 
$T_O$) будут принимать ненулевые значения. Каждый сайт использует 
собственный подход к оформлению и выбору набора тегов, что не позволяет 
определить универсальный критерий по\-лез\-ности каждой из этих характеристик 
и их совокупностей. Поэтому было решено все перечисленные величины 
включить в пятимерный вектор~$Y_1$.
  
    Каждый из оставшихся четырех параметров является важной 
характеристикой структуры документа, поэтому в качестве элементов 
остальных векторов использовались все попарные сочетания величин~$P$, $S$, 
$N$ и~$V$. Так были получены 6~двумерных векторов $Y_2, \ldots , Y_7$.
  
    Таким образом, модель документа, под\-вер\-га\-юща\-я\-ся обработке 
<<оперативным детектором>>, представляет собой совокупность из следующих 
семи случайных векторов:
  \begin{equation}
  \left.
  \begin{array}{c}
  Y_1 = (T_H, T_B, T_S, T_I, T_O)\,; \enskip Y_2=(P,S)\,;\\[9pt]
  Y_3=(P,N)\,;\enskip Y_4=(P,V)\,;\enskip Y_5=(S,N)\,;\\[9pt]
  Y_6=(S,V)\,;\enskip Y_7=(N,V)\,.
  \end{array}
  \right\}
  \label{e4-and}
  \end{equation}

\subsection{Кластеризация документов}
  
    Выделение областей необходимо производить таким образом, чтобы 
максимально облегчить последующую проверку принадлежности точек этим 
областям. Поэтому нет смысла выбирать зоны сложной формы~--- более 
эффективным решением является нахождение плотных групп точек и 
построение простых ограничивающих поверхностей для этих групп. Для 
разбиения всего множества документов из обучающей выборки на группы 
нужно решить задачу кластеризации. Существует множество подходов к 
кластерному анализу, и применение\linebreak различных алгоритмов к одним и тем же 
входным данным может дать совершенно разные результаты~[14, 15]. 
Основным требованием, опре\-де\-ля\-ющим пригодность метода для кластеризации 
документов, является простая, гиперсферическая\linebreak
 форма кластеров, 
позволяющая получить с по\-мощью простых ограничивающих поверхностей 
плотные области без разреженных участков. Среди популярных методов 
кластеризации (k-means~[16, 17], иерархические методы~[18]) наилучшим 
образом отвечает требованиям к виду формируемых клас\-те\-ров иерархический 
метод средней связи~[19]. Однако он имеет серьезный недостаток, характерный 
для всех иерархических методов~--- высокую вычислительную сложность 
(O(n2)). Тем не менее в данной работе за основу был взят этот метод, в который 
были внесены следующие модификации.
  
    Ограничим число элементов, подвергающихся кластеризации методом 
средней связи, числом~$n$. Тогда кластеризация $N$ элементов ($N\hm>n$) 
будет осуществляться следующим образом.
  \begin{enumerate}[1.]
\item Выбрать из множества документов $n$~элементов.
\item Произвести кластеризацию этих элементов методом средней связи.
\item Найти центроиды кластеров.
\item Поместить центроиды в множество точек в качестве новых элементов.
\item Повторять п.~1--4 пока в множестве не останется необходимое 
число элементов.
\item Определить принадлежность исходных элементов найденным 
кластерам.
\end{enumerate}

    Результат кластеризации, произведенной описанным способом при 
$n\hm = 20$, приведен на рис.~4.



  В качестве ограничивающих поверхностей для областей рассматривались 
гиперпараллелепипед, гиперсфера и гиперэллипсоид.  Выбор
 был сделан\linebreak\vspace*{-12pt}
\begin{center}  %fig4
\vspace*{12pt}
\mbox{%
 \epsfxsize=78.877mm
 \epsfbox{and-4.eps}
 }
 \vspace*{6pt}
{{\figurename~4}\ \ \small{Ограничивающие поверхности кластеров}}

 \end{center}


%\pagebreak

%\vspace*{15pt}

\addtocounter{figure}{1}


\noindent
 в пользу наиболее простых в построении гиперпараллелепипедов, показавших 
хорошие результаты при оценке плотности точек. Таким образом,\linebreak каж\-дый 
кластер задается набором пар ($z_{\min}, z_{\max}$), определяющих 
граничные значения со\-от\-вет\-ст\-ву\-ющего гиперпараллелепипеда по 
па\-ра\-мет\-ру~$Z$. \mbox{Элемент} принадлежит кластеру, если для каж\-до\-го 
па\-ра\-мет\-ра~$Z$ выполняется $z_{\min}\hm\leq z\hm\leq z_{\max}$, где $z$~--- 
значение параметра~$Z$ для рассматриваемого элемента. При классификации 
документ считается подозрительным, если он не попадает ни в один из 
кластеров.
  
    Кластеризация и построение ограничивающих поверхностей и последующая 
классификация загружаемых документов производятся отдельно для каждого 
из семи выделенных векторов~(\ref{e4-and}). Таким образом, результатом 
классификации является набор из семи двоичных значений. Решение о 
корректности документа принимается на основании этого набора: статья 
считается корректной, если она успешно прошла проверку по каждому из семи 
критериев. 
  
\section{Отложенный детектор}
  
    Второй компонент системы обнаружения сбоев осуществляет оценку набора 
документов. Оценка осуществляется на основе статистических 
  рядов~(\ref{e3-and}), которые можно рассматривать как приближения к 
функциям вероятности соответствующих случайных величин. Идея, лежащая в 
основе функционирования отложенного детектора, заключается в следующем: 
рассматриваемые случайные величины, составляющие вектор~(\ref{e1-and}), 
подчиняются некоторым законам распределения, которые при отсутствии сбоя 
остаются неизменными. Изменение же верстки с высокой вероятностью 
повлияет на эти законы распределения. Следовательно, две разных выборки, 
состоящие из корректных документов, будут обладать высокой степенью 
сходства. Если же одна из них будет содержать <<плохие>> статьи, то различие 
между выборками будет значительно сильнее. Таким образом, задача детектора 
заключается в определении степени сходства проверяемой выборки и выборки, 
состоящей из гарантированно корректных статей, сформированной в процессе 
обучения (назовем ее эталонной). На основе полученного результата 
принимается решение о наличии/отсутствии сбоя. 
  
    Для примера рассмотрим три выборки случайной величины~$S$ 
(суммарный размер параграфов документа), соответствующие наборам 
новостей с сайта {\sf lenta.ru}: эталонную~(\textit{а}); тестовую выборку, состоящую из 
<<хороших>> документов~(\textit{б}) и тесто-\linebreak\vspace*{-12pt}
\begin{center}  %fig5
\vspace*{1pt}
 \mbox{%
 \epsfxsize=71.266mm
 \epsfbox{and-5.eps}
 }
 \vspace*{4pt}
{{\figurename~5}\ \ \small{Гистограммы выборок}}
 \end{center}


%\pagebreak

\vspace*{6pt}

\addtocounter{figure}{1}

\noindent
вую выборку, содержащую некорректные 
статьи~(\textit{в}). В~качестве последних использовались новости с сайта {\sf cnews.ru}. 
  
  На рис.~5 показаны гистограммы, соответствующие этим выборкам. Первые 
две из них обладают высокой степенью сходства, в то время как третья 
значительно от них отличается.

    \begin{figure*}[b]
  \begin{minipage}[t]{82mm}
    \vspace*{1pt}
  \begin{center}  %fig6
\mbox{%
 \epsfxsize=78.994mm
 \epsfbox{and-6.eps}
 }
 \end{center}
 \vspace*{-9pt}
\Caption{Зависимость максимального значения KLIC от мощности набора}
%\end{figure}
\end{minipage}
\hfill
\begin{minipage}[t]{82mm}
\vspace*{1pt}
\begin{center}
 \mbox{%
 \epsfxsize=78.994mm
 \epsfbox{and-7.eps}
 }
 \end{center}
 \vspace*{-9pt}
 \Caption{График пороговой функции}
 \end{minipage}
\end{figure*}

  Для оценивания сходства выборок используется относительная энтропия 
(расстояние Куль\-ба\-ка--Лейб\-ле\-ра, KLIC~\cite{20-and}). Для дискретных 
случайных величин с функциями вероятности~$p$ и~$q$, принимающих 
значения в одном множестве $\mathcal{M}\subset \mathbb{R}$, это расстояние 
задается формулой 
  $$
  D_{KL}(p,q) =\sum\limits_{x\subset \mathcal{M}} p(x) \ln \fr{p(x)}{q(x)}\,.
  $$
  

  
    Вместо функций вероятности используются час\-то\-ты рядов~(\ref{e3-and}). 
При этом $p(x)$ соответствует эталонной выборке, а $q(x)$~--- проверяемой. 
  
    Результатом расчета KLIC для рядов~(\ref{e3-and}) являются значения 
$D_P$, $D_S$, $D_N$, $D_V$ и $D_N$ соответственно.
  
    После расчета расстояния Куль\-ба\-ка--Лейб\-ле\-ра встает вопрос: как по 
найденному значению определить, произошел сбой или нет? Необходимо 
задать некоторое пороговое значение~$K$, такое что наличие сбоя можно 
определить как
  $$
  f(D_{\mathrm{KL}} ) = 
  \begin{cases}
  0\,, &\ D_{\mathrm{KL}}\leq K (\mbox{сбоя нет});\\
  1\,, &\ D_{\mathrm{KL}}>K (\mbox{произошел сбой}).
  \end{cases}
  $$
  
  Данный порог не является фиксированной величиной, его значение зависит 
от числа документов в тестовой выборке. Поясним это утверждение на 
примере. Выберем множество $\mathbf{A}=\{A_i\}$ наборов документов $A_i$ 
различной мощности и вычислим для каждого из них расстояние 
  Куль\-ба\-ка--Лейб\-ле\-ра ~$d_i$ 
  от эталонного закона распределения. Сопоставим натуральным числам~$j$, 
соответствующим мощностям наборов из множества $\mathbf{A}\hm=\{A_i\}$, 
числа~$K_j$, определяемые как
  $$
  K_j=\max\limits_{A_i\in \mathbf{A}}\{ D_i:\vert A_i\vert=j\}\,.
  $$
  
  Рассмотрим зависимость максимального расстояния 
  Куль\-ба\-ка--Лейб\-ле\-ра от мощности набора. На рис.~6 приведена такая 
зависимость для новостей с {\sf kp.ru}. При этом использовалась оценка 
характеристики~$P$, отражающей объем веб-стра\-ни\-цы, но аналогичная 
зависимость имеет место и для других характеристик. 
  


    Такой вид зависимости легко объясним: чем больше выборка, тем меньше на 
нее влияют локальные колебания значений параметров. Таким образом, при 
выборе порогового значения необходимо учитывать мощность анализируемого 
набора. Для этого необходимо определить пороговую функцию $K\hm = h(x)$, 
устанавливающую соответствие между количеством документов в наборе и 
пороговым значением для этого набора. 
  
    Анализ рис.~6 ведет к предположению об обратно пропорциональной 
зависимости значения~$K_j$ от~$j$ и целесообразности использования 
аппроксимирующей функции вида $h(x)\hm=a/x^b$. Однако проведение 
подобного исследования для других источников и параметров показывает, что 
такая функция не всегда дает приемлемый результат: в некоторых случаях 
зависимость имеет более сложный характер. Чтобы сделать метод определения 
пороговой функции пригодным для различных случаев и при этом учесть 
общую закономерность (постепенное уменьшение значения функции при 
воз\-рас\-та\-нии
 аргумента), было решено использовать для аппроксимации 
функцию $h(x)\hm= \sum\limits_{i=0}^k a_i/x^i$. Коэффициенты~$a_i$ 
определяются в процессе обучения (с по\-мощью метода наименьших 
квадратов (МНК)~\cite{21-and}), а значение $k\hm = 7$ было выбрано на основе 
исследования зависимостей, характерных для различных источников. Таким 
образом, пороговая функция принимает вид:
  $$
  h(x)=\sum\limits_{i=0}^7 \fr{a_i}{x^i}\,.
  $$
  




  
  Для минимизации числа ложных срабатываний было решено подвергнуть 
функцию преобразованию, которое бы обеспечило выполнение условия 
$h_j\hm\geq K_j$ для всех узлов аппроксимации. Для этого коэффициент~$a_0$ 
необходимо увеличить на величину $\Delta\hm= \max\limits_j(K_j-h_j)$. На 
рис.~7 приведены графики пороговой функции до (пунктирная линия) и после 
(сплошная линия) коррекции.
  
  \begin{figure*}[b] %fig8
%  \vspace*{9pt}
 \begin{center}
 \mbox{%
 \epsfxsize=113.506mm
 \epsfbox{and-8.eps}
 }
 \end{center}
 \vspace*{-6pt}
  \Caption{Этапы обнаружения сбоев}
  \end{figure*}
  
    С помощью приведенной пороговой функции на основании показателей 
$D_P$, $D_S$, $D_N$, $D_V$ и $D_T$ получим набор из пяти двоичных 
значений: ($F_P$, $F_S$, $F_N$, $F_V$, $F_T$). В~зависимости от количества 
единиц в этом наборе и от того, какие именно критерии приняли единичное 
значение, делается заключение о вероятности сбоя. В~разработанной системе 
используется следующий подход:
  \begin{itemize}
\item количество единиц в наборе равно~0 или~1~--- низкая вероятность (сбоя 
нет);
\item 2 или 3~--- средняя вероятность (нельзя с уверенностью судить о наличии 
или отсутствии сбоя);
\item 4 или~5~--- высокая вероятность (произошел сбой).
\end{itemize}

\section{Взаимодействие детекторов}
  
    Отдельной задачей является организация взаимодействия двух детекторов с 
целью достижения максимально эффективного функционирования системы 
отслеживания сбоев. Поскольку отложен\-ный детектор осуществляет более 
качественный анализ и менее склонен к ложным срабатываниям, он 
используется для контроля работы оперативного классификатора. Этот 
контроль подразумевает две основные функции:
  \begin{enumerate}[(1)]
\item проверку правильности результатов, полученных классификатором 
оперативного детектора; 
\item обучение классификатора. Если оперативный детектор обнаружил 
подозрительный документ, а отложенный детектор в результате проверки 
установил отсутствие сбоя, значит, произошло ложное срабатывание. Это 
свидетельствует о недостаточной обученности оперативного детектора. 
Поэтому необходимо произвести его переобучение с использованием 
документов, определенных им в категорию подозрительных.
\end{enumerate}

    В некоторых случаях ложные срабатывания могут быть обнаружены без 
участия отложенного детектора. Для этого оперативный классификатор был 
оснащен функцией самопроверки. Он способен самостоятельно отличить 
единичный выброс от массового поступления некорректных статей путем 
анализа частоты появления таких статей среди последних скачанных 
документов. Если эта частота меньше заданного порогового значения 
(например, 50\%), делается вывод о ложном срабатывании и запускается 
переобучение. В~качестве анализируемого набора при самопроверке 
используется группа документов, полученных в рамках последней транзакции, 
т.\,е.\ при последней загрузке документов с сайта.
  
    Рассмотрим итоговый метод обнаружения изменений структуры 
  веб-сай\-тов, реализованный в работе подсистемы обнаружения сбоев с 
учетом выбранного подхода к реализации взаимодействия детекторов. Этапы 
функционирования подсистемы приведены на рис.~8. 
  

  
  На этапе классификации оперативный детектор проверяет поступающие 
статьи. Документы классифицируются на корректные и подозрительные. 
Необходимые для классификации данные о кластерах и ограничивающих 
поверхностях извлекаются из базы данных.
  
    После поступления от источника группы документов оперативный детектор 
выполняет самопроверку: вычисляется частота детектирования подозрительных 
статей в пределах текущей транзакции. Если она ниже порогового значения, но 
не равна нулю, делается заключение о ложном срабатывании и выполняется 
переход к блоку переобучения. Если частота выше порогового значения~--- к 
блоку отложенной проверки.
  
        Работа блока отложенной проверки начинается с оповещения отложенного 
детектора о необходимости выполнения анализа. Выполнение проверки 
непосредственно после получения оповещения не имеет смысла, поскольку 
сбой может быть зафиксирован только после накапливания достаточного числа 
некорректных статей. После поступления необходимого числа документов 
отложенный детектор выполняет проверку этого набора. Для ее проведения из 
базы данных извлекаются статистические ряды эталонных выборок и 
коэффициенты~$a_i$ пороговой функции. Результат проверки передается блоку 
принятия решения. 
  
    Блок принятия решения определяет дальнейшие действия подсистемы в 
зависимости от результата отложенной проверки. Если она показала высокую 
вероятность сбоя, администратор системы оповещается о необходимости 
корректировки системы сбора документов. Если вероятность сбоя низка, 
делается заключение о ложном срабатывании оперативного классификатора и 
выполняется переход к блоку переобучения. Если же результат анализа не 
позволяет с высокой долей уверенности судить о наличии или отсутствии сбоя, 
выполняется повторная отложенная проверка.
  
    На этапе переобучения для оперативного детектора заново определяются 
кластеры и строятся ограничивающие поверхности с использованием нового, 
дополненного набора данных. Количество кластеров и граничные значения 
гиперпараллелепипедов заносятся в базу данных.

\vspace*{-9pt}
  
\section{Экспериментальная проверка системы}

\vspace*{-2pt}
  
    В рамках данной работы были проведены эксперименты, направленные на 
анализ качества работы разработанной системы обнаружения сбоев. 
Эксперименты проводились на ПЭВМ со следующими основными 
параметрами: процессор Intel Core~2 Duo 1,8~ГГц, объем ОЗУ 2~ГБ.
  
    Для проведения экспериментов использовалась коллекция новостей, 
извлеченных со следующих сайтов: {\sf mail.ru}, {\sf itar-tass.com}, {\sf kp.ru}, {\sf 
rbc.ru}, \mbox{\sf kommersant}.{\sf ru}, {\sf ria.ru}, {\sf rambler.ru} 
(табл.~1). Для обучения 
использовалось в общей сложности 72\,888 корректных документов. При 
обучении оперативного детектора формировалось 10~кластеров.
  
    При самопроверке оперативного детектора было использовано пороговое 
значение, равное 10\%. Накопленные за время тестирования документы 
использовались в качестве тестовой выборки для отложенного детектора.
  
    Целью первого эксперимента была оценка работы системы на корректных 
данных. В~качестве входных данных использовались гарантированно 
корректные статьи, полученные с использованием
правильных настроек 
системы сбора. Для проведения эксперимента использовалось в общей 
сложности 5169~документов. 

  В рамках эксперимента проверке были подвергнуты 5169 корректных статей. 
При первичной классификации 65 из них (1,26\%) были определены как\linebreak\vspace*{-12pt}

\vspace*{6pt}
% \begin{table*}

  \noindent{{\normalsize\tablename~1}\ \ \small{Ложные срабатывания оперативного детектора}}

{\small  \begin{center} 
  \tabcolsep=5pt
  \begin{tabular}{|l|c|c|c|c|c|}
  \hline
\multicolumn{1}{|c|}{Источник}&$M_L$&$M_T$&$M_S$&$N_D$&$N_S$\\
\hline
{\sf mail.ru}&25\,296&2631\hphantom{9}&20&14&0\\
{\sf itar-tass.com}&11\,548&560&76&\hphantom{9}0&0\\
{\sf kp.ru}&\hphantom{9}7\,220&218&24&\hphantom{9}4&1\\
{\sf rbc.ru}&\hphantom{9}3\,517&227&25&14&5\\
{\sf kommersant.ru}&\hphantom{9}5\,288&260&47&\hphantom{9}4&0\\
{\sf ria.ru}&16\,519&1115&29&12&5\\
{\sf rambler.ru}&\hphantom{9}3\,500&\hphantom{9}158&15&17&13\hphantom{9}\\
\hline
Всего&72\,888&5169&34&65&24\hphantom{9}\\
\hline
\multicolumn{6}{p{220pt}}{\footnotesize \textbf{Примечания:} $M_L$~--- размер обучающей 
выборки; $M_T$~--- размер тестовой выборки; $M_S$~--- средний размер 
анализируемого набора документов при самопроверке; $N_D$~--- количество 
подозрительных статей; $N_S$~--- количество подозрительных статей после 
самопроверки.}
\end{tabular}
\end{center}
}
%\end{table*} 

\addtocounter{table}{1}



\setcounter{table}{1}
\begin{table*}\small %tabl2
\begin{center}
\Caption{Ложные срабатывания отложенного детектора}
\vspace*{2ex}

\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Источник}&$M_L$&$M_T$&$F_P$&$F_S$&$F_N$&$F_V$&$F_T$&$
N_F$&$P_F$\\
\hline
{\sf mail.ru}&25\,296&2631&0&0&0&0&0&0 из 5&$L$\\
{\sf itar-tass.com}&11\,548&\hphantom{9}560&0&0&0&0&0&0 из 5&$L$\\
{\sf kp.ru}&\hphantom{9}7\,220&\hphantom{9}218&1&0&0&0&0&1 из 5&$L$\\
{\sf rbc.ru}&\hphantom{9}3\,517&\hphantom{9}227&0&0&0&0&0&0 из 5&$L$\\
{\sf kommersant.ru}&\hphantom{9}5\,288&\hphantom{9}260&0&0&0&0&0&0 из 5&$L$\\
{\sf ria.ru}&16\,519&1115&0&0&0&0&0&0 из 5&$L$\\
{\sf rambler.ru}&\hphantom{9}3\,500&\hphantom{9}158&0&0&0&0&0&0 из 5&$L$\\
\hline
Всего&72\,888&5169&1&0&0&0&0&\hphantom{9}1 из 35&\\
\hline
\multicolumn{10}{p{320pt}}{\footnotesize \textbf{Примечания:} $N_F$~--- количество критериев, 
показавших наличие сбоя; $P_F$~--- заключение детектора: вероятность сбоя 
($L$~--- низкая, $M$~--- средняя, $H$~--- высокая).}
\end{tabular}
\end{center}
\end{table*}

\setcounter{table}{3}  
\begin{table*}[b]\small %tabl4
\vspace*{-12pt}
\begin{center}
\Caption{Оценка пропуска сбоев отложенным детектором}
\vspace*{2ex}

\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Источник}&$M_L$&$M_T$&$F_P$&$F_S$&$F_N$&$F_V$&$F_T$&$
N_F$&$P_F$\\
\hline
{\sf mail.ru}&25\,296&356&1&1&1&0&0&3 из 5&$M$\\
{\sf itar-tass.com}&11\,548&356&1&1&1&0&0&3 из 5&$M$\\
{\sf kp.ru}&\hphantom{9}7\,220&356&1&0&1&0&1&3 из 5&$M$\\
{\sf rbc.ru}&\hphantom{9}3\,517&356&1&1&1&0&1&4 из 5&$H$\\
{\sf kommersant.ru}&\hphantom{9}5\,288&356&1&1&1&1&1&5 из 5&$H$\\
{\sf ria.ru}&16\,519&356&1&0&1&1&1&4 из 5&$H$\\
rambler.ru&\hphantom{9}3\,500&356&1&1&1&1&1&5 из 5&$H$\\
\hline
Всего&72\,888&2492\hphantom{9}&7&5&7&3&5&27 из 35&\\
\hline
\end{tabular}
\end{center}
\vspace*{-12pt}
\end{table*}



%\begin{table*}
%tabl3
{\small
  \noindent{{\normalsize\tablename~3}\ \ \small{Оценка пропуска сбоев оперативным детектором}}
  
  \begin{center}

\tabcolsep=5pt
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Источник}&$M_L$&$M_T$&$M_S$&$N_D$&$N_S$\\
\hline
{\sf mail.ru}&25\,296&356&25&356&356\\
{\sf itar-tass.com}&\hphantom{9}3\,500&356&25&356&356\\
{\sf kp.ru}&11\,548&356&25&356&356\\
{\sf rbc.ru}&\hphantom{9}7\,220&356&25&356&356\\
{\sf kommersant.ru}&16\,519&356&25&356&356\\
{\sf ria.ru}&\hphantom{9}3\,517&356&25&356&356\\
{\sf rambler.ru}&\hphantom{9}5\,288&356&25&356&356\\
\hline
Всего&72\,888&2492\hphantom{9}&25&2492\hphantom{9}&2492\hphantom{9}\\
\hline
\end{tabular}
\end{center}
}
%\end{table*} 

\addtocounter{table}{1}
  
\noindent
подозрительные. В~результате самопроверки 41 из них был переведен в 
категорию корректных. Оставшиеся 24 (0,46\% от общего числа) были 
ошибочно признаны некорректными. 
  

  Отложенный детектор показал правильный результат при проверке тестовой 
выборки каждого сайта (табл.~2). Ошибочное значение критерия было 
зафиксировано лишь в 1~случае из~35 (2,86\%). 
  
  В рамках второго эксперимента (см.\ табл.~3) оценивалась способность 
системы обнаруживать сбои. Ввиду отсутствия для многих сайтов достаточного 
числа негативных примеров тестовые наборы были созданы искусственно: в 
качестве <<плохих>> документов использовались комментарии к новостям, 
полученные с сайта {\sf championat.com}. Такой выбор тестовых данных 
обусловлен тем, что возможным последствием изменения верстки является 
извлечение из веб-стра\-ниц не новостей, а текстов с других участков сайта, в 
частности комментариев. Для проведения эксперимента использовалось 
356~документов (для всех источников использовался одинаковый тестовый 
набор).
  
  В рамках эксперимента проверке были подвергнуты 356~некорректных 
статей. При первичной классификации все они были определены как 
подозрительные для каждого из семи источников. В~результате самопроверки 
никаких изменений произведено не было. 
  
  Отложенный детектор показал правильный результат для 4~источников из~7. 
Для оставшихся 3 источников он не смог сделать вывод о наличии или 
отсутствии сбоя. В~8~случаях из~35 (22,85\%) значение критериев было 
неверным. Данным ситуациям соответствуют значения~0 соответствующего 
критерия в табл.~4.
  
  Если в ходе первого эксперимента система обнаружения сбоев 
продемонстрировала свою работоспособность при выполнении как 
оперативной, так и отложенной проверки корректных данных, то с задачей 
обнаружения сбоев она справилась значительно хуже. Возможной причиной 
низкого качества работы системы при анализе некорректных документов 
является неудачный подход к определению результата проверки. Анализ 
результатов экспериментов показывает необходимость понижения порога 
фиксации сбоя. Кроме того, при проведении второго эксперимента 
критерии~$F_P$, $F_S$, $F_N$, $F_V$ и~$F_T$ были приняты равнозначными, 
однако оказалось, что некоторые из них показывают наличие сбоя значительно 
точнее, чем другие. Так, критерии~$F_P$ и~$F_N$ приняли верное значение в 
7~случаях из~7, а $F_V$~--- лишь в~3. Чтобы учесть различную значимость 
критериев, для каждого из них может быть установлен весовой коэффициент, 
определяющий влияние значения соответствующего критерия на результат 
проверки.





\vspace*{-6pt}

\section{Заключение}

\vspace*{-2pt}

    В работе предложен подход к автоматизированному контролю работы 
системы извлечения данных с веб-сай\-тов. В~его основе лежит двухуровневая 
проверка корректности веб-стра\-ниц, обеспе\-чи\-ва\-ющая быстроту реакции и 
высокое качество оценки документов.
  
    В основе первичной классификации лежит проверка схожести документа с 
элементами обуча-\linebreak ющей выборки. Это позволяет системе адекватно реа\-ги\-ровать 
на любые нетипичные для сайта веб-стра\-ни\-цы. Простота выполнения такой 
проверки достигается с помощью предложенного метода клас\-те\-ри\-за\-ции. Он 
относится к иерархическим методам, но имеет меньшую вычислительную 
сложность по сравнению с другими алгоритмами этого класса.
  
    Отложенная проверка корректности основана на сравнении законов 
распределения. Для правильной интерпретации полученного результата 
используется пороговая функция, полученная путем аппроксимации  
МНК. Такой подход обеспечивает высокую точность проверки вне зависимости 
от размера оцениваемой выборки.
  
    Проведенные эксперименты показали эффективность совместного 
использования двух детекторов. Предложенный подход был реализован в виде 
подсистемы отслеживания сбоев в системе сбора новостной информации. 
Данная система успешно внедрена в Совете Федерации Федерального 
Собрания РФ в рамках комплекса <<Обзор СМИ>>, решающего задачу сбора, 
накопления и классификации новостей об\-ще\-ст\-вен\-но-по\-ли\-ти\-че\-ской 
тематики. 

\vspace*{-6pt}

{\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{99}

\vspace*{-2pt}
  
\bibitem{1-and}
\Au{Nikovski D., Esenther A., Baba~A.} Semi-supervised information extraction from 
variable-length web-page lists~// ICEIS 2009: 11th Conference (International) on 
Enterprise Information Systems Proceedings.~--- Milan, Italy, 2009. P.~261--266.
\bibitem{2-and}
\Au{Oro E., Ruffolo M., Staab~S.} SXPath~--- Extending XPath towards spatial 
querying on web documents~// VLDB Endowment Proceedings, 2011. Vol.~4. 
No.\,2. P.~129--140.

\bibitem{4-and} %3
\Au{Chidlovskii B., Ragetli J., de Rijke~M.} Wrapper generation by reversible 
grammar induction~// Machine learning~--- ECML 2000:  11th European Conference 
on Machine Learning Proceedings (Barcelona,  2000). 
Lecture notes in computer sci. ser. Vol.~1810.~--- Springer, 2000. P.~96--108.

\bibitem{5-and} %4
\Au{Kushmerick N.} Wrapper induction: Efficiency and expressiveness~// Artificial 
Intelligence, 2000. No.\,118. P.~15--68.

\bibitem{3-and} %5
\Au{Tobias A.} XPath-Wrapper Induction by generalizing tree traversal patterns~// 
Workshopwoche der GI-Fachgruppen/Arbeitskreise.~--- GI-Fachgruppen ABIS, 
AKKD, FGML, 2005. P.~126--133.


\bibitem{6-and}
\Au{Kushmerick N., Weld D.\,S., Doorenbos~R.\,B.} Wrapper induction for 
information extraction~//  IJCAI 97: 15th Joint Conference (International)  on 
Artificial Intelligence Proceedings.~--- Nagoya, Japan, 1997. Vol.~1. P.~729--737.
\bibitem{7-and}
\Au{Kushmerick N.} Wrapper verification~// World Wide Web~J., 2000. Vol.~3. 
No.\,2. P.~79--94.
\bibitem{8-and}
\Au{Lerman K., Minton S, Knoblock~C.} Wrapper maintenance: A~machine learning 
approach~// J.~Artificial Intelligence Research, 2003. Vol.~18. P.~149--181.
\bibitem{9-and}
\Au{Кендалл М., Стьюарт А.} Статистические выводы и связи.~--- М.: Наука, 
1973.
\bibitem{10-and}
\Au{Kriegel H.-P., Kr$\ddot{\mbox{o}}$ger~P., Zimek~A.} Outlier detection 
techniques~// PAKDD 2009: 13th Pacific-Asia Conference on Knowledge Discovery 
and Data Mining Proceedings.~--- Bangkok, Thailand, 2009.
\bibitem{11-and}
Process Mining. {\sf http://www.processmining.org}.
\bibitem{12-and}
\Au{Van der Aalst W.\,M.\,P.} Process mining: Discovery, conformance and 
enhancement of business processes.~--- Springer-Verlag, 2011.
\bibitem{13-and}
\Au{Sturges H.} The choice of a class-interval~// J.~Amer. Statistical Association, 
1926. Vol.~21. No.\,153. P.~65--66.

\bibitem{15-and} %14
\Au{Дюран Б., Оделл П.} Кластерный анализ.~--- М.: Статистика, 1977. 128~с. 

\bibitem{14-and} %15
\Au{Мандель И.\,Д.} Кластерный анализ.~--- М.: Финансы и статистика, 1988. 
176~с.

\bibitem{16-and}
\Au{Jain A., Dubs R.} Clustering methods and algorithms.~--- Prentice-Hall, 1988.
\bibitem{17-and}
\Au{Андреев А.\,М., Березкин Д.\,В., Морозов~В.\,В., Симаков~К.\,В.} Метод 
кластеризации документов текстовых коллекций и синтеза аннотаций кластеров~// 
Электронные библиотеки: перспективные методы и технологии, электронные 
коллекции (RCDL'2008): Труды 10-й Всеросс. научной конф.~--- Дубна, 2008. 
С.~220--229.
\bibitem{18-and}
\Au{Жамбю М.} Иерархический класс\-тер-ана\-лиз и соответствия.~--- М.: 
Финансы и статистика, 1988. 342~с. 
\bibitem{19-and}
\Au{Бериков В.\,Б., Лбов Г.\,С.} Современные тенденции в кластерном 
анализе.~--- Новосибирск: Институт математики им.\ С.\,Л.~Соболева, 2008. 
26~с.
\bibitem{20-and}
\Au{Kullback S., Leibler R.\,A.} On information and sufficiency~// The Annals of 
Math. Stat., 1951. Vol.~22. No.\,1. P.~79--86.

\label{end\stat}

\bibitem{21-and}
Аппроксимация методом наименьших квадратов (МНК). {\sf 
http://alglib.sources.ru/interpolation/\linebreak linearleastsquares.php}.

\end{thebibliography} } }

\end{multicols}

  
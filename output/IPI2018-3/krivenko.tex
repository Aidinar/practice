\def\stat{krivenko}

\def\tit{ОБУЧАЕМАЯ КЛАССИФИКАЦИЯ ДАННЫХ\\ С~УЧЕТОМ АНАЛИЗА ГЛАВНЫХ 
КОМПОНЕНТ}

\def\titkol{Обучаемая классификация данных с~учетом анализа главных 
компонент}

\def\aut{М.\,П.~Кривенко$^1$}

\def\autkol{М.\,П.~Кривенко}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Кривенко М.\,П.}
\index{Krivenko M.\,P.}




%{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
%{Работа выполнена при финансовой поддержке Российского научного фонда (проект 18-11-00155).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Институт проблем информатики Федерального исследовательского центра 
<<Информатика и~управление>>
Российской академии наук, \mbox{mkrivenko@ipiran.ru}}

\vspace*{8pt}





\Abst{Рассматриваются вопросы обучаемой классификации с~учетом результатов анализа 
главных компонент (PCA~--- Principal Component 
Analysis). Построение байесовского классификатора становится возможным 
после представления ковариаций через параметры вероятностной модели PCA. Выделен 
случай сингулярных распределений данных, для него оценивание параметров модели 
предлагается проводить при ограничениях на собственные значения ковариационных 
матриц. Исследуется качество классификации с~учетом реальной размерности данных. 
Продемонстрировано, что при ее правильном задании классификатор обладает наименьшими 
вероятностями ошибки. Превышение наилучшего значения размерности обычно ухудшает 
качество классификации в~меньшей степени, чем его занижение. Смесь вероятностных 
анализаторов главных компонент позволяет моделировать объемные данные с~помощью 
относительно небольшого числа свободных параметров. Число свободных параметров 
можно контролировать с~помощью выбора латентной размерности данных.}

\KW{анализ главных компонент; смеси нормальных распределений; EM-ал\-го\-ритм; 
обучаемая классификация}

\DOI{10.14357/19922264180308}
  
%\vspace*{4pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}

\section{Введение}

     Один из способов снижения размерности данных заключается 
в~применении анализа главных компонент (PCA). 
Популярность PCA определяется рядом свойств, важнейшим из 
которых является его оптимальность при сжатии множества векторов высокой 
размерности в~множество векторов более низкой размерности, а~затем их 
восстановления.
     
     Использовать PCA в~задаче обучаемой классификации данных можно 
двояко. Во-пер\-вых, без-\linebreak от\-но\-си\-тель\-но к~сложной структуре 
результатов\linebreak 
наблюдений, подразумевающей наличие классов данных. В~этом случае 
данные без уточнения их статистической модели сжимаются, а~затем 
подвергаются анализу. Более сложным оказывается второй подход, когда PCA 
проводится индивидуально для каждого класса в~отдельности. В~связи с~его 
применением возникают два вопроса:
     \begin{enumerate}[1.]
\item Как проводить классификацию данных, объединяя результаты PCA для 
каждого класса в~отдельности?
\item Может ли подобное сжатие данных стать источником повышения качества 
классификации данных?
\end{enumerate}

     Задача классификации данных становится традиционной после перехода 
к~PPCA$(k)$~--- вероятностной модели PCA (PPCA~--- probabilistic PCA) для 
сниженной размерности~$k$:
     $$
     \mathbf{y}=\mathbf{Wx}+\mathbf{a}+\boldsymbol{\varepsilon}\,,
     $$
где $\mathbf{y}$~--- $d$-мер\-ная наблюдаемая переменная, $\mathbf{y}\sim$\linebreak
$\sim 
N(\mathbf{a},\mathbf{C}(k))$; $\mathbf{W}$~---  $(d\times k)$-мат\-ри\-ца 
преобразования; $\mathbf{x}$~--- $k$-мер\-ная латентная переменная, 
$\mathbf{x}\sim$\linebreak $\sim N(\mathbf{0},\mathbf{I})$; $\boldsymbol{\varepsilon$}~--- 
$d$-мер\-ная переменная, 
$\boldsymbol{\varepsilon}\sim N(\mathbf{0},\sigma^2\mathbf{I})$; 
$\mathbf{C}(k)\hm=\mathbf{W}\mathbf{W}^{\mathrm{T}}\hm 
+\sigma^2\mathbf{I}$. Здесь $d$~--- исходная размерность данных; $k$~--- 
сниженная размерность сжатых данных; $\sigma^2$ и~$\mathbf{W}$ суть 
параметры модели (данные принимаются центрированными).

     Пусть задана $(d\times N)$-мат\-ри\-ца  
<<при\-знак--объ\-ект>>~$\mathbf{Y}$ и~найдена выборочная ковариационная 
мат\-ри\-ца~$\mathbf{S}$. Справедливо спектральное разложение вида 
$\mathbf{S}\hm= \mathbf{UVU}^{\mathrm{T}}$, где $\mathbf{V}$~--- 
диагональная матрица, ее элементы $v_1,\ldots , v_d$ суть собственные 
значения матрицы~$\mathbf{S}$, а~$\mathbf{U}$ является ортогональной 
матрицей, столбцы которой~--- ортонормированные собственные векторы  
мат\-ри\-цы~$\mathbf{S}$. Тогда согласно~[1] могут быть найдены оценки 
параметров модели:
     $$
     \hat{\sigma}^2=\fr{1}{d-k}\sum\limits^d_{i=k+1} v_i\,;\enskip 
\hat{\mathbf{W}}= \mathbf{U}_k\left( \mathbf{V}_k-
\hat{\sigma}^2\mathbf{I}\right)^{1/2}\,,
     $$
где столбцы $(d\times k)$-мат\-ри\-цы~$\mathbf{U}_k$ суть оси 
первых~$k$~главных компонент; $\mathbf{V}_k$~--- диагональная  
$(k\times k)$-мат\-ри\-ца соответствующих дисперсий. После этого можно 
рассматривать случайную нормально распределенную величину 
$\mathbf{y}\sim N(\mathbf{a},\hat{\mathbf{C}}(k))$,\linebreak где
$$
\hat{\mathbf{C}}(k) 
=\hat{\mathbf{W}}\hat{\mathbf{W}}^{\mathrm{T}}+\hat{\sigma}^2\mathbf{I}\,,
$$
и становится возможным построение байесовского классификатора. 
Заметим, что
для различных значений~$k$ ковариационная матрица 
$\hat{\mathbf{C}}(k)\not=\mathbf{S}$, кроме случаев $k\hm= d\hm- 1,d$.

\section{Смесь вероятностных моделей анализа главных компонент}

     Анализ главных компонент 
     определяет только линейную проекцию данных, по этой причине 
область его применения несколько ограничена. Это, естественно, мотивировало 
различные разработки нелинейного PCA. 

Связь вероятностной модели со 
стандартным PCA открывает заманчивую перспективу моделировать сложные 
структуры данных с~по\-мощью комбинации локальных подмоделей PCA 
и~реализации механизма смеси вероятностных анализаторов главных 
компонент. 

Этот подход позволяет определять все параметры модели путем 
максимизации правдоподобия, в~ходе которого автоматически происходит 
раз\-би\-ение данных и~определение соответствующих главных осей. Логарифм 
правдоподобия для такой модели смеси есть
     $$
     L=\sum\limits^N_{n=1} \ln \left\{ p\left( \mathbf{y}_n\right)\right\} 
=\sum\limits^N_{n=1}\ln \left\{ \sum\limits^M_{j=1} \pi_i p(\mathbf{y}_n\vert 
j)\right\}\,,
     $$
где $p(\mathbf{y}_n\vert j)$ отвечает элементарной PPCA-мо\-де\-ли;  
$\pi_j$~--- соответствующий вес элемента смеси с~$\pi_j\hm\geq 0$ 
и~$\sum\nolimits^M_{j=1} \pi_j\hm=1$. Заметим, что с~каждым $j$-м элементом 
смеси связаны свои параметры~$\mathbf{a}_j$, $\mathbf{W}_j$ и~$\sigma_j^2$.

     При этом генерирующая модель для смеси требует случайного выбора 
элемента в~соответствии с~пропорциями~$\pi_j$, после чего формирование 
наблюдений для~$\mathbf{x}$ и~$\boldsymbol{\varepsilon}$ происходит согласно модели 
PPCA$(k)$  с~соответствующими параметрами. Кроме того, для некоторой 
точки~$\mathbf{y}$ теперь имеется апостериорное распределение, связанное 
с~каждым латентным пространством.
     
     Можно разработать итеративный EM (expectation-maximization)
     алгоритм для оценивания всех 
параметров модели $\pi_j$, $\mathbf{a}_j$, $\mathbf{W}_j$ и~$\sigma_j^2$. 
Если\linebreak
\vspace*{-12pt}

\columnbreak

\noindent
$q_{nj}\hm= p(j\vert \mathbf{y}_n)$~--- вероятность 
принадлежности~$\mathbf{y}_n$ к~$j$-му элементу смеси и
     $$
     q_{nj}= \fr{\pi_j p(\mathbf{y}_n\vert j)}{p(\mathbf{y}_n)}\,,
     $$
     то согласно приложению~C~[1] обновления для параметров принимают 
обычный вид для смеси нормальных распределений. Более того, 
в~приложении~C~[1] также показано: комбинация E-\linebreak и~M-ша\-гов приводит 
к~интуитивно ясному результату, что оси~$\mathbf{W}_j$ и~дисперсии 
шума~$\sigma_j^2$ определяются из взвешенной ковариационной матрицы
     $$
     \mathbf{S}_j= \fr{1}{\tilde{\pi}_j N} \sum\limits^N_{n=1} q_{nj} \left( 
\mathbf{y}_n -\hat{\mathbf{a}}_j\right) \left( \mathbf{y}_n-
\hat{\mathbf{a}}_j\right)^{\mathrm{T}}
     $$
с помощью обычной факторизации так же, как и~для элементарного PPCA. 
Однако, как отмечено в~разд.~3.4 и~приложении~A.5~[1], для больших 
значений размерности данных~$d$ могут быть получены вычислительные 
преимущества, если оценки~$\mathbf{W}_j$ и~$\sigma^2_j$ обновляются 
итеративно в~соответствии со схемой EM-ал\-го\-ритма.

     До сих пор предполагалось, что $\vert \mathbf{S}_j\vert\not= 0$. Иная 
ситуация с~наличием сингулярных распределений может возникать при малых 
выборках, когда их объем не превосходит размерности выборочного 
пространства, или при применении EM-ал\-го\-рит\-ма оценивания параметров, 
характеризующих смесь нормальных распределений. Если это так, то на 
помощь может прийти подход и~результаты из разд.~2.2.2~[2], заключающиеся 
в~использовании уточненной (невырожденной) модели многомерного 
нормального распределения, для которой введены ограничения на множество 
возможных значений ковариационной матрицы.
     
     Пусть плотность распределения смеси нормальных распределений есть
     $$
     f(\mathbf{u}) =\sum\limits^M_{j=1} \pi_j \varphi\left( \mathbf{u}, 
\mathbf{a}_j, \mathbf{C}_j\right)\,,
     $$ 
где $\varphi(\mathbf{u},\mathbf{a}_j,\mathbf{C}_j)$~--- плотность нормального 
рас\-пределения со средним~$\mathbf{a}_j$ и~ковариационной\linebreak 
мат\-ри\-цей~$\mathbf{C}_j$. При этом все собственные 
значения~$v_i(\mathbf{C}_j)$ ковариационных матриц~$\mathbf{C}_j$ 
ограничены снизу некоторой положительной константой~$v_0$, т.\,е.\ 
$v_i(\mathbf{C}_j)\hm\geq v_0\hm>0$, $i\hm=1,\ldots ,d$. Доказано, что при этих 
условиях на $t$-м шаге итерации EM-ал\-го\-рит\-ма максимум функции 
$\sum\nolimits^M_{j=1} \sum\nolimits^N_{n=1} q_{ij}^{(t)} \ln \varphi 
(\mathbf{y}_j, \mathbf{a}_j, \mathbf{C}_j)$ достигается при значениях 
параметров, которые для каждого значения допустимого~$j$ последовательно 
находятся следующим образом (реализация М-шага):
\begin{enumerate}[(1)]
\item вычислить 
$$
\mathbf{a}_j^{(t+1)}= \fr{\sum\nolimits^N_{n=1} q_{nj}^{(t)} \mathbf{y}_n} 
{\sum\nolimits^N_{n=1} q_{nj}^{(t)}}\,;
     $$
\item вычислить 
$$
\hspace*{-4.69887pt}\tilde{\mathbf{C}}_j=
\fr{\sum\nolimits^N_{n=1}\! q_{nj}^{(t)} \left( 
\mathbf{y}_n -\mathbf{a}_j^{(t+1)}\right) \left( \mathbf{y}_n-
\mathbf{a}_j^{(t+1)}\right)^{\mathrm{T}}} {\sum\nolimits^N_{n=1} 
q_{nj}^{(t)}}\,;
     $$
\item найти матрицы $\tilde{\mathbf{U}}_j$ и~$\tilde{\mathbf{V}}_j$, которые 
задают спектральное разложение матрицы~$\tilde{\mathbf{C}}_j$;
\item определить элементы~$v_{jl}^{(t+1)}$ диагональной 
мат\-ри\-цы~$\mathbf{V}_j^{(t+1)}$ через элементы~$\tilde{v}_{jl}$ 
мат\-ри\-цы~$\tilde{\mathbf{V}}_j$ по формулам $v_{jl}^{(t+1)}\hm= \max 
\left\{ \tilde{v}_{jl}, v_0\right\}$, $l\hm=1, \ldots , d$;
\item вычислить $\mathbf{C}_j^{(t+1)} \hm= \tilde{\mathbf{U}}_j 
\mathbf{V}_j^{(t+1)} \left( \tilde{\mathbf{U}}_j\right)^{\mathrm{T}}$.
     \end{enumerate}
     
     Заметим, что в~рассматриваемом случае пространство параметров 
ограничено и~максимум функции правдоподобия может лежать на границе. 
В~силу этого полученные оценки не подпадают под обычные условия 
о~сходимости EM-ал\-го\-рит\-ма (см., например,~[3]). Требуемые результаты 
были получены в~[4].
     
     Задание ограничения снизу на собственные значение ковариационных 
матриц необходимо для предотвращения появления недопустимо больших 
(малых) значений функции правдоподобия. При этом возникает необходимость 
выбора этого ограничения~$v_0$. С~одной стороны, значение~$v_0$ должно 
быть достаточно большим, чтобы обеспечить корректное выполнение операций 
с~плавающей точкой. С~другой стороны, неразумное увеличение этого 
значения может дать снижение качества классификации данных на основе 
модели смеси (например, слишком большие значения~$v_0$ могут привести 
к~потере индивидуальности отдельных элементов смеси).
     
\section{Последствия неправильного выбора размерности}

     Исследуем влияние ошибочного представления о реальной модели 
данных PPCA, принятой при классификации. Подобная постановка задачи 
актуальна в~связи с~вопросом, может ли снижение размерности данных на 
основе анализа главных компонент привести к~повышению качества 
классификации данных.
     
     Аналогичная ситуация рассматривалась в~[1]. На примере задачи 
распознавания рукописных цифр исследовалась эффективность представления 
плотности распределения данных с~помощью модели смеси PPCA. Было 
продемонстрировано снижение ошибочной классификации за счет 
рассмотрения не просто нормального распределения, а~смеси нормальных 
распределений (увеличение числа элементов смеси с~$M\hm=1$ 
до~10) и~снижения размерности данных (с~$d\hm=64$ 
до~10). Но данное улучшение, скорее всего, является просто 
результатом использования смеси, а~выбор малых значений~$k$ лишь 
обеспечивает снижение вычислительной слож\-ности, но не ясно, как он влияет 
на качество классификации. Таким образом, фактически было лишь показано, 
что качество классификации повышается в~связи с~усложнением модели 
данных.
     
     Чтобы составить представление о поведении качества классификации 
с~учетом PPCA, рассмотрим различение двух классов~$\omega_1$ 
и~$\omega_2$ нормально распределенных данных $N(\mathbf{a}_1, 
\mathbf{C}_1(k))$ и~$N(\mathbf{a}_2, \mathbf{C}_2(k))$, формируемых 
в~соответствии с~моделью PPCA$(k)$. При этом байесовский классификатор 
будет строиться с~помощью модели PPCA$(q)$. В~этом случае он будет 
обучаться по выборке~$\mathbf{Y}_n$ объема~$n$ в~соответствии с~моделью 
PPCA$(q)$, что даст для каждого класса оценки~$\hat{\mathbf{a}}_i$ 
и~$\hat{\mathbf{C}}_i(q)$, $i\hm=1,2$. Поэтому решающая функция для 
некоторого вектора~$\mathbf{x}$ примет вид:
    \begin{multline*}
     d_i(\mathbf{x})=\ln \pi_i -{}\\
     {}-\fr{1}{2}\ln \left\vert 
\hat{\mathbf{C}}_i(q)\right\vert -\fr{1}{2}\left( \mathbf{x}-
\hat{\mathbf{a}}_i\right)^{\mathrm{T}}\hat{\mathbf{C}}^{-1}(q) \left( \mathbf{x}-
\hat{\mathbf{a}}_i\right)\,,
\end{multline*}
где $\pi_i$~--- вероятности появления классов.

     Для построенного классификатора теперь можно найти условную 
вероятность ошибки классификатора~$P_e$ или ее оценку~$\hat{P}_e$, т.\,е.\ 
реализовать следующие шаги:
     \begin{itemize}
\item генерирование выборки~$\mathbf{Y}_n$ для 
$\pi_1 N(\mathbf{a}_1,\mathbf{C}_1(k))\hm+ \pi_2 N(\mathbf{a}_2, 
\mathbf{C}_2(k))$;
\item обучение классификатора в~соответствии с~моделью PPCA$(q)$, т.\,е.\ 
нахождение на основе сгенерированной выборки~$\mathbf{Y}_n$ оценок 
параметров модели PPCA$(q)$ и~с их помощью~$\hat{\mathbf{C}}_i(q)$;
\item нахождение~$P_e$ или~$\hat{P}_e$.
\end{itemize}
     
     Данные шаги могут быть многократно повторены для различных 
выборок~$\mathbf{y}_n$, что позволит \mbox{найти} оценки безусловных 
характеристик вероятности ошибки классификации.
     
     Нахождение $\hat{P}_e$ можно реализовать с~помощью метода 
моделирования, т.\,е.\ реализовать следующие шаги:
     \begin{itemize}
\item генерирование обучающей выборки~$\mathbf{X}_m$ для 
$f(\mathbf{u})\hm= \hat{\pi}_1 N\left( \hat{\mathbf{a}}_1, 
\hat{\mathbf{C}}_1(q)\right) +\hat{\pi}_2 N\left( \hat{\mathbf{a}}_2, 
\hat{\mathbf{C}}_2(q)\right)$;
\item классификация элементов~$\mathbf{X}_m$ на основе $f(\mathbf{u})$, 
затем получение значения~$\hat{P}_e$ путем сравнения смоделированной 
и~оцененной классификаций.
\end{itemize}
     
     С помощью некоторого упрощения постановки задачи классификации 
удается добиться того, что~$P_e$ можно найти аналитически, в~част\-ности, 
обобщая прием разд.~4.4~[5].
     
     Действительно, рассмотрим различение двух классов   и~нормально 
распределенных данных с~различными векторами средних~$\mathbf{a}_1$ 
и~$\mathbf{a}_2$, но одинаковыми ковариационными матрицами. Пусть 
наблюдения формируются в~соответствии с~моделью PPCA$(k)$, а~для их 
классификации используется модель PPCA$(q)$. Обучение байесовского 
классификатора по выборке~$\mathbf{Y}_n$ в~соответствии с~моделью 
PPCA$(q)$ дает оценку~$\hat{\mathbf{C}}(q)$. После этого классификация 
некоторого вектора~$\mathbf{x}$ осуществляется с~помощью функции
     \begin{multline*}
     u_{12}(\mathbf{x}) =2\mathbf{x}^{\mathrm{T}} \hat{\mathbf{C}}^{-1}(q) 
\left( \mathbf{a}_1 - \mathbf{a}_2\right)^{\mathrm{T}}-{}\\
{}-\left( \mathbf{a}_1 
+\mathbf{a}_2\right)^{\mathrm{T}}\hat{\mathbf{C}}^{-1} (q) \left(\mathbf{a}_1-
\mathbf{a}_2\right)\,.
   \end{multline*}
     
     При единичной функции потерь и~вероятностях появления 
классов~$\pi_i$ условие, определяющее принадлежность~$\mathbf{x}$ 
к~$\omega_1$, имеет вид $u_{12}(\mathbf{x})\hm>t$, где $t\hm= \ln 
(\pi_2/\pi_1)$. Случайная величина $u_{12}(\mathbf{x})$, где\linebreak $\mathbf{x}\sim 
N(\mathbf{a}_i, \mathbf{C}(k))$ для $i\hm=1,2$, как линейная ком\-бинация 
нормально распределенных случайных ве\-личин имеет также нормальное 
распределение.\linebreak Поэтому достаточно найти первые моменты 
распределений~$u_{12}(\mathbf{x})$ для каждого из двух классов, а~именно:
     $$
     2E_i\left\{ u_{12}(\mathbf{x})\right\} =\begin{cases}
     \rho\,, & i=1\,;\\
     -\rho\,, & i=2\,,
     \end{cases}
     $$
     где 
     $$
     \rho=(\mathbf{a}_1-\mathbf{a}_2)^{\mathrm{T}}\hat{\mathbf{C}}^{-
1} (\mathbf{a}_1-\mathbf{a}_2)\,;
$$
\vspace*{-12pt}

\noindent
     \begin{multline*}
\hspace*{-9pt}E_i\!\left\{ \!\left( u_{12}(\mathbf{x})-E_i\left\{ 
u_{12}(\mathbf{x})\right\}\right)^2\right\}^{\!2}=\left( \mathbf{a}_1-
\mathbf{a}_2\right)^{\mathrm{T}}\hat{\mathbf{C}}^{-1} (q) \times{}\\
{}\times E_i \left\{ \left( 
\mathbf{x}-\mathbf{a}_i\right) \left( \mathbf{x}-\mathbf{a}_i\right)^{\mathrm{T}} 
\hat{\mathbf{C}}^{-1} (q) \left( \mathbf{a}_1-\mathbf{a}_2\right)\right\}= {}\\
     {}=
     \left( \mathbf{a}_1-\mathbf{a}_2\right)^{\mathrm{T}}\hat{\mathbf{C}}^{-1}
     (q)\cdot \mathbf{C}(k)\hat{\mathbf{C}}^{-1}(q)\cdot \left( \mathbf{a}_1-
\mathbf{a}_2\right) = v^2\\ 
\mbox{(по\ определению)}.
     \end{multline*}
          В результате имеем:
     \begin{multline*}
     \hspace*{-6.08414pt}P_e=\pi_1\mathrm{Pr}\left\{ u_{12}(\mathbf{x}) < t\vert \omega_1\right\}+
     \pi_2\mathrm{Pr}\left\{ u_{12}(\mathbf{x})>t\vert \omega_2\right\}={}\\
     {}=\pi_1 \Phi\left( \fr{t-\rho/2}{\sqrt{v^2}}\right)+\pi_2 \left( 1-\Phi\left( 
\fr{t+\rho/2}{\sqrt{v^2}}\right)\right)\,,
     \end{multline*}
где $\Phi(u)$~--- функция стандартного нормального распределения.

     При $\pi_1=\pi_2=1/2$
     
     \noindent
     $$
     P_e=\Phi\left( -\fr{\rho}{2\sqrt{v^2}}\right)\,.
     $$
     Если $q=k$, то $v^2\hm=\rho$ и~получаем ранее известную формулу:
     
     \noindent
$$
P_e=\Phi\left(-
\fr{\sqrt{\rho}}{2}\right).
$$
     
     Для того чтобы составить представление о реальной зависимости 
качества классификации от знания фактической размерности данных, 
рассматривался случай: $d\hm=50$; $k\hm=5$;  $\mathbf{a}_1\hm-
\mathbf{a}_2\hm=(0{,}1; \ldots ; 0{,}1)^{\mathrm{T}}$; $\mathbf{C}$~--- 
некоторая случайно выбранная ковариационная матрица. Параметры выборок 
были таковы: $n\hm=300$; $N_{\mathrm{exp}}\hm=100$. Результаты моделирования 
позволили получить оценки~95\%-ных доверительных интервалов для оценки 
вероятности ошибки байесовского классификатора при различных значениях~$q$. 

В~первую очередь ковариационные матрицы для классов принимались 
одинаковыми: 
$$
\mathbf{C}_1=\mathbf{C}_2= (\mathbf{u}_1,\ldots 
,\mathbf{u}_k)\mathbf{V}_k(\mathbf{u}_1,\ldots , \mathbf{u}_k)^{\mathrm{T}}\,.
$$ 
Соответствующие результаты отражены на рисунке в~виде практически 
слившихся линий с~пометкой~\textit{1} (границы доверительных интервалов 
приблизительно равны).

Затем рассматривался случай ковариационных матриц: 

\noindent
\begin{align*}
\mathbf{C}_1&= (\mathbf{u}_1, \ldots ,\mathbf{u}_k)\mathbf{V}_k 
(\mathbf{u}_1, \ldots ,\mathbf{u}_k)^{\mathrm{T}};\\
\mathbf{C}_2&= 
(\mathbf{u}_2, \ldots ,\mathbf{u}_{k+1})\mathbf{V}_k (\mathbf{u}_2, \ldots 
,\mathbf{u}_{k+1})^{\mathrm{T}},
\end{align*}

{ \begin{center}  %fig1
 \vspace*{6pt}
  \mbox{%
 \epsfxsize=77.961mm 
 \epsfbox{kri-1.eps}
 }


\end{center}

\vspace*{-3pt}

\noindent
{\small{Зависимость качества классификации~$P_e$ от размерности 
данных~$q$, принятой при анализе данных}}
}

%\vspace*{11pt}
\pagebreak

\noindent
 использующих несколько разные 
подпространства главных компонент и~совпадающие дисперсии для них. 

Соответствующие результаты представлены на рисунке уже в~виде двух линий, 
помеченных как~\textit{2}. При оценивании вероятности ошибки классификации 
бралась контрольная выборка объема $N_{\mathrm{contr}}=300$. Заметим, что только 
изменение взаимной ориентации главных компонент для классов приводит 
к~существенному повышению качества классификации.
     
     Продемонстрированный пример, а~также множество дополнительно 
проведенных экспериментов позволяют сформулировать следующие 
результаты:
     \begin{itemize}
     \item оценка $P_e$ как функция от размерности~$q$ для модели данных 
имеет минимум для $q\hm=k$, т.\,е.\ при правильном задании реальной 
раз\-мер\-ности данных классификатор обладает наилучшим качеством (на 
рисунке наилучшее значение   выделено вертикальной штриховой прямой);
\item превышение наилучшего значения~$q$ обычно ухудшает качество 
классификации в~меньшей степени, чем его занижение (на рисунке это более 
ярко проявляется для графика~\textit{2}).
\end{itemize}


\section{Заключение}

     Внимание к~модели смеси в~рамках PPCA определяется в~первую очередь 
необходимостью повышения эффективности сжатия и~восстановления\linebreak данных. 
Но эта модель позволяет также детализировать описание реальных данных 
и~тем самым\linebreak создать предпосылки для повышения качества классификации. 
Как показывают проведенные эксперименты, вероятность ошибок 
классификатора может снижаться достаточно существенно.
     
Модель смеси нормальных распределений является основой для 
популярного подхода к~комбинированной оценке плотности. 

Однако такая 
модель %\linebreak 
обладает следующим недостатком: если каж\-дая гауссовская компонента 
описывается полной ковариационной мат\-ри\-цей, то для каждой компоненты 
смеси должны оцениваться $d(d+1)/2$ отдельных ковариационных\linebreak па\-ра\-метров.
Очевидно, что по мере роста размер\-ности пространства данных, да еще при 
естест\-вен\-ном желании увеличить число элементов смеси,\linebreak количество точек 
данных, необходимых для надежного определения этих па\-ра\-мет\-ров, становится 
непомерно высоким.
%
 Альтернативный подход заключается в~уменьшении  
чис\-ла параметров путем введения ограничения на форму ковариационной 
мат\-ри\-цы (другой прием со\-сто\-ял во введении предположений  
о~па\-ра\-мет\-рах полной ковариационной матрицы~\cite{6-kri}). При этом 
обычно используются два общих ограничения: задать мат\-ри\-цу ковариаций 
изотропной или диагональной. 

Изотропная модель сильно ограничена, 
поскольку она присваивает только один параметр для описания всей структуры 
ковариации для полноразмерных данных. Диагональная модель более гибкая, 
с~$d$ па\-ра\-мет\-ра\-ми, но главные оси эллипсоидов для элементов смеси 
должны быть выровнены\linebreak
 с~осями данных, и,~таким образом, каж\-дый 
от\-дельный элемент смеси не способен описывать корреляции меж\-ду 
переменными. Поэтому смесь моделей PPCA, где ковариации каждого элемента\linebreak 
пара\-мет\-ри\-зуются с~по\-мощью соотношения $\mathbf{C}\hm= 
\sigma^2\mathbf{I}\hm+\mathbf{WW}^{\mathrm{T}}$, может содержать 
существенно\linebreak меньшее чис\-ло па\-ра\-метров.
     
     Одним из преимуществ методологии PPCA является то, что определение 
модели плотности позволяет вы\-чис\-лять апостериорные вероятности 
принадлежности некоторого наблюденного значения элементу смеси 
(некоторому классу) и~использовать их для последующей классификации, не 
находя ошибку восстановления.
     
     Возможным недостатком вероятностного подхо\-да к~объединению 
локальных моделей PCA является то, что, оптимизируя функцию 
правдоподобия, модель смеси PPCA напрямую не минимизирует квадратичную 
ошибку реконструкции. Для приложений, где это ключевой критерий, следует 
ожидать, что алгоритмы, которые явно минимизируют ошибку восстановления, 
будут эффективнее. Эксперименты действительно показали, что это, как 
правило, имеет место, но важны две оговорки, прежде чем можно будет сделать 
ка\-кие-ли\-бо твердые выводы относительно пригодности данной модели.  

Во-пер\-вых, есть задачи, где окончательная модель смеси PPCA оказывалась 
фактически лучше в~смысле ошибки реконструкции, даже на обуча\-ющем 
наборе.

 Второе соображение заключается в~том, что имеются также 
свидетельства того, что сглаживание, подразумеваемое мягкой кластеризацией, 
присущей модели смеси PPCA, помогает уменьшить переобучение, особенно 
в~случае эксперимента сжатия данных, где статистика набора \mbox{тестовых} данных 
отличается от данных обучения гораздо больше, чем для других примеров (см., 
например,~[1]).

     
     В терминах модели гауссовой смеси смесь вероятностных анализаторов 
главных компонент позволяет моделировать данные больших размеров 
с~относительно небольшим числом свободных параметров, не налагая в~целом 
неуместного ограничения на структуру ковариации. 
%
Число свободных 
параметров можно контролировать с~по\-мощью выбора скрытой 
пространственной размерности~$k$, что позволяет проводить интерполяцию по 
сложности модели от изотропной до полной ковариационной структуры.
     
{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{9}
 
 \bibitem{1-kri}
\Au{Tipping M.\,E., Bishop~C.\,M.} Mixtures of probabilistic principal component 
analyzers~// Neural Comput., 1999. Vol.~11. Iss.~2. P.~443--482.
\bibitem{2-kri}
\Au{Кривенко М.\,П.} Прикладные методы оценивания распределения 
многомерных данных малой выборки.~--- М.: ИПИ РАН, 2011. 146~с.
\bibitem{3-kri}
\Au{Wu C.\,F.\,J.} On convergence properties of the EM algorithm~// Ann. Stat., 
1983. Vol.~11. P.~95--103.

%\columnbreak

\bibitem{4-kri}
\Au{Nettleton D.} Convergence properties of the EM algorithm in constrained 
parameter spaces~// Can. J.~Stat., 1999. Vol.~27. Iss.~3. P.~639--648.



\bibitem{5-kri}
\Au{Ту Дж., Гонсалес~Р.} Принципы распознавания образов~/
Пер. с~англ.~--- М.: Мир, 1978. 
414~с. (\Au{Tou~J., Gonzalez~R.\,C.}  {Pattern 
recognition principles}.~--- Reading, MA, USA: Addison-Wesley Publ. 
Co., 1974.\linebreak 377~p.)
\bibitem{6-kri}
\Au{Ormoneit D., Tresp~V.} Improved gaussian mixture density estimates using 
Bayesian penalty terms and network averaging~// Advances in neural information 
rocessing systems~/
Eds. D.\,S.~Touretzky, M.\,C.~Mozer, M.\,E.~Hasselmo.~--- 
Cambridge, MA, USA: MIT Press, 1996. Vol.~8. P.~542--548.
 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-7pt}

\hfill{\small\textit{Поступила в~редакцию 30.05.18}}

\vspace*{4pt}

%\newpage

%\vspace*{-24pt}

\hrule

\vspace*{2pt}

\hrule

\vspace*{-2pt}


\def\tit{SUPERVISED LEARNING CLASSIFICATION OF~DATA\\ TAKING INTO~ACCOUNT 
PRINCIPAL COMPONENT ANALYSIS}

\def\titkol{Supervised learning classification of~data taking into~account 
principal component analysis}

\def\aut{M.\,P.~Krivenko}

\def\autkol{M.\,P.~Krivenko}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-13pt}


\noindent
Institute of Informatics Problems, Federal Research Center ``Computer 
Science and Control'' of the Russian Academy of Sciences, 44-2~Vavilov Str., 
Moscow 119333, Russian Federation


\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2018\ \ \ volume~12\ \ \ issue\ 3}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2018\ \ \ volume~12\ \ \ issue\ 3
\hfill \textbf{\thepage}}}

\vspace*{1pt}



\Abste{The article examines questions of supervised learning 
classification of data taking into account principal component 
analysis (PCA) results. Сonstruction of a Bayesian classifier 
becomes possible after representation of covariances through the 
parameters of the probabilistic PCA model. The case of singular 
data distributions is singled out; for this case, it is suggested to 
estimate the parameters of the model under constraints on the 
eigenvalues of covariance matrices. The quality of classification is 
studied in respect to the actual data dimension. It is demonstrated 
that, when correctly assigned, the classifier has the least error 
probabilities. Exceeding the best value of the dimension usually 
worsens the quality of the classification to a lesser extent than its 
underestimation. The mixture of probabilistic principal component 
analyzer allows modeling big data by means of a relatively small 
number of free parameters. The number of free parameters can be 
controlled by choosing the latent dimension of the data.}

\KWE{principal component analysis; mixtures of normal 
distributions; EM algorithm; supervised learning classification}

\DOI{10.14357/19922264180308}

%\vspace*{-14pt}

 %\Ack
%\noindent



\vspace*{-4pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{9}
\bibitem{1-kri-1}
\Aue{Tipping, M.\,E., and C.\,M.~Bishop.} 1999. Mixtures of 
probabilistic principal component analyzers. \textit{Neural 
Comput.} 11(2):443--482.
\bibitem{2-kri-1}
\Aue{Krivenko, M.\,P.} 2011. \textit{Prikladnye metody 
otsenivaniya raspredeleniya mnogomernykh dannykh maloy 
vyborki} [Applied methods for estimating the distribution of small 
sample multidimensional data].~--- Moscow: IPI RAN. 146~p.
\bibitem{3-kri-1}
\Aue{Wu, C.\,F.\,J.} 1983. On convergence properties of the EM 
algorithm. \textit{Ann. Stat.} 11:95--103.
\bibitem{4-kri-1}
\Aue{Nettleton, D.} 1999. Convergence properties of the EM 
algorithm in constrained parameter spaces. \textit{Can. 
J.~Stat.} 27(3):639--648.
\bibitem{5-kri-1}
\Aue{Tou, J., and R.\,C.~Gonzalez.} 1974.  \textit{Pattern 
recognition principles}. Reading, MA: Addison-Wesley Publ. 
Co. 377~p.
\bibitem{6-kri-1}
\Aue{Ormoneit, D., and V.~Tresp.} 1996. Improved gaussian 
mixture density estimates using Bayesian penalty terms and 
network averaging. Eds. D.\,S.~Touretzky, M.\,C.~Mozer, and 
M.\,E.~Hasselmo. \textit{Advances in neural information 
processing systems}.  Cambridge, MA: MIT Press.  
8:542--548. 
 %1120 p.
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-9pt}

\hfill{\small\textit{Received May 30, 2018}}

%\pagebreak

\vspace*{-18pt}

\Contrl

\noindent
\textbf{Krivenko Michail P.} (b.\ 1946)~--- Doctor of Science in 
technology, professor, leading scientist, Institute of Informatics 
Problems, Federal Research Center ``Computer Science and 
Control'' of the Russian Academy of Sciences, 44-2~Vavilov Str., 
Moscow 119333, Russian Federation; 
\mbox{mkrivenko@ipiran.ru}


\label{end\stat}


\renewcommand{\bibname}{\protect\rm Литература} 
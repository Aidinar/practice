\def\stat{vilziter}

\def\tit{ОБУЧЕНИЕ АЛГОРИТМОВ ВЫДЕЛЕНИЯ КОЖИ НА ЦВЕТНЫХ ИЗОБРАЖЕНИЯХ ЛИЦ$^*$}

\def\titkol{Обучение алгоритмов выделения кожи на цветных изображениях лиц}

\def\autkol{Ю.\,В.~Визильтер, В.\,С.~Горбацевич, С.\,Л.~Каратеев, 
Н.\,А.~Костромов}
\def\aut{Ю.\,В.~Визильтер$^1$, В.\,С.~Горбацевич$^2$, С.\,Л.~Каратеев$^3$, 
Н.\,А.~Костромов$^4$}

\titel{\tit}{\aut}{\autkol}{\titkol}

{\renewcommand{\thefootnote}{\fnsymbol{footnote}}\footnotetext[1]
{Работа выполнена при поддержке РФФИ, гранты №\,11-08-01114-а, №\,11-08-01039-а.}}

\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Государственный научно-исследовательский институт авиационных систем, viz@gosniias.ru}
\footnotetext[2]{Государственный научно-исследовательский институт авиационных систем, gvs@gosniias.ru}
\footnotetext[3]{Государственный научно-исследовательский институт авиационных систем, goga@gosniias.ru}
\footnotetext[4]{Государственный научно-исследовательский институт авиационных систем}

\vspace*{4pt}

\Abst{Рассмотрены два способа обучения алгоритмов выделения кожи на цветных 
изображениях лиц~--- на основе самоорганизующейся нейронной сети типа <<растущий 
нейронный газ>> и морфологической классификации путем построения минимальных 
разрезов графов соседства на обучающей выборке. В~качестве рабочего цветового 
пространства использовалось пространство CIE Lab. Показана эффективность обоих 
использованных методов, исследованы различия полученных результатов обучения.}

\vspace*{2pt}

\KW{биометрия; обнаружение кожи; самоорганизующиеся нейронные сети; морфологическая 
классификация}

\vspace*{9pt}

 \vskip 14pt plus 9pt minus 6pt

      \thispagestyle{headings}

      \begin{multicols}{2}

            \label{st\stat}


\section{Введение}

      Задача выделения человеческих лиц на цифровых изображениях получила широкое 
распространение в связи с бурным развитием информационных сетей и охранных систем. 

Существует много алгоритмов и методов выделения человеческих лиц на изображениях, но 
наиболее широко применяемым является известный алгоритм Вио\-лы--Джон\-са, основанный на 
использовании процедуры обучения типа AdaBoost и хааро-по\-доб\-ных признаков~[1]. 
Недостатком этого алгоритма является необходимость практически попиксельного сканирования 
изображения окнами различных размеров, что приводит к заметной потере про\-из\-во\-ди\-тель\-ности 
при обработке изображений большого размера. 

Для преодоления описанных выше трудностей 
применяются алгоритмы предобработки, позволяющие сузить область поиска и тем самым 
повысить производительность. При этом широкое распространение получили методы, основанные 
на цветовой сегментации изображений по признаку принадлежности человеческой коже.
      
      В работе рассматриваются два различных способа построения подобных 
      классификаторов~--- на основе самоорганизующейся нейронной сети типа <<растущий 
нейронный газ>> и морфологическое обучение методом минимального разрезания графа 
соседства для обучающей выборки.

\section{Морфологическое обучение методом минимального разрезания графа 
соседства для~обучающей выборки}

\vspace*{-1pt}
      
      Морфологический подход к синтезу классификаторов основан на рассмотрении задачи 
синтеза метрического классификатора как задачи оптимальной сегментации (optimal labeling) 
конечной выборки точек метрического пространства. При этом <<форма>> и <<сложность>> 
классификаторов трактуются в терминах <<формы>> и <<сложности>> изоб\-ра\-же\-ний 
(образованных метками классов на точках выборки), т.\,е.\ в терминах математической 
морфологии~[2--6]. %\cite{3v}--\cite{7v}.
      
      Для алгоритмической реализации процедур синтеза метрических классификаторов 
используется техника построения минимальных разрезов графов~[7--11], %\cite{8v}--\cite{12v}, 
применяемая к графам соседства элементов обучающей выборки.
      
      Рассмотрим задачу обучения с учителем. Пусть дано пространство 
объектов~$\mathcal{A}$, конечное множество классов $C\hm=\{c_1, \ldots , c_l\}$ и известно 
разбиение объектов по классам: $c_{\mathcal{A}}(a): a\hm\in\mathcal{A}\mapsto c\hm\in C$. 
Обозначение~$c_{\mathcal{A}}$ указывает на то, что функция определена на~$\mathcal{A}$.

Производится описание объектов из~$\mathcal{A}$ дескрипторами из пространства описаний 
(признаков) $\mathcal{X}$: $x_{\mathcal{A}}(a): a\hm\in\mathcal{A}\mapsto 
x\hm\in\mathcal{X}$. Случайным образом формируется конечная выборка объектов $A\subseteq 
\mathcal{A}$, $\Vert A\Vert \hm<+\infty$ и соответствующая выборка описаний 
$X\subseteq\mathcal{X}$, $\Vert X\Vert\hm<+\infty$. Каждому значению~$x$ ставится в 
соответствие класс~$c$ породившего его объекта~$a$:
$$
   c_X(x): x_{\mathcal{A}}(a)\in X\mapsto c_{\mathcal{A}}(a)\in C\,.
   $$
По обучающей выборке $c_X$ требуется построить такой распознающий алгоритм, или 
классификатор,
$$
f_{\mathcal{X}}(x):\ x\in\mathcal{X}\mapsto c\in C\,,
$$ 
который обеспечивает наилучшее разбиение~$\mathcal{X}$ на классы из~$C$. <<Наилучшее 
разбиение>> формализуется при помощи тестовой выборки
\begin{gather*}
c^\prime_Y(x):\ x\in Y\mapsto c\in C\,,\\
Y\subseteq \mathcal{X}\,,\ Y\cap X=\varnothing\,, \Vert 
Y\Vert <+\infty\,,
\end{gather*}
и критерия эмпирического риска на выборке~$Y$:
\begin{align*}
J_Y(f_{\mathcal{X}}) &= \fr{d_H(f_Y, c^\prime_Y)}{\Vert Y\Vert}\,;\\
d_H(f_Y, c_Y^\prime) &=\sum\limits_{x\in Y} {1}(f(x)\not= c^\prime(x))\,,
\end{align*}
где 1(true)\;=\;1, 1(false)\;=\;0, $\Vert Y\Vert\hm=\sum\limits_{x\in Y}1$. Здесь расстояние Хэмминга 
$d_{\mathrm{H}}$ имеет смысл числа ошибок классификации на тестовой выборке~$Y$.
Отсюда критерий среднего ожидаемого эмпирического риска имеет вид:
$$
J_{\mathcal{X}}(f_{\mathcal{X}})=E_{Y\subseteq \mathcal{X}} \{J_Y(f_{\mathcal{X}})\}\,,
$$
где $E_{Y\subseteq \mathcal{X}}\{\cdot\}$~--- математическое ожидание по всем возможным 
выборкам $Y\subseteq \mathcal{X}$.

Таким образом, может быть сформулирована задача построения оператора оптимального 
синтеза~$\theta$, доставляющего минимум критерию 
$J_{\mathcal{X}}(f_{\mathcal{X}})\hm=J_{\mathcal{X}}(\theta_{c_X})$:
\begin{equation}
\left.
\begin{array}{l}
\Theta:\ c_X\in \Omega_X\mapsto f_{\mathcal{X}}\in\Omega_{\mathcal{X}}\,;\\[9pt]
\theta=\mathrm{arg}\,\min\limits_{\theta^\prime} \{J_{\mathcal{X}}(\theta^\prime c{X}\}\,.
\end{array}
\right\}
\label{e1v}
\end{equation}
Здесь $\Omega_X$ и $\Omega_{\mathcal{X}}$~--- множества всех возможных разбиений выборки 
$X$ и пространства~$\mathcal{X}$ по классам из~$C$.

В~большинстве известных подходов от задачи синтеза~(\ref{e1v}) сразу переходят к задаче 
обучения классификаторов заданного класса при помощи обучающего правила известного типа:
\begin{equation}
\left.
\begin{array}{l}
\theta \in \Theta:\ c_X\in \Omega_X\mapsto F_{\mathcal{X}}\subseteq 
\Omega_{\mathcal{X}}\,;\\[9pt]
\theta = \mathrm{arg}\,\min\limits_{\theta^\prime \in \Theta} \{ J_Y(\theta^\prime c_X)\}\,,
\end{array}
\right\}
\label{e2v}
\end{equation}
где $F_{\mathcal{X}}$~--- класс классификаторов; $\Theta$~--- класс алгоритмов обучения 
классификаторов из~$F_{\mathcal{X}}$ на выборках $X\subseteq \mathcal{X}$. 

Кроме того, вместо недоступного критерия $J_{\mathcal{X}}(f_{\mathcal{X}})$ на практике 
используется критерий наблюдаемого эмпирического риска $J_X(\theta c_X)$, который имеет 
глобальный минимум в точке $f_X\hm\equiv c_X$, заведомо не пригодный для неизвестной 
тестовой выборки~$Y$. Этой проблеме посвящена теория оценки и контроля переобучения, 
созданная Вапником и Червоненкисом~\cite{2v}. Здесь эмпирический риск оценивается по 
обучающей выборке, но сложность решающего правила искусственно ограничивается. Для этого 
вводится понятие сложности классификатора~$Q(f_{\mathcal{X}})$, а точнее сложности класса 
классификаторов $Q(F_{\mathcal{X}})$. Соответственно, вместо задачи~(\ref{e2v}) решается 
задача минимизации наблюдаемого риска с регуляризацией по сложности класса обучаемого 
классификатора:
\begin{gather*}
\theta \in \Theta:\ c_X\in\Omega_X\mapsto f_{\mathcal{X}}\in F_{\mathcal{X}}\subseteq 
\Omega_{\mathcal{X}}\,;\\
\theta = \mathrm{arg}\,\min\limits_{\theta^\prime\in \Theta} \{J_X(\theta^\prime c_X)+\alpha 
Q(F_{\mathcal{X}})\}\,,
\end{gather*}
где $\alpha \geq 0$~--- параметр регуляризации, опре\-де\-ля\-ющий компромисс между точностью на 
обучающей выборке~$X$ и сложностью решающего правила, от которой зависит 
поведение~$f_{\mathcal{X}}$ на тестовой выборке~$Y$ из~(\ref{e2v}).

Морфологический подход к машинному обучению направлен непосредственно на решение 
задачи~(\ref{e1v}) и позволяет исключить из рассмотрения априорно заданные классы 
классификаторов и алгоритмов обучения. При этом решение задачи~(\ref{e1v}) отыскивается в 
виде композиции решений подзадач
\begin{equation}
\theta_\alpha = \delta_\alpha \psi_\alpha\,,
\label{e4v}
\end{equation}
где $\psi_\alpha$~--- оператор (процедура) синтеза оптимального отклика классификатора на 
обучающей выборке~$X$ с учетом его сложности (локальной некомпактности),
\begin{equation}
\left.
\begin{array}{c}
\psi_\alpha:\ c_X\in \Omega_X\mapsto f_X\in\Omega_X\,;\\[9pt]
\psi_\alpha=\mathrm{arg}\, \min\limits_{\psi^\prime} \{J_X(\psi^\prime c_X)+\alpha Q_X(\psi^\prime c_X)\}\,;
\end{array}
\right \}
\label{e5v}
\end{equation}
$\delta_\alpha$~--- оператор (процедура) оптимальной корректной интерполяции (расширения) 
классификатора $f_X$ на $\mathcal{X}$ с учетом сложности получаемого 
классификатора~$f_{\mathcal{X}}$,
\begin{gather*}
\delta_\alpha:\ f_X\in \Omega_X\mapsto f_{\mathcal{X}}\in \Omega_{\mathcal{X}}\,;\\
\delta_\alpha =\mathrm{arg}\,\min\limits_{\delta^\prime}\{ J_{NN}(\delta^\prime f_X)+\beta 
Q(\delta^\prime f_X)\}\,.
\end{gather*}
Здесь
\begin{multline*}
J_{NN}(\delta f_X)={}\\
\!\!{}=
\begin{cases}
+\infty\,, &\hspace*{-54pt}\mbox{если}\ \exists x\in X:\ \delta f_X(x)\not= f_X(x)\,;\\
d_{\mathrm{H}}\left(\delta f_X(x),\,\delta^{NN}f_X(x)\right) &\hspace*{-6pt}\mbox{в\ противном\ случае}\,;
\end{cases}
\end{multline*}
$d_{\mathrm{H}}$~--- расстояние Хэмминга; $\delta^{NN}$~--- простейший оператор интерполяции 
классификатора, соответствующий правилу ближайшего соседа (nearest neighbor).

Поскольку в задаче~(\ref{e1v}) функционал~$J$ имеет вид расстояния Хэмминга, из 
утверждения~1 следует, что оператор $\theta_\alpha$~(\ref{e4v}) является алгебраическим 
проектором:
\begin{equation*}
\theta_\alpha^2=\theta_\alpha\Rightarrow \forall x\in X:\ \theta_{\alpha} f_X(x)=f_X(x)\,.
%\label{e7v}
\end{equation*}
Кроме того, на основе $\theta_\alpha$ образуется система вложенных классов решающих правил, 
монотонная относительно~$\alpha$: 
\begin{equation*}
\forall \alpha \geq \beta \Rightarrow F^\alpha_{\mathcal{X}}\subseteq F^\beta_{\mathcal{X}}:\ 
Q(F^\alpha_{\mathcal{X}}\leq Q(F^\beta_{\mathcal{X}})\,,
%\label{e8v}
\end{equation*}
где $F^\alpha_{\mathcal{X}}= \{ f_X:\ \theta_\alpha\ f_X=f_X\}$~--- множество классификаторов 
(разбиений), стабильное относительно проектора~$\theta_\alpha$. В~морфологиях изображений 
такая система вложенных проективных классов рас\-смат\-ри\-ва\-ет\-ся как множество пытьевских 
<<форм>> на\-рас\-та\-ющей сложности. В~задаче синтеза классификаторов последовательность 
<<форм>> используется для решения проблемы переобучения методом минимизации 
структурного риска.

Определим такой критерий $Q_X(f_X)$, который отдает предпочтение решающим 
правилам~$f_X$, более компактным на выборке~$X$. Для этого определим систему вложенных 
окрестностей $O_k(x)\subseteq X$, $x\in X\subseteq \mathcal{X}$, $k\hm=1, \ldots , \Vert X\Vert\hm-
1$, состоящих из $k$~ближайших соседей. Введем локальную меру некомпактности~$f_X$ в 
окрестности~$O_k(x)$:
\begin{align*}
Q_k(x, f_X) &= \fr{q_{\mathrm{H}}(O_k(x))}{\Vert O_k(x)\Vert}\,;\\
q_{\mathrm{H}}(O_k(x)) &= \sum\limits_{y\in O_k(x)} 1(f_X(x)\not= f_X(y))\,;\\
\Vert O_k(x)\Vert &= \sum\limits_{y\in O_k(x)} 1\,.
\end{align*}
Тогда глобальная мера $k$-не\-ком\-пакт\-ности имеет вид:
\begin{equation}
\left.
\begin{array}{l}
Q^k_X(f_X)=\fr{Q_{\mathrm{H}}(X, f_X)}{\Vert X\Vert}\,;\\[9pt]
Q_{\mathrm{H}}(X, f_X)=\sum\limits_{x\in X} Q_k(x, f_X)\,.
\end{array}
\right\}
\label{e10v}
\end{equation}
Значение $Q^k_X(f_X)$ характеризует эмпирическую оценку вероятности того, что 
один из $k$ ближайших соседей в разбиении $f_X(x)$ будет отнесен к другому классу. При любых 
фиксированных~$k$ и~$X$ усложнению классификатора~$f_X$ соответствует нарастание меры 
$k$-не\-ком\-пакт\-ности $Q^k_X(f_X)$. С~другой стороны, при увеличении параметра~$k$ 
в~(\ref{e10v}) преимущество получают более простые и <<гладкие>> разделяющие 
поверхности~\cite{9v}.

С учетом критерия~(\ref{e10v}) задача~(\ref{e5v}) сводится к хорошо известной задаче 
оптимальной разметки графа на основе скрытой марковской модели~\cite{10v}, для которой 
существует эффективное приближенное решение методом минимального разреза графа, 
вычислимого за низко полиномиальное время относительно числа узлов графа (объектов в 
выборке) при любом конечном числе классов. 

Более того, для случая двух классов метод разреза 
графов может давать точное глобально оптимальное решение.

Алгоритм нахождения минимального разреза на графе с двумя терминальными вершинами 
позволяет находить минимум функционала энергии вида
\begin{equation}
E(T)= E_0 +\! \sum\limits_{i=1,\ldots , N}\! E_i (t_i) + \sum\limits_{(i,j)\in V} E_{ij}(t_i,\,t_j)\,,
\label{e11v}
\end{equation}
где $N$~--- число нетерминальных вершин графа; $T\hm=\langle t_1, \ldots , t_N\rangle$, $t_1, \ldots 
,t_N \in \{0,\,1\}$~--- метки ассоциирования каждой нетерминальной вершины с одной из 
терминальных; $E_i(0)$, $E_i(1)\in\{0,\,1\}$~--- унарные потенциалы; $E_{ij}(t_i, t_j)$~--- парные 
потенциалы, задаваемые четверкой действительных коэффициентов $E_{ij}(0,\,0)$, $E_{ij}(0,\,1)$, 
$E_{ij}(1,\,0)$, $E_{ij}(1,\,1)$; $V$~--- подмножество пар индексов переменных, за\-да\-ющее систему 
соседства на~$T$.
   
   Энергия~(\ref{e11v}) считается субмодулярной~\cite{11v}, \mbox{если}
   \begin{multline}
   \forall (i,j)\in V:\ E_{ij}(0,\,0)+E_{ij}(1,\,1)\leq{}\\
   {}\leq E_{ij}(0,\,1)+E_{ij}(1,\,0)\,.
   \label{e12v}
   \end{multline}

Для субмодулярной энергии~(\ref{e11v})--(\ref{e12v}) метод построения минимального разреза 
графа~\cite{12v, 11v} гарантирует нахождение точного минимума~\cite{12v}.

Для реализации задачи синтеза двухклассового классификатора~(\ref{e5v}), (\ref{e10v}) примем: 
$C\hm \{0,1\}$, $N\hm=\Vert X\Vert$, $T\hm=\langle t_1, \ldots , t_N\rangle$, $t_1\hm = f_X(x_1), 
\ldots , t_N \hm =f_X(x_N)$; $E_i(x)\hm= 1(f_X(x) \hm\not= c_X(x))$, $E_{ij}(t_i, t_j)\hm=1(f_X(x_i) 
\not=$\linebreak $\not= f_X(x_j))$; $V\hm = \{(i, j): j\in O_k(x_i)\}$. 

Легко убедиться, что соответствующая 
энергия~(\ref{e11v}) будет субмодулярной, а значит, метод минимального разреза графа 
$k$-со\-сед\-ст\-ва для выборки~$c_X$ действительно оптимален и порождает 
$\alpha$-се\-мей\-ст\-ва проекторов.



\section{Самоорганизующаяся нейронная сеть~--- растущий нейронный газ}
      
      В качестве второго способа цветовой сег\-мен\-тации рассматривается алгоритм 
кластеризации цвето\-во\-го пространства, основанный на аппроксимации цветового пространства 
изображения самоорганизующейся сетью, обучаемой по алгоритму рас\-ту\-ще\-го нейронного 
газа~\cite{14v, 13v}. Главным преимуществом этого алгоритма является осуществление так 
называемой <<адаптивной>> кластеризации входных данных, т.\,е.\ пространство не только 
разделяется на кластеры, но и определяется необходимое их количество исходя из топологии 
распределения самих данных. Начиная всего с двух нейронов, алгоритм последовательно изменяет 
(по большей час\-ти увеличивает) их чис\-ло, одновременно создавая набор связей между нейронами, 
наилучшим образом отвечающий распределению входных векторов, используя подход 
соревновательного хеббовского обучения. Каждый нейрон характеризуется так называемой 
<<локальной ошибкой>>. Соединения между узлами характеризуются <<возрастом>>. Пример 
такой структуры показан на рис.~1.

\begin{center} %fig1
\vspace*{6pt}
\mbox{%
 \epsfxsize=79mm
 \epsfbox{viz-1.eps}
}
\end{center}
%\begin{center}
\vspace*{1pt}
{{\figurename~1}\ \ \small{Структура нейронного газа: распределение клас\-теров (зеленый), связей (оранжевый) и 
топология данных (синий), конкретные сигналы показаны в виде отдельных точек}}
%\end{center}
\vspace*{6pt}

%\smallskip
\addtocounter{figure}{1}

Алгоритм работы растущего нейронного газа кратко можно описать следующим образом:
\begin{enumerate}
\item Инициализация: создать два узла с векторами весов, разрешенными распределением входных 
векторов, и нулевыми значениями локальных ошибок; соединить узлы связью, установив ее 
возраст равным~0.
\item Подать на вход нейросети вектор~$x$.
\item Найти два нейрона $s$ и~$t$, ближайших к~$x$, т.\,е.\ узлы с векторами весов~$w_s$ и 
$w_t$ такими, что $\Vert w_s\hm-x\Vert^2$~--- минимальное, а $\Vert w_t\hm-x\Vert^2$~--- второе 
минимальное значение расстояния среди всех узлов.
\item Обновить локальную ошибку ней\-ро\-на-по\-бе\-ди\-те\-ля~$s$ путем добавления к ней 
квадрата расстояния между векторами~$w_s$ и~$x$: $E_s\leftarrow E_s\hm+\Vert w_s\hm-
x\Vert^2$.\\[-13pt]
\item Сместить нейрон-по\-бе\-ди\-тель~$s$ и всех его топологических соседей (т.\,е.\ все нейроны, 
име\-ющие соединение с победителем) в сторону входного вектора~$x$ на расстояния, равные долям 
$\varepsilon_w$ и~$\varepsilon_n$ от полного:

\noindent
\begin{align*}
w_s&\leftarrow w_s+\varepsilon_w(w_s-x)\,;\\
w_n&\leftarrow w_n+\varepsilon_n(w_n-x)\,.
\end{align*}
\item Увеличить на~1 возраст всех соединений, исходящих от победителя~$s$.\\[-13pt]

\item Если два лучших нейрона~$s$ и~$t$ соединены, обнулить возраст их связи. В~противном 
случае создать связь между ними.\\[-13pt]
\item Удалить все соединения, возраст которых превышает age$_{\max}$. Если после этого остаются 
нейроны, не имеющие связей с другими узлами, удалить эти нейроны.\\[-13pt]
\item Если номер текущей итерации кратен~$\lambda$ и предельный размер сети не достигнут, 
создать новый нейрон $r$ по следующим правилам:\\[-13pt]
\begin{itemize}
\item найти нейрон~$u$ с наибольшей локальной ошибкой;\\[-14pt]
\item среди соседей $u$ найти нейрон~$v$ с максимальной ошибкой;\\[-14pt]
\item создать узел $r$ <<посредине>> между~$u$ и~$v$:
$$
w_r =\fr{w_u+w_v}{2}\,;
$$
\item заменить связь между~$u$ и~$v$ на связи~$u$ и~$r$, $v$ и~$r$;\\[-14pt]
\item уменьшить ошибки нейронов~$u$ и~$v$, установить значение ошибки нейрона~$r$:

\noindent
$$
E_u\leftarrow E_u\alpha\,;\ E_v \leftarrow E_v\alpha\,;\ E_r\leftarrow E_u\,;$$
\item уменьшить ошибки всех нейронов~$j$ на долю~$\beta$:

\noindent
$$
E_j\leftarrow E_j-E_j\beta\,.
$$
      \end{itemize}
\item Если критерий останова не выполнен, перейти к шагу~2.\\[-13pt]
\end{enumerate}

\vspace*{-9pt}

\section{Результаты экспериментального исследования}

      Тестирование проводилось на базе изображений людей, снятых при различных условиях
      съемки. Изображения были предварительно размечены вручную на области принадлежности\linebreak\vspace*{-12pt}
      
      \pagebreak
      
      
      
      \noindent 
пикселов классу кожи. Выборка точек для обработки получена путем перевода тестовых 
изображений в цветовое пространство CIE Lab с целью отделения цветовых компонент от яркости. 
Это повышает компактность представления, так как  кожа имеет характерный цвет, а не 
яркостную составляющую. Обучение производилось на 10\% точек от общего объема выборки в 
800\,000~точек. 
      
      При построении графа соседства использовался алгоритм триангуляции Делоне с 
динамическим кэшированием треугольников~\cite{15v}. Нахождение оптимальных разрезов 
графов осуществлялось с использованием библиотеки~\cite{16v}. 

Полученная вероятность 
правильной классификации цвета пиксела (кожа\,/\,не кожа) на тестовой выборке~--- 0,937.

      Для самоорганизующейся нейронной сети были рассмотрены результаты при 32, 128 и 
256~клас\-те\-рах,  полученных после кластеризации обуча\-ющей выборки. Полученная 
вероятность правильной классификации цвета пиксела (кожа\,/\,не кожа) на тес\-то\-вой выборке~--- 
0,925.

Как видно, численные значения результатов обучения, полученные двумя описанными 
методами в задаче цветовой сегментации кожи на изоб\-ра\-же\-ни\-ях лиц, оказались достаточно 
близки.  %\linebreak\vspace*{-12pt}


      \end{multicols}
      
      \begin{figure}[b] %fig2
\vspace*{-18pt}
 \begin{center}
 \mbox{%
 \epsfxsize=160.218mm
 \epsfbox{viz-2.eps}
 }
 \end{center}
 \vspace*{-9pt}
\Caption{Обучающая выборка (\textit{а}) и ее увеличенный фрагмент~(\textit{б}), содержащий точки 
<<кожи>> (красные) и <<не кожи>> (зеленые); желтым показаны точки, имеющие обе метки}
%\end{figure}
%\begin{figure*} %fig3
\vspace*{12pt}
\begin{minipage}[t]{81mm}
 \begin{center}
 \mbox{%
 \epsfxsize=79mm
 \epsfbox{viz-3.eps}
 }
 \end{center}
 \vspace*{-9pt}
\Caption{Результат переразметки обучающей выборки после морфологического обучения на основе 
разреза графа}
%\end{figure*}
\end{minipage}
\hfill
%\begin{figure*} %fig4
\vspace*{1pt}
\begin{minipage}[t]{81mm}
 \begin{center}
 \mbox{%
 \epsfxsize=79mm
 \epsfbox{viz-4.eps}
 }
 \end{center}
 \vspace*{-9pt}
\Caption{Результат переразметки обучающей выборки после обучения на основе <<растущего 
нейронного газа>>}
\end{minipage}
\end{figure}

      
      \begin{multicols}{2}
      

%\noindent

\begin{center} %fig5
%\vspace*{12pt}
\mbox{%
 \epsfxsize=79mm
 \epsfbox{viz-5.eps}
}
\end{center}
%\begin{center}
%\vspace*{3pt}
{{\figurename~5}\ \ \small{Различия в результатах обучения (см.\ рис.~3 и~4). Жирным выделены точки, классифицируемые 
по-разному}}
%\end{center}
\vspace*{12pt}

%\smallskip
\addtocounter{figure}{1}


      
%      \noindent
Одна\-ко более подробное рассмотрение выделенных  клас\-теров демонстрирует 
существенные различия в их форме. На рис.~2 показана обучающая выборка в плоскости~$ab$ 
цветового пространства CIE Lab. Красными точками помечены пикселы кожи, зелеными~--- 
других классов.
      

На рис.~3 приведен результат переразметки обуча\-ющей выборки после применения процедуры 
обучения на основе разреза графа соседства, а на рис.~4~--- после обучения на основе 
<<растущего нейронного газа>>. Рисунок~5 демонстрирует различия в форме кластеров, 
полученных двумя способами обучения.


Как видно, значительные отличия в форме полученных кластеров цвета кожи указывают на 
существенно различную природу этих процедур обуче\-ния, что позволяет в дальнейшем 
рассматривать возможность их комплексирования с целью повышения вероятности правильной 
классификации.

{\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{99}

\bibitem{1v}
\Au{Viola P., Jones M.}
Robust real-time object detection~//  IEEE Workshop on Statistical and Computational Theories of 
Vision Proceedings.~--- Vancouver, CA,  2001.


\bibitem{4v} %2
\Au{Serra J.}
Image analysis and mathematical morphology.~--- London: Academic Press, 1982. 

\bibitem{5v} %3
\Au{Пытьев Ю.\,П.}
Морфологический анализ изображений~// Доклады АН СССР, 1983. Т.~269. №\,5. 
С.~1061--1064.

\bibitem{3v} %4
\Au{Pavel M.}
Fundamentals of pattern recognition.~--- New York: Marcel Dekker, Inc., 1989.

\bibitem{7v} %5
\Au{Визильтер Ю.\,В.}
Обобщенная проективная морфология~// Компьютерная оптика, 2008. Т.~32. №\,4. 
С.~384--399.

\bibitem{6v} %6
\Au{Пытьев Ю.\,П., Чуличков А.\,И.}
Методы морфологического анализа изображений.~--- М.: Физматлит, 2010. 336~с.

\bibitem{8v} %7
\Au{Ford L., Fulkerson D.} 
Flows in networks.~--- Princeton University Press, 1962.

\bibitem{9v} %8
\Au{Greig D., Porteous B., Seheult~A.}
Exact maximum \textit{a posteriori} estimation for binary images~// J.~Roy. Statistical Soc., 1989. 
Vol.~51. No.\,2. P.~271--279.

\bibitem{10v} %9
\Au{Boykov Y., Kolmogorov V.}
Computing geodesics and minimal surfaces via graph cuts~// IEEE  Conference (International) 
Computer Vision (ICCV) Proceedings, 2003. P.~26--33.

\bibitem{12v} %10
\Au{Kolmogorov V., Zabih R.}
What energy functions can be minimized via graph cuts?~// IEEE Trans. Pattern Anal. 
Machine Intelligence (PAMI), 2004. Vol.~26. No.\,2. P.~147--159.

\bibitem{11v} %11
\Au{Boykov Y., Kolmogorov V.}
An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision~// 
IEEE Trans. Pattern Anal. Machine Intelligence (PAMI), 2004. Vol.~26. No.\,9. 
P.~1124--1137.

\bibitem{2v} %12
\Au{Вапник В.\,Н.}
Восстановление зависимостей по эмпирическим данным.~--- М.: Наука, 1979. 

\bibitem{14v} %13
\Au{Fritzke B.}
Fast learning with incremental RBF networks~// Neural Processing Lett., 1994. Vol.~1. No.\,1. 
P.~2--5.

\bibitem{13v} %14
\Au{Fritzke B.}
A~growing neural gas network learns topologies~// Advances in neural information processing 
systems~7~/ Eds.\ G.~Tesauro, D.\,S.~Touretzky, T.\,K.~Leen.~--- Cambridge MA: MIT Press, 
1995. P.~625--632.


\bibitem{15v}
\Au{Скворцов А.\,В.}
Обзор алгоритмов построения триангуляции Делоне~// Вычислительные методы и 
программирование, 2002. Т.~3. С.~14--39.

\label{end\stat}

\bibitem{16v}
\Au{Boykov Y., Kolmogorov V.} 
MAXFLOW~--- software for computing mincut/maxflow in a graph. V.~3.01. {\sf 
http:// www.cs.ucl.ac.uk/staff/V.Kolmogorov/software.html}.
 \end{thebibliography}
}
}


\end{multicols}
\def\stat{kriv}

\def\tit{РАСЩЕПЛЕНИЕ СМЕСИ ВЕРОЯТНОСТНЫХ РАСПРЕДЕЛЕНИЙ НА~ДВЕ 
СОСТАВЛЯЮЩИЕ}
\def\titkol{Расщепление смеси вероятностных распределений на~две 
составляющие}

\def\autkol{М.\,П.~Кривенко}
\def\aut{М.\,П.~Кривенко}

\titel{\tit}{\aut}{\autkol}{\titkol}

%{\renewcommand{\thefootnote}{\fnsymbol{footnote}}\footnotetext[1]
%{Работа выполнена при поддержке
%Российского фонда фундаментальных исследований,
%гранты 06-07-89056 и 08-07-00152.}}

\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Институт проблем
информатики Российской академии наук, mkrivenko@ipiran.ru}

\Abst{Рассматривается представление плотности распределения в виде смеси двух 
составляющих и задача оценивания параметров этой смеси при имеющихся выборках из самой 
плотности и из одной из составляющих. Предлагаются два метода построения оценок, строятся 
соответствующие алгоритмы и проводится их сравнительный анализ.}
      
      \KW{смесь нормальных распределений; расщепление смеси распределений; 
EM-алгоритм}

 %     \vskip 36pt plus 9pt minus 6pt
      
            \vskip 24pt plus 9pt minus 6pt

      \thispagestyle{headings}

      \begin{multicols}{2}

      \label{st\stat}

      
\section{ Введение}
     
     Рассмотрим представление плотности распределения в виде смеси, 
состоящей из двух составляющих:
     \begin{equation}
    g_{B,S} (u) = pg_B (u) +(1-p)g_S (u)\,,\ \mbox{где}\ p\in [0,\,1]\,,
     \label{e1kr}
     \end{equation}
и задачу оценивания элементов этого представления при условии наличия двух 
выборок: $X_{B,S}$ из  $g_{B,S}(u)$ и $X_B$ из $g_B(u)$.
     
     Дадим пример практической ситуации, в рамках которой может возникнуть 
сформулированная задача~\cite{1kr}. Речь идет об обработке изображений, когда 
интенсивности серого у пикселов для изоб\-ра\-же\-ний знаков на некотором фоне 
отличаются от интенсивностей пикселов самого фона. Необходимо по 
изображению знаков на некотором фоне (выборка $X_{B,S}$ из $g_{B,S}(u)$) и 
изображению только фона (выборка $X_B$ из $g_B(u)$) составить представление 
об уровне фона (оценить $p$) и описать распределения интенсивностей серого как 
знаков, так и фона (оценить $g_S(u)$ и $g_B(u)$). После получения 
перечисленных оценок становится возможным оценить порог~$T$, отделяющий 
<<оптимальным>> образом пикселы фона и знаков, и, например, перейти к 
     черно-белому изображению. Важность подобного автоматического метода 
оценивания порога~$T$ с помощью экспериментальных данных для практики 
объясняется следующим: качество реальных изоб\-ра\-же\-ний может быть невысоким 
и существенно меняться в процессе распознавания определенного текста, в силу 
чего процесс экспертного установления значения~$T$ становится достаточно 
трудоемким, а основанные на черно-белом изображении процедуры 
распознавания оказываются достаточно эффективными. При этом~$T$ 
понимается как решение уравнения 

\noindent
$$
\alpha (T) =\beta (T)\,,
$$ 
где $\alpha (T)$, $\beta 
(T)$~--- ошибки при разделении двух классов, а именно первого класса, 
соответствующего фону с вероятностью появления $p$ и плотностью 
распределения $g_B(u)$, и второго класса, соответствующего пикселам знаков с 
вероятностью появления $1-p$ и плотностью распределения~$g_S(u)$.
     
     Рассмотрим~(\ref{e1kr}) как уравнение относительно $g_S(u)$. Если 
$g_B(u)>0$ для $\forall u$, то 
     $$
     g_S (u) = \fr{g_B(u)}{1-p}\left (\fr{g_{B,S}(u)}{g_B(u)} -p\right )\,.
     $$
     Тогда решение~(\ref{e1kr}) является либо единственным и $p=0$, либо 
     неединственным и существует для всех 
     $p\in [0,\,\underset{u}{\mathrm{inf}} (g_{B,S}(u)/g_B(u))]$. Вряд ли такой 
вывод практически интересен. Введем дополнительные ограничения, 
позволяющие сначала определиться со значением $p$, а затем пытаться находить 
$g_S(u)$. Пусть существуют значения~$u$, для которых $g_S(u)=0$ и $g_{B,S} 
\not= 0$. Для таких точек (\ref{e1kr}) дает уравнение
     \begin{equation} %2
     g_{B,S}(u) =pg_B(u)\,,
     \label{e2kr}
     \end{equation}
после чего при известных $g_{B,S}(u)$ и $g_B(u)$ становится возможным найти 
искомое $p$ и, следовательно, $g_S(u)$, если, конечно, соотношения~(\ref{e1kr}) 
и~(\ref{e2kr}) не становятся противоречивыми для различных точек~$u$.
     
     Приведенные рассуждения носят прикидочный характер, позволяют 
прояснить идеи, которые положены автором в алгоритмы решения поставленной 
задачи.
     
     При оценивании элементов представления~(\ref{e1kr}) можно следовать 
одной из следующих схем:
     \begin{itemize}
\item от частного к общему, когда сначала по $X_B$ находится оценка 
$g_B^*(u)$, а затем по $X_{B,S}$ оцениваются величины  $p^*$, $g_S^*(u)$;
\item от общего к частному, когда сначала по $X_{B,S}$ оценивается 
$g_{B,S}^*(u)$, а затем с учетом $X_B$ так подбираются $p^*$, $g_B^*(u)$, 
$g_S^*(u)$, чтобы  
$$
p^*g_B^*(u)+(1-p^*)g_S^*(u)\approx g_{B,S}^*(u)\,.
$$
     \end{itemize}
     
     Для оценивания $g_{B,S}(u)$, $g_B(u)$ и $g_S(u)$ удобно представить их в 
виде смеси нормальных распределений с неизвестными параметрами. Таким 
образом, модель смеси распределений используется как для описания сложной 
структуры данных, так и для аппроксимации реальных распределений данных с 
помощью плотностей нормального распределения.

\section{Модель смеси нормальных распределений и 
оценивание ее~параметров}
     
     Рассмотрим модель смеси распределений, когда плотность некоторого 
распределения $f(u)$ представима в виде 
     \begin{equation}
     f(u) = \sum\limits_{j=1}^k p_{f_j} h\left ( u,\vartheta_{f_j}\right )\,.
     \label{e3kr}
     \end{equation}
     
     В представлении~(\ref{e3kr}) неизвестными являются все или часть 
следующих характеристик (параметров смеси): число $k$ элементов смеси (число 
компонентов смеси), вероятности $p_{f_j}$ элементов смеси (веса элементов 
смеси), параметры $\vartheta_{f_j}$ элементов смеси. Вид плотности 
     $h(u,\vartheta )$ предполагается известным: 
     $$
     h(u,\vartheta ) \equiv h(u,a,c)=\fr{1}{\sqrt{2\pi}\,c}\,\exp\left \{ -\fr{(u-
a)^2}{2c^2}\right \}\,.
     $$
     
     Для оценивания неизвестных параметров~(\ref{e3kr}) используется 
метод максимального правдоподобия. При решении задачи оценивания 
параметров смеси далее предполагается, что число $k$ элементов смеси задано.
     
     Согласно принципу максимального правдоподобия центральным 
становится решение оптимизационной задачи вида
     \begin{multline}
     \ln L(p_f,\vartheta_f )\equiv  \sum\limits_{i=1}^N \ln \left (
     \sum\limits_{j=1}^k p_{f_j}h\left ( x_i,\vartheta_{f_j}\right )\right )\rightarrow{}\\
     {}\rightarrow
     \underset{\substack{{p_{f_1},\ldots , p_{f_k}, \sum\limits_{j=1}^k p_{f_j} 
=1,}\\{\vartheta_{f_1},\ldots , \vartheta_{f_k}} }}{\max}\,.
     \label{e4kr}
     \end{multline}

Наиболее работоспособную общую схему построения процедур, позволяющих 
находить решения задачи~(\ref{e4kr}), обычно называют 
EM-ал\-го\-рит\-мом~\cite{2kr}.

     Введем в рассмотрение так называемые апостериорные вероятности 
$q_{ij}$ принадлежности наблюдения $x_i$ к $j$-му элементу смеси. Если 
известны значения параметров $p_{f_j}$ и $\vartheta_{f_j}$, $j=1,\ldots , k$, то 
при наблюденном значении $x_i$, $i=1,\ldots , N$, апостериорная вероятность 
принадлежности этого значения к  $j$-му элементу смеси принимает вид:
     \begin{equation}
     q_{ij} (x_i, p_{f_j}, \vartheta_{f_j}) = \fr{p_{f_j} h(x_i, 
\vartheta_{f_j})}{\sum\limits_{l=1}^k p_{f_l} h(x_i, \vartheta_{f_l})}\,.
     \label{e5kr}
     \end{equation}
     
     Если это не будет искажать смысл, то в записи апостериорных вероятностей 
будем далее опускать перечисление аргументов, т.\,е.\ вместо $q_{ij}(x_i, p_{f_j}, 
\vartheta_{f_j})$ будем просто записывать $q_{ij}$. Из определения следует, что 
для всех допустимых $i$ и $j$ выполняется следующее: 
     $$
     q_{ij} \geq 0\ \ \mbox{и}\  \ \sum\limits_{j=1}^k q_{ij}=1\,.
     $$
     
     Преобразуем так логарифм функции правдоподобия, чтобы в получившемся 
для него выражении фигурировали апостериорные вероятности $q_{ij}$, а именно 
при $p_{f_j} h(x_i,\vartheta_{f_j})\not=0$ имело место представление:
     \begin{multline}
     \ln L(p_f,\vartheta_f ) = \sum\limits_{j=1}^k\sum\limits_{i=1}^N q_{ij}\ln 
p_{f_j} +{}\\
{}+\sum\limits_{j=1}^k\sum\limits_{i=1}^N q_{ij}\ln h(x_i,\vartheta_{f_j})-
 \sum\limits_{j=1}^k\sum\limits_{i=1}^N q_{ij} \ln q_{ij}\,.
     \label{e6kr}
     \end{multline}
     
     Для доказательства справедливости этого представления достаточно в 
правую часть~(\ref{e6kr}) подставить выражение для апостериорной 
вероятности. Теперь для нахождения оценки параметров смеси распределений 
появляется возможность сформулировать
\textbf{ЕМ-ал\-го\-ритм:}


\noindent
\begin{enumerate}[1.]
\item Положить $t=0$ и задать для параметров смеси начальные значения 
$p^{(0)}_{f_j}$ и $\vartheta^{(0)}_{f_j}$, $j=1,\ldots , k$. 
\item По формуле~(\ref{e5kr}) вычислить значения 
$q_{ij}^{(t+1)}\left ( x_i,p_{f_j}^{(t)}, \vartheta_{f_j}^{(t)}\right )$, $i=1,\ldots , N$ и 
$j=1,\ldots ,k$.
\item С помощью выражения~(\ref{e6kr}) определить значения 
$p_{f_j}^{(t+1)}$ и $\vartheta_{f_j}^{(t+1)}$ для $j=1,\ldots ,k$ из условия 
максимизации отдельно каждого из первых двух слагаемых правой 
части~(\ref{e6kr}), поскольку при фиксированных апостериорных вероятностях 
первое %\linebreak 
слагаемое $\sum\limits_{j=1}^k\sum\limits_{i=1}^N q_{ij}\ln p_{f_j}$ 
зависит только от па\-ра\-мет\-ров $p_f$, а второе слагаемое 
$\sum\limits_{j=1}^k\sum\limits_{i=1}^N q_{ij}\ln h(x_i,\vartheta_{f_j})$ зависит 
только от па\-ра\-мет\-ров $\vartheta_f$.
\item Завершить процесс нахождения оценки параметров смеси, если эти оценки 
являются подходящими, или положить $t=t+1$ и перейти к шагу~2 в противном 
случае. 
\end{enumerate}
\bigskip
     
     Решение оптимизационной задачи
     $$
     \sum\limits_{j=1}^k\sum\limits_{i=1}^N q_{ij} \ln p_{f_j}\rightarrow
     \underset{\substack{{p_{f_1},\ldots ,p_{f_k}}\\{\sum\limits_{j=1}^k 
p_{f_j}=1}}}{\max}
     $$
имеет вид:
\begin{equation*}
p_{f_j}^* = \fr{w_j}{N}\,,\quad j=1,\ldots ,k\,, \ \ \mbox{где}\ \ w_j = 
\sum\limits_{i=1}^N q_{ij}\,.
%\label{e7kr}
\end{equation*}
Для доказательства этого достаточно рассмотреть функцию Лагранжа.
     
     Решение оптимизационной задачи
     $$
     \sum\limits_{j=1}^k\sum\limits_{i=1}^N q_{ij}\ln h(x_i, \vartheta_{f_j})\rightarrow 
     \underset{\vartheta_{f_j},\ldots , \vartheta_{f_k}}{\max}
     $$
распадается на решения для $j=1,\ldots ,k$ отдельных задач: 
\begin{equation}
\sum\limits_{i=1}^N q_{ij}\ln h(x_i, \vartheta_{f_j})\rightarrow
\underset{\vartheta_{f_j}}{\max}\,,
\label{e8kr}
\end{equation}
в случае нормального распределения они могут быть получены аналитическим 
путем:
\begin{equation*}
a^*_{f_j} = \fr{\sum\limits_{i=1}^N q_{ij} x_i}{w_j}\,,\quad \left ( c^2_{f_j}\right )^* 
= \fr{\sum\limits_{i=1}^N q_{ij} (x_i -a^*_{f_j})^2}{w_j}\,.
\end{equation*}
     
     Постановка задачи в виде~(\ref{e8kr}) без дополнительных ограничений 
на параметр $\vartheta_{f_j}$ может привести к появлению бесконечно большого 
вклада в значение функции правдоподобия. Причина этого может состоять в том, 
что к $j$-му элементу смеси в ходе работы ЕМ-алгоритма могут быть отнесены 
либо единственное наблюденное значение, либо совпадающие наблюденные 
значения, при этом выборочная дисперсия оказывается равной нулю. В этом 
случае можно ограничивать множество допустимых значений дисперсии и тем 
самым ограничивать значение максимума функции правдоподобия. Таким 
образом, задача~(\ref{e8kr}) принимает вид:
     \begin{equation*}
     \sum\limits_{i=1}^N q_{ij}\ln h(x_i,a_{f_j},c_{f_j})\rightarrow
     \underset{\substack{{a_{f_j}, c_{f_j},}\\{c_{f_j}\geq c_{\min}}}}{\max}\,,
     \end{equation*}
где $c_{\min}$ задано, а ее решение, согласно~\cite{1kr}, имеет следующий вид:
\begin{equation*}
a_{f_j}^{**}=a_{f_j}^*\,,\quad
\left ( c^2_{f_j}\right )^{**} = 
\begin{cases}
\left ( c^2_{f_j}\right )^*\,, & \left ( c_{f_j}^2\right )^* > c^2_{\min}\,,\\
 c^2_{\min}\,, & \left ( c_{f_j}^2\right )^* \leq c^2_{\min}\,.
\end{cases}
     \end{equation*}
     
     Заметим, что если вместо~(\ref{e4kr}) решается задача
     \begin{equation*}
     \sum\limits_{i=1}^N\ln \left ( \sum\limits_{j=1}^k p_{f_j} h\left (x_i, 
\vartheta_{f_j}\right )\right ) \rightarrow \underset{\substack{{p_{f_1},\ldots 
,p_{f_k},}\\{\sum\limits_{j=1}^k p_{f_j}=1}}}{\max}\,,
 %    \label{e10kr}
     \end{equation*}
то это просто приводит к упрощению 3-го шага ЕМ-алгоритма.

\section{Процедуры расщепления смеси}

     Реализация описанной схемы от частного к общему оказалась 
неработоспособной, так как при нахождении $p^*$, $g_S^*(u)$ при уже 
полученной $g_B^*(u)$ часто возникает следующая ситуация: $p^*$ устремляется 
к нулю и $g_S^*(u)$ становится оценкой для $g_{B,S}(u)$, а не для $g_S(u)$.
     
     При оценивании элементов модели~(\ref{e1kr}) в соответствии со схемой 
от общего к частному сначала для выборки $X_{B,S}$ решается задача 
нахождения оценки $g_{B,S}^*(u)$ в виде смеси
     \begin{equation}
g_{B,S}^* (u) = \sum\limits_{j=1}^{k_{B,S}} p_{B,S_j}^* h\left ( 
u,\vartheta_{B,S_j}^* \right )\,,
     \label{e11kr}
     \end{equation}
а затем решается задача расщепления смеси $g_{B,S}^*(u)$ на две части 
вида~(\ref{e1kr}). Последнее предлагается реализовать одним из двух 
возможных способов: 
\begin{enumerate}[(1)]
\item одновременное оценивание $p^*$, $g_{B}^*(u)$, $g_S^*(u)$ путем 
выбора такого набора индексов элементов смеси~(\ref{e11kr}), при 
котором достигается максимум функции правдоподобия для выборки~$X_B$;
\item последовательное оценивание сначала значения $p^*$, затем 
вероятностей $p_{B_j}^*$ в разложении 
$$
g_B^*(u) = \sum\limits_{j=1}^{k_{B,S}}p_{B_j}^* h\left (u, \vartheta_{B,S_j}^*\right )
$$ 
для выборки $X_B$, а затем формирование оценок вероятностей $p_{S_j}^*$ в 
разложении 
$$
g_S^*(u) = \sum\limits_{j=1}^{k_{B,S}}p_{S_j}^* h  \left (u,\vartheta_{B,S_j}^*\right )
$$ 
с помощью представления $$
g_S^*(u) \approx \fr{1}{1-p^*}\left ( g_{B,S}^*(u) -p^* g_B^*(u)\right )\,.
$$
     \end{enumerate}
     
     При реализации 1-го способа можно предложить два варианта выбора 
набора индексов элементов смеси~(\ref{e11kr}):
     \begin{itemize}
\item полный перебор всех возможных комбинаций индексов и выбор из них той 
комбинации, для которой достигается наибольшее значение функции 
правдоподобия (оптимальный выбор); 
\item последовательное построение комбинации путем перемещения в 
формируемую смесь $g_B^*(u)$ из оставшихся элементов смеси~(\ref{e11kr}) 
того элемента, для которого сформированная комби\-нация приводит к 
наибольшему значению функции правдоподобия; завершение этого процесса 
происходит, когда значение функции правдоподобия станет уменьшаться 
(субоптимальный выбор). 
\end{itemize}
     
     В данной работе рассматривается использование только оптимального 
выбора набора индексов.
     
     Дадим более подробное описание конкретных вариантов соответствующих 
процедур обработки данных. Их работа определяется следующими основными 
входными параметрами: 
     \begin{itemize}
\item заданная выборка $X_{B,S}$;
\item заданная выборка $X_B$;
\item заданные параметры ЕМ-алгоритма (число элементов смеси $k$, 
критическое значение дисперсии $c^2_{\min}$, максимальное число итераций, 
ошибка нахождения максимума функции правдоподобия);
\end{itemize}
а также выходными параметрами:
\begin{itemize}
\item формируемая оценка $p^*$;
\item формируемые оценки параметров смеси $g_B^*$;
\item формируемые оценки параметров смеси $g_S^*$.
\end{itemize}

\subsection{Процедура одновременного оценивания} %3.1
     
     В основе этого алгоритма лежит предположение, что составляющие 
$g_S(u)$ и $g_B(u)$ <<отделимы>> друг от друга в том смысле, что в разложении 
$g_{B,S}(u)$ по некоторой системе плотностей одна его часть относится к 
$g_S(u)$, а другая~--- к $g_B(u)$. Поэтому группирование элементов смеси с 
целью одновременного формирования составляющих $g_S^*(u)$ и $g_B^*(u)$ 
может быть реализовано с помощью следующей последовательности действий
     (\textbf{алгоритм А1}):

\noindent
\begin{enumerate}[1.]
\item Оценивание с помощью ЕМ-алгоритма па\-ра\-мет\-ров смеси~(\ref{e11kr}) 
по выборке~$X_{B,S}$.
\item Выбор подмножества множества элементов 
$\left \{ p_{B,S_j}^* h\left( u,\vartheta_{B,S_j}^*\right ), j=1,\ldots ,k_{B,S}\right \}$ так, чтобы 
при~$X_B$ получить максимальное значение функции правдоподобия (см.\ 
далее~(\ref{e12kr})).
\item Нахождение значения $p^*$ и $g_B^*(u)$ (см.\ далее~(\ref{e13kr})).
\item Формирование оценки для $g_S^*(u)$ (см.\ далее~(\ref{e14kr})). 
\end{enumerate}

\bigskip
     Пусть $I_B$~--- подмножество множества индексов элементов смеси 
заданного набора $\{ p_{B,S_j}^* h(u, \vartheta_{B,S_j}^*), j=1,\ldots , k_{B,S}\}$, 
а $N$~--- объем выборки~$X_B$. Обозначив с целью упрощения записи 
$$
p_j =  p_{B,S_j}^*\,,\  
a_j = a_{B,S_j}^*\,,\  
c_j=c_{B,S_j}^*\,, \ j=1,\ldots ,k_{B,S}\,,
$$ 
выпишем формулы для подсчета функции правдоподобия:
     \begin{multline}
   \ln L (p,a,c\vert I_B) ={}\\
   {}= \sum\limits_{i=1}^N \ln\left ( \sum\limits_{j\in I_B }
     \fr{p_j}{\sum\limits_{j\in I_B}p_{j}}\,h(x_i, a_j, c_j )\right )={}\\
     {}=\sum\limits_{i=1}^N\ln \left ( \sum\limits_{j\in I_B} p_j 
\fr{1}{\sqrt{2\pi}\,c_j}\,\exp\left [
     -\fr{(x_i -a_j)^2}{2c_j^2}\right ]\right ) -{}\\
     {}- N\ln \sum\limits_{j\in I_B} p_j =
    \sum\limits_{i=1}^N \ln \left ( \sum\limits_{j\in I_B} h_{ij}\right ) -{}\\
    {}-N\ln 
\sum\limits_{j\in I_B} p_j -\fr{N}{2}\,\ln\left (2\pi\right )\,,
     \label{e12kr}
     \end{multline}
где 
$$
h_{ij} = \fr{p_j}{c_j}\,\exp \left [ -\fr{(x_i-a_j)^2}{2c_j^2}\right ]
$$
для $i=1,\ldots ,N$ и $j=1,\ldots ,k_{B,S}$.
     
     После того как найдено подмножество $\{ p_{B_j}^* h(u,\vartheta_{B_j}^*),\, 
j=1,\ldots ,k_B\}$ и соответствующее $I_B^*$, оценки для $g_B^*(u)$ и $p^*$ 
принимают следующий вид:
     \begin{align}
     p^* & = \sum\limits_{j\in I_B^*} p_{B,S_j}^*\equiv \sum\limits_{j=1,\ldots , 
k_B} p_{B_j}^*\,;\notag\\[-6pt]
     &\label{e13kr}\\[-6pt]
     g_B^*(u) & = \fr{1}{p^*}\sum\limits_{j\in I_B^*}  p_{B,S_j}^* h\left ( u, 
\vartheta_{B,S_j}^* \right)\,.\notag
     \end{align}
     И, наконец, 
     \begin{multline}
     g_S^*(u) = \left \{ p_{B,S_j}^* h\left ( u,\vartheta_{B,S_j}^*\right )\,, \ j=1,\ldots ,k_{B,S}\right \} 
\backslash\\
\left  \{p_{B_j}^* h\left ( u, \vartheta_{B_j}^*\right )\,,\  j=1,\ldots ,k_B\right \}\,.
     \label{e14kr}
     \end{multline}

\subsection{Процедура последовательного оценивания} %3.2
     
     В основе этого алгоритма лежит предположение, что существует множество 
значений $u$, для которых $g_S(u)=0$ и $g_{B,S}(u)\not= 0$. В данной работе это 
множество~--- область $u\in [b,\,+\infty)$, где $b$~--- некоторое значение, и 
задается оно либо априори (например, исходя из сущности решаемой задачи), 
либо по имеющимся данным (например, как найденная по выборке $X_B$ оценка 
среднего распределения $g_B(u)$ или как найденная по выборке $X_B$ оценка 
квантиля распределения $g_B(u)$ некоторого порядка). Выбирая тем или иным 
способом $b$, необходимо быть только уверенным, что $g_S(u)\approx 0$ при 
$u>b$.
     
     Тогда нахождение требуемых оценок осуществляется с помощью 
следующих шагов
(\textbf{алгоритм А2}):


\noindent
\begin{enumerate}[1.]
\item Нахождение значения $p^*$ (см.\ далее~(\ref{e15kr})).
\item Оценивание с помощью ЕМ-алгоритма па\-ра\-мет\-ров смеси~(\ref{e11kr}) 
по выборке~$X_{B,S}$.
\item Оценивание с помощью EM-алгоритма па\-ра\-мет\-ров смеси для $g_B^*(u)$ по 
выборке~$X_B$ путем коррекции только весов элементов смеси, найден\-ной на 
2-м шаге.
\item Оценивание параметров смеси для $g_B^*(u)$ путем <<вычитания>> из 
оценки $g_{B,S}^*$ взвешенной оценки $p^* g_B^*(u)$. 
\end{enumerate}

\bigskip
     Пусть $1-\overline{G}_{B,S}(u)$~--- функция распределения для 
$g_{B,S}(u)$, а $1-\overline{G}_B(u)$~--- для $g_B(u)$. Тогда имеем следующие 
соотношения:
     \begin{multline*}
     \overline{G}_{B,S}(b) =\int\limits_{b}^{+\infty} g_{B,S}(u)\,du = 
     \int\limits_b^{+\infty} \left (\left ( 1-p\right ) g_S(u)\right. +{}\\
\left.     {}+
     pg_B(u)\right )\,du\approx
      \int\limits_b^{+\infty} pg_B(u)\,du = p\overline{G}_B(b)\,.
     \end{multline*}
Отсюда получаем:
\begin{align}
\widetilde{p}^* & = \fr{\overline{G}_{B,S}^*(b)}{\overline{G}_B^*(b)}\,;\notag\\[-6pt]
&\label{e15kr}\\[-6pt]
p^* & = 
\begin{cases}
\widetilde{p}^*\,, & \overline{G}_{B,S}^*(b)\not= 0\ \mbox{и}\ \widetilde{p}^*\leq 
1\,,\\
1\,, & \mbox{в иных случаях}\,,
\end{cases}\notag
\end{align}
      где оценка $\overline{G}_{B,S}^*(b)$ получена по выборке~$X_{B,S}$, а 
$\overline{G}_B^*(b)$~--- по выборке~$X_B$.
     
     После выполнения 2-го и 3-го шагов алгоритма~А2 получены следующие 
два разложения:
     \begin{align*}
     g_{B,S}^* (u) & = \sum\limits_{j=1}^{k_{B,S}}p_{B,S_j}^* h\left (u, 
\vartheta_{B,S_j}^*\right )\,;\\
     g_{B}^* (u) & = \sum\limits_{j=1}^{k_{B,S}}p_{B_j}^* h\left (u, 
\vartheta_{B,S_j}^*\right )\,.
     \end{align*}

Тогда оценка для составляющей $g_S(u)$ принимает вид:
$$
g_S^*(u) = \sum\limits_{j=1}^{k_{B,S}}p_{S_j}^* h(u,\vartheta_{B,S_j})\,,
$$
где для нахождения  $p_{S_j}^*$ необходимо последовательно получить 
следующее:
\begin{align*}
\widetilde{p}_{S_j}^* & =
\begin{cases}
p_{B,S_j}^* -p^* p_{B_j}^*\,, & p_{B,S_j}^*-p^* p_{B_j}^* >0\,,\\
0\,, & p_{B,S_j}^* - p^* p_{B_j}^*\leq 0\,,
\end{cases} \\
&\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\mbox{для}\ j=1,\ldots ,k_{B,S}\,;\\
\widetilde{P}_S^* & = \sum\limits_{j=1}^{k_{B,S}}\widetilde{p}_{S_j}^*\,;\\
p_{S_j}^* &= \fr{\widetilde{p}^*_{S_j}}{\widetilde{P}_S^*}\ \mbox{для}\ j=1,\ldots ,k_{B,S}\,.
\end{align*}
Мерой точности такого приближенного решения~(\ref{e1kr}) как уравнения 
относительно $g_S(u)$ может служить следующая величина: 
$$
\varepsilon = \left (1-p^*\right )\sum\limits_{\substack{{j=1,\ldots 
,k_{B,S}}\\{p_{B,S_j}^* -p^*p_{B_j}^* <0}}}\left\vert p^*_{B,S_j} -p^*p_{B_j}^*\right\vert\,.
$$
     
     Для оценки $p^*$ можно, в принципе, построить ее распределение. 
Действительно, оценки $\overline{G}_B^*(b)$ и~$\overline{G}_{B,S}^*(b)$ 
основываются на числе успехов в последовательности испытаний Бернулли: для 
$\overline{G}_B^*(b)$ это $\widetilde{v}$ в~$N$ испытаниях с ве\-ро\-ят\-ностью 
успеха $q$, для $\overline{G}_{B,S}^*(b)$ это $\widetilde{w}$ в $M$ испытаниях 
с ве\-ро\-ят\-ностью успеха~$pq$. Здесь $N$~--- объем выборки~$X_B$ и 
$q=\int\limits_b^{+\infty} g_B(u)\,du$, $M$~--- объем выборки~$X_{B,S}$ и $pq = 
\int\limits_b^{+\infty} g_{B,S}(u)\,du$. 
     
  
Предполагается, что выборки~$X_B$ и~$X_{B,S}$ являются 
независимыми. Тогда совместное распределение числа успехов в двух 
испытаниях Бернулли есть
     \begin{multline*}
     \mathrm{Pr}\{\widetilde{V}=\widetilde{v},\widetilde{W}=\widetilde{w}\}={}\\
     {}= 
     \begin{pmatrix}
     N\\
     \widetilde{v}
     \end{pmatrix} q^{\widetilde{v}}\left (1-q\right ) ^{N-\widetilde{v}}
     \begin{pmatrix}
     M\\ \widetilde{w}
     \end{pmatrix}
     \left ( pq\right )^{\widetilde{w}}\left (1-pq\right )^{M-\widetilde{w}}\,.
     \end{multline*}
Это позволяет в явном виде записать вероятность появления любого допустимого 
значения для $W/V$, где $W = \widetilde{W}/M$ и $v=\widetilde{V}/N$, а 
следовательно, получить все необходимые характеристики случайной величины 
$p^*$, определенной в~(\ref{e15kr}). Асимптотический подход может дать 
более простые результаты. Если через $\Phi (a,c^2)$ обозначить нормальное 
распределение с параметрами $(a,c^2)$, а знак $\Leftarrow$ упо\-треб\-лять между 
обозначениями случайной величины и ее распределением, то при $M,N\rightarrow 
\infty$ имеем: 
\begin{align*}
V & \Leftarrow \Phi\left (q, \fr{q(1-q)}{N}\right )\,;\\
W & \Leftarrow \Phi\left ( pq, \fr{pq(1-pq)}{M}\right )\,.
\end{align*} 
     
     Найдем вероятность $\mathrm{Pr}\{W-uV\leq y\}$ для некоторых значений 
$u$,  $y$. Имеем: 
     $$
     W-uV\Leftarrow \Phi\left ( pq-uq,\fr{pq(1-pq)}{M}+u^2\fr{q(1-q)}{N}\right )
     $$
     и, следовательно, 
     \begin{multline*}
     \mathrm{Pr} \left \{ W-uV\leq y\right \} ={}\\
     {}= \Phi_{0,1}\left ( \fr{y-(pq-
uq)}{\sqrt{pq(1-pq)/M+u^2q(1-q)/N}}\right )\,,
     \end{multline*}
где $\Phi_{0,1}$~--- функция стандартного нормального распределения. Но
$$
\mathrm{Pr}\{W-uV\leq 0\} = \mathrm{Pr}\{W\leq uV\}\approx \mathrm{Pr}\left 
\{\fr{W}{V}\leq u\right \}\,.
$$
Таким образом,
\begin{multline}
\mathrm{Pr}\{P^*\leq u\} \equiv \mathrm{Pr}\left \{\fr{W}{v}\leq u\right 
\}\approx{}\\
{}\approx \Phi_{0,1}\left ( \fr{uq-pq}{\sqrt{pq(1-pq)/M+u^2q(1-
q)/N}}\right )\,.
\label{e16kr}
\end{multline}

    
С помощью~(\ref{e16kr}) можно, в частности, проанализировать 
зависимость свойств оценки~$p^*$ от выбора значения~$q$. В качестве 
выборочных свойств оценки рассмотрим смещение и стандартное отклонение, а 
именно: если $p^*$ есть оценка параметра~$p$, принимающего значение~$p_0$, 
то смещение этой оценки есть $\mathrm{Bias} (p^*) = E\{p^*\} -p_0$; а стандартное 
отклонение~--- $\mathrm{St}D (p^*) =\sqrt{E\{(p^*-E\{p^*\})^2\}}$. Для вычисления 
смещения и стандартной среднеквадратичной ошибки 
используем~(\ref{e16kr}) и приближенный метод вычисления 
соответствующих интегралов. На рис.~1 дан график зависимостей 
введенных выборочных характеристик оценки от значения $q$ для $p_0 = 50\%$, 
$M = N = 800$. Из него, в частности, видно, что с точки зрения качества оценки $p^*$
значение $q$ следует выбирать как можно большим;
но при этом не следует забывать, что с точки зрения коррект-\linebreak\vspace*{-12pt} 

%\begin{figure*} %fig1
\begin{center}
\vspace*{2pt}
\mbox{%
\epsfxsize=76.433mm
\epsfbox{kri-1.eps}
}
\end{center}
\vspace*{1pt}
%\Caption{
{{\figurename~1}\ \ \small{Зависимости свойств оценки $p^*$ от выбора~$q$: \textit{1}~--- 
$\mathrm{Bias} (p^*)$; \textit{2}~--- $\mathrm{St}D(p^*)$}}
%\label{f1s}}
%\end{figure*}
\bigskip
\addtocounter{figure}{1}  

\noindent
ности отделения $g_S(u)$ и $g_B(u)$ 
значение~$q$ следует выбирать как можно меньшим (соответствующее значение 
$b$ должно быть как можно большим).

\section{Эксперименты}

Цель экспериментов состояла в сравнительном анализе свойств 
построенных алгоритмов. В качестве модели обрабатываемых данных была 
выбрана смесь распределений $g_B(u)$ и $g_S(u)$. Каждая со\-став\-ля\-ющая 
$g_B(u)$ и $g_S(u)$ описывалась, в свою очередь, с помощью смесей нормальных 
распределений следующего общего вида: 
     \begin{align*}
     g_B(u) & = \sum\limits_{j=1}^{16} p_{B_j}h\left(u,a_{B_j},c_{B_j}\right)\,;\\
     g_S(u) & = \sum\limits_{j=1}^6 p_{S_j} h\left(u, a_{S_j}, c_{S_j}\right)
     \end{align*}
     (значения параметров приведены в табл.~\ref{t1kr}); с по\-мощью этих 
смесей аппроксимировались распределения реальных данных, встречающихся 
при распознавании зашумленного изображения текста. 

     
     Качество процедур расщепления смеси на две составляющие 
характеризовалось выборочными свойствами оценок веса $p^*$ и порога~$T^*$, 
а также временем обработки выборок. Параметрами задачи сравнительного 
анализа свойств алгоритмов являлись следующие: значение~$p_0$, значение 
объема выборок (объемы~$X_{B,S}$ и~$X_B$ здесь и далее брались равными), 
значение $k$~--- числа элементов смеси, принятого для описания выборочных 
распределений. Для обеих процедур многократно проводились эксперименты по 
моделированию выборок из $g_{B,S}(u) = (1-p_0)g_S(u) +p_0g_B(u)$ и 
построению оценок $p^*$(A1) и $p^*$(A2). После получения\linebreak\vspace*{-12pt}
\pagebreak

\end{multicols}

\begin{table}\small
\begin{center}
\Caption{Параметры моделей
\label{t1kr}}
\vspace*{2ex}

\begin{tabular}{|r|c|c|c|c|c|c|}
\hline
$j$&\multicolumn{3}{c|}{$g_B(u)$ }&\multicolumn{3}{c|}{$g_S(u)$}\\
\cline{2-7}
&&&&&&\\[-9pt]
&$p_{B_j}$&$a_{B_j}$&$c_{B_j}^2$&$p_{S_j}$&$a_{S_j}$&$c_{S_j}^2$\\
&&&&&&\\[-9pt]
\hline
1&0,01&120,280&45,085\hphantom{9}&0,15&\hphantom{9}77,000&\hphantom{9}0,050\\
2&0,02&129,915&26,361\hphantom{9}&0,20&\hphantom{9}86,263&28,335\\
3&0,02&135,846&5,493&0,35&103,902&70,201\\
4&0,08&143,709&6,024&0,15&120,280&45,085\\
5&0,11&150,824&2,660&0,10&129,915&26,361\\
6&0,04&153,636&0,239&0,05&135,846&\hphantom{9}5,493\\
7&0,08&156,556&0,264&&&\\
8&0,05&158,000&0,050&&&\\
9&0,07&159,000&0,050&&&\\
10&0,08&160,000&0,050&&&\\
11&0,08&161,000&0,050&&&\\
12&0,10&162,000&0,050&&&\\
13&0,08&164,000&0,050&&&\\
14&0,05&165,000&0,050&&&\\
15&0,05&166,000&0,050&&&\\
16&0,08&167,418&0,482&&&\\
\hline
     \end{tabular}
     \end{center}
     \vspace{6pt}
     \end{table}
     
     \begin{figure}[b] %fig2
\vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=160.172mm
\epsfbox{kri-2.eps}
}
\end{center}
\vspace*{-9pt}
     \Caption{Сравнительные характеристики качества оценок $p^*$ и~$T^*$
     \label{f2kr}}
     \end{figure}


\begin{multicols}{2}

\noindent
 100~пар этих 
значений по ним строились оценки для $\mathrm{Bias}(p^*(\mathrm{A1}))$ и 
$\mathrm{Bias}(p^*(\mathrm{A2}))$, $\mathrm{St}D(p^*(\mathrm{A1}))$ и 
$\mathrm{St}D(p^*(\mathrm{A2}))$ (с целью упрощения записи далее под $\mathrm{Bias}(\ldots)$ 
и $\mathrm{St}D(\ldots)$ понимаются их оценки). Далее в плоскости значений 
$(\mathrm{abs}(\mathrm{Bias}(p^*(\mathrm{A2}))) -$ %\linebreak 
$-\;\mathrm{abs}(\mathrm{Bias}(p^*(\mathrm{A1})))$, $\mathrm{St}D(p^*(\mathrm{A2})) - 
\mathrm{St}D(p^*(\mathrm{A1})))$ для $p_0 = 25\%$, 50\%, 75\%, $N = M = 200$, 800, 
3\,200, 12\,800, $k = 8$, 12, 16 отмечались точки, соответствующие результатам 
применения двух процедур. 
{%\looseness=1

}

Аналогичные действия проводились для 
оценок~$T^*$; полученные результаты представлены в графическом виде на 
рис.~\ref{f2kr}. Из них на множестве рассмотренных вариантов видно 
преимущество последовательного оценивания (процедура~А2) по сравнению с 
процедурой одновременного оценивания (процедура~А1) параметров %\linebreak 
расщепленной смеси, так как показатели качества оценок в основном выше 
для~А2 по сравнению с~А1.
{%\looseness=1

}


     Кроме того, временные показатели~А2 также лучше, чем у~А1, что 
отображено на рис.~\ref{f3kr}, где в плоскости (log(сред\-нее\;вре\-мя\;А1), 
log(сред\-нее\;вре\-мя\;А2)) точками отмечены перечисленные варианты 
моделирования и обработки данных. Таким образом, предпочтение следует отдать 
последовательной процедуре оценивания параметров расщепленной смеси.

     
%     \begin{multicols}{2}

    
     Самостоятельный интерес представляет поведение качества оценок при 
увеличении объема выборок, соответствующие результаты представлены на 
рис.~\ref{f4kr} и~\ref{f5kr}. 

    
     Обращает на себя внимание <<необычное>> поведение свойств оценок при 
различных значени-\linebreak 
\vspace*{-12pt}
\pagebreak
\end{multicols}

\begin{figure} %fig3
\vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=162.586mm
\epsfbox{kri-3-4.eps}
}
\end{center}
\vspace*{-9pt}
\begin{minipage}[t]{82mm}
%\vspace*{-9pt}
\Caption{Сравнительные характеристики среднего времени расщепления смеси
\label{f3kr}}
%\end{figure*}
\end{minipage}
\hfill
\begin{minipage}[t]{76mm}
%\begin{figure*} %fig4
%\vspace*{1pt}
%\begin{center}
%\mbox{%
%\epsfxsize=73.966mm
%\epsfbox{kri-4.eps}
%}
%\end{center}
%\vspace*{-9pt}
     \Caption{Зависимости свойств оценки $p^*$ от объема выборок~$N$: 
\textit{1}~--- $\mathrm{Bias} (p^*)$; \textit{2}~--- $\mathrm{St}D(p^*)$
     \label{f4kr}}
%     \end{figure*}
\end{minipage}
     \vspace*{6pt}
\end{figure}

     \begin{figure} %fig5
     \vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=165.777mm
\epsfbox{kri-5.eps}
}
\end{center}
\vspace*{-9pt}
     \Caption{Зависимости свойств оценки $T^*$ от объема выборок~$N$:
     \textit{1}~--- $k=8$; \textit{2}~--- $k=12$; \textit{3}~--- $k=16$
     \label{f5kr}}
     \vspace*{6pt}
     \end{figure}
     
     \begin{multicols}{2}


\noindent 
ях чис\-ла элементов смеси~$k$. Представляется, что увеличение этого параметра должно приводить к улучшению аппроксимации и 
поэтому к более качественным результатам (заметим, что оценка $p^*$ не зависит 
от~$k$). В связи с чем отдельно проводилось моделирование зависимости 
свойств~$T^*$ от числа элементов смеси (рис.~6). Полученные 
результаты позволяют сформулировать следующее предположение: существует 
некоторое оптимальное значение~$k$, позволяющее наилучшим образом решить 
задачу аппроксимации распределений~$g_B(u)$ и~$g_S(u)$. При этом, к счастью, 
для практических применений увеличение значения~$k$ несущественно 
сказывается на свойствах получающихся оценок.


\section{Заключение}

Полученные решения задачи расщепления смеси на две составляющие 
представляют самосто-\linebreak\vspace*{-12pt}

%\bigskip
%\begin{figure} %fig6
\begin{center}
\vspace*{1pt}
\mbox{%
\epsfxsize=76.855mm
\epsfbox{kri-6.eps}
}
\end{center}
\vspace*{2pt}
%\Caption{
{{\figurename~6}\ \ \small{Зависимости свойств оценки $T^*$ от числа элементов смеси~$k$: 
\textit{1}~--- $\mathrm{Bias} (T^*)$; \textit{2}~--- $\mathrm{St}D(T^*)$}}
%\label{f1s}}
%\end{figure*}
%\smallskip
\bigskip
\addtocounter{figure}{1}  

\noindent
ятельный интерес, достаточны для их применения на 
практике. 
Проведенные эксперименты не только продемонстрировали 
работоспособность предложенных решений, но и дали схемы исследования в духе 
бутстреп-метода свойств оценок, сущест\-вен\-но зависящих от множества таких 
параметров, как объемы выборок, значения $q$ (или~$b$, где 
$q=\int\limits_b^{+\infty} g_B(u)\,du$), параметры ЕМ-алгоритма (число элементов 
смеси~$k$, критическое значение дис\-пер\-сии $c^2_{\min}$, максимальное число 
итераций, ошибка нахождения максимума функции правдоподобия).
{%\looseness=1

}
     
{\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{9}
\bibitem{1kr}
\Au{Кривенко М.\,П.}
Распознавание элементов изображения, имеющих различные размеры~// Системы 
и средства информатики.~--- М.: ИПИ РАН, 2007. Вып.~17. С.~30--51.

\label{end\stat}

\bibitem{2kr}
\Au{McLachlan G.\,J., Peel~D.}
Finite mixture models.~--- New York: Wiley, 2000.  419~p.
\end{thebibliography}
}
}
\end{multicols} 
 
 
 
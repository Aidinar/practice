\def\stat{donskoy}

\def\tit{ИЗВЛЕЧЕНИЕ ОПТИМИЗАЦИОННЫХ МОДЕЛЕЙ ИЗ~ДАННЫХ}

\def\titkol{Извлечение оптимизационных моделей из данных}

\def\aut{В.\,И.~Донской$^1$}

\def\autkol{В.\,И.~Донской}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Донской В.\,И.}
\index{Donskoy V.\,I.}
 

%{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
%{Работа выполнена при финансовой поддержке РФФИ (проект 19-07-00352);
%исследования проводились в~рамках программы Московского Цент\-ра 
%фундаментальной и~прикладной математики.}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Крымский федеральный университет им.\ В.\,И.~Вернадского, 
\mbox{vidonskoy@mail.ru}}

\vspace*{3pt}


\Abst{Изложены основные принципы, методы и~алгоритмы, 
представляющие новую информационную технологию  извлечения 
оптимизационных математических моделей из данных (ИОМД). 
Эта технология позволяет автоматически строить математические 
модели планирования и~управления  на основе использования массивов 
прецедентов (наблюдений) над объектами управления и~внешней средой, 
что дает возможность решать задачи интеллектуального управления и~определять 
целесообразное поведение экономических и~других объектов в~сложных средах. 
Технология  ИОМД позволяет получать объективные модели управления, 
отражающие реально существующие связи, цели, ограничения и~процессы. 
В~этом заключается ее главное преимущество по сравнению с~традиционным, 
субъективным подходом к~управлению. Разработаны линейные и~нелинейные 
алгоритмы синтеза моделей по прецедентной информации.}

\KW{машинное обучение; извлечение моделей из данных; оптимизация; 
нейронные сети; градиентные методы}


\DOI{10.14357/19922264200316} 
 
\vspace*{-6pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}



\section{Введение}

\vspace*{-3pt}


В данной работе изложены математические модели и~алгоритмы, 
составляющие основной аппарат \textit{информационной технологии извлечения 
оптимизационных моделей из эмпирических данных} (обучающих выборок)  
в~рамках парадигмы \textit{неклассического информационного моделирования}, 
сложившейся в~научных работах Ю.\,И.~Журавлева, К.\,В.~Рудакова, Вл.\,Д.~Мазурова, 
а~также их учеников и~последователей~[1--5], к~которым относится и~автор 
данной статьи.
%
Исследования в~указанном направлении предполагают применение методов 
машинного обучения для построения целевых функций и~ограничений  по 
достоверным обучающим выборкам~--- эмпирической информации, отражающей 
регулярные явления, объекты и~процессы~[6--10].

 При таком подходе 
\textit{построенные оптимизационные модели оказываются согласованными 
с~реальной информацией о~моделируемых объектах} в~отличие от 
субъективных математических моделей, которые обычно предлагаются экспертами. 
Точные решения задач, соответствующих субъективным моделям,  вследствие 
субъективности экспертов могут на практике давать ошибки, превышающие 
ошибки принятия решений на основе согласованных моделей, извлеченных 
из объективных эмпирических данных.

Будем обозначать $X^n\hm=\mathcal{X}_1\times \cdots \times \mathcal{X}_i 
\times\cdots \times \mathcal{X}_n$  ограниченную область в~$\mathbb{R}^n$, 
на\-зы\-ва\-емую пространством  признаков размерности~$n$;  $X$ 
или~$\tilde{x}\hm=(x_1, \ldots , x_i, \ldots , x_n)$~--- 
произвольную точку в~пространстве признаков, служащую описанием\linebreak  
допус\-ти\-мо\-го объекта.
Каждая координата~$x_i$ описания объекта $\tilde{x}$
принадлежит зафиксированному\linebreak \textit{ограниченному множеству 
допустимых значений}:  $m_i\hm \leq x_i \hm\leq M_i$.
Будем далее обозначать %\linebreak
$\mathfrak{T}_{\mathrm{Opt}}\hm=\{ (\tilde{a}_j,\gamma_j,y_j)\}_{j=1}^l$ 
\textit{стандартную}\linebreak достоверную эмпирическую выборку  для 
\textit{задачи  обучения~--- извлечения модели  на основе частичной  
информации о некотором существующем, но неизвестном скалярном критерии 
$F:X^n \hm\rightarrow \mathbb{R}$ и~неизвестных ограничениях, 
которые формально могут быть представлены в~виде $\Omega(\tilde{x})\hm=0$}. 
Полагается, что множество~$X^n$ разбито  на два класса: класс~$\mathfrak{L}_0$, 
состоящий из точек, заведомо удовлетворяющих\linebreak некоторой  системе ограничений 
задачи наилучшего выбора, и~класс~$\mathfrak{L}_1$, содержащий точки, 
заведомо не удовлетворяющие этой системе ограничений. Тогда 
$\Omega: X^n \hm\rightarrow \{0;1 \}$~--- 
\textit{\mbox{характеристическая} функция ограничений, которая частично 
задана обуча\-ющей выборкой}; $\Omega(\tilde{x})\hm=0 
\hm\Leftrightarrow \tilde{x}\hm\in \mathfrak{L}_0$; 
$\Omega(\tilde{x})\hm=1 \hm\Leftrightarrow \tilde{x}\hm\in \mathfrak{L}_1$. 
Будем полагать, что в~стандартной обучающей выборке~$\mathfrak{T}_{\mathrm{Opt}}$ 
содержится достоверная информация  $\gamma_j\hm=\Omega(\tilde{a}_j)$, 
$\gamma_j \hm\in \{0;1 \}$; $y_j\hm=F(\tilde{a}_j)$, $\tilde{a}_j\hm=
(a_{j1},\dots, a_{jn})\hm\in X^n$.

В процессе обучения следует построить алгоритм, позволяющий выбрать 
такое решение~$\hat{\tilde{x}}^{*}$, которое удовлетворяло 
бы ограничениям ($\Omega(\hat{\tilde{x}}^{*})\hm=0$) и~при этом 
значение~$F(\hat{\tilde{x}}^{*})$ было бы как можно б$\acute{\mbox{о}}$льшим (или меньшим~--- 
по смыслу задачи).

 \renewcommand{\figurename}{\protect\bf Алгоритм}
\setcounter{figure}{0}

\begin{figure*}[b] %алгоритм1
\vspace*{-12pt}
\Caption{Поиск исходного набора отделяющих гиперплоскостей}
\vspace*{2ex}

{\small 
\begin{center}
\begin{tabular}{lp{140mm}}
\hline
1: &Положить пустым множество отобранных гиперплоскостей: 
$\mathfrak{L}:=\emptyset$.\\
&Очистить  пометки всех точек множества $W_1$. \ \ \ $i:=1$.\\
2: &Выбрать из множества~$W_1$ непомеченную точку~$X$, ближайшую к~множеству~$W_0$,
и пометить ее.\\
3: &Используя ПЛКР, построить гиперплоскость~$\mathcal{L}_i$, 
отделяющую эту точку~$X$ от всех точек
множества~$W_0$.  $i:=i+1$.\\
4: &Пометить все такие непомеченные точки множества~$W_1$, 
которые (как и~точка~$X$) оказались
отделенными гиперплоскостью~$\mathcal{L}_i$ от множества~$W_0$.\\
5: &Запомнить построенную гиперплоскость: $\mathfrak{L}:=\mathfrak{L} 
\cup {\mathcal{L}_i}$.\\
6: &Если в~множестве $W_1$ остались непомеченные точки, перейти к~2.\\
7: &Конец алгоритма. Построен набор гиперплоскостей 
$\mathfrak{L}:=\{ \mathcal{L}_1,\dots,\mathcal{L}_s\}$,  где $s\hm=i-1$.\\
\hline
\end{tabular}
\end{center}
}
\end{figure*}


\textit{В рассматриваемых  задачах критерий $F$ и~ограничения 
$($характеристическая функция~$\Omega)$ не заданы точно. Они отражены 
в~наборе данных~$\mathfrak{T}_{\mathrm{Opt}}$ и~являются частично заданными}.

\textit{Постановка задачи} состоит в~следующем. Требуется, используя 
частичную начальную информацию~$\mathfrak{T}_{\mathrm{Opt}}$, 
синтезировать модель 
и,~соответственно, алгоритм, позволяющий выбрать решение~$\hat{\tilde{x}}^{*}$, 
как можно более близкое к~оптимальному решению~$\tilde{x}^{*}$, 
определяемому неизвестными, но существующими истинными объектами~$F$ и~$\Omega$, 
которые должны быть аппроксимированы в~процессе машинного обучения.
Если скалярный критерий и~характеристическая функция ограничений 
аппроксимируются независимо друг от друга отдельными алгоритмами, 
вычисляющими как можно более точные в~ка\-ком-ли\-бо смыс\-ле 
приближения~$\hat{F}$ и~$\hat{\Omega}$, то 
\textit{восстановленная по обучающей выборке задача 
$($извлеченная математическая модель$)$ нахождения наилучшего 
решения имеет следующий вид}: 
$$
\max(\min) \ \hat{F}(\tilde{x}): 
\hat{\Omega}(\tilde{x})=0 \wedge  \tilde{x}\hm\in X^n\,.
$$

Полученная в~результате машинного обучения пара функций 
$\langle \hat{F},\hat{\Omega}\rangle$ называется 
\textit{эмпирической информационной моделью}.

\section{Извлечение линейных оптимизационных моделей}

Может оказаться, что знания о проблемной области заведомо определяют 
целесообразность\linebreak извлечения линейной модели с~вещественными переменными. 
Тогда можно сразу переходить к~решению задачи, применяя  методы, 
изложенные в~данном разделе. Если же исходной информации\linebreak о~линейности нет, 
то сначала следует проверить: \textit{не противоречат ли данные 
выборки~$\mathfrak{T}_{\mathrm{Opt}}$ гипотезе о~линейности неизвестных объектов~$F$ 
и~$\Omega$, породивших эту выборку}. Гипотезу о~линейности целевой 
функции можно проверить при помощи алгоритма, представленного в~работе~\cite{8-don}. 
Гипотеза о~ли\-ней\-ности ограничений проверяется в~процессе по\-стро\-ения 
отделяющих ги\-пер\-плос\-ко\-стей алгоритмом~1. Если данные противоречат 
гипотезе линейности, то следует применять методы извлечения нелинейных 
моделей, в~част\-ности основанные на обучении нейронных сетей.

В случае извлечения линейной оптимизационной модели с~вещественными 
переменными из данных обучающей выборки  
$\mathfrak{T}_{\mathrm{Opt}}\hm=\{ (X_k,\gamma_k,y_k)\}_{k=1}^l$ 
  неизвестными полагаются и~линейная целевая функция
  $$
  f(X)=\langle 
  \Lambda,X \rangle = \lambda_1x_1+\dots + \lambda_ix_i
  +\dots +\lambda_nx_n +\lambda_0\,,
  $$
    и~линейные ограничения
\begin{multline*}
 AX \leq B;\quad
   A=\left[a_{ji}\right]_{m \times n}\,,\\
 X=\left[x_1,\dots,x_i,\dots,x_n\right]^{\mathrm{T}},\
B=\left[b_1,\dots,b_j,\dots,b_m\right]^{\mathrm{T}}\!.\hspace*{-4.8543pt}
\end{multline*}
В~результате синтеза модели путем машинного обуче\-ния предполагается 
получить аппроксимации~$\hat{A}$ и~$\hat{B}$ матрицы~$A$ и~вектора~$B$. 
Значения~$\gamma_k$ выделяют точки обучающей выборки, соответствующие 
допустимым решениям, т.\,е.\ заведомо удовлетворяющие неравенствам 
$ AX \hm\leq B$,  среди остальных точек выборки, не являющихся допустимыми. 
Иначе говоря, исходная информация представлена в~форме 
\begin{multline*}
f(X_k)=y_k\,;\enskip 
(\gamma_k = 0)\Leftrightarrow AX_k \leq B\,;\\ 
(\gamma_k = 1)\Leftrightarrow AX_k > B;\enskip k\in \{ 1,\dots,l\}.
\end{multline*}

Обозначим 
$$
W_0=\left\{ X_k: \gamma_k=0 \right\}\,;\enskip
W_1=\left\{ X_k: \gamma_k=1\right\}.
$$

{\bfseries\textit{Построение линейных отделяющих гиперплоскостей}} 
проводится при помощи обучающей процедуры линейной коррекции 
Ро\-зен\-блат\-та--Но\-ви\-ко\-ва (ПЛКР)~\cite{11-don}:

\noindent
\begin{multline*}
\Lambda_{t+1}={}\\
{}=
\begin{cases}
\Lambda_t, & \!\!\mbox{если } (\langle \Lambda_t,X_t\rangle> 0)\wedge 
( X_t\in W_1 )\vee{}\hspace*{-3.45738pt}\hspace*{-0.18182pt}\\
&\hspace*{14pt}{}\vee (\langle \Lambda_t,X_t \rangle \leq 0)\wedge( X_t\in W_0 );\\
\Lambda_t+cX_t, & \!\!\mbox{если } (\langle \Lambda_t,X_t\rangle  \leq 0)\wedge (X_t\in W_1 ); \hspace*{-0.18182pt}\\
\Lambda_t-cX_t, & \!\!\mbox{если } (\langle \Lambda_t,X_t\rangle  > 0)\wedge (X_t\in W_0 ),\hspace*{-0.18182pt}
\end{cases}
\end{multline*}
где $0<c\leq 1$. К~точкам обучающей выборки, предъявляемым на шагах 
 $t\hm=0,1,2,\dots$, добавляется дополнительная координата с~единичным 
 значением: 
 \begin{multline*}
 X_t=\left(x_1^t,\dots,x_n^t,1 \right);\\  
\Lambda_t=\left\langle \lambda_1^t,\dots,\lambda_n^t,\lambda_{n+1}^t\right);\enskip 
 \Lambda_0  = (0,\ldots,0,0)\,.
 \end{multline*}
Если выпуклые оболочки конечных множеств~$W_0$ и~$W_1$ не пересекаются, 
то найдется такой единичный вектор~$\Lambda^{*}$ и~такое 
положительное вещественное число~$\rho$, что $\langle \Lambda^{*},X\rangle 
\hm< -\rho $ для $X\hm\in W_0$  и~$\langle \Lambda^{*},X\rangle \hm> \rho $ 
для $X\hm\in W_1$~\cite{11-don}.

При заведомой линейности модели каждая точка множества~$W_1$ 
может быть отделена некоторой  гиперплоскостью от всех точек множества~$W_0$. 
Идея синтеза аппроксимации области допустимых решений состоит в~нахождении  
исходного набора гиперплоскостей, в~совокупности отделяющих все точки 
 множества~$W_1$ от множества~$W_0$, с~по\-сле\-ду\-ющим выделением 
 из этого набора минимального числа гиперплоскостей, обеспечивающих 
 такое же совокупное отделение.
 

После выполнения алгоритма~1 осуществляется выбор минимального 
подмножества гиперплоскостей из построенного множества~$\mathfrak{L}$, 
которого достаточно для линейного разделения множеств~$W_0$  и~$W_1$.

Введем логические значения $\beta_{p,j}\hm=1$, если гиперплоскость~$\mathcal{L}_j$ 
отделяет точку $X_p\hm\in W_1$ от всех точек множества~$W_0$; 
 $p\hm=\overline{1,|W_1 |}$,  $j\hm=\overline{1,s}$. 
 Условие отделимости точки~$X_p$ хотя бы одной гипер\-плос\-костью имеет вид:
\begin{equation}
\label{Log1}
\beta_{p,1}\mathcal{L}_1\vee \dots \vee \beta_{p,j}\mathcal{L}_j\vee \dots \vee 
\beta_{l,s}\mathcal{ L}_s.
\end{equation}
Символы $\mathcal{L}_j$, $j\hm=\overline{1,s}$, в~выражении~(\ref{Log1}) 
понимаются \textit{как формальные логические переменные}, 
обозначающие соответствующие гиперплоскости. Если $\beta_{p,j}\hm=1$, 
то гиперплоскость~$\mathcal{L}_j$ включается в~набор гиперплоскостей, 
отделяющих точку~$X_p$, а если $\beta_{p,j}\hm=0$, то не включается. Формула
\begin{equation}
\label{Log2}
\bigwedge\limits_{p=\overline{1,|W_1 |}} 
\left(\beta_{p,1}\mathcal{L}_1\vee \dots \vee \beta_{p,j}\mathcal{L}_j\vee 
\dots \vee \beta_{l,s}\mathcal{ L}_s \right)
\end{equation}
выражает требование отделимости каждой точки множества~$W_1$ от всех 
точек множества~$W_0$.
Выполняя логическое  перемножение  в~формуле~(\ref{Log2}), а~затем~--- 
все возможные поглощения, можно получить все варианты наборов отделяющих 
гиперплоскостей и~выбрать из них кратчайший набор.

\smallskip

\noindent
\textbf{Пример~1}. Пусть $n\hm=2$ и~19~точек представлены 
в~табл.~1 ($W_0$) и~2 ($W_1$).

Множество из семи прямых $\mathcal{L}_1\hm - \mathcal{L}_7$, 
от\-де\-ля\-ющих каждая по построению одну точку с~номером\linebreak\vspace*{-12pt}

{ \begin{center}  %tabl1
 
\noindent
{{\tablename~1}\ \ \small{
Множество точек $W_0$
}}
\end{center}}

{\small %tabl1
\begin{center}
\vspace*{3pt}
  \tabcolsep=17pt
  \begin{tabular}{|c|c|c|}
 \hline
   $ W_0$ & $x_1$ & $x_2$\\
   \hline
   \hphantom{9}1 & 2 &10\hphantom{9}\\
   \hphantom{9}2 & 2&9\\
   \hphantom{9}3 & 3&5\\
   \hphantom{9}4 & 4& 9\\
   \hphantom{9}5 & 4&8\\
   \hphantom{9}6 & 5&2\\
   \hphantom{9}7 & 6&9\\
   \hphantom{9}8 & 6&6\\
   \hphantom{9}9 & 9&8\\
   10 & 9&5\\
   11 & 11\hphantom{9}& 4\\
   12 &12\hphantom{9}&2 \\
\hline
\end{tabular}
\end{center}
}
%\end{table*}

\vspace*{6pt}

{ \begin{center}  %tabl2
 
\noindent
{{\tablename~2}\ \ \small{
Множество точек $W_1$
}}
\end{center}}

{\small %tabl2
\begin{center}
\vspace*{3pt}
  \tabcolsep=17pt
   \begin{tabular}{|c|c|c|}
 \hline
$W_1$ & $x_1$ & $x_2$\\
\hline
13 & \hphantom{9}1&12\\
14 & \hphantom{9}3& 14\\
15 & \hphantom{9}5& 11\\
16 & \hphantom{9}8&14\\
17 & 12&10\\
18 & 12& \hphantom{9}6\\
19 &17& \hphantom{9}5\\
\hline
\end{tabular}
\end{center}
}
%\end{table*}

\vspace*{6pt}

{ \begin{center}  %tabl3
 
\noindent
{{\tablename~3}\ \ \small{
Построенное исходное множество прямых
}}
\end{center}}

{\small %tabl3
\begin{center}
\vspace*{3pt}
     \tabcolsep=5pt
  \begin{tabular}{|c|l|c|}
 \hline
\tabcolsep=0pt\begin{tabular}{c}№\\ точки\\ $p$\end{tabular}& 
\multicolumn{1}{c|}{Отделяющая эту точку прямая}
 &  \tabcolsep=0pt\begin{tabular}{c}Все\\ отделенные\\ точки\end{tabular}    \\
 \hline
13 & $\mathcal{L}_1: 15x_1 - 3x_2 + 2 = 0 $ & 13  \\
%\hline
14 & $\mathcal{L}_2: 6x_1 - 3x_2 + 19 = 0 $ & 13--14  \\
%\hline
15 & $\mathcal{L}_3:  - 5x_2 + 52 = 0 $ & 13--16  \\
%\hline
16 & $\mathcal{L}_4: -8x_1 - 4x_2 + 115 = 0 $ & 16--19  \\
%\hline
17 & $\mathcal{L}_5: -19x_1 - 2x_2 + 244 = 0 $ & 17--19  \\
%\hline
18 & $\mathcal{L}_6: -36x_1 - 2x_2 + 440 = 0 $ & 17--19  \\
%\hline
19 & $\mathcal{L}_7: -18x_1 - 26x_2 + 170 = 0 $ & 19  \\
\hline
\end{tabular}
\end{center}
}
%\end{table*}

\vspace*{9pt}







\noindent   
$p \hm\in \{13,\dots,19\}$ (в~первом столбце таблицы) и,~как 
оказалось после построения, некоторые другие точки множества~$W_1$ 
(третий столбец), представлено в~табл.~3. 
Это множество избыточно.
Логическое выражение, которое позволяет найти все возможные 
несократимые наборы отделяющих прямых,  выписывается непосредственно по 
табл.~4 и~имеет вид:

\vspace*{-2pt}

\noindent
\begin{multline*}
(\mathcal{ L}_1 \vee \mathcal{ L}_2 \vee \mathcal{ L}_3)
\wedge (\mathcal{ L}_2 \vee \mathcal{ L}_3)\wedge \mathcal{ L}_3 
\wedge (\mathcal{ L}_3 \vee \mathcal{ L}_4)\wedge {}\\
{}\wedge (\mathcal{ L}_4 \vee \mathcal{ L}_5 \vee \mathcal{ L}_6)\wedge 
 (\mathcal{ L}_4 \vee \mathcal{ L}_6)\wedge (\mathcal{ L}_4 
 \vee \mathcal{ L}_5\vee \mathcal{ L}_6\vee \mathcal{ L}_7)={}\\
{} =\mathcal{ L}_3\mathcal{ L}_4 \vee \mathcal{ L}_3\mathcal{ L}_6.
\end{multline*}

\vspace*{-2pt}

Пара прямых $\mathcal{ L}_3\mathcal{ L}_4$ более предпочтительна, 
поскольку прямая~$\mathcal{ L}_4$ отделяет~4~точки, а прямая~$\mathcal{ L}_6$~--- 
три точки. Прямая~$\mathcal{ L}_3$ отделяет точки $\{X_{13}, X_{14}, 
X_{15}, X_{16}\} \hm=\Omega_3$ от множества~$W_0$. 
Вновь используя процедуру ПЛКР, находим уточненную\linebreak\vspace*{-12pt}

\pagebreak


{ \begin{center}  %tabl4
 
\noindent
\parbox{70mm}{{{\tablename~4}\ \ \small{
Логическая таблица отделимости множества~$W_0$
}}}
\end{center}}

{\small %tabl4
\begin{center}
\vspace*{-3pt}
     \begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\raisebox{-6pt}[0pt][0pt]{Прямые}}
 & \multicolumn{7}{c|}{Точки} \\
                                \cline{2-8}
                        & 13  &  14 & 15  & 16  &  17 & 18  &  19 \\
\hline
$\mathcal{L}_1$ & 1  &   &   &   &  &  &  \\
%\hline
$\mathcal{L}_2$ & 1  & 1  &   &   &  &  &  \\
%\hline
$\mathcal{L}_3$ & 1  &  1 & 1   & 1  &  & &  \\
%\hline
$\mathcal{L}_4$ &   &   &   & 1  & 1 & 1 & 1 \\
%\hline
$\mathcal{L}_5$ &    &   &   &   & 1 &  & 1 \\
%\hline
$\mathcal{L}_6$ &    &   &   &   & 1 & 1 & 1 \\
%\hline
$\mathcal{L}_7$ &    &   &   &   &  &  & 1 \\
\hline
\end{tabular}
\end{center}
}
%\end{table*}

\vspace*{3pt}

\renewcommand{\figurename}{\protect\bf Рис.}
\setcounter{figure}{1}

{ \begin{center}  %fig1
 \vspace*{1pt}
    \mbox{%
 \epsfxsize=79mm 
 \epsfbox{don-1.eps}
 }


\vspace*{3pt}

\noindent
{{\figurename~1}\ \ \small{
Построенные отделители 
}}
\end{center}}


\vspace*{9pt}


\noindent
  прямую,
 разделяющую множества~$\Omega_3$
и $W_0$:  $\mathcal{L}_3^{*}$:  $2x_1\hm-17x_2\hm+167=0$. 
Аналогично находим  $\mathcal{ L}_4^{*}$: $-12x_1\hm-6x_2\hm+158=0$. 
Построенная область до\-пус\-ти\-мых решений представлена на рис.~1.

\textit{Начальная обучающая  информация не противоречит гипотезе 
линейности ограничений, если ПЛКР обеспечивает нахождение отделяющего 
множества гиперплоскостей. Если число шагов коррекции превышает 
заданную величину, указанная линейная гипотеза отвергается}.

{\bfseries\textit{Построение линейной целевой функции}} 
$$
\hat{f}(X)=\langle \hat{\Lambda},X \rangle = \hat{\lambda}_1x_1
+\dots + \hat{\lambda}_ix_i+\dots +\hat{\lambda}_nx_n 
+\hat{\lambda}_0
$$ 
по обучающей выборке $\mathfrak{T}_{\mathrm{Opt}}\hm=
\{ (X_k,\gamma_k,y_k)\}_{k=1}^l$  несложно осуществить по методу 
наименьших квад\-ра\-тов. Но более предпочтительной для этой цели 
представляется линейная SVM (support vector machine) ре\-грес\-сия~\cite{12-don}, применение 
которой сводится к~оптимизационной задаче
$$
\fr{1}{2}\,\langle \hat{\Lambda},\hat{\Lambda} \rangle + C \sum_{k=1}^l
(\xi_k^{+}+\xi_k^{-}) \rightarrow 
\underset{\hat{\Lambda},\hat{\lambda}_0,\tilde{\xi}_k^{+},\tilde{\xi}_k^{-}} {\min}; 
$$

\noindent
\begin{gather*}
y_k-\varepsilon -\xi_k^{-} \leq \langle \hat{\Lambda},X_k\rangle + \hat{\lambda}_0 \leq
y_k + \varepsilon + \xi_k^{+}; \\
\xi_k^{+} \geq 0; \enskip \xi_k^{-} \geq 0; \enskip k=1,\dots,l,
\end{gather*}
где $C$~--- параметр регуляризации;  $\xi_k^{+}$ и~$\xi_k^{-}$~--- 
дополнительные переменные, характеризующие величину ошибки в~точках~$X_k$; 
$\varepsilon$~--- параметр задачи.
SVM-ре\-грес\-сия обеспечивает минимизацию нормы вектора~$\hat{\Lambda}$, 
что приводит к~уменьшению верхней оценки колмогоровской сложности~\cite{6-don}  
синтезируемой модели и~увеличению оценки неслучайности ее извлечения из данных.

\vspace*{-6pt}

\section{Извлечение нелинейных оптимизационных моделей}

\renewcommand{\figurename}{\protect\bf Рис.}
\setcounter{figure}{1}



Если имеется априорная информация о нелинейности искомой модели 
или в~результате анализа обучающей информации гипотеза линейности 
отвергается, то полезным инструментом для извлечения эмпирических 
информационных моделей становятся  нейронные сети. Известно, что 
нейронная сеть даже с~одним скрытым слоем позволяет 
аппроксимировать с~необходимой точностью широчайший класс 
функций, включая нелинейности, вообще говоря, любого вида~\cite{13-don}.

 \renewcommand{\figurename}{\protect\bf Алгоритм}
\setcounter{figure}{1}

\begin{figure*}
\vspace*{-6pt}
\Caption{Поиск условного (локального) минимума  по $\hat{F}_{NN1}$ 
и~$\hat{\Omega}_{NN2}(\tilde{x})$}
\vspace*{2ex}

{\small 
\begin{center}
\begin{tabular}{lp{130mm}}
\hline
&\\[-9pt]
Вход: & обучающая выборка $\mathfrak{T}_{\mathrm{Opt}}
\hm=\{ (\tilde{a}_j,\gamma_j,y_j)\}_{j=1}^l$ и~нейронные 
аппроксимации~$\hat{F}_{NN1}$
и~$\hat{\Omega}_{NN2}(\tilde{x})$.\\
Выход: &
$\tilde{x}^{*}: \hat{F}_{NN1}(\tilde{x}^{*})\approx 
\min \hat{F}_{NN1}(\tilde{x}) /\hat{\Omega}_{NN2}(\tilde{x})\hm = 0$~--- 
точка экстремума эмпирической
информационной модели и~значение~$y^{*}$ функции~$\hat{F}_{NN1}$ в~этой точке.\\
\hline
\hphantom{9}1: &Взять из обучающей выборки точку $\tilde{x}_0\hm=\tilde{a}_{j^{*}}: 
y_{j^{*}}= \min_j y_j$  в~качестве
начального приближения и~вычислить~$\hat{F}_{NN1}(\tilde{x}_{0}) $.
\\
\hphantom{9}2: &Выбрать начальное значение $\eta_0$ и~значение $\varepsilon$.
\\
\hphantom{9}3: &Выбрать скорость уменьшения шага $\delta$: $0{,}8 < \delta <1$.
\\
\hphantom{9}4: &\textbf{for} $t:= 1,2, 3,\dots $ \textbf{do}
\\
\hphantom{9}5: &\hspace*{5mm}$ \tilde{x}_t:=\tilde{x}_{t-1} -\eta_{t-1} \ 
\mathrm{grad} \ \hat{F}_{NN1}(\tilde{x}_{t-1}) $;\\
\hphantom{9}6: &\hspace*{5mm}$\eta_t:=\eta_{t-1}\cdot \delta$;\\
\hphantom{9}7: &\hspace{5mm}\textbf{if} $\hat{\Omega}_{NN2}(\tilde{x_t})=1$ 
\textbf{then} \textbf{goto}~10;\\
\hphantom{9}8: &\hspace{5mm}\textbf{if} $\|\hat{F}_{NN1}(\tilde{x}_{t})
  - \hat{F}_{NN1}(\tilde{x}_{t-1})  \| < \varepsilon $ \textbf{then  goto}~11;\\
\hphantom{9}9: &\textbf{end for};\\
10: &$\tilde{x}^{*}:= \tilde{x}_{t-1}$; \ $y^{*}:=\hat{F}_{NN1}(\tilde{x}_{t-1})$; 
\textbf{stop.}\\
11: &$\tilde{x}^{*}:= \tilde{x}_t$; \ $y^{*}:=\hat{F}_{NN1}(\tilde{x}_t)$; 
\textbf{stop.}\\
\hline
\end{tabular}
\end{center}
}
\end{figure*}

Будем полагать, что по прецедентам, описывающим неизвестную целевую функцию~$F$, 
строится аппроксимирующая ее нейронная сеть~$NN1$ с~одним скрытым слоем 
и~функцией активации  $\varphi(z)\hm=
\left(1\hm+\exp(-z)\right)^{-1}$ (рис.~2), 
реализующая функцию $\hat{F}_{NN1}(\tilde{x})\hm=\varphi(\tilde{x})$.  
А~по прецедентам, опи\-сы\-ва\-ющим ограничения, обучается ка\-кая-ни\-будь 
другая классифицирующая нейронная сеть~$NN2$, аппроксимирующая 
характеристическую функцию ограничений~$\Omega(\tilde{x})$:
$\hat{\Omega}_{NN2}(\tilde{x})$. Если сети~$NN1$ и~$NN2$\linebreak\vspace*{-12pt}

\renewcommand{\figurename}{\protect\bf Рис.}
\setcounter{figure}{1}

{ \begin{center}  %fig2
 \vspace*{1pt}
    \mbox{%
 \epsfxsize=66.616mm 
 \epsfbox{don-2.eps}
 }

\vspace*{6pt}

\noindent
{{\figurename~2}\ \ \small{
Сеть с~одним скрытым слоем 
}}
\end{center}}


\noindent 
обучены, то получена нейросетевая эмпирическая  модель 
$\langle \hat{F}_{NN1}(\tilde{x}), \hat{\Omega}_{NN2}(\tilde{x}) \rangle $ 
и~можно перейти к~нахождению определяемого ею экстремального решения:
$$
\hat{F}_{NN1}(\tilde{x})=v_{m+1}+\sum\limits_{j=1}^m v_j 
\varphi\left(\omega_{n+1,j}+\sum_{i=1}^n\omega_{ij}x_i \right).
$$
Обозначим $A(\tilde{\omega},\tilde{x})\hm=
\omega_{n+1,j}+\sum\nolimits_{i=1}^n\omega_{ij}x_i$. Тогда
\begin{align*}
\fr{\partial \hat{F}_{NN1}(\tilde{x})}{\partial x_i} &={}\\ 
&\hspace*{-15mm}{}=\sum\limits_{j=1}^m v_j \omega_{ij} \varphi
\left(
A(\tilde{\omega},\tilde{x})\right)
\left( 1-\varphi\left(A(\tilde{\omega},\tilde{x})\right)\right), 
\ i=\overline{1,n};
\\
\mathrm{grad}  \,\hat{F}_{NN1} &= {}\\
&\hspace*{-15mm}{}=\left( 
\fr{\partial \hat{F}_{NN1}(\tilde{x})}{\partial x_1}, 
\fr{\partial \hat{F}_{NN1}(\tilde{x})}{\partial x_2}, \dots, 
\fr{\partial \hat{F}_{NN1}(\tilde{x})}{\partial x_n} \right).
\end{align*}

Нахождение экстремума эмпирической информационной модели 
$\langle \hat{F}_{NN1}, \hat{\Omega}_{NN2}\rangle$ 
можно осуществить градиентным алгоритмом~2.
В~общем случае функция~$\hat{F}_{NN1}$ может оказаться многоэкстремальной,
 и~тогда этот алгоритм будет отыскивать локальный экстремум.



{\bfseries\textit{При использовании полносвязных многослойных нейронных сетей}}~\cite{14-don}
изложенный метод принципиально не изменяется: требуется только 
умение вычислять градиент  аппроксимирующей нейронной функции. 
Покажем, как  вычислять этот градиент, полагая функции активации 
логистическими: $\varphi(z)\hm=(1+e^{-z})^{-1}$; 
$\varphi^{'}(z)\hm=\varphi(z)(1\hm-\varphi(z))$.

Будем использовать следующие обозначения:
\begin{description}
\item[\,] $v_j^l$~--- 
взвешенная сумма всех входов нейрона~$j$ слоя~$l$, называемая его 
\textit{индуцированным локальным полем}~\cite{14-don};
\item[\,]
$l$~--- номер слоя,  $0\hm\leq l \hm\leq L$; 
слой с~номером~0~--- добавленный для облегчения понимания хода 
вычислений градиента сети слой входов;  выходной слой имеет номер~$L$;
\item[\,]
$m_1,m_2,\dots,m_{L-1}$~--- число нейронов в~скрытых слоях $1,2,\dots,L-1$;
\item[\,]
$v_L$~--- суммарный вход выходного нейрона;
\item[\,]
$\tilde{x}=(x_1,\dots,x_i,\dots,x_n)$~--- вход нейронной сети;
\item[\,]
$y_j^{l}=\varphi(v_j^l)$~--- выход нейрона~$j$ слоя~$l$; 
$Y \hm= Y(\tilde{x})$~--- выход сети.
\end{description}

Для нахождения градиента функции, реализованной обученной нейронной сетью,  
будем использовать рекуррентную схему, которая лежит в~основе алгоритма 
обучения нейронной сети методом обратного распространения ошибки~\cite{14-don}.

 \renewcommand{\figurename}{\protect\bf Алгоритм}
\setcounter{figure}{2}

\begin{figure*}[b] %алгоритм3
\vspace*{-9pt}
\Caption{Эвристический поиск условного глобального минимума 
 эмпирической информационной модели 
 $\langle \hat{F}_{NN1}, \hat{\Omega}_{NN2}(\tilde{x})\rangle$}
 \vspace*{4pt}
 
{\small 
\begin{center}
\begin{tabular}{l p{146.7mm}}
\hline
Вход: & обучающая выборка $\mathfrak{T}_{\mathrm{Opt}}=\{ 
(\tilde{a}_j,\gamma_j,y_j)\}_{j=1}^l$; нейронные аппроксимации 
$\hat{F}_{NN1}$
и~$\hat{\Omega}_{NN2}(\tilde{x})$; алгоритм~1 поиска локального 
минимума в~качестве  используемой
внут\-рен\-ней процедуры; дерево кластеризации точек по значениям целевой
функции в~качестве второй используемой внутренней процедуры; наибольшее
допустимое суммарное число шагов локального поиска~$S$.\\
Выход: &
$\tilde{x}^{*}: \hat{F}_{NN1}(\tilde{x}^{*})\approx 
\min \hat{F}_{NN1}(\tilde{x})  / \hat{\Omega}_{NN2}(\tilde{x}) = 0$~--- 
точка, являющаяся
приближением к~глобальному экстремуму эмпирической  модели и~значение
$y^{*}$ функции $\hat{F}_{NN1}$ в~этой точке.\\
\hline
1:& Очистить пометки просмотра всех листьев дерева (сброс всех флажков в~ноль).\\
2: &Взять из обучающей выборки точку $\tilde{x}_0=\tilde{a}_{j^{*}}: \ y_{j^{*}}= \min_j y_j$ \ в~качестве
начального приближения и~выполнить алгоритм~1.\\
3: &Запомнить найденную точку  $\tilde{x}^{*}$ и~значение локального минимума $y^{*}$ в~этой точке.\\
4: &Пометить лист дерева, в~который попала точка $\tilde{x}^{*}$ (установить флажок в~единицу).\\
5: &Выбрать непомеченный лист дерева с~наименьшим значением $\bar{y}_q$ и~попавшую
в него точку обучающей выборки с~наименьшим значением целевой функции.\\
6: &Применить к~выбранной точке алгоритм~1, 
получая экстремум~$y^{\diamond}$ в~точке~$\tilde{x}^{\diamond}$.\\
7: &Пометить выбранный лист как просмотренный (установить флажок в~единицу).\\
8: &\textbf{if}  $y^{\diamond} < y^{*}$  \textbf{then} 
$\{y^{*}:=y^{\diamond}; \tilde{x}^{*}:= \tilde{x}^{\diamond} \}$;\\
9: &Если все еще есть непомеченные листья дерева и~число шагов локального поиска
не превысило заданной величины~$S$, то перейти на~5.\\
\hline
\end{tabular}
\end{center}
}
\end{figure*}

Локальный градиент выходного нейрона определяется выражением:
\begin{multline}
\label{Del1}
\delta_L^{(L)}=\fr{\partial Y} {\partial v_L}=
\varphi^{'}(v_L)={}\\
{}=\varphi(v_L)(1-\varphi(v_L))= Y(1-Y),
\end{multline}
где верхний индекс в~скобках обозначает номер слоя сети.
Локальный градиент  нейрона~$j$ скрытого слоя с~номером~$l$:
\begin{multline}
\label{Del2}
\delta_j^{(l)}=\varphi^\prime
\left(v_j^l\right)\sum\limits_{k} \delta_k^{(l+1)} 
\omega_{jk}={}\\
{}=y_j^{(l)}\left(1-y_j^{(l)}\right)
\sum\limits_{k} \delta_k^{(l+1)} \omega_{jk}\,,
\end{multline}
где сумма берется по всем номерам нейронов слоя, 
непосредственно следующего за слоем, в~котором содержится нейрон~$j$. 
Здесь и~далее в~формулах величины~$\omega_{jk}$ понимаются как веса 
от $j$-го нейрона слоя~$l$ к~нейрону с~порядковым номером~$k$ слоя~$l+1$:
\begin{align*}
%\label{Del3}
\delta_j^{(1)}&=\varphi^\prime\left( 
v_j^1 \right)\sum\limits_{k} \!\delta_k^{(2)} \omega_{jk} = 
y_j^{(1)}\left(1-y_j^{(1)}\right)\sum\limits_{k}\! \delta_k^{(2)} \omega_{jk};
\\
%\label{Del4}
\delta_i^{(0)}&=\varphi_0^\prime ( x_i )\sum\limits_{k} \!
\delta_k^{(1)} \omega_{ik}=\sum\limits_{k} \!\delta_k^{(1)} \omega_{ik};
\end{align*}

\vspace*{-12pt}
\noindent
\begin{equation}
\fr{\partial Y} {\partial x_i}=\delta_i^{(0)}; \quad 
\mathrm{grad} \ Y = (\delta_1^{(0)},\dots, \delta_n^{(0)}).
\label{Del4}
\end{equation}

Как и~при вычислениях по методу обратного распространения ошибки, для 
заданного входа~$\tilde{x}$ сначала в~прямом направлении от входа к~выходу 
сети вычисляются локальные поля и~выходы всех нейронов. Затем в~обратном 
направлении, начиная с~уравнения~(\ref{Del1}), рекуррентно выполняются 
вычисления локальных градиентов по формуле~(\ref{Del2}) и~завершаются 
вычислением градиента по формулам~(\ref{Del4}).



\section{Эвристический подход к~поиску глобального экстремума}

Поиск глобального экстремума  задачи, определенной 
мо\-делью~$\langle\hat{F}_{NN1}, \hat{\Omega}_{NN2}\rangle$, 
основан на повторяющемся применении  алгоритма~2 на\-хож\-де\-ния 
локального экстремума, начиная с~различных\linebreak исходных точек об\-ласти 
допустимых решений.
Предварительно осуществляется  кластеризация точек обучающей 
выборки $\mathfrak{T}_{\mathrm{Opt}}\hm=\{ (\tilde{a}_j,\gamma_j,y_j)\}_{j=1}^l$
 по значениями целевой функции~$y_j$. Для этой цели используется алгоритм
  построения решающего дерева, определяющего разбиение области 
  допустимых значений пе\-ре\-мен\-ных-при\-зна\-ков~\cite{6-don} на 
  гиперпараллелепипеды~\cite{15-don}. В~об\-ласти, соответствующей 
  каждому такому гиперпараллелепипеду, значения целевой функции~$y_j$ 
  принадлежат зафиксированному полуинтервалу. Число таких полуинтервалов и~есть 
  число классов в~предварительной задаче кластеризации. После построения 
  классифицирующего дерева каждый класс получает логическое описание в~терминах 
  пороговых предикатов вида $[x_i \leq b]$,  $b\hm\in \mathbb{R}$, 
   $i\hm\in \{1,\ldots ,n \}$.

Необходимо из содержательных соображений, определяемых 
проблемной областью, определить две константы~$m$ и~$M$ такие, что $m \hm< y_j \hm< M$ 
для всех $j\hm=\overline{1,l}$. Затем отрезок $[m,M ]$ разбить на~$k$ 
равных сегментов $[m, m+\Delta),\dots,[m+k\Delta,M]$, где $\Delta\hm=(M-m)/k$.
Если точка~$\tilde{a}_j$ обучающей выборки попадает в~сегмент с~номером~$q$, 
 $q\hm=\overline{1,k}$, то эта точка считается относящейся к~классу~$\mathcal{K}_q$.
Таким образом, в~каждом из  полученных классов окажутся точки, значения
 целевой функции в~которых отличаются не более чем на~$\Delta$.

Обозначим $\bar{y}_q = m\hm+q  \Delta \hm- \Delta/2$ середину $q$-го
 промежутка. Каждая концевая вершина дерева содержит значение~$\bar{y}_q$, 
 соответствующее номеру класса~$q$, и~специальную по\-мет\-ку-фла\-жок, 
 исполь\-зу\-емую для запоминания просмотренных листьев при поиске глобального 
 экстремума.

\setcounter{table}{4}
\begin{table*}\small %tabl5
\begin{center}
\Caption{Выборка для обучения нейронной сети}
\label{TabData}
\vspace*{2ex}

\begin{tabular}{|c|c|c|c|c|c||c|} 
\hline
№ предприятия & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $y$ \\
\hline 
\hphantom{9}1 & 1404,5 &345,9& 170 &0,313 & 0,061 & 112,5 \\
\hphantom{9}2 & 1709,8 &431,9& 225 &0,285 & 0,067 & 113,7 \\
\hphantom{9}3 & 1808,7 &886,2& 238 &0,398 & 0,089 & 193,0 \\
\hphantom{9}4 & 1437,1 &484,2& 181 &0,322 & 0,076 & 125,0 \\
\hphantom{9}5 & 1496,1 &724,6& 177 &0,367 & 0,085 & 173,4 \\
\hphantom{9}6 & 1034,3 &200,7& 179 &0,206 & 0,039 & \hphantom{1}81,4 \\
\hphantom{9}7 & 1335,0 &317,6& 162 &0,314 & 0,049 & 106,4 \\
\hphantom{9}8 & 1256,1 &156,1& 159 &0,187 & 0,038 & \hphantom{1}72,6 \\
\hphantom{9}9 & 1581,4 &364,3& 255 &0,319 & 0,054 & 110,7 \\
10& 1826,5 &554,2& 275 &0,338 & 0,083 & 146,3 \\
11& 1697,7 &387,7& 251 &0,219 & 0,064 & 112,9 \\
12& 1294,6 &302,5& 154 &0,214 & 0,044 & 105,9 \\
13& 1174,7 &483,9& 215 &0,324 & 0,075 & 134,5 \\
14& 1180,9 &220,1& 165 &0,217 & 0,039 & \hphantom{1}91,4 \\
15& 1319,0 &243,6& 156 &0,207 & 0,040 & \hphantom{1}98,4 \\
16& 1460,0 &347,3& 202 &0,316 & 0,053 & 107,6 \\
17& 1478,3 &313,5& 186 &0,211 & 0,044 & 102,3 \\
\hline
\multicolumn{7}{p{100mm}}{\footnotesize \hspace*{2mm}\textbf{Примечание:} 
Стоимостные показатели~$x_1$, $x_2$ и~$y$ в~тыс.\
 долл.\ США приведены за один месяц.}
\end{tabular}
\end{center}
\end{table*}

Обоснование применения классифицирующего дерева при 
поиске глобального экстремума в~составе алгоритма~3 состоит в~следующем.
\begin{enumerate}[1.]
\item  Число точек~$l$ в~обучающей выборке, как правило, многократно 
превышает число листьев дерева классификации, и~вместо перебора всех~$l$ точек 
для инициализации локального поиска осуществляется повторение локального 
поиска не более $\mu\hm \ll l$~раз~--- каждый раз с~начальной точкой, 
взятой из сегмента, соответствующего выбранному листу дерева.

\item 
Листьям дерева соответствуют разные, в~том числе значительно удаленные 
друг от друга, сегменты области глобального поиска, что  расширяет поиск, в~то 
время как произвольный упорядоченный перебор точек обучающей выборки 
для инициализации  поиска может привести к~вычислениям в~окрест\-ности 
одного и~того же локального минимума.
\end{enumerate}

\noindent
\textbf{Пример~2}. Чтобы проиллюстрировать подход к~синтезу 
нейронной нелинейной оптимизационной
модели, была использована обучающая выборка по~17~производственным 
предприятиям 
(табл.~5). Наблюдения,  содержащиеся в~табл.~5,
 соответствовали  только допустимым точкам  ($\Omega(\tilde{x})\hm=1$) 
 и~описывались
пятью признаками, обозначаемыми далее как  $x_1$--$x_5$:
$x_1$~--- объем валовой продукции в~стоимостном выражении;
$x_2$~--- стоимость основных фондов;
$x_3$~--- число работников;
$x_4$~--- коэффициент специализации\footnote{Отношение 
стоимости готовой продукции профильного направления производства 
к~стоимости всей готовой продукции, выпускаемой предприятием за месяц.};
$x_5$~--- коэффициент маркетинга\footnote{Линейная свертка коэффициентов, 
определяемых значениями маркетинговых индикаторов.}.
Целевым атрибутом~$y$  выступала прибыль.
Использовалась нейронная сеть с~одним скрытым слоем 
(см.\ рис.~2)\footnote{Доказано, что сети  такого класса 
позволяют получать требуемую точность при обучении регрессии~\cite{13-don}.} 
и~сигмоидными активационными функциями. Ошибка нейронной сети оценивалась как
$$
E(\tilde{w},\tilde{V})= \fr{1}{2} 
\sum\limits_{j=1}^{17} \left( f_{NN}(X_j^{\mathrm{sample}})-
y_j^{\mathrm{sample}} \right)^2,$$
где $f_{NN}(X_j^{\mathrm{sample}})$~---
выход нейронной сети на примере с~номером~$j$:
$$
f_{NN}(X_j^{\mathrm{sample}})=-V_o+\sum\limits_{q=1}^m \! V_q  \varphi 
\left(\! -w_{0,q}+ \sum\limits_{i=1}^n \! w_{i,q}x_i\! \right);
$$  
$n\hm=5$~--- 
число переменных;  $m\hm=6$~--- число нейронов в~скрытом слое;
$ \tilde{w}=\{w_{i,q}, i\hm=\overline{0,n}; q\hm=\overline{1,m};\}$~--- 
входные веса сети; $X_j^{\mathrm{sample}}\hm= (x_1^j,\dots,x_n^j)$~--- $j$-я 
строка (пример) таблицы обучения; $V_q$,  $q\hm=\overline{1,m}$,~--- 
выходные веса скрытого слоя сети;  $-V_0$~--- смещение; 
$y_j^{\mathrm{sample}}$~--- значение прибыли на $j$-м примере обучающей выборки.
Для вычислений были использованы следующие формулы: 

\noindent
\begin{gather*}
\eta_k=\varphi \left(-w_{0,k} +\sum\limits_{s=1}^m w_{s,k}x_s \right); \\
\fr{\partial f_{NN}} {\partial w_{i,q}}= 
\sum\limits_{k=1}^m V_k x_i \eta_k (1 - \eta_k), \enskip i=\overline{1,n}; \
\mathrm{}q=\overline{1,m};
\end{gather*}

\noindent
\begin{gather*}
\fr{\partial f_{NN}} {\partial w_{0,q}}=
-\sum\limits_{k=1}^m V_k \eta_k (1 - \eta_k), \ q=\overline{1,m}; \\
\fr{\partial f_{NN}} {\partial V_k}=\eta_k, \ k=\overline{1,m}; \\
\fr{\partial f_{NN}} {\partial V_0}=-1;\\
\fr{\partial E(\tilde{w},\tilde{V})} {\partial w_{i,q}}= 
\left( f_{NN}(X_j^{\mathrm{sample}})-y_j^{\mathrm{sample}} \right) 
\fr{\partial f_{NN}} {\partial w_{i,q}}; \\
\fr{\partial E(\tilde{w},\tilde{V})} {\partial V_{k}}=\left( 
f_{NN}(X_j^{\mathrm{sample}})-y_j^{\mathrm{sample}} \right)\eta_k. 
\end{gather*}
Для обучения был использован градиентный спуск:
\begin{align*}
w_{i,q}^t:&=w_{i,q}^{t-1}-\gamma_t \fr{\partial E(\tilde{w},\tilde{V})}
{\partial w_{i,q}}; \\
 V_q^t:&=V_q^{t-1}-\gamma_t \fr{\partial E(\tilde{w},\tilde{V})} 
 {\partial V_{k}},
 \end{align*}
 где $\gamma_t$, $t=1,2,\ldots ,$~---  уменьшающаяся с~ростом~$t$ 
 скорость обучения.
В~ходе 94~эпох обучения наблюдалось монотонное уменьшение средней ошибки
 $({1}/{l})\sum\nolimits_{j=1}^{17}\left(f_{NN}(X_j^{\mathrm{sample}})\hm-
 y_j^{\mathrm{sample}}\right)^2$   
 от величины~1,0878   до~0,0277. После того как нейронная сеть была 
 обучена, решалась задача нахождения максимума реализованной ею
  функции прибыли  при следующих искусственно введенных 
  ограничениях, позволяющих задать об\-ласть до\-пус\-ти\-мых 
  решений: значения при\-зна\-ков-фак\-то\-ров ограничивались сегментами  
   $$
   0{,}9\min\limits_{j=\overline{1{,}17}} x_{j,i}^{\mathrm{sample}} \leq x_i 
   \hm\leq 1,1\max\limits_{j=\overline{1,17}} x_{j,i}^{\mathrm{sample}}.
   $$
    
   Для градиентного подъема использовались формулы:
\begin{align*}
\fr{\partial f_{NN}}{\partial x_i}&= 
\sum\limits_{q=1}^m V_q \eta_q (1-\eta_q) w_{i,q}\,; \\
 x_i^t:&=x_i^{t-1}+\gamma_t \Delta y^t \fr{\partial f_{NN}}{\partial x_i}\,,
 \end{align*}
где $\Delta y^t$~--- изменение значения на выходе нейронной сети на шаге~$t$.
Было выполнено~250~шагов  оптимизационного градиентного поиска и~найдено 
экстремальное значение прибыли $Y \hm=  {\mathbf{195{,}54}}$  
при значениях аргументов $x_1\hm= 1096{,}56$,   
$x_2\hm= 934{,}67$,   $x_3\hm= 199{,}94$,  
$x_4 =  0{,}392$ и~$x_5 = 0{,}0371$. Заметим, что максимальное 
значение прибыли в~обучающей выборке $\left(\max_{j=\overline{1{,}17}} 
y_j^{\mathrm{sample}}\right)$ равно~193,0.




\section{Заключение}

Исследования в~области применения методов машинного обучения и~распознавания 
для синтеза оптимизационных моделей из данных с~целью построения 
интеллектуализированных сис\-тем управ\-ле\-ния  позволили получить 
результаты в~виде ряда готовых алгоритмов и~программ, 
обеспечивающих синтез таких моделей и~их обоснование~\cite{6-don, 7-don, 8-don}. 
Изложенные в~данной статье результаты расширяют указанные 
исследования и~обобщают их до уровня 
\textit{новой информационной технологии}~--- 
ИОМД, в~англоязычной 
версии~--- \textit{Building Optimization Models from Data} (BOMD). 
В~рамках технологии ИОМД созданы все необходимые алгоритмы синтеза 
оптимизационных моделей, и~они могут быть широко использованы на 
практике, позволяя расширить  методы искусственного интеллекта, 
применяемые в~цифровой экономике и~других прикладных областях.

Ограниченный объем статьи не позволил включить 
в~изложенный материал методы оценивания надежности моделей, 
извлеченных из данных. Подход к~такому оцениванию, основанный 
на теории колмогоровской алгоритмической сложности, изложен 
в~работе~\cite{6-don}.

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}


\bibitem{2-don} %1
\Au{Мазуров Вл.\,Д.} Применение методов теории распознавания 
образов в~оптимальном планировании и~управ\-ле\-нии~// Тр. I~Всесоюзн. 
конф. по оптимальному планированию и~управ\-ле\-нию народным хозяйством.~--- 
М.: ЦЭМИ, 1971. С.~1--49.

\bibitem{1-don} %2
\Au{Журавлёв Ю.\,И.} 
Экстремальные алгоритмы в~математических моделях для задач 
распознавания и~классификации~// Докл. Акад. наук СССР, 1976. Т.~231. №\,3. 
C.~532--535.

\bibitem{3-don}
\Au{Ерёмин И.\,И., Мазуров Вл.\,Д.} 
Нестационарные процессы математического программирования.~--- M.: Наука, 1979. 288~c.

\bibitem{4-don}
\Au{Рудаков К.\,В.} Об алгебраической теории универсальных и~локальных 
ограничений для задач классификации~// Распознавание, классификация, 
прогноз.~--- М.: Наука, 1989. C.~176--201.

\bibitem{5-don}
\Au{Воронцов К.\,В.} Аддитивная регуляризация тематических моделей 
коллекций текстовых документов~// Докл. Акад. наук, 2014. Т.~456. №\,3. С.~268--271.

\bibitem{10-don} %6
\Au{Махина Г.\,А.} Нейросетевой подход к~задачам слабоопределенной оптимизации~// 
Искусственный интеллект, 2000. №\,2. C.~145--148.

\bibitem{9-don} %7
\Au{Таратынова Н.\,Ю.} Построение оптимизационной модели по прецедентной 
начальной информации как задача нелинейной регрессии~// Искусственный 
интеллект, 2006. №\,2. C.~238--241.

\bibitem{6-don} %8
\Au{Донской В.\,И.} Синтез согласованных  линейных оптимизационных моделей  
по прецедентной информации: подход на основе колмогоровской сложности~// 
Таврический вестник информатики и~математики, 2012. №\,1. C.~13--25.

\bibitem{7-don} %9
\Au{Донской В.\,И.}  Извлечение оптимизационных моделей из данных: 
подход на основе решающих деревьев и~лесов~// 
Таврический вестник информатики и~математики, 2017. №\,4. C.~59--86.

\bibitem{8-don} %10
\Au{Donskoy V.\,I.} A~synthesis of pseudo-boolean empirical models 
by precedential information~// 
Вестник Юж\-но-Ураль\-ско\-го государственного университета. Сер.
 Математическое моделирование и~программирование, 2018. Т.~11. №.\,2. С.~96--107.


\bibitem{11-don}
\Au{Nilsson N.\,J.} Learning machines.~--- New York, NY, USA: 
McGraw-Hill, 1965. 137~p.

\bibitem{12-don}
\Au{Rokach L., Scholkopf B.} 
A~tutorial on support vector regression~// Stat. Comput., 2004. Vol.~14. 
No.\,4. P.~199--222.

\bibitem{13-don}
\Au{Hornik K., Stinchcombe~M., White~H.} 
Universal approximation of an unknown mapping and derivatives using 
multilayer feedforward networks~// Neural Networks, 1990. Vol.~3. No.\,1. P.~551--560.

\bibitem{14-don}
\Au{Haykin S.} Neural networks and learning machines.~--- 
New York, NY, USA: Prentice Hall,  2008. 906~p.

\bibitem{15-don}
\Au{Breiman L., Friedman J.\,H., Olshen~R., Stone~C.\,J.} 
Classification and regression trees.~--- New York, NY, USA: 
Chapman and Hall, 1984. 369~p.

%\bibitem{16-don}
%\Au{Донской В.\,И.} Сложность семейств алгоритмов обучения и~оценивание 
%неслучайности извлечения эмпирических закономерностей~// 
%Кибернетика и~системный анализ, 2012. Т.~48. №\,2. C.~86--96.
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 20.05.19}}

\vspace*{8pt}

%\pagebreak

%\newpage

%\vspace*{-28pt}

\hrule

\vspace*{2pt}

\hrule

%\vspace*{-2pt}

\def\tit{OPTIMIZATION MODELS EXTRACTION FROM~DATA}

\def\titkol{Optimization models extraction from~data}

\def\aut{V.\,I.~Donskoy}

\def\autkol{V.\,I.~Donskoy}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-9pt}


\noindent
V.\,I.~Vernadsky Crimean Federal University, 4~Vernadsky Av., 
Simferopol 295007, Russian Federation

\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2020\ \ \ volume~14\ \ \ issue\ 3}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2020\ \ \ volume~14\ \ \ issue\ 3
\hfill \textbf{\thepage}}}

\vspace*{3pt} 

\Abste{The basic principles, methods and algorithms representing 
a~new information technology for building optimization mathematical 
models from data (BOMD) are presented. This technology allows one to 
automatically build mathematical models of planning and control on 
the basis of use of precedents (observations) over objects that gives 
the chance to solve the problems of intellectual control and to define 
expedient behavior of economic and other objects in difficult 
environments. The BOMD technology allows one to obtain objective 
control models that reflect real-life relationships, goals, 
constraints, and processes. This is its main advantage over the 
traditional, subjective approach to control. Linear and nonlinear 
algorithms for synthesis of models based on precedent information 
are developed.}

\KWE{machine learning; model extraction from data; optimization; 
neural networks; gradient methods}

\DOI{10.14357/19922264200316} 

%\vspace*{-20pt}

%\Ack
%\noindent


%\vspace*{6pt}

 \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}

\bibitem{2-don-1} %1
\Aue{Mazurov, Vl.\,D.}
1971. Primenenie metodov teorii ras\-po\-zna\-va\-niya obrazov v~optimal'nom 
planirovanii i~uprav\-le\-nii [Application of pattern recognition methods 
in optimal planning and management]. 
\textit{Tr. I~Vsesoyuzn. konf. po optimal'nomu planirovaniyu i~upravleniyu 
narodnym khozyaystvom} [1st All-Union Conference on 
Optimal Planning and Management of the National Economy Proceedings]. Moscow. 1--49.

\bibitem{1-don-1} %2
\Aue{Zhuravlev, Yu.\,I.} 1976. Ekstremal'nye algoritmy v~ma\-te\-ma\-ti\-che\-skikh 
modelyakh dlya zadach raspoznavaniya i~klassifikatsii 
[Extremal algorithms in mathematical models for discrimination and 
classification problems]. \textit{Dokl. Akad. Nauk SSSR} 231(3):532--535.

\bibitem{3-don-1}
\Aue{Eremin, I.\,I., and V.\,D.~Mazurov.}
1979. \textit{Ne\-sta\-tsi\-o\-nar\-nye protsessy matematicheskogo programmirovaniya} 
[Nonstationary processes of mathematical programming]. Moscow: Nauka. 288~p.
\bibitem{4-don-1}
\Aue{Rudakov, K.\,V.}
 1989. Ob algebraicheskoy teorii universal'nykh i~lokal'nykh ogranicheniy 
 dlya zadach klassifikatsii [On the algebraic theory of universal and 
 local constraints for classification problems]. 
 \textit{Raspoznavanie, klassifikatsiya, prognoz} [Recognition,
  classification, forecast]. Moscow: Nauka. 176--201.
\bibitem{5-don-1}
\Aue{Vorontsov, K.\,V.} 2014. 
Additive regularization for topic models of text collections. 
\textit{Dokl. Math.} 89(3):301--304.

\bibitem{10-don-1} %6
\Aue{Makhina, G.\,A.} 2000. Neyrosetevoy podkhod k~zadacham slaboopredelennoy 
optimizatsii [Neural network approach to weakly defined optimization problems].
\textit{Iskusstvennyy intellekt} [Artificial Intelligence] 2:145--148.

\bibitem{9-don-1} %7
\Aue{Taratynova, N.\,Yu.} 2006. Postroenie optimizatsionnoy modeli po 
pretsedentnoy nachal'noy informatsii kak zadacha nelineynoy regressii 
[Optimization model construction   by precedents 
as a~nonlinear regression problem]. 
\textit{Iskusstvennyy intellekt} [Artificial Intelligence] 2:238--241.
\bibitem{6-don-1} %8
\Aue{Donskoy, V.\,I.} 2012. Sintez soglasovannykh lineynykh
op\-ti\-mi\-za\-tsi\-on\-nykh modeley po pretsedentnoy informatsii: podkhod na 
osnove Kolmogorovskoy slozhnosti [Synthesis of coordinated linear
optimization 
models according to precedential information: An
approach based on Kolmogorov complexity]. 
\textit{Tavricheskiy vestnik informatiki i~matematiki}
[Taurida J.~Computer Science Theory Mathematics] 2:13--25.
\bibitem{7-don-1} %9
\Aue{Donskoy, V.\,I.} 2017. Izvlechenie optimizatsionnykh mo\-de\-ley iz dannykh: 
podkhod na osnove reshayushchikh derev'ev i~lesov [Extracting optimization 
models from data: An approach based on
decision trees and forests].
\textit{Tav\-ri\-che\-skiy vestnik informatiki i~matematiki} 
[Taurida J.~Computer Science Theory Mathematics] 4:59--86.
\bibitem{8-don-1} %10
\Aue{Donskoy, V.\,I.} 2018. A~synthesis of pseudo-boolean empirical 
models by precedential information. \textit{Vestnik Yuzhno-Ural'skogo 
gosudarstvennogo universiteta. Ser.
 Ma\-te\-ma\-ti\-che\-skoe modelirovanie i~programmirovanie} 
 [Bull. South Ural State University. Ser. 
 Mathematical Modelling Programming Computer Software] 11(2):96--107.


\bibitem{11-don-1}
\Aue{Nilsson, N.\,J.} 1965. \textit{Learning machines}. New York, NY: 
McGraw-Hill. 137~p.
\bibitem{12-don-1}
\Aue{Rokach, L., and B.~Scholkopf.} 2004. A~tutorial on support vector 
regression. \textit{Stat. Comput.} 14(4):199--222.
\bibitem{13-don-1}
\Aue{Hornik, K., M.~Stinchcombe, and H.~White.}
 1990. Universal approximation of an unknown mapping and derivatives using 
 multilayer feedforward networks. \textit{Neural Networks} 3(1):551--560.
\bibitem{14-don-1}
\Aue{Haykin, S.} 2008. \textit{Neural networks and learning machines}. 
New York, NY: Prentice Hall. 906~p.
\bibitem{15-don-1}
\Aue{Breiman, L., J.\,H.~Friedman, R.~Olshen, and C.\,J.~Stone.}
1984. \textit{Classification and regression trees}.
New York,  NY: Chapman and Hall. 369~p.
%\bibitem{16-don-1}
%\Aue{Donskoy, V.\,I.} 2012. Slozhnost' semeystv algoritmov obucheniya 
%i~otsenivanie nesluchaynosti izvlecheniya empiricheskikh zakonomernostey 
%[Complexity of families of learning algorithms and estimation of 
%the nonrandomness of extraction of empirical regularities]. 
%\textit{Kibernetika i~sistemnyy analiz} [Cybernetics and Systems Analysis] 
%48(2):233--241.
 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Received May 20, 2019}}

%\pagebreak

%\vspace*{-24pt}

\Contrl

\noindent
\textbf{Donskoy Vladimir I.} (b.\ 1948)~--- 
Doctor of Science in physics and mathematics, professor, V.\,I.~Vernadsky 
Crimean Federal University, 4~Vernadsky Av., Simferopol 295007, 
Russian Federation; \mbox{vidonskoy@mail.ru}
\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература} 

\renewcommand{\figurename}{\protect\bf Рис.}
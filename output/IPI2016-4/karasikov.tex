
\let\varvec\vec
%\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}

\def\stat{karasikov}

\def\tit{КЛАССИФИКАЦИЯ ВРЕМЕННЫХ РЯДОВ В~ПРОСТРАНСТВЕ ПАРАМЕТРОВ ПОРОЖДАЮЩИХ МОДЕЛЕЙ$^*$}

\def\titkol{Классификация временных рядов в~пространстве параметров порождающих моделей}

\def\aut{М.\,Е.~Карасиков$^1$, В.\,В.~Стрижов$^2$}

\def\autkol{М.\,Е.~Карасиков, В.\,В.~Стрижов}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Карасиков М.\,Е.}
\index{Стрижов В.\,В.}
\index{Karasikov M.\,E.}
\index{Strijov V.\,V.}


{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
{Работа выполнена при финансовой поддержке РФФИ (проект 16-37-00485).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Московский физико-технический институт, Сколковский институт науки и~технологий, 
    \mbox{karasikov@phystech.edu}}
\footnotetext[2]{Вычислительный центр им.\ А.\,А.~Дородницына Федерального исследовательского 
    центра <<Информатика и~управ\-ле\-ние>> Российской академии наук, 
    \mbox{strijov@ccas.ru}}

      
    

\Abst{Работа посвящена задаче многоклассовой признаковой классификации временных рядов.
    Признаковая классификация временных рядов заключается в~сопоставлении каждому 
    временному ряду его краткого признакового описания и~позволяет решать задачу 
    классификации в~пространстве признаков.
Исследуются методы построения пространства признаков временн$\acute{\mbox{ы}}$х рядов,
    при этом временной ряд рассматривается как последовательность сегментов, 
    аппроксимируемых некоторой параметрической моделью, параметры которой используются 
    в~качестве их признаковых описаний.
    Построенное признаковое описание сегмента временного ряда наследует от 
    модели аппроксимации такое полезное свойство, как инвариантность относительно 
    сдвига.
    Для решения задачи классификации в~качестве признаковых описаний временн$\acute{\mbox{ы}}$х рядов 
    предлагается использовать распределения параметров аппроксимирующих сегменты 
    моделей, что обобщает базовые методы, использующие непосредственно сами параметры 
    аппроксимирующих моделей.
    Проведен ряд вычислительных экспериментов на реальных данных, показавших 
    высокое качество решения задачи многоклассовой классификации.
    Эксперименты показали превосходство предлагаемого метода над базовым 
    и~многими распространенными методами классификации временных рядов на всех 
    рассмотренных наборах данных.}

\KW{временные ряды; многоклассовая классификация; сегментация временных рядов; 
гиперпараметры аппроксимирующей модели; модель авторегрессии; дискретное 
преобразование Фурье}

\DOI{10.14357/19922264160413} 


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}

\section{Введение}
%\label{sec:introduction}

Временн$\acute{\mbox{ы}}$м рядом~$x$ будем называть конечную упорядоченную 
последовательность чисел
$$
x = \left[x^{(1)}, \dots, x^{(t)}\right]\,.
$$
Временн$\acute{\mbox{ы}}$е ряды являются объектом исследования 
в~таких задачах анализа данных, как прогнозирование,
  обнаружение аномалий, сегментация~\cite{geurts2005segment},
  клас\-те\-ри\-за\-ция и~классификация~\cite{geurts2005segment}.
Обзор по задачам и~методам анализа временн$\acute{\mbox{ы}}$х рядов дается 
в~\cite{Esling:2012:TDM:2379776.2379788}.
Последние годы связаны с~ростом интереса к~данной области, проявляющемся 
в~непрекращающемся предложении новых методов анализа временных рядов~--- метрик, 
алгоритмов сегментации, кластеризации и~др.

В данной работе рассматривается задача классификации временн$\acute{\mbox{ы}}$х рядов, 
возникающая во многих приложениях
  (медицинская диагностика по электрокардиограммам~\cite{basil2014automatic} 
  и~электроэнцефалограммам~\cite{alomari2013automated},
  классификация типов физической активности по данным 
  акселерометра~\cite{Kwapisz:2011:ARU:1964897.1964918},
  верификация динамических подписей~\cite{gruber2006signature}~и~т.\,д.).

Формально задача классификации в~общем виде ставится следующим образом.
Пусть~$X$~--- множество описаний объектов произвольной природы,
$Y$~--- конечное множество меток классов.
Предполагается существование целевой функции~--- отоб\-ра\-же\-ния~$y:\;X
\hm\to Y$,
значения которого известны только на~объектах обучающей выборки
$$
    \mathfrak{D} = \left\{(x_1,y_1),\dots,(x_m,y_m)\right\} \subset X\times Y\,.
$$
Требуется построить классификатор~$a:\;X\to Y$~--- отображение,
приближающее целевую функцию~$y$ на~множестве~$X$.
При $|Y|\hm>2$ задачу классификации будем называть многоклассовой.
Задачей классификации временн$\acute{\mbox{ы}}$х рядов будем называть задачу классификации, 
в~которой объектами классификации являются временн$\acute{\mbox{ы}}$е ряды.

Задание метрики, или функции расстояния~\cite{Ding:2008:QMT:1454159.1454226}, 
на парах временн$\acute{\mbox{ы}}$х рядов позволяет применять метрические методы классификации.
При удачном выборе метрики классификация может производиться простейшими метрическими 
алгоритмами классификации, например методом ближайшего соседа~\cite{jeong2011weighted}.
Данный подход к~решению задачи классификации временн$\acute{\mbox{ы}}$х рядов чрезвычайно распространен 
в~силу того, что позволяет свести исходную задачу классификации временн$\acute{\mbox{ы}}$х рядов 
к~задаче выбора метрики.

Второй подход к~решению задачи классификации состоит в~построении для 
каждого временн$\acute{\mbox{о}}$го ряда его информативного признакового 
описания~$\mathbf{f}:\;X\hm\to\mathbb{R}^n$, позволяющего строить точные 
классификаторы с~хорошей обобщающей способностью.
Построение информативного пространства признаков исходных объектов
 множества~$X$,\linebreak
  позволяющего добиться заданной точности классификации и~значительно 
 упрощающего по\-сле\-ду\-ющий анализ, является важнейшим этапом решения задачи классификации.
Признаки могут задавать\-ся экспертом.
Так, в~работе~\cite{Nanopoulos01feature-basedclassification} предлагается использовать 
в~качестве признаков статистические функции (среднее, отклонения от среднего, 
коэффициенты эксцесса и~др.).
Стоит заметить, что при таком подходе к~построению пространства признаков 
час\-то удается добиться необходимого качества классификации путем выбора 
соответствующих конкретной задаче признаков (см., например,~\cite{wiens2012patient}), 
а~сам выбор признаков становится важной технической задачей.
Другой метод построения пространства признаков заключается в~задании 
параметрической регрессионной или аппроксимирующей модели временн$\acute{\mbox{о}}$го ряда.
Тогда в~качестве признаков временн$\acute{\mbox{ы}}$х рядов будут выступать параметры 
настроенной модели.
В~работе~\cite{morchen2003time} в~качестве признаков предлагается 
использовать коэффициенты дискретного преобразования Фурье (DFT) 
и~дискретного вейв\-лет-пре\-обра\-зо\-ва\-ния (DWT), 
а~в~\cite{kini2013large, kuznetsov2015description}~--- модели авторегрессии.
%В~\cite{kalliovirta2015gaussian} исследуются свойства смеси моделей авторегрессии.
Таким образом, при данном методе построения признаковых описаний 
возникает задача выбора аппроксимирующей модели временн$\acute{\mbox{о}}$го ряда.
%Об исчерпывающих исследованиях этой задачи авторам неизвестно.

В работе исследуются методы классификации временн$\acute{\mbox{ы}}$х рядов, использующие 
в~качестве их признаковых описаний параметры аппроксимирующих моделей.
Приводится сравнение моделей аппроксимации.
Из временн$\acute{\mbox{о}}$го ряда могут извлекаться сегменты~--- его 
подпоследовательности, для которых признаковые описания строятся так же, как и~для 
исходных временн$\acute{\mbox{ы}}$х рядов.
Использование подпоследовательностей позволяет обобщить алгоритмы классификации.
 Так, в~работе~\cite{geurts2005segment} предлагается алгоритм классификации 
 временн$\acute{\mbox{ы}}$х\linebreak
  рядов методом голосования их случайных сегментов 
 (непрерывных подпоследовательностей со\linebreak случайным начальным элементом).
В~данной\linebreak
 работе предлагается алгоритм классификации вре\-мен\-н$\acute{\mbox{ы}}$х рядов в~пространстве 
параметров распределений признаков их сегментов, который сравнивается с~родственным 
ему алгоритмом голосования сегментов~\cite{geurts2005segment}.
В~разд.~7 приводятся результаты экспериментов на реальных данных, показывающие 
высокое качество и~общность предлагаемого алгоритма в~сочетании с~методом 
признаковых описаний временн$\acute{\mbox{ы}}$х рядов параметрами аппроксимирующих их моделей.


\section{Постановка задачи}
%\label{sec:problem_statement}

Поставим задачу многоклассовой классификации временн$\acute{\mbox{ы}}$х рядов в~общем виде.
Пусть $(X,\rho)$~--- метрическое пространство временн$\acute{\mbox{ы}}$х рядов, 
$Y$~--- множество меток классов, $\mathfrak{D}\subset X\times Y$~--- 
конечная обучающая выборка.

Пусть~$S$~--- процедура сегментации:
  \begin{equation}
  \label{eq:segmentation}
  S(x)\subset 2^{\mathbf{S}(x)}\,,
  \end{equation}
  где $\mathbf{S}(x)$~--- множество всех сегментов временн$\acute{\mbox{о}}$го ряда~$x\hm\in X$;
  $\mathbf{f}(S(x))\hm\in\mathbb{R}^n$~--- процедура по\-стро\-ения 
  признакового описания набора сегментов;
  $b$~--- алгоритм многоклассовой классификации:
  \begin{equation}
  \label{eq:classification}
  b:\;\mathbb{R}^n\to Y\,.
  \end{equation}

Рассмотрим семейство~$A=\left\{a:\;X\hm\to Y\right\}$ алгоритмов классификации вида
\begin{equation}
\label{eq:classifiers}
a=b\circ \mathbf{f}\circ S\,.
\end{equation}

Пусть задана функция потерь
$
\mathscr{L}:\;X\times Y\times Y\hm\to \mathbb{R}
$
и функционал качества
\begin{equation}
\label{eq:empirical_risk}
Q(a,\mathfrak{D})=
\fr{1}{|\mathfrak{D}|}
\sum\limits_{(x,y)\in\mathfrak{D}}\mathscr{L}\left(x, a(x),y\right)\,.
\end{equation}

В качестве методов обучения~$\mu(\mathfrak{D})\in A$ будем использовать следующие:
$$
\mu_{\mathbf{f},S}(\mathfrak{D})=\hat{b}\circ \mathbf{f}\circ S\,,
$$
где~$\hat{b}$~--- минимизатор эмпирического риска:
$$
\hat{b}=\argmin_{b}Q(b\circ \mathbf{f}\circ S,\mathfrak{D})\,.
$$

Оптимальный метод обучения определяется по скользящему контролю:
$$
\mu^* = \argmin_{\mathbf{f},\,S}\widehat{\mathrm{CV}}(\mu_{\mathbf{f},S},\mathfrak{D})\,,
$$
где $\widehat{\mathrm{CV}}(\mu,\mathfrak{D})$~--- внешний критерий качества метода 
обучения~$\mu$;
при этом исходная обучающая выборка~$\mathfrak{D}$ случайно разбивается~$r$~раз 
на обучающую и~контрольную~($\mathfrak{D}\hm=\mathfrak{L}_1\sqcup\mathfrak{T}_1=
\dots=\mathfrak{L}_r\sqcup\mathfrak{T}_r$),
\begin{equation}
\label{eq:cross_validation}
\widehat{\mathrm{CV}}(\mu,\mathfrak{D})=
\fr{1}{r}\sum\limits_{v=1}^{r}Q(\mu(\mathfrak{L}_v),\mathfrak{T}_v)\,,
\end{equation}
где
\begin{equation}
\label{eq:total_quality}
Q(a,\mathfrak{T})=\fr{1}{|\mathfrak{T}|}
\sum\limits_{(x,y)\in\mathfrak{T}}\vec1\{a(x)=y\}\,.
\end{equation}
Средняя точность (precision) классификации объектов класса~$c\hm\in Y$ оценивается функционалом скользящего контроля~\eqref{eq:cross_validation} с~модифицированным функционалом качества~$Q$:
\begin{equation}
\label{eq:class_quality}
Q_c(a,\mathfrak{T})=
\fr{\left|\left\{(x,y)\in\mathfrak{T}\,|\,a(x)=y=
c\right\}\right|}{\left|\left\{(x,y)\in\mathfrak{T}\,|\,y=c\right\}\right|}\,.
\end{equation}

\section{Сегментация временных рядов}
%\label{sec:segmenting}

\noindent
\textbf{Определение 1.}\
Сегментом временн$\acute{\mbox{о}}$го ряда~$x\hm=[x^{(1)},\dots,x^{(t)}]$ будем 
называть любую его непрерывную подпоследовательность~$s\hm=[x^{(i)}]_{i=t_0}^{t_1}$, 
$1\hm\leqslant t_0\hm\leqslant t_1\hm\leqslant t.$

\smallskip

\noindent
\textbf{Определение 2.}\
Под сегментацией будем понимать отображение временн$\acute{\mbox{ы}}$х рядов 
во множество их сегментов~\eqref{eq:segmentation}.


\smallskip

\subsection*{Примеры}

\begin{enumerate}[1.]
\item
  Тривиальная сегментация
  \begin{equation}
  \label{eq:equal_fragmenting}
  S(x)=\{x\},\ \forall x\in X\,.
  \end{equation}

\item
  Случайное выделение сегментов некоторой длины~$\ell$~\cite{geurts2005segment}.

\item
  Важным является случай квазипериодичности временн$\acute{\mbox{о}}$го 
  ряда, когда сам ряд состоит из похожих в~определенном смысле сегментов, 
  называемых периодами:
  \begin{multline*}
%  \label{eq:periodic}
  x=\left[\underbrace{x^{(1)},\dots,x^{(t_1)}}_{s^{(1)}},\underbrace{x^{(t_1+1)},
\dots,x^{(t_2)}}_{s^{(2)}},\dots\right.\\
\left.\dots,\underbrace{x^{(t_{p-1}+1)},\dots,x^{(t)}}_{s^{(p)}}
  \right]\,.
  \end{multline*}
  Тогда в~качестве процедуры сегментации можно взять разбиение на периоды:
  \begin{equation*}
  %\label{eq:period_segmentation}
  S(x)= \left\{s^{(1)},\dots,s^{(p)}\right\}\,.
  \end{equation*}
\end{enumerate}


\section{Аппроксимирующая модель сегмента временного ряда}
%\label{sec:regression_model}

Поскольку сегмент временн$\acute{\mbox{о}}$го ряда сам является временн$\acute{\mbox{ы}}$м 
рядом, в~этом разделе слово сегмент будем опускать.

\smallskip

\noindent
\textbf{Определение 3.}\
Параметрической аппроксими\-ру\-ющей моделью временн$\acute{\mbox{о}}$го ряда~$x$ будем называть отображение
\begin{equation}
\label{eq:regression}
g:\;\mathbb{R}^n\times X\to X\,.
\end{equation}

\smallskip

В слово <<аппроксимирующая>> вкладывается тот смысл, что модель должна приближать 
временн$\acute{\mbox{о}}$й ряд в~пространстве $(X,\rho)$, т.\,е.\ 
для некоторого $\mathbf{w}\hm\in \mathbb{R}^n$
$$
g(\mathbf{w},x)=\hat{x}\,,
$$
где
$$
\rho(\hat{x},x)<\varepsilon\,.
$$
При этом естественно взять в~качестве признакового описания временн$\acute{\mbox{о}}$го ряда~$x$
 вектор оптимальных параметров его модели.

\smallskip
%\label{def:feature_description}

\noindent
\textbf{Определение 4.}\
Признаковым описанием вре\-мен\-н$\acute{\mbox{о}}$го ряда~$x$, порожденным 
параметрической моделью~$g(\mathbf{w},x)$, назовем вектор оптимальных 
па\-ра\-мет\-ров этой модели:
\begin{equation}
\label{eq:feature_solution}
\mathbf{w}_g(x)=
\argmin_{\mathbf{w}\in \mathbb{R}^n} \rho\left(g(\mathbf{w},x),x\right)\,.
\end{equation}


В качестве аппроксимирующих моделей предлагается использовать следующие.
\begin{enumerate}[1.]
\item \textbf{Модель линейной регрессии}.
Пусть задан $r$-ком\-по\-нент\-ный временной 
ряд (например, время и~три пространственные координаты):
$$
x = [\vec{x}^{(1)}, \dots, \vec{x}^{(t)}]\,,
$$
где
$$
\vec{x}^{(k)}=[x_1^{(k)},\dots,x_r^{(k)}]^{\mathrm{T}},\enskip k=1,\dots,t\,.
$$
Рассмотрим модель линейной регрессии одной из компонент 
временн$\acute{\mbox{о}}$го ряда на остальные компоненты как аппроксимирующую модель:
$$
g(\mathbf{w},x)=\left[\hat{\vec{x}}^{(1)},\dots,\hat{\vec{x}}^{(t)}\right]\,,
$$
 где 
 $$
\hat{\vec{x}}^{(k)}=\left[x_1^{(k)},\dots,x_{r-1}^{(k)},\hat{x}_r^{(k)}\right]^{\mathrm{T}},\enskip 
k\hm=1,\dots,t\,,
$$
$$
\underbrace{
\begin{bmatrix}
\hat{x}_r^{(1)} \\
\vdots  \\
\hat{x}_r^{(t)}
\end{bmatrix}
}_{\hat{\mathbf{x}}_r}
=
\underbrace{
\begin{bmatrix}
x_1^{(1)} & \cdots & x_{r-1}^{(1)} \\
\vdots    & \ddots & \vdots       \\
x_1^{(t)} & \cdots & x_{r-1}^{(t)}
\end{bmatrix}
}_{\mathbf{X}}
\underbrace{
\begin{bmatrix}
w_1 \\
\vdots  \\
w_{r-1}
\end{bmatrix}
}_{\mathbf{w}}.
$$
Тогда, выбрав в~качестве~$\rho$ евклидово расстояние, по 
определению~4 получим признаковое описание объекта~$x$:

\noindent
\begin{multline}
\label{eq:linear_regression}
\hspace*{-1mm}\mathbf{w}_g(x)=
\argmin\limits_{\mathbf{w}\in \mathbb{R}^n} \|\mathbf{x}_r-\hat{\mathbf{x}}_r\|^2_2={}\\
\hspace*{-1mm}{}=
\argmin\limits_{\mathbf{w}\in \mathbb{R}^n} \|\mathbf{x}_r-\mathbf{X}\mathbf{w}\|^2_2=
\left(\mathbf{X}^{\mathsf{T}}\mathbf{X}\right)^{-1}\mathbf{X}^{\mathsf{T}}\mathbf{x}_r\,.
\end{multline}

\item \textbf{Модель авторегрессии {\boldmath{$\mathbf{AR}(p)$}}}.

Задан временной ряд
$$
x = [x^{(1)},\dots,x^{(t)}],\ x^{(k)}\in\mathbb{R}\,,\enskip k=1,\dots,t\,.
$$
Выберем в~качестве модели аппроксимации авторегрессионную модель порядка~$p$:
\begin{equation*}
g(\mathbf{w},x)=\left[\hat{x}^{(1)},\dots,\hat{x}^{(t)}\right]\,,
\label{eq:autoregressive_model}
\end{equation*}
где
\begin{equation*}
\hat{x}^{(k)}=
\begin{cases}
x^{(k)}\,, & k=1,\dots,p\,;\\
w_0 + \sum\limits_{i=1}^{p} w_i x^{(k-i)}\,, & k=p+1,\dots,t\,.
\end{cases}
\end{equation*}
Далее признаковое описание определяется аналогично случаю линейной 
регрессии~\eqref{eq:linear_regression}.

\item \textbf{Дискретное преобразование Фурье}.
Задан временной ряд
$$
x = \left[x^{(0)},\dots,x^{(t-1)}\right],\ x^{(k)}\in\mathbb{C},\ k=0,\dots,t-1.
$$
Взяв в~качестве аппроксимирующей модели обратное преобразование Фурье
$$
g(\mathbf{w},x)=\left[\hat{x}^{(0)},\dots,\hat{x}^{(t-1)}\right]\,,
$$
где
\begin{multline}
\label{eq:fourier_approximation}
\hat{x}^{(k)}=\fr{1}{t}\sum\limits_{j=0}^{t-1} 
\left(w_{2j}+iw_{2j+1}\right) e^{({2\pi i}/t)kj}\,,\\ k=0,\dots,t-1\,,
\end{multline}
получим, что признаковым описанием вре\-мен\-н$\acute{\mbox{о}}$го ряда~$x$ является прямое преобразование:
\begin{equation}
\label{eq:fourier}
\mathbf{w}_g(x)=\left[w_0,\dots,w_{2t-1}\right]\,,
\end{equation}
где 
\begin{multline*}
w_{2k}+iw_{2k+1}=\sum\limits_{j=0}^{t-1} x^{(j)} 
e^{-({2\pi i}/{t})kj}\,,\\ k=0,\dots,t-1\,.
\end{multline*}
Переписывая~\eqref{eq:fourier_approximation} в~матричном виде, 
заметим, что, как и~в предыдущих случаях, параметры модели~$\mathbf{w}$ 
эквивалентно находятся при помощи линейной регрессии временн$\acute{\mbox{о}}$го 
ряда на столбцы матрицы Фурье.
Выбор лишь некоторых комплексных амплитуд соответствует регрессии временн$\acute{\mbox{о}}$го 
ряда на соответствующие столбцы матрицы Фурье.
Случай дискретного вейв\-лет-пре\-об\-ра\-зо\-ва\-ния аналогичен.
\end{enumerate}

Заметим, что в~первых двух случаях используются билинейные аппроксимирующие 
моде-\linebreak\vspace*{-12pt}

\columnbreak

\noindent
ли~$g(\mathbf{w},x)$, а~в~третьем~--- линейная.
Приведенные примеры демонстрируют большую общность построения пространства 
признаков при помощи моделей типа~\eqref{eq:regression} и~решения оптимизационной\linebreak 
задачи~\eqref{eq:feature_solution}.
Вообще говоря, при $|X|\hm\geqslant 2$ любая процедура построения признаковых 
описаний~$\mathbf{f}:\;X\hm\to \mathbb{R}^n$ задается эквивалентно решением 
оптимизационной задачи~\eqref{eq:feature_solution} при выборе соответствующей 
пары~$(g,\rho)$.

\section{Распределения признаков сегментов}
%\label{sec:distribution}

Объединим идеи, изложенные в~предыдущих разделах.
Согласно аппроксимирующей модели~\eqref{eq:regression}\linebreak получим для 
каж\-до\-го сегмента~$s^{(k)}\hm\in S(x)\hm=\left\{s^{(1)},\ldots,s^{(p)}\right\}$ 
временн$\acute{\mbox{о}}$го ряда~$x$ его признаковое 
описание~$\mathbf{w}^{(k)}:=\mathbf{w}_g(s^{(k)})$, решив оптимизационную 
задачу~\eqref{eq:feature_solution}.
Тогда всему набору \mbox{сегментов}~$S(x)$ будет соответствовать выборка:
\begin{equation}
\label{eq:segments_features}
\vec{F}=\left(\mathbf{w}^{(1)},\dots,\mathbf{w}^{(p)}\right)\,.
\end{equation}
Примем гипотезу простоты выборки~\eqref{eq:segments_features}.

\smallskip

\noindent
\textbf{Гипотеза~1.}\
\textit{Выборка~$\vec{F}\hm=\left(\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right)$~--- 
прос\-тая, т.\,е.\ случайная, независимая и~однородная, где}
 $\mathbf{w}^{(k)}\sim\mathsf{P}_0$.


\smallskip

Пусть имеется параметрическое семейство 
распределений~$\left\{\mathsf{P}_{\vec\theta}\right\}_{\vec\theta\in 
\Theta}$.
Будем рассматривать вероятностную модель, в~которой объект~$x$ 
зависит от случайного параметра~$\vec\theta$.
\smallskip

\noindent
\textbf{Гипотеза~2.}\
$p(x|\vec\theta,y)=p(x|\vec\theta)$.

\smallskip
Тогда
\begin{multline*}
p(x,y)=
p(\vec{F},y)={}\\
{}=
\int\limits_{\Theta}p(\vec{F},\vec\theta,y)\,d\vec\theta=
\int\limits_{\Theta}p(\vec{F}|\vec\theta)p(\vec\theta,y)\,d\vec\theta\,.
\end{multline*}
При этом распределение~$p(\vec\theta,y)$ предлагается оценивать на этапе 
обучения, где признаковыми описаниями объектов~$x_i$ задачи классификации являются 
оценки параметров~$\vec\theta_i$:
$$
\hat{\vec\theta}_i=T(x_i)=T(\vec{F}_i)\,.
$$
Получив оценку~$\hat{p}(\vec\theta,y)$, находим оценку плот\-ности~$\hat{p}(x,y)$:
$$
\hat{p}(x,y)=
\int\limits_{\Theta}p(\vec{F}|\vec\theta)\hat{p}(\vec\theta,y)\,d\vec\theta\,,
$$
по которой строится байесовский классификатор.

\pagebreak

В алгоритмической постановке задачи классификации получим~$\hat{p}(y|\vec\theta)
\hm=\delta(a(\vec\theta),y)$ и
$$
\hat{p}(\vec\theta,y)=\delta(a(\vec\theta),y)p(\vec\theta)\,.
$$
Тогда
\begin{multline*}
\hat{p}(x,y)=
\int\limits_{\Theta}\!p\left(\vec{F}|\vec\theta\right)\hat{p}
(\vec\theta,y)d\vec\theta={}\\
{}=
\int\limits_{\Theta}\!p\left(\vec{F}|\vec\theta\right)\delta\left(a\left(\vec\theta\right),y\right)
p\left(\vec\theta\right)\,d\vec\theta={}\\
{}=\int\limits_{a^{-1}(y)}\!p\left(\vec{F}|\vec\theta\right)p\left(\vec\theta\right)\,d\vec\theta=
\int\limits_{a^{-1}(y)}\!p\left(\vec\theta|\vec{F}\right)p\left(\vec{F}\right)\,d\vec\theta\,.
\end{multline*}
Приближая распределение~$p(\vec\theta|\vec{F})$ вырожденным 
$\delta(\vec\theta-T(\vec{F}))$, получим
\begin{multline*}
\hat{p}(y|x)=
\!\int\limits_{a^{-1}(y)}\!\!\! \!p\left(\vec\theta|\vec{F}\right)\,d\vec\theta=
\!\int\limits_{a^{-1}(y)}\!\!\!\!\delta\left(\vec\theta-T\left(\vec{F}\right)\right)\,
d\vec\theta={}\\
{}=
\delta\left(a(T(\vec{F})),y\right).
\end{multline*}
Таким образом, задача классификации временн$\acute{\mbox{ы}}$х рядов свелась к~задаче 
классификации оценок параметров распределений 
семейства~$\left\{\mathsf{P}_{\vec\theta}\right\}_{\vec\theta\in \Theta}$.

В качестве оценок параметров~$\vec\theta$ предлагается брать оценки максимального 
правдоподобия:
\begin{multline*}
\hat{\vec\theta}=
T(x)=
\argmax_{\vec\theta\in\Theta}\mathcal{L}\left(\vec\theta\,|\,x\right)=
\argmax_{\vec\theta\in\Theta}p(\vec{F}|\vec\theta)={}\\
{}=
\argmax_{\vec\theta\in\Theta}\prod_{k}p(\mathbf{w}^{(k)}|\vec\theta).
\end{multline*}

Заметим, что в~частном случае тривиальной сегментации~\eqref{eq:equal_fragmenting} и~семейства вырожденных распределений оценка~$\hat{\vec\theta}$ является исходным признаковым описанием.
Таким образом, предложенный подход к~построению признакового описания временн$\acute{\mbox{о}}$го ряда
\begin{equation*}
%\label{eq:parameter_estimation}
\mathbf{f}:\;x\mapsto\hat{\vec\theta}
\end{equation*}
является достаточно общим и~при этом хорошо интерпретируется.


\section{Алгоритм классификации}
%\label{sec:classification}

Для завершения построения классификатора временн$\acute{\mbox{ы}}$х 
рядов~\eqref{eq:classifiers} построим многоклассовый 
классификатор~$b$~\eqref{eq:classification} по обучающей 
выборке~$\left\{(\mathbf{f}(x),y)\,|\,(x,y)\hm\in\mathfrak{D}\right\}$.

Сведем задачу многоклассовой классификации к~задачам бинарной классификации 
при помощи стратегий One-vs-All и~One-vs-One.

В~данной работе для решения задач бинарной классификации, где~$Y=\{-1,+1\}$, 
берутся различные модификации SVM (support vector machine).

\vspace*{-6pt}


\section{Вычислительный эксперимент}
%\label{sec:computational_experiment}

\vspace*{-2pt}

Вычислительный эксперимент проводился на данных для задачи классификации 
типов физической активности человека.

\vspace*{-6pt}

\subsection{Датасет WISDM}

\vspace*{-2pt}

Датасет (набор данных) WISDM~\cite{Kwapisz:2011:ARU:1964897.1964918} содержит 
показания акселерометра для~6~видов человеческой активности.
Необработанные данные, пред\-став\-ля\-ющие собой последовательность размеченных 
показаний акселерометра (по тройке чисел на каждый отсчет времени с~интервалом 
в~50~мс), были разбиты на временн$\acute{\mbox{ы}}$е ряды длиной 
по~200~отсчетов~(10~с).
Распределение полученных временн$\acute{\mbox{ы}}$х рядов по классам приведено 
в~табл.~1.

\vspace*{12pt}

\noindent
 %tabl1
%\vspace*{3pt}
{{\tablename~1}\ \ \small{Распределение временн$\acute{\mbox{ы}}$х рядов по классам. Набор данных
  WISDM}}

{\small
\begin{center}
   \tabcolsep=14pt
  \begin{tabular}{|l|c|}
    \hline
    \multicolumn{1}{|c|}{Классы} & Число объектов\\
    \hline
    1.\ Jogging (бежит) &  1624\hphantom{9}\\
    2.\ Walking (идет) &  2087\hphantom{9}\\
    3.\ Upstairs (поднимается)& 549\\
    4.\ Downstairs (спускается)& 438\\
    5.\ Sitting (сидит) & 276\\
    6.\ Standing (стоит)& 231\\
     \hline
  \end{tabular}
  \end{center}}
  
  \addtocounter{table}{1}
%\end{table*}

\subsubsection{Ручное выделение признаков}

\paragraph*{Выбор признаков.}
%\label{par:manual_feature_selection}
Каждая компонента вре\-мен\-н$\acute{\mbox{о}}$\-го ряда описывалась ее средним, 
стандартным отклонением, средним модулем отклонения от среднего, гистограммой 
с~10~областями равной \mbox{ширины}.
Полученные признаки для каждой компоненты объединялись, и~к~ним добавлялся признак 
средней величины ускорения.
Таким образом, каждый временной ряд описывался~40~признаками.


\vspace*{-12pt}

\paragraph*{Классификатор.}
Задача многоклассовой классификации сводилась к~задаче бинарной 
классификации при помощи подхода One-vs-One.
В качестве бинарного классификатора использовался SVM с~RBF (radial basis function)
яд\-ром 
и~па\-ра\-мет\-ра\-ми $C\hm=8{,}5$ и~$\gamma\hm=0{,}12$.

\vspace*{-12pt}

\paragraph*{Результаты.}
На диаграмме рис.~1 демонстрируется качество классификации при усреднении по 
$r\hm=50$  случайным разбиениям исходной выборки на тес\-то\-вую и~контрольную 
в~пропорции~7 к~3.





Как видно из~табл.~2, классы~2, 3 и~4 недостаточно хорошо отделяются друг от друга.

\pagebreak

\end{multicols}
\begin{figure*} %fig1-2
 \vspace*{1pt}
 \begin{minipage}[t]{80mm}
 \begin{center}  
 \mbox{%
\epsfxsize=78.057mm
\epsfbox{kar-1.eps}
}
\end{center}
\vspace*{-11pt}
\Caption{Набор данных WISDM.
Средняя точность~0,9726~--- вычисляется по формуле~\eqref{eq:total_quality}.
Средние точности классификации для каждого класса вычисляются по формуле~\eqref{eq:class_quality}}
%\end{figure}
%\begin{figure}
\end{minipage}
\hfill
 \vspace*{1pt}
  \begin{minipage}[t]{80mm}
 \begin{center}  
 \mbox{%
\epsfxsize=78.057mm
\epsfbox{kar-2.eps}
}
\end{center}
\vspace*{-11pt}
\Caption{Точность классификации для параметров модели авторегрессии в~качестве признаковых описаний}
\end{minipage}
\end{figure*}

\begin{table*}\small %tabl2
\begin{minipage}[t]{80mm}
\begin{center}
\Caption{Усредненная матрица неточностей. Ручное выделение признаков. 
Набор данных WISDM\newline
}
\vspace*{2ex}

\tabcolsep=6.4pt
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Класс & \multicolumn{6}{c|}{Предсказанный класс} \\ 
  \cline{2-7}
объекта & $1$ & $2$ & $3$ & $4$ & $5$ & $6$\\ 
\hline
 1 & \textbf{1,00} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 2 & 0,00 & \textbf{0,99} & 0,01 & 0,00 & 0,00 & 0,00\\ 
 3 & 0,03 & 0,04 & \textbf{0,89} & $0,04$ & 0,00 & 0,00\\ 
 4 & 0,02 & 0,05 & 0,05 & \textbf{0,88} & 0,00 & 0,00\\ 
 5 & 0,01 & 0,00 & 0,00 & 0,00 & \textbf{0,98} & 0,00\\ 
 6 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & \textbf{1,00}\\ 
 \hline
\end{tabular}
\end{center}
%\end{table*}
\end{minipage}
\hfill
%\begin{table}\small %tabl3
\begin{minipage}[t]{80mm}
\begin{center}
\Caption{Усредненная матрица неточностей. Признаки, порожденные моделью 
авторегрессии. Набор данных ~WISDM}
\vspace*{2ex}


\tabcolsep=6.5pt
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Класс & \multicolumn{6}{c|}{Предсказанный класс} \\ 
  \cline{2-7}
объекта & 1 & 2 & 3 & 4 & 5 & 6\\ 
  \hline
 1 & \textbf{1,00} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 2 & 0,00 & \textbf{0,99} & 0,00 & 0,00 & 0,00 & 0,00\\ 
 3 & 0,01 & 0,02 & \textbf{0,95} & 0,02 & 0,00 & 0,00\\ 
 4 & 0,00 & 0,02 & 0,04 & \textbf{0,94} & 0,00 & 0,00\\ 
 5 & 0,01 & 0,00 & 0,00 & 0,00 & \textbf{0,97} & 0,01\\ 
 6 & 0,01 & 0,00 & 0,00 & 0,00 & 0,01 & \textbf{0,97}\\ 
\hline
\end{tabular}
\end{center}
\end{minipage}
\end{table*}

\begin{multicols}{2}


\subsubsection{Модель авторегрессии} %~(\ref{eq:autoregressive_model})}

\paragraph*{Признаковое описание.}
%\label{par:ar_feature_selection}
Во втором эксперименте в~качестве признаковых описаний временн$\acute{\mbox{ы}}$х 
рядов использовались все статистические функции, что брались в~первом эксперименте, 
за исключением гистограммы, вместо которой использовалось~7~коэффициентов 
модели авторегрессии AR($6$)~(см.~\eqref{eq:autoregressive_model}).
Таким образом, каждый временной ряд описывался~31~числом.
Также проводилась предварительная нормализация признаков.

\vspace*{-6pt}

\paragraph*{Классификатор.}
Задача многоклассовой классификации сводилась к~задаче 
бинарной классификации при помощи подхода One-vs-All.
В~качестве бинарного классификатора использовалась SVM с~RBF-яд\=ром 
и~параметрами $C\hm=8$ и~$\gamma\hm=0{,}8$.

\vspace*{-6pt}

\paragraph*{Результаты.}
На диаграмме рис.~2 и~в~табл.~3 показано качество классификации при усреднении по
$r\hm=50$ случайным разбиениям исходной выборки на тестовую и~контрольную 
в~отношении~7 к~3.




Несмотря на неравномерное распределение объектов по классам, 
использование признакового описания, порожденного моделью авторегрессии, 
позволяет значительно повысить качество классификации.
Точность построенного классификатора минимальна для~4-го класса~--- Downstairs~--- 
и~со\-став\-ля\-ет~94,3\%.

\subsection{Датасет USC-HAD}
%\label{seq:usc_had_dataset}

\begin{figure*}[b] %fig3
\vspace*{6pt}
\begin{center}
\mbox{%
\epsfxsize=128.626mm
\epsfbox{kar-3.eps}
}
\end{center}
\vspace*{-9pt}
\Caption{Точность классификации для параметров модели авторегрессии 
в~качестве признаковых описаний}
\label{fig:USCHAD_AR_FOURIER}
\end{figure*}


Датасет USC-HAD~\cite{mi12:ubicomp-sagaware} содержит показания акселерометра 
для~12~типов физической активности человека:
\begin{enumerate}[1)]
  \item walk forward (идет вперед);
  \item walk left (идет влево);
  \item walk right (идет вправо);
  \item go upstairs (подъем по лестнице);
  \item go downstairs (спуск по лестнице);
  \item run forward (бежит вперед);
  \item jump up and down (делает прыжок);
  \item sit and fidget (сидит);
  \item stand (стоит);
  
  \pagebreak
  
  
  \item sleep (спит);
  \item elevator up (поднимается в~лифте);
  \item elevator down (спускается в~лифте).
\end{enumerate}

Выборка содержит примерно по~70~шестикомпонентных временн$\acute{\mbox{ы}}$х 
рядов для каждого класса, а средняя длина временн$\acute{\mbox{о}}$го ряда~--- 3300. 
Частота записи измерений сенсора~100~Гц.

\vspace*{-6pt}


\subsubsection{Модель авторегрессии %~(\ref{eq:autoregressive_model}) 
и~Фурье} %~(\ref{eq:fourier})}

\vspace*{-2pt}

\paragraph*{Признаковое описание.}
%\label{par:ar_fourier_feature_selection_USCHAD}
Исходные временн$\acute{\mbox{ы}}$е ряды приводились к~частоте~10~Гц 
при помощи осреднения.

В качестве признаковых описаний преобразованных временн$\acute{\mbox{ы}}$х рядов брались 
статистические функции, описанные в~п.~7.1.1, 
за исключением гистограммы.
Также для каждой компоненты отдельно и~для модуля результирующего 
ускорения и~поворота добавлялось по~11~параметров авторегрессионной 
модели~$\text{AR}(10)$~(см.~\eqref{eq:autoregressive_model}).
Затем проводилась нормализация признаков и~добавлялись коэффициенты 
Фурье~\eqref{eq:fourier} с~индексами~3--12.
Таким образом, каждый~6-ком\-по\-нент\-ный временной ряд описывался~128~признаками.

\vspace*{-12pt}

\paragraph*{Классификатор.}
Задача многоклассовой классификации сводилась к~задаче бинарной 
классификации при помощи подхода One-vs-One.
В~качестве бинарного классификатора использовалась SVM с~RBF-яд\-ром и~параметрами 
$C\hm=10$ и~$\gamma\hm=0{,}13$.

%\vspace*{-12pt}

\paragraph*{Результаты.}
На диаграмме рис.~3 показано качество классификации 
при усреднении по $r\hm=500$ случайным разбиениям исходной выборки на тес\-то\-вую 
и~контрольную в~отношении~7 к~3.






Из~табл.~4 видно, что использование коэффициентов Фурье значительно повысило 
качество классификации.
Хуже всего класс~8 (sit and fidget) отделяется от класса~9 (stand).
Точность классификации для него составляет~92,2\%.

\vspace*{-6pt}


\subsubsection{Классификация голосованием и~классификация в~пространстве 
распределений параметров}

\vspace*{-2pt}

Рассмотрим алгоритм классификации в~сочетании с~процедурой сегментации временн$\acute{\mbox{ы}}$х 
рядов.
В качестве процедуры сегментации~$S(x)$ (см.~\eqref{eq:segmentation}) 
будем использовать выделение сегментов фиксированной длины.
Решим задачу классификации для первых~10~классов (за исключением <<elevator up>> 
и~<<elevator down>>, которые плохо отделяются друг от друга при малой длине сегментов) 
двумя алгоритмами.

В алгоритме голосования классификатор~$b:\;\mathbb{R}^n\hm\to Y$ обучается на 
новой обучающей выборке для сегментов исходных временн$\acute{\mbox{ы}}$х рядов

\noindent
$$
\mathfrak{D}_S=\left\{(\mathbf{w}_g(s),y):\;(x,y)\in\mathfrak{D},\,s\in S(x)\right\}.
$$
Далее производится голосование
$\hat{y}\hm=\argmax_{y}\sum\limits_{s\in S(x)}1\left[b(\mathbf{w}_g(s))=y\right].$

Алгоритм классификации в~пространстве гиперпараметров 
(распределений параметров аппрокси-\linebreak\vspace*{-12pt}

\pagebreak

\end{multicols}

\begin{table*}\small %tabl4
\begin{center}
\parbox{396pt}{\Caption{Усредненная матрица неточностей. Признаки, порожденные моделью авторегрессии. 
Набор данных USC-HAD}
\label{tbl:USCHAD_AR_FOURIER_confusion}
}

\vspace*{2ex}

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
Класс & \multicolumn{12}{c|}{Предсказанный класс} \\ 
\cline{2-13}
объекта & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12\\ 
\hline
 1& \textbf{0{,}99} & 0,00 & 0,00 & 0,00 & 0,01 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 2& 0,01 & \textbf{0{,}97} & 0,01 & 0,00 & 0,01 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 3& 0,00 & 0,00 & \textbf{1{,}00} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 4& 0,00 & 0,00 & 0,00 & \textbf{0{,}99} & 0,01 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 5& 0,00 & 0,00 & 0,00 & 0,01 & \textbf{0{,}97} & 0,02 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 6& 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & \textbf{1{,}00} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 7& 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & \textbf{0{,}99} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 8& 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & \textbf{0{,}92} & 0,08 & 0,00 & 0,00 & 0,00\\ 
 9& 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,01 & \textbf{0{,}99} & 0,00 & 0,00 & 0,00\\ 
 10\hphantom{9} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & \textbf{1{,}00} & 0,00 & 0,00\\ 
 11\hphantom{9} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00& 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 
 \textbf{1{,}00} & 0,00\\ 
 12\hphantom{9} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 &0,00 & 0,00 & 0,00 & 0,02 & 0,00 & 0,01 & \textbf{0{,}97}\\ 
 \hline
\end{tabular}
\end{center}
\end{table*}
\begin{figure*} %fig4
\vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=87.865mm
\epsfbox{kar-4.eps}
}
\end{center}
\vspace*{-9pt}
\Caption{Зависимость средней точности классификации от длины сегментов:
\textit{1}~--- голосование;
\textit{2}~--- гиперпараметры.
Набор данных USC-HAD, первые $10$~классов.
Точность классификации вычисляется по формуле~\eqref{eq:total_quality}}
\label{fig:USCHAD_AR_VOTING_VS_DISTR}
\end{figure*}

\begin{multicols}{2}

\noindent
мирующих моделей был описан 
в~разд.~5).
В~эксперименте использовалось семейство нормальных распреде\-лений с~диагональной 
ковариационной мат\-рицей.

Задача многоклассовой классификации решалась при помощи подхода One-vs-One 
бинарными классификаторами SVM с~RBF-яд\-ром и~параметрами $C\hm=100$
и~$\gamma\hm=0{,}017$.

На графике рис.~4 приведены результаты для средней 
точности решения задачи многоклассовой классификации обоими алгоритмами.

Из графика можно видеть, что оба алгоритма позволяют повысить качество 
классификации, причем алгоритм классификации в~пространстве гиперпараметров 
при длине сегмента~50 достигает качества~98,2\% и~показывает результат выше, 
чем алгоритм голосования.

Объединим результаты из последних двух экспериментов.
Будем обучать два классификатора.
Первый классификатор~$a_1$~--- One-vs-One SVM с~RBF-яд\-ром и~параметрами
$C\hm=10$ и~$\gamma\hm=0{,}13$~--- будет разделять классы~11, 12 и~первые 
десять классов для исходных временн$\acute{\mbox{ы}}$х рядов.
Второй классификатор~$a_2$~--- One-vs-One SVM с~RBF-яд\-ром 
и~параметрами $C\hm=100$ и~$\gamma\hm=0{,}017$~--- 
классификатор в~пространстве гиперпараметров, описанный в~предыду\-щем эксперименте.

Итоговый классификатор выглядит следующим образом:
\begin{equation}
\label{eq:final_classifier}
a(x)=\begin{cases}
a_1(x), &\ a_1(x)\in\{11, 12\}\,;\\
a_2(x)\ &\ \mbox{иначе}\,.
\end{cases}
\end{equation}


\paragraph*{Результаты.}
На диаграмме рис.~5 и~в~табл.~5 демонстрируется качество 
классификации построенного классификатора~\eqref{eq:final_classifier} 
при усреднении по $r\hm=500$ случайным разбиениям исходной выборки на тес\-то\-вую 
и~контрольную в~отношении~7 к~3.

\pagebreak

\end{multicols}

\begin{figure*} %fig5
\vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=128.626mm
\epsfbox{kar-5.eps}
}
\end{center}
\vspace*{-9pt}
\Caption{Точность классификации для гиперпараметров в~качестве признаковых описаний.
Набор данных USC-HAD}
\label{fig:hyperparams}
\end{figure*}


\begin{table*}\small %tabl5
\begin{center}
\parbox{380pt}{\Caption{Усредненная матрица неточностей. Признаки~--- гиперпараметры. Набор данных
USC-HAD}

}
\label{tbl:hyperparams_confusion}
\vspace*{2ex}

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
Класс & \multicolumn{12}{c|}{Предсказанный класс} \\ 
\cline{2-13}
объекта & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8& 9 & 10 & 11 & 12\\ 
\hline
 1 & \textbf{1{,}00} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 2 & 0,01 & \textbf{0,98} & 0,01 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 3 & 0,00 & 0,00 & \textbf{0{,}99} & 0,01 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 4 & 0,00 & 0,00 & 0,00 & \textbf{0{,}99} & 0,01 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 5 & 0,01 & 0,01 & 0,00 & 0,00 & \textbf{0{,}97} & 0,01 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 6 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & \textbf{1{,}00} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 7 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & \textbf{0{,}99} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00\\ 
 8 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & \textbf{0{,}93} & 0,06 & 0,00 & 0,00 & 0,00\\
 9 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,03 & \textbf{0{,}97} & 0,00 & 0,00 & 0,00\\
 10\hphantom{9} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & \textbf{1{,}00} & 0,00 & 0,00\\ 
 11\hphantom{9} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & \textbf{0{,}99} & 0,00\\
 12\hphantom{9} & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,00 & 0,02 & 0,00 & 0,01 & 
 \textbf{0{,}97}\\
 \hline
\end{tabular}
\end{center}
\end{table*}

\begin{multicols}{2}

\section{Заключение}

В работе показано, что метод признакового описания временн$\acute{\mbox{о}}$го 
ряда оптимальными параметрами аппроксимирующих его моделей дает высокое 
качество решения задачи классификации.
Предложенный метод вычислительно эффективен и~не требователен к~памяти 
вычислительного устройства.

В~работе также предложен алгоритм классификации временн$\acute{\mbox{ы}}$х рядов в~пространстве 
распределений параметров моделей, порождающих их\linebreak сегменты. 
Он обобщает предыдущий метод классификации временн$\acute{\mbox{ы}}$х рядов и~позволяет 
производить более тонкую настройку алгоритма клас\-си\-фи\-кации.
{ %\looseness=1

}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{geurts2005segment}
\Au{Geurts~P., Wehenkel~L.}
 Segment and combine approach for non-parametric time-series classification~//
{Knowledge discovery in databases: PKDD 2005}.~--- Berlin--Heidelberg: Springer, 2005. 
P.~478--485.

\bibitem{Esling:2012:TDM:2379776.2379788}
\Au{Esling~P., Agon~C.}
Time-series data mining~//
\newblock {ACM Comput. Surv.}, 2012. Vol.~45. No.\,1. Article~12. P.~1--34.

\bibitem{basil2014automatic}
\Au{Basil~T., Lakshminarayan~C.}
Automatic classification of heartbeats~//
{22nd European Signal Processing Conference Proceedings}, 2014. P.~1542--1546.

\bibitem{alomari2013automated}
\Au{Alomari~M.\,H., Samaha~A., AlKamha~K.}
 Automated classification of l/r hand movement eeg signals using advanced 
 feature extraction and machine learning~//
{Int. J.~Adv. Comput. Sci. Appl.}, 2013. Vol.~4. No.\,6. P.~207--212.

\bibitem{Kwapisz:2011:ARU:1964897.1964918}
\Au{Kwapisz~J.\,R., Weiss~G.\,M., Moore~S.\,A.}
Activity recognition using cell phone accelerometers~//
{ACM SigKDD Explorations Newsletter}, 2011. Vol.~12. No.\,2. P.~74--82.

\bibitem{gruber2006signature}
\Au{Gruber~C., Coduro~M., Sick~B.}
 Signature verification with dynamic rbf networks and time series motifs~//
{10th  Workshop (International) on Frontiers in Handwriting Recognition}.
 La Baule, 2006. 
P.~455--460.

\bibitem{Ding:2008:QMT:1454159.1454226}
\Au{Ding~H., Trajcevski~G., Scheuermann~P., Wang~X., Keogh~E.}
Querying and mining of time series data: Experimental comparison of representations and distance measures~//
{Proc. VLDB Endow}, 2008. Vol.~1. No.\,2. P.~1542--1552.
 doi:10.14778/1454159.1454226.

\bibitem{jeong2011weighted}
\Au{Jeong~Y.\,S., Jeong~M.\,K., Omitaomu~O.\,A.}
Weighted dynamic time warping for time series classification~//
{Pattern Recogn.}, 2011. Vol.~44. No.\,9. P.~2231--2240.
doi:10.1016/j.patcog.2010.09.022.

\bibitem{Nanopoulos01feature-basedclassification}
\Au{Nanopoulos~A., Alcock~R., Manolopoulos~Y.}
Feature-based classification of time-series data~//
{Int. J.~Comput. Res.}, 2001. Vol.~10. P.~49--61.

\bibitem{wiens2012patient}
\Au{Wiens~J., Horvitz~E., Guttag~J.\,V.}
Patient risk stratification for hospital-associated c. diff as a time-series classification task~//
{Adv. Neur. Inform. Proc. Syst.}, 2012. Vol.~25. P.~467--475.

\bibitem{morchen2003time}
\Au{M$\ddot{\mbox{o}}$rchen~F.}
 Time series feature extraction for data mining using dwt and dft,
 2003. Unpubl.

\bibitem{kini2013large}
\Au{Kini~B.\,V., Sekhar~C.\,C.}
 Large margin mixture of ar models for time series classification~//
{Appl. Soft Comp.}, 2013. Vol.~13. No.\,1. P.~361--371.

\bibitem{kuznetsov2015description}
\Au{Кузнецов М.\,П., Ивкин Н.\,П.}
 Алгоритм классификации временных рядов акселерометра по комбинированному признаковому описанию~//
 {Машинное обучение и~анализ данных}, 2015. Т.~1. №\,11. С.~1471--1483.

\bibitem{mi12:ubicomp-sagaware}
\Au{Zhang~M., Sawchuk~A.\,A.}
 USC-HAD: A~daily activity dataset for ubiquitous activity recognition 
 using wearable sensors~//
 {ACM Conference (International) on Ubiquitous Computing Workshop on Situation, 
 Activity and Goal Awareness}.~--- Pittsburgh, PA, USA, 2012.
 \end{thebibliography}

 }
 }

\end{multicols}

%\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 10.05.16}}

\vspace*{14pt}

%\newpage

%\vspace*{-24pt}

\hrule

\vspace*{2pt}

\hrule

\vspace*{8pt}


\def\tit{FEATURE-BASED TIME-SERIES CLASSIFICATION}

\def\titkol{Feature-based time-series classification}

\def\aut{M.\,E.~Karasikov$^{1,2}$ and V.\,V.~Strijov$^3$}

\def\autkol{M.\,E.~Karasikov and V.\,V.~Strijov}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-9pt}


    
\noindent
   $^1$Moscow Institute of Physics and Technology,
    9~Institutskiy Per., 
Dolgoprudny, Moscow Region 141700, Russian\linebreak
$\hphantom{^1}$Federation
    
\noindent
$^2$Skolkovo Institute of Science and Technology, Skolkovo Innovation Center,
Building~3, Moscow 143016, Russian\linebreak
$\hphantom{^1}$Federation

\noindent
$^3$A.\,A.~Dorodnicyn Computing Center, 
Federal Research Center ``Computer Science and Control'' 
of the Russian\linebreak
$\hphantom{^1}$Academy of Sciences, 44-2~Vavilov Str., Moscow 119333, 
Russian Federation



\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2016\ \ \ volume~10\ \ \ issue\ 4}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2016\ \ \ volume~10\ \ \ issue\ 4
\hfill \textbf{\thepage}}}

\vspace*{3pt}


     
\Abste{The paper is devoted to the multiclass time series classification problem.
The feature-based approach that uses meaningful and concise representations for feature space construction is applied.
    A~time series is considered as a sequence of segments 
    approximated by parametric models, and their parameters are used as time series 
    features.
        This feature construction method inherits from the
        approximation model such unique properties as shift invariance.
    The authors propose an approach to solve the time series classification problem 
    using distributions of parameters of the approximation model.
    The proposed approach is applied to the human activity classification problem.
    The computational experiments on real data demonstrate superiority of
    the proposed algorithm over baseline solutions.}

\KWE{time series; multiclass classification; time series segmentation; hyperparameters of approximation model; autoregressive model; discrete Fourier transform}

\DOI{10.14357/19922264160413} 


%\vspace*{-9pt}

\Ack
\noindent
The work was supported by the Russian Foundation for Basic Research 
(project 16-37-00485).

\pagebreak

%\vspace*{3pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}

\bibitem{geurts2005segment-1}
\Aue{Geurts,~P., and L.~Wehenkel}.
2005.
 Segment and combine approach for non-parametric time-series classification.
\textit{Knowledge Discovery in Databases: PKDD 2005}. 
Berlin--Heidelberg: Springer. 478--485.

\bibitem{Esling:2012:TDM:2379776.23797880-1}
\Aue{Esling,~P., and C.~Agon}.
2012.
Time-series data mining.
\textit{ACM Comput. Surv}. 45(1):12:1--12:34.

\bibitem{basil2014automatic-1}
\Aue{Basil,~T., and C.~Lakshminarayan}.
2014.
Automatic classification of heartbeats.
\textit{22nd European Signal Processing Conference Proceedings}. 1542--1546.

\bibitem{alomari2013automated-1}
\Aue{Alomari,~M.\,H., A.~Samaha, and K.~AlKamha}.
2013.
Automated classification of l/r hand movement eeg signals using advanced feature extraction and machine learning.
\textit{Int. J.~Adv. Comput. Sci. Appl.} 4(6):207--212.

\bibitem{Kwapisz:2011:ARU:1964897.1964918-1}
\Aue{Kwapisz,~J.\,R., G.\,M.~Weiss, and S.\,A.~Moore}.
2011.
Activity recognition using cell phone accelerometers.
\textit{ACM SigKDD Explorations Newsletter} 12(2):74--82.

\bibitem{gruber2006signature-1}
\Aue{Gruber,~C., M.~Coduro, and B.~Sick}.
2006.
 Signature verification with dynamic rbf networks and time series motifs.
\textit{10th  Workshop (International) on Frontiers in Handwriting Recognition}.
 La Baule. 455-460.

\bibitem{Ding:2008:QMT:1454159.1454226-1}
\Aue{Ding,~H., G.~Trajcevski, P.~Scheuermann, X.~Wang, and E.~Keogh}.
2008.
 Querying and mining of time series data: Experimental comparison of 
 representations and distance measures.
\textit{Proc. VLDB Endow} 1(2):1542--1552.
 doi: 10.14778/1454159.1454226.

\bibitem{jeong2011weighted-1}
\Aue{Jeong,~Y.\,S., M.\,K.~Jeong, and O.\,A.~Omitaomu}.
2011.
 Weighted dynamic time warping for time series classification.
\textit{Pattern Recogn.}. 44(9):2231--2240.
doi: 10.1016/j.patcog.2010.09.022.

\bibitem{Nanopoulos01feature-basedclassification-1}
\Aue{Nanopoulos,~A., R.~Alcock, and Y.~Manolopoulos}.
2001.
Feature-based classification of time-series data.
\textit{Int. J.~Comput. Res.} 10:49--61.

\bibitem{wiens2012patient-1}
\Aue{Wiens,~J., E.~Horvitz, and J.\,V.~Guttag.}
2012.
Patient risk stratification for hospital-associated c. diff as a time-series classification task.
\textit{Adv. Neur. Inform. Proc. Syst.} 25:467--475.

\bibitem{morchen2003time-1}
\Aue{M$\ddot{\mbox{o}}$rchen,~F.}
2003.
Time series feature extraction for data mining using dwt and dft.
Unpubl.

\bibitem{kini2013large-1}
\Aue{Kini,~B.\,V., and C.\,C.~Sekhar}.
2013.
 Large margin mixture of ar models for time series classification.
\textit{Appl. Soft Comp.} 13(1):361--371.

\bibitem{kuznetsov2015description-1}
\Au{Kuznetsov,~M.\,P., and N.\,P.~Ivkin.}
2015.
Algoritm klassifikatsii vremennykh ryadov akselerometra po kombinirovannomu 
priznakovomu opisaniyu [Time series classification algorithm using combined feature description].
\textit{Mashinnoe obuchenie i~analiz dannykh} [Machine Learning and Data Analysis]
 1(11):1471--1483.

\bibitem{mi12:ubicomp-sagaware-1}
\Aue{Zhang,~M., and A.\,A.~Sawchuk.}
2012.
USC-HAD: A~daily activity dataset for ubiquitous activity recognition
  using wearable sensors.
\textit{ACM Conference (International)
on Ubiquitous Computing Workshop on Situation, Activity and Goal Awareness}. 
Pittsburgh, PA.
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-3pt}

\hfill{\small\textit{Received May 10, 2016}}


\Contr

\noindent
\textbf{Karasikov Mikhail E.} (b.\ 1992)~--- 
student, Moscow Institute of Physics and Technology, 9~Institutskiy Per., 
Dolgoprudny, Moscow Region 141700, Russian Federation;  
student, Skolkovo Institute of Science and Technology, Skolkovo Innovation Center,
Building~3, Moscow 143016, Russian Federation; \mbox{karasikov@phystech.edu}

\vspace*{3pt}

\noindent
\textbf{Strijov Vadim V.} (b.\ 1967)~--- 
Doctor of Science in physics and mathematics, leading scientist, 
A.\,A.~Dorodnicyn Computing Center, Federal Research Center ``Computer Science and Control'' 
of the Russian Academy of Sciences, 44-2~Vavilov Str., Moscow 119333, 
Russian Federation; \mbox{strijov@ccas.ru}
\label{end\stat}


\renewcommand{\bibname}{\protect\rm Литература} 
\def\stat{krivenko}

\def\tit{КРИТЕРИИ ВЫБОРА РАЗМЕРНОСТИ МОДЕЛИ ФАКТОРИЗАЦИИ}

\def\titkol{Критерии выбора размерности модели факторизации}

\def\aut{М.\,П.~Кривенко$^1$}

\def\autkol{М.\,П.~Кривенко}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Кривенко М.\,П.}
\index{Krivenko M.\,P.}


%{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
%{Исследование выполнено за счет Российского научного фонда (грант №\,21-79-00142).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Федеральный исследовательский центр <<Информатика и~управление>> 
Российской академии наук, \mbox{mkrivenko@ipiran.ru}}

\vspace*{-12pt}




\Abst{Работа посвящена выбору размерности модели факторизации матрицы 
с~пропущенными элементами. Задача оценивания параметров принятой модели данных 
решается путем многомерной оптимизации квадратичной целевой функции. Оценивание 
значения сниженной размерности~--- типичный пример задачи выбора модели, когда в~ходе 
анализа данных возникает альтернатива, а выбор означает либо выяснение предпочтений 
отдельных вариантов, либо выделение <<лучшего>> представителя. Обычно применяемые 
критерии выбора основываются на функции правдоподобия, для чего требуются 
вероятностные предположения относительно данных. Но при оценивании параметров 
рассматриваемой факторной модели они не задаются, а вводить их нецелесообразно, ибо 
можно нарушить общность сформулированной задачи снижения размерности. Поэтому была 
предпринята попытка обратиться к~идее использовать имеющиеся данные для целей 
статистического вывода повторно. Ни один из существующих подходов (бутстреп, складного 
ножа, перепроверки, а~также перестановочные тесты) не подходит, поэтому был предложен 
оригинальный метод формирования новых данных путем дополнительных пропусков 
элементов исходной матрицы. Для обработки сформированных выборок предлагается 
использовать комбинацию модели смеси нормальных распределений совместно с ядерным 
сглаживанием. Предложенные решения позволяют корректно проводить процедуру 
обоснования размерности принятой модели факторизации. Изложение иллюстрируется 
примером обработки синтетических данных.}

\KW{понижающая ранг аппроксимация матрицы; пропущенные данные; критерии выбора 
модели; методы повторной выборки; ядерное сглаживание}

\DOI{10.14357/19922264230207}{NQXYDC} 
  
\vspace*{3pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}

\section{Введение}

     Факторизация матриц данных хорошо зарекомендовала себя как метод 
снижения размерности в~таких областях, как разведочный анализ данных, 
сжатие передаваемой информации, визуализация, распознавание образов 
и~прогнозирование временн$\acute{\mbox{ы}}$х рядов. При этом все б$\acute{\mbox{о}}$льшую востребованность 
стали приобретать задачи с наличием пропусков в~данных. С~общей 
характеристикой этой проблемы можно ознакомиться, например, в~[1]. В~[2] 
обращено внимание на построение алгоритмов, учитывающих в~ходе 
оценивания параметров факторизации специфические особенности 
обрабатываемых матриц.
     
     Модель факторизации ($m\times n$)-мат\-ри\-цы 
наблюдений~$\mathbf{Y}$~--- это представление ее в~виде 
$\tilde{\mathbf{Y}}\hm= \mathbf{UV}^{\mathrm{T}}$, где $\mathbf{U}$~--- 
($m\times r$)-мат\-ри\-ца; $\mathbf{V}$~--- ($n\times r$)-мат\-ри\-ца; $r$~--- 
размерность модели (Factorization Model Dimension, FMD). Наличие пропусков 
в~данных отражается в~($m\times n$)-мат\-ри\-це~$\mathbf{H}$:
     $$
     h_{ij}= \begin{cases}
     1\,, & y_{ij}\ \mbox{присутствует};\\
     0\,, & y_{ij}\ \mbox{пропущено}.
     \end{cases}
     $$
Для суммы всех ее элементов примем обозначение~$p$ (число 
присутствующих элементов матрицы наблюдений~$\mathbf{Y}$).

     \textbf{Задача построения}~$\tilde{\mathbf{Y}}$ формулируется как 
минимизация целевой функции:
     $$
     \varphi(\mathbf{U},\mathbf{V}) =\left\| \mathbf{H}\odot (\mathbf{Y}-
\tilde{\mathbf{Y}} )\right\|^2_F \to \min\limits_{\mathbf{U},\mathbf{V}}.
     $$
Перепишем целевую функцию в~более удобном для аналитических 
преобразований виде, исключив~$\mathbf{H}$. Построчная запись матрицы 
$\mathbf{U}\hm= [\bm{u}_1, \ldots, \bm{u}_m]^{\mathrm{T}}$, где $\bm{u}_i$ 
суть $r$-век\-то\-ры, даст $mr$-век\-тор $\bm{u}\hm= [\bm{u}_1^{\mathrm{T}}, 
\ldots , \bm{u}_m^{\mathrm{T}}]^{\mathrm{T}}$. Также для 
матрицы~$\mathbf{V}$ определим $nr$-век\-тор~$\bm{v}$. В~результате 
$\varphi(\mathbf{U},\mathbf{V})$ можно переписать как
$$
\varphi(\mathbf{U},\mathbf{V}) \equiv \varphi(\bm{u},\bm{v}) =\vert 
\mathbf{F}\bm{u} -\bm{y}\vert^2 =\vert \mathbf{G}\bm{v} -\bm{y}\vert^2\,,
$$
где $p$-век\-тор~$\bm{y}$ формируется из соответствующих 
элементов~$\mathbf{Y}$, а ($p\times mr$)-мат\-ри\-ца~$\mathbf{F}$~--- из 
векторов~$\bm{v}_i$ и~($p\times nr$)-мат\-ри\-ца~$\mathbf{G}$~--- из 
векторов~$\bm{u}_i$ согласно только значениям $h_{ij}\hm=1$. 

     Для нахождения минимума $\hat{\varphi}(\mathbf{U},\mathbf{V})$ 
целевой функции принят альтернирующий алгоритм наименьших квадратов 
(Alternating Least Squares, ALS). Задание начального~$\bm{v}$ осуществляется 
с~по\-мощью методов обработки неполных данных (в данной работе~--- это 
заполнение пропусков средними значениями наблюденных значений 
с~последующим нахождением сингулярного разложения теперь уже полной 
матрицы данных и~формирование начального значения с помощью правых 
сингулярных векторов). Поблочная обработка возникающих в~ходе 
итерационного процесса матриц позволяет в~полной мере воспользоваться 
возможностями методов многократной повторной выборки~[3]. 
     
     Сравнение моделей для разных~$r$ осуществляется с помощью 
нормированной формы целевой функции $\hat{\varphi}(r)\hm= 
\hat{\varphi}(\mathbf{U},\mathbf{V})/p$, которая позволяет сопоставлять 
результаты оценивания параметров модели для различных значений 
интенсивности пропусков.
     
     Оценивание FMD~--- типичный пример задачи выбора модели, когда 
в~ходе анализа данных возникает альтернатива, а выбор означает либо 
выяснение предпочтений отдельных вариантов, либо выделение <<лучшего>> 
представителя. Как правило, альтернативные модели имеют вероятностный 
характер и~включают разное число параметров, причем чем больше параметров 
используется, тем лучше подгонка, которой можно достичь. Критерий выбора 
модели (Model-Selection Criterion, MSC) должен одновременно учитывать как 
пригодность модели, так и~число параметров, обеспечивающих ее 
использование. Фактически он дает ответ на вопрос: какая степень 
соответствия должна быть достигнута, чтобы оправдать включение 
дополнительных параметров в~модель? Примерами близких по содержанию 
и~достаточно проработанных задач могут служить оценивание количества 
элементов смеси распределений или выбор числа регрессоров. 
     
     Для выбора вероятностной модели привлекаются две основные группы 
методов~[4].
     
     \textbf{Критерии, основанные на функции правдоподобия со 
штрафами.} Они используют предельные свойства максимального 
правдоподобия и~в качестве целевой рассматривают функцию вида
\begin{multline*}
\mathrm{MSC}= {}\\
     {}=\left[ \begin{array}{c}
     \mbox{качество}\\ 
     \mbox{подгонки}
     \end{array}
     \right] + \left[ \begin{array}{c}
     \mbox{штраф,\ включающий\ число}\\ 
     \mbox{используемых\ параметров}
     \end{array}
     \right].
     \end{multline*}
Вариантов подобных критериев множество, в~частности сюда входят 
информационный критерий Акаике (Akaike's Information Criterion, AIC) и~байесовский информационный 
критерий Шварца (Bayesian Information Criterion, BIC). Заметим, что как обобщение при оценивании качества 
подгонки можно рассматривать любую целевую функцию (например, 
квадратичное отклонение).

     \textbf{Байесовский подход} к~проб\-ле\-ме выбора модели заключается 
не в~выборе какой-то одной модели, а~в~том, чтобы указать подходящее 
достоверное распределение для полностью исчерпывающего набора моделей, 
обновить эту информацию на основе данных и~использовать в~последующем 
анализе все модели с ненулевыми апостериорными вероятностями. Если 
подобный вывод не отвечает потребностям практики из-за своей нечеткости, 
можно ограничиться одной моделью с максимальной апостериорной 
вероятностью (Maximum \textit{a~Posteriori} Probability, MAP). 
     
     Между этими подходами существует достаточно тесная связь. Дело не 
только в~нормальном распределении, методах максимального правдоподобия 
и~наименьших квадратов. За счет подбора априорного распределения можно 
добиться, чтобы AIC или BIC, применяемые к~вложенным вероятностным 
моделям, имели тот же эффект, что и~оценка MAP, несмотря на различия 
в~происхождении подходов~[4].
     
     Наряду с двумя представленными базовыми подходами на практике 
используются как их расширения (в~частности, максимизация <<коэффициента 
эффективности>>~--- правдоподобия для каждой модели в~качестве меры 
точности, деленного на вычислительные затраты), так и~разной степени 
строгости эвристические приемы (в частности, последовательный перебор от 
простых к~более сложным альтернативам вкупе с бут\-стреп-ме\-то\-дами). 
     
     При любом из базовых способов выбора факторной модели требуются 
вероятностные предположения относительно данных, которые при оценивании 
параметров рассматриваемой факторной\linebreak модели не задавались. Вводить их 
напрямую нецелесообразно, ибо можно нарушить общность сформулированной 
задачи снижения размерности.\linebreak Поэтому объяснима попытка обратиться 
к~процедурам, которые для целей статистического вывода используют 
имеющиеся данные повторно. При этом обычно рассматриваются 
следующие конкретные методы~[5]: бутстреп, складного ножа, перепроверка, 
тесты перестановки.

\vspace*{-6pt}
     
\section{Повторная выборка путем дополнительных пропусков}

     Содержание задачи факторизации матрицы с~пропусками и~желание 
сохранить общность ее постановки не позволяют напрямую использовать 
перечисленные подходы: вместо выборки имеется только единственный 
структурированный элемент данных~--- матрица, относительно вероятностных 
характеристик которой ничего не предполагается. В~ходе экспериментов~[2] 
было обращено внимание на то, что значения квадратичной целевой функции 
при близких значениях вероятности пропуска практически не отличаются, 
поэтому дополнительное исключение отдельных элементов~$\mathbf{Y}$ по 
отношению к~уже имеющимся не должно существенно повлиять на результаты 
подгонки модели. Это в~совокупности со сформировавшимися принципами 
управления обработкой данных становится основой нового метода, когда 
образование выборок, назовем их RSM-выборками (ReSample by Missing), 
осуществляется путем повторного случайного пропуска элементов исходной 
матрицы данных. 
     
     Реализация дополнительных пропусков должна сохранить постановку 
исходной задачи, т.\,е.\ удалять элементы исходной матрицы надо так, чтобы не 
изменились значения~$m$ и~$n$. Простейшим алгоритмом в~этом случае 
оказывается достижение успеха при случайном выборе элемента матрицы 
среди непропущенных с отбраковкой его в~случае, когда удаление приводит 
к~уменьшению значений~$m$ и~$n$. Он не обременителен при малых 
значениях~$m$ и~$n$ и~небольшом числе пропусков в~исходной матрице. 
Удобно набор кандидатов на удаление сформировать заранее, а~затем 
использовать по усмотрению. 
     
     Сложности, в~первую очередь вычислительные, возрастают при росте 
значений~$m$ и~$n$ и~степени разреженности матрицы~$\mathbf{H}$. 
Обозначим через~$p_m$ число возможных дополнительных удалений 
элементов исходной матрицы, а через $N_{\mathrm{RSM}}$~--- требуемый 
размер RSM-вы\-бор\-ки. Далее необходимо рассмотреть два случая: $p_m$ 
известно (посчитано заранее) или нет.
     
     Если~$p_m$ известно, то получаем задачу случайной выборки 
$N_{\mathrm{RSM}}$ элементов из~$p_m$ имеющихся кандидатов на 
удаление. Соответствующий последовательный алгоритм формирования 
выборки из\-вес\-тен~[6]. Применительно к~рассматриваемой задаче факторизации 
требуется напомнить, что пропуск касается только элементов мат\-ри\-цы, 
которые можно удалять. 
     
     Если $p_m$ неизвестно, то можно посчитать его значение и~тем самым 
свести ситуацию к~уже описанной. При этом обращение к~перебору элементов 
исходной матрицы произойдет дважды. Но~[6] содержит оригинальный прием, 
подготавливающий при первом переборе резервуар (авторский термин), 
в~который входят менее чем~$p_m$~элементов~--- возможных кандидатов 
в~окончательную выборку. Правда, при этом на второй стадии алгоритма для 
того, чтобы выявить $N_{\mathrm{RSM}}$ нужных элементов, понадобится 
хоть и~частично, но отсортировать резервуар. 
     
     Многовариантность базовых алгоритмов случайной выборки 
дополнительно пропускаемых элементов матрицы вкупе с их различными 
модификациями и~солидный набор параметров $m$, $n$, $p$ 
и~$N_{\mathrm{RSM}}$ с~богатым спектром значений оставляют возможность 
выбора конкретного алгоритма только с~по\-мощью экспериментов.
     
     Для иллюстрации постановок задач и~методов их решения проводились 
эксперименты с искусственными данными по следующей схеме: задание 
объемов анализируемых данных~$m$ и~$n$, назначение сниженной 
размерности~$r_M$, генерирование случайных данных с фиксированными 
средним и~дисперсией, получение из них матрицы данных~$\mathbf{Y}$ 
в~подпространстве сниженной размерности, генерирование 
матрицы~$\mathbf{H}$ для определенной вероятности~$p_m$ пропуска, 
применение того или иного метода анализа данных. Для определенности 
рассматривалось ограничение $n\hm<m$. В~качестве критерия завершенности 
итерационного процесса использовался контроль относительного изменения 
значений целевой функции на последовательных шагах итераций $\mathrm{Tol}\hm= 
0{,}01$ вместе с ограничением числа шагов итераций $t_{\max}\hm= 100$.
     
     Построив RSM-выборки из элементов~$\bm{x}_i$ размерности~$d$, 
можно оценивать соответствующее распределение в~виде плотности $f(\bm{t})$ 
и~обращаться к~классическим вероятностным методам выбора модели. При 
этом доступны параметрические и~непараметрические подходы с оговоркой, 
что общность привлекаемых моделей не должна искажать постановку исходной 
задачи. Последнее в~первую очередь касается параметрической точки зрения, 
вследствие чего внимание было обращено на описание данных с помощью 
смеси нормальных распределений. Она позволяет обеспечивать желаемую 
точность аппроксимации благодаря наращиванию числа элементов смеси, 
сохраняя при этом в~силе преимущество относительно простых аналитических 
решений типовых вероятностных задач. 
     
     Использование смеси многомерных нормальных распределений  
с~плот\-ностью $f(\bm{t}\vert \bm{\mu}, \bm{\Sigma})$ основывается на 
представлении
     $$
     f(\bm{t}) =\sum\limits^k_{l=1} w_l f(\bm{t}\vert 
\bm{\mu}_l,\bm{\Sigma}_l).
     $$
     
     \begin{figure*} %fig1
\vspace*{1pt}
\begin{center}
   \mbox{%
\epsfxsize=155.726mm 
\epsfbox{kri-1.eps}
}

\end{center}
\vspace*{-9pt}
\Caption{Попарное сравнение гистограмм для $r\hm= 3$~(\textit{1}) и~4~(\textit{2})~(\textit{а}) и~$r\hm= 4$~(\textit{2}) 
и~5~(\textit{3})~(\textit{б})}
\end{figure*}
     
     Вместе с оценками для параметров смеси $\hat{w}_l$, 
$\hat{\bm{\mu}}_l$ и~$\hat{\bm{\Sigma}}_l$, $l\hm= 1,\ldots , k$, становится 
известна матрица апостериорных вероятностей принадлежности $i$-го 
наблюденного значения к~$j$-му элементу смеси, что, в~свою очередь, 
позволяет стратифицировать исходный набор данных 
и~получить~$k$~подвыборок. Для каждой $l$-й из них объемом~$n_l$ можно 
построить ядерную оценку плотности в~следующих предположениях~[7]:
     \begin{itemize}
\item распределение данных~--- $N(\hat{\bm{\mu}}_l, \hat{\bm{\Sigma}}_l)$;
\item $\hat{\hat{f}} (\bm{t},\mathbf{H} = (1/n_l) \sum\nolimits^{n_l}_{i=1} 
K_{\mathbf{H}} (\bm{t} -\hat{\bm{x}}_i)$ при 
$K_{\mathbf{H}}(\bm{t}) =\vert \mathbf{H}\vert^{-1/2} K(\mathbf{H}^{-
1/2} \bm{t})$ и~$K(\bm{t}) \hm= (2\pi)^{-d/2}\exp (-(1/2) \bm{t}^{\mathrm{T}} 
\bm{t})$, 
 $\mathbf{H} = \mathbf{H}_{\mathrm{AMISE}} = ( 4/(d+2))^{2/(d+4)} 
\hat{\bm{\Sigma}}_l n_l^{-2/(d+4)}$.
\end{itemize}
Здесь AMISE~--- Asymptotic Mean Integrated Squared error и~речь идет об 
асимптотически оптимальной сглаживающей 
матрице~$\mathbf{H}_{\mathrm{AMISE}}$. В~принятых обозначениях 
$$
f\left( \bm{t}\vert \bm{x}_i, \mathbf{H}_i\right) =K_{\bm {H}} (\bm{t} - 
\bm{x}_i).
$$
     
     Ключевым в~выборе оптимального параметра сглаживания~$\mathbf{H}$ 
становится предположение о~виде оцениваемой плотности распределения, что, 
на первый взгляд, лишено всякого смысла: оцениваем то, что знаем. Но это не 
так, и~причин здесь несколько: предположение о~конкретном виде 
распределения (в данном случае речь идет о~смеси нормальных) есть способ, 
пусть грубой, аппроксимации данных, которая, в~свою очередь, позволяет 
относительно просто найти для параметра окна сглаживания некоторое 
начальное приближение для его последующего уточнения. Кроме того, 
полученные оценки можно использовать для формирования представления 
о~свойствах непараметрических оценок в~наилучшем случае, когда известна 
оцениваемая плот\-ность (например, для иллюстрации эффекта <<проклятия 
размерности>>).
     
     После предварительных общих замечаний перейдем к~предлагаемым 
критериям выбора FMD.
     
     \textbf{Критерий попарного сравнения эмпирических 
распределений.} Для каждого значения~$r$ можно получить значение 
$\hat{\varphi}(r)$ нормированной целевой функции и~построить 
RSM-вы\-бор\-ку значений $\hat{\varphi}^{\mathrm{RSM}}(r)$, которая позволяет получить 
эмпирическую плотность распределения $\hat{f}(t;r)$ (далее речь идет 
о~гистограммной оценке) и~представление о~значимости отличий 
значений $\hat{\varphi}(r)$ для разных~$r$. Если рас\-смот\-реть пару  
RSM-рас\-пре\-де\-ле\-ний $\hat{\varphi}(r)$  и~$\hat{\varphi}(r+1)$, то видно, 
что они могут заметно различаться или нет в~зависимости от того, существенно 
или несущественно уменьшились значения целевой функции. Такое парное 
сравнение можно сделать шагом последовательной процедуры от меньшего 
значения~$r$ к~большему.
     
     Формализовать меру различия двух распределений можно, рассмотрев 
простейший однопороговый критерий значимости, статистику которого будем 
оценивать с помощью двух RSM-выборок из условия минимизации ошибки 
классификации $\mathrm{Err}_{\min}$ принятого критерия. Далее, если получившаяся 
оценка ошибки классификации мала, то будем считать целесообразным переход 
от случая~$r$ к~случаю $r\hm+1$, если же нет (около~0,5), то процесс 
усложнения модели факторизации прекратим и~в качестве оценки структурного 
параметра факторизации примем найденное~$r$ (не что иное, как реализация 
принципа <<брит\-вы Ок\-ка\-ма>>). 

Для иллюстрации рассмотрим случай 
$m\hm=50$, $n\hm=10$, $r_M\hm=4$, $p_m\hm=40\%$ и~сравним пары 
одномерных гистограмм для $r\hm=3$ и~$4$ (рис.~1). Если для $r\hm=3$ 
видно, что усложнение модели до $r\hm=4$ оправдано (плотности 
распределения RSM-зна\-че\-ний отчетливо отделимы друг от друга 
и~$\mathrm{Err}_{\min}\hm= 0\%$), то для $r\hm=4$ в~нем нет необходимости 
($\mathrm{Err}_{\min} \hm= 50\%$).
     




     Подобную процедуру можно рекомендовать только в~качестве 
разведочного анализа данных по ряду причин:
     \begin{itemize}
\item внешне парное сравнение очень схоже с условиями и~приемами 
двухвыборочных критериев значимости, но на поверку последние 
демонстрируют неприемлемые результаты; это связано с тем, что нулевая 
гипотеза должна формулироваться в~более общем виде, чем совпадение 
распределений; как результат~--- для сравнения приходится использовать 
упрощенные критерии значимости; 
\item требуется уточнить правило задания уровня значимости на каждом шаге, 
если принимается как условие задачи итоговый уровень зна\-чи\-мости всей 
последовательной процедуры;
\item неизвестные распределения $\hat{\varphi}(r)$, скорее всего, зависят 
от~$r$, но это не принимается во внимание в~описанной схеме принятия 
решения;
\item полностью не учитывается многомерность статистики критерия выбора 
FMD (многократно сравнивается лишь пара <<соседних>> одномерных 
распределений).
\end{itemize}

     Но перечисленные недостатки компенсируются простотой использования 
попарных сравнений,\linebreak в~особенности при больших размерах исходных мат\-риц 
данных. Например, при исследовании возможности прогнозирования 
химического состава мочевых камней у~пациентов с~уролитиазом~[2] без\linebreak 
особых затрат можно показать, что желание снизить количество анализируемых 
метаболических показателей мочи и~сыворотки крови только на основе 
матрицы исходных данных несостоятельно.
     
     \textbf{Критерий максимального правдоподобия} позволяет полнее 
описать альтернативу и~учесть реальную размерность при построении 
эмпирических распределений. 
     
     Если при статистическом выводе удастся предусмотреть фактическую 
размерность~$r_M$, то это \mbox{найдет} отражение в~обозначениях как $\varphi(r\vert 
r_M)$. Для того чтобы учесть многомерность статистики критерия выбора 
FMD, надо перейти к~анализу $r_{\max}$-мер\-но\-го набора 
     $$
     \left(\hat{\varphi}(1\vert r_M), \hat{\varphi}(2\vert r_M),\ldots , 
\hat{\varphi}(r_{\max}\vert r_M)\right)= \hat{\bm{\varphi}}(r_M),
$$
 где  
$r_{\max}$~--- заранее выбранная верхняя граница рассматриваемых 
возможных значений размерности. Тогда станет возможным сравнивать 
различные варианты выбора FMD, рассчитывая при этом на эффективность 
предлагаемых решений. Зависимость статистического вывода от фактической 
размерности FMD реализуется следующим образом. Для каждого 
значения~$r_M$ могут быть получены оценки параметров модели 
$\hat{\mathbf{U}}$ и~$\hat{\mathbf{V}}$, которые приводят к~аппроксимации 
исходных данных $\hat{\mathbf{Y}}\hm= \hat{\mathbf{U}} 
\hat{\mathbf{V}}^{\mathrm{T}}$ и~к~возможности для нее построить  
RSM-вы\-бор\-ку $\hat{\bm{\varphi}}^{\mathrm{RSM}}(r_M)$. В~результате 
эмпирическая плотность распределения, построенная на основе 
$\hat{\bm{\varphi}}^{\mathrm{RSM}}(r_M)$, даст оценку функции 
правдоподобия в~точке $\hat{\varphi}(r_M)$ для принятого значения~$r_M$.
     
     Для рассматриваемого иллюстративного примера результативность 
предлагаемой схемы обработки исходных данных (рис.~2) несомненна: 
     \begin{itemize}
\item ярко выраженный экстремум функции правдоподобия точно указывает на 
истинное значение~$r$;
\item более тщательная аппроксимация распределений RSM-вы\-бо\-рок, когда 
использование смеси из двух нормальных распределений (случай NM) 
дополняется ядерным сглаживанием (случай NM\&K), выглядит убедительней.
     \end{itemize}
     

     
   
     
     Дополнительные многократные эксперименты в~рамках иллюстративного 
примера показывают, что скудость априорной информации, относительно 
небольшие объемы исходных данных, а также лишь наброски идей о~том, как 
справляться со слож\-ностью возникающих проб\-лем, приводят к~понятному 
результату: в~среднем оценки реального значения~$r$ оказываются несколько 
завышенными, но не тривиальными. 



     Кроме использования зависимости $\hat{\varphi}(r\vert r_M)$ от~$r_M$, 
можно попробовать привлечь дополнительную информацию об априорном 
распределении размерности~$R_M$. Но чаще всего ее нет, и~чтобы как-то 
снять проблему роста набора параметров задачи факторизации, можно ввести 
упрощенную схему задания нужных вероятностей, используя лишь 
единственный параметр. Например, для этого принимается предположение, что 
с ростом~$r_M$ вероятности уменьшаются (т.\,е.\ более сложная модель 
оказывается менее вероятной) в~соответствии с геометрическим 
распределением:
     $$
     \mathrm{Pr}\left\{ R_M=r\right\} = p_1(1-p_1)^{r-1}, \enskip r=1,2,\ldots ,
     $$
где $p_1$~--- вероятность выбора одномерной модели. В~рассматриваемой 
задаче факторизации $r\hm\leq n$. Поэтому формально, если говорить о~$R_M$ и~за основу брать геометрическое распределение, соответствующие 
вероятности надо нормировать, но для байесовского вывода это 
непринципиально. Когда $p_1$ при\-бли\-жа\-ет\-ся к~0, распределение~$R_M$
начинает походить на равномерное, что фактически приводит к~оценке 
максимального правдоподобия. С~рос\-том~$p_1$ акцент на малых значениях\linebreak\vspace*{-12pt}

     { \begin{center}  %fig2
 \vspace*{6pt}
    \mbox{%
\epsfxsize=73.476mm 
\epsfbox{kri-2.eps}
}

\end{center}



\noindent
{{\figurename~2}\ \ \small{Правдоподобие как функция от размерности~$r$: \textit{1}~--- случай NM, когда 
распределение  
RSM-вы\-бор\-ки аппроксимируется смесью из двух нормальных распределений; \textit{2}~--- случай 
NM\&K, когда указанная аппроксимация дополняется ядерным сглаживанием
}}}

%\vspace*{6pt}

\addtocounter{figure}{1}

\noindent 
раз\-мер\-ности становится отчетливей. Таким образом, параметр гео\-мет\-ри\-че\-ско\-го 
распределения дает полный контроль над ско\-ростью уменьшения ве\-ро\-ят\-ности 
по\-сле\-до\-ва\-тель\-ных моделей. 

\section{Заключение}
     
     Сжатие матрицы данных с пропусками (например, в~фиксируемом 
изображении потеряны его фрагменты, в~ходе клинического обследования 
часть результатов лабораторного исследования отсутствует и~т.\,п.)\ обычно 
осуществляется на предварительном этапе обработки информации при 
отсутствии обоснованных предположений о~вероятностных характеристиках 
сведений. В~связи с этим возникает вопрос: можно ли и~как бо\-лее-ме\-нее 
формально определиться со значением сниженной размерности? Если работ по 
оцениванию па\-ра\-мет\-ров факторной модели (матрицы~$\mathbf{U}$ 
и~$\mathbf{V}$) достаточно много, то по выбору числа факторов (значения 
сниженной размерности) нет вообще. Кроме того, в~имеющихся публикациях 
присутствовала некоторая несогласованность деталей модели данных и~методов 
их обработки. Поэтому пришлось обращаться к~истокам темы и~в~\cite{2-kri} 
удалось определиться с~моделью и~методами оценивания ее па\-ра\-мет\-ров. Но 
стала явной проб\-ле\-ма объемных по памяти и~времени вычислений. Однократно 
найти оценку па\-ра\-мет\-ров модели возможно, но проанализировать ее с 
помощью многократного пересчета оказывается уже нереальным. Пришлось 
заниматься методами мат\-рич\-ных вы\-чис\-ле\-ний: полученные в~\cite{3-kri} 
результаты относительно  
блоч\-но-диа\-го\-наль\-но\-го пред\-став\-ле\-ния мат\-риц и~пра\-во\-мер\-ности 
операций с ними в~таком представлении привели к~реальным вы\-чис\-ли\-тел\-ьным 
преимуществам по\-блоч\-ной обработки. Кроме того, открылись новые 
горизонты, например воз\-мож\-ность использования по\-блоч\-ной версии 
сингулярного разложения мат\-риц в~чис\-том виде без перестановочных мат\-риц, 
а~также привлечения иных способов факторизации мат\-риц.
     
     Уточнение модели факторизации матрицы данных с пропусками, выбор 
устойчивого варианта алгоритма оценивания параметров модели, по\-стро\-ение 
его эффективной реализации позволили в~\mbox{данной} статье предложить критерии 
выбора адекватной размерности модели факторизации. Ключевым моментом 
стала оригинальная идея получения повторных выборок путем дополнительных 
пропусков элементов данных. Таким образом, не вводя дополнительных 
ограничений на исходные данные, удалось привлечь фактор случайности 
и~методы обработки эмпирических распределений.
     
     При полном отсутствии знаний о~вероятностных свойствах ге\-не\-ри\-ру\-емых 
данных продемонстрирована дей\-ст\-вен\-ность ап\-прок\-си\-ма\-ции распределений 
с~по\-мощью смеси многомерных \mbox{распределений} с~по\-сле\-ду\-ющим уточнением 
посредством ядерного сглаживания. В~данной работе построение 
и~использование модели данных\linebreak RSM-вы\-бор\-ки преследовало утилитарную 
цель~--- обосновать жиз\-не\-спо\-соб\-ность оригинального метода формирования 
новых данных. При этом \mbox{схема} комбинированного описания выборочных 
распределений через смеси распределений и~ядерные оценки остав\-ля\-ет 
воз\-мож\-ность для совершенствования путем рас\-смот\-ре\-ния других критериев 
эф\-фек\-тив\-ности оценок и~иных ядер, посредством учета вы\-чис\-ли\-тель\-ных 
аспектов. 
     
{\small\frenchspacing
 {\baselineskip=11.5pt
 %\addcontentsline{toc}{section}{References}
 \begin{thebibliography}{9}
  \bibitem{1-kri}
  \Au{Chen P.} Optimization algorithms on subspaces: Revisiting missing data problem in  
low-rank matrix~// Int. J. Comput. Vision, 2008. Vol.~80. Iss.~1. P.~125--142. doi: 
10.1007/s11263-008-0135-7.
  \bibitem{2-kri}
\Au{Кривенко М.\,П.} Выбор модели при факторизации мат\-ри\-цы данных с пропусками~// 
Информатика и~её применения, 2022. Т.~16. Вып.~3. С.~52--58. doi: 
10.14357/ 19922264220307.
  
  \bibitem{3-kri}
  \Au{Кривенко М.\,П.} Эффективные вычисления при факторизации матричных данных 
с~пропусками~// Сис\-те\-мы и~средства информатики, 2023. Т.~33. Вып.~1. С.~78--89.
doi: 10.14357/08696527230108.
  
  \bibitem{4-kri}
  \Au{Poland W.\,B., Shachter~R.\,D.} Three approaches to probability model selection~// 
Uncertainty in artificial intelligence.~---  Seattle, WA, USA: Morgan Kaufmann, 1994. P.~478--483.
  doi: 10.1016/B978-1-55860-332-5.50065-1.
  
  \bibitem{5-kri}
\Au{Chernick M.\,R.} Resampling methods~// WIREs Data Min. 
Knowl., 2012. Vol.~2. Iss.~3. P.~255--262. doi: 10.1002/ widm.1054.
  
  \bibitem{6-kri}
\Au{Fan C.\,T., Muller~M.\,E., Rezucha~I.} Development of sampling plans by using sequential 
(item by item) selection techniques and digital computers~// J.~Am. Stat. Assoc., 1962. Vol.~57. 
No.\,298. P.~387--402. doi: 10.1080/ 01621459.1962.10480667.
  
  \bibitem{7-kri}
  \Au{Wand M.\,P.} Error analysis for general multivariate kernel estimators~// J.~Nonparametr. 
Stat., 1992. Vol.~2. Iss.~1. P.~1--15. doi: 10.1080/10485259208832538.
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 27.01.23}}

%\vspace*{8pt}

%\pagebreak

\newpage

\vspace*{-28pt}

%\hrule

%\vspace*{2pt}

%\hrule

%\vspace*{-2pt}

\def\tit{CRITERIA FOR CHOOSING THE~FACTORIZATION MODEL 
DIMENSIONALITY}


\def\titkol{Criteria for choosing the~factorization model 
dimensionality}


\def\aut{M.\,P.~Krivenko}

\def\autkol{M.\,P.~Krivenko}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-10pt}


\noindent
Federal Research Center ``Computer Science and Control'' of the Russian Academy 
of Sciences, 44-2~Vavilov Str., Moscow 119333, Russian Federation

\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2023\ \ \ volume~17\ \ \ issue\ 2}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2023\ \ \ volume~17\ \ \ issue\ 2
\hfill \textbf{\thepage}}}

\vspace*{3pt}


 

\Abste{The paper is devoted to the choice of model dimension of matrix 
factorization in the presence of missing elements. The problem of estimating the 
parameters of the adopted data model is solved by multidimensional optimization. 
Estimating the value of reduced dimensionality is a typical example of the problem of 
choosing a~model when an alternative arises during data analysis and the choice 
means either finding out the preferences of individual options or highlighting the 
``best'' representative. Typically, applied selection criteria are based on likelihood 
function which requires probabilistic assumptions about the data. But when 
evaluating the parameters of the factor model under consideration, they are not set 
and it is impractical to introduce them, so as not to violate the commonality of the 
formulated task of reducing dimensionality. Therefore, an attempt was made to turn to 
the idea of reusing the available data for the statistical output. None of the existing 
approaches (bootstrap, folding knife, rechecks, as well as permutation tests) is 
suitable; so, an original method for generating new data by additional omissions of 
elements of the original matrix was proposed. To process the formed samples, it is 
suggested to use a~combination of the model of a~mixture of normal distributions in 
conjunction with nuclear smoothing. The proposed solutions make it possible to 
correctly carry out the procedure for justifying the dimensionality of the adopted 
factorization model. The exposition is illustrated by an example of synthetic data 
processing.}

\KWE{lower rank matrix approximation; missing data; criteria for model selection; 
resampling methods; kernel smoothing}

\DOI{10.14357/19922264230207}{NQXYDC} 

%\vspace*{-18pt}

%\Ack
%  \noindent
 
\vspace*{6pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{9}
  \bibitem{1-kri-1}
\Aue{Chen, P.} 2008. Optimization algorithms on subspaces: Revisiting missing data 
problem in low-rank matrix. \textit{Int. J.~Comput. Vision} 80(1):125--142. doi: 
10.1007/s11263-008-0135-7.
  \bibitem{2-kri-1}
\Aue{Krivenko, M.\,P.} 2022. Vybor modeli pri faktorizatsii mat\-ri\-tsy dannykh 
s~propuskami [Model selection for matrix factorization with missing components]. 
\textit{Informatika i~ee Primeneniya~--- Inform. Appl.} 16(3):52--58. doi: 
10.14357/ 19922264220307.
  \bibitem{3-kri-1}
\Aue{Krivenko, M.\,P.} 2023. Effektivnye vychisleniya pri faktorizatsii matrichnykh 
dannykh s propuskami [Efficient computations in a matrix factorization with missing 
components]. \textit{Sistemy i~Sredstva Informatiki~--- Systems and Means of 
Informatics} 33(1):78--89. doi: 10.14357/ 08696527230108.
  \bibitem{4-kri-1}
\Aue{Poland, W.\,B., and R.\,D.~Shachter.} 1994. Three approaches to probability 
model selection. \textit{Uncertainty in artificial intelligence}.
Seattle, WA: Morgan Kaufmann. 478--483. doi: 10.1016/B978-1-55860-332-5.50065-1.

\vspace*{2pt}

  \bibitem{5-kri-1}
\Aue{Chernick, M.\,R.} 2012. Resampling methods. \textit{WIREs Data Min. 
Knowl.} 2(3):255--262. doi: 10.1002/widm.1054.

\vspace*{2pt}

  \bibitem{6-kri-1}
\Aue{Fan, C.\,T., M.\,E.~Muller, and I.~Rezucha.} 1962. Development of sampling 
plans by using sequential (item by item) selection techniques and digital computers. 
\textit{J.~Am. Stat. Assoc.} 57(298):387--402. doi: 
10.1080/01621459.1962. 10480667.

\vspace*{2pt}

  \bibitem{7-kri-1}
\Aue{Wand, M.\,P.} 1992. Error analysis for general multivariate kernel estimators. 
\textit{J.~Nonparametr. Stat.} 2(1):1--15. doi: 10.1080/10485259208832538.
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Received January 27, 2023}} 


\Contrl

\noindent
\textbf{Krivenko Michail P.} (b.\ 1946)~--- Doctor of Science in technology, 
professor, leading scientist, Institute of Informatics Problems, Federal Research 
Center ``Computer Science and Control'' of the Russian Academy of Sciences,  
44-2~Vavilov Str., Moscow 119333, Russian Federation; 
\mbox{mkrivenko@ipiran.ru}


\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература} 
\def\stat{sevast}

\def\tit{О МЕТОДАХ ПОВЫШЕНИЯ ТОЧНОСТИ МНОГОКЛАССОВОЙ КЛАССИФИКАЦИИ 
НА~НЕСБАЛАНСИРОВАННЫХ ДАННЫХ$^*$}

\def\titkol{О методах повышения точности многоклассовой классификации 
на~несбалансированных данных}

\def\aut{Л.\,А.~Севастьянов$^1$, Е.\,Ю.~Щетинин$^2$}

\def\autkol{Л.\,А.~Севастьянов, Е.\,Ю.~Щетинин}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Севастьянов Л.\,А.}
\index{Щетинин Е.\,Ю.}
\index{Sevastianov L.\,A.}
\index{Shchetinin E.\,Yu.}


{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
{Работа выполнена при поддержке РФФИ (проект 18-07-00567).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Российский университет дружбы народов, leonid.sevast@gmail.com}
\footnotetext[2]{Финансовый университет при Правительстве РФ, riviera-molto@mail.ru}

\vspace*{12pt}

  
  
      
  
  \Abst{Проведены исследования методов преодоления разбалансированности классов 
в~данных с~целью повышения качества классификации с~точностью, более высокой, чем при 
непосредственном использовании алгоритмов классификации к~несбалансированным 
данным. Для повышения точности классификации в~работе предложена схема, состоящая 
в~использовании комбинации алгоритмов классификации и~методов отбора признаков RFE
(Recursive Feature Elimination), 
Random Forest и~Boruta с~предварительным использованием балансирования классов 
методами случайного семплирования, SMOTE (Synthetic Minority
Oversamplimg TEchnique) и~ADASYN (ADAptive SYNthetic sampling). 
На примере данных 
о~заболеваниях кожи проведены компьютерные эксперименты, показавшие, что применение 
алгоритмов семплирования для устранения дисбаланса классов, а также отбора наиболее 
информативных признаков значительно повышает точность результатов классификации. 
Наиболее эффективным по точности классификации оказался алгоритм случайного леса при 
семплировании данных с~использованием алгоритма ADASYN.}
  
  \KW{классификация; несбалансированные данные; семплирование; случайный лес; 
ADASYN; SMOTE}

\DOI{10.14357/19922264200109} 
  
\vspace*{6pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}
  
\section{Введение}

  Задачи классификации относятся к~наиболее популярным в~анализе 
данных~[1]. В~качестве методов, используемых для установления 
принадлежности объекта к~тому или иному классу, чаще всего используют 
машинное обучение с~учителем. Основная идея этого подхода~--- индуктивный 
вывод функции на основе размеченных данных для обучения. Это означает, что 
успешность применения алгоритма машинного обучения с~учителем во многом 
зависит от той выборки объектов, на основе которых он <<обучается>>. 
Большинство подобных алгоритмов требуют от исследователя включения 
сопоставимого числа примеров для каждого из классов, однако зачастую 
сделать сбалансированные наборы данных не представляется возможным 
в~связи с~рядом факторов. Нередко возникают ситуации, когда в~наборе 
данных доля примеров некоторого класса незначительна (этот класс будем 
называть миноритарным, а~другой, пре\-об\-ла\-да\-ющий над первым,~--- 
мажоритарным). Ключевые из них~--- специфика целевой области 
(балансировка данных может понизить показатель их репрезентативности) 
и~разная цена ошибок первого и~второго рода при классификации. Такие 
тенденции хорошо заметны, например, в~кредитном скоринге, медицине, 
маркетинге~[2, 3].  
  
  Вследствие этого возникает проблема обучения модели на 
несбалансированных данных (таковыми являются данные, в~распределении 
которых наблюдается асимметрия, а показатели моды и~среднего значения не 
равны): в~соответствии с~базовыми предположениями, заключенными 
в~большинстве алгоритмов, целью обучения ставится максимизация доли 
правильных решений по отношению ко всем принятым решениям, а~данные 
для обучения и~генеральная совокупность подчиняются одному и~тому же 
распределению. Однако учет данных предположений и~несбалансированности 
выборки приводит к~тому, что модель оказывается неспособна 
классифицировать данные лучше, чем тривиальная модель, полностью 
игнорирующая менее представленный класс и~маркирующая все объекты для 
классификации как принадлежащие к~мажоритарному классу.
  
  С другой стороны, возможно построение слишком сложной модели, 
включающей большое мно\-же\-ство правил, которое при этом будет охватывать\linebreak 
малое число объектов. Такой классификатор может оказаться неэффективным, 
что приведет модель к~переобучению и~некорректным оценкам прогноза. 
Следует отметить, что могут отличаться и~последствия ошибочной 
классификации, причем неверная классификация примеров миноритарного 
класса, как правило, обходится в~разы дороже, чем ошибочная классификация 
объекта из мажоритарного класса. Правильный выбор признаков может 
оказаться более значимой задачей, чем уменьшение времени обработки данных 
или повышения точности классификации. К~примеру, в~медицине нахождение 
минимального набора признаков, оптимального для задачи классификации, 
может стать необходимым условием для постановки диагноза. Таким образом, 
чтобы избежать подобного явления и~достичь хорошего результата, 
необходимо исследовать методы работы с~несбалансированными данными.
  
  В настоящей работе проведены исследования методов преодоления 
разбалансированности классов с~целью повышения качества классификации 
с~точ\-ностью, более высокой, чем при непосредственном применении 
алгоритмов классификации к~несбалансированным классам. Для повышения 
точности классификации в~работе предложена схема, состоящая 
в~использовании комбинации алгоритмов классификации и~методов отбора 
признаков RFE, Random Forest и~Boruta с~предварительным использованием 
балансирования классов методами случайного семплирования, SMOTE 
и~ADASYN.

%\vspace*{-6pt}
  
\section{Алгоритмы балансирования классов}

  Один из подходов к~решению указанной проблемы~--- применение различных 
стратегий семплинга, которые можно разделить на две группы: случайные 
и~специальные~\cite{3-sev}. В~первом случае удаляют некоторое число 
примеров мажоритарного класса (undersampling), во втором~--- увеличивают 
число примеров миноритарного класса (oversampling). 

%\vspace*{-9pt}
  
\subsection{Удаление примеров мажоритарного класса. Алгоритм 
случайного семплирования мажоритарного класса (random 
undersampling)}

  Сначала рассчитывается~$K$~--- число мажоритарных примеров, которые 
необходимо удалить для достижения требуемого уровня соотношения 
различных классов. Затем случайным образом 
выбираются~$K$~мажоритарных примеров и~удаляются. В~случае 
исследуемых данных естественными представляются методы по увеличению 
миноритарного класса. Перейдем к~рассмотрению таких стратегий.

%\vspace*{-9pt}
  
  \subsection{Увеличение миноритарного класса. Дублирование примеров 
миноритарного класса (oversampling). Случайная наивная выборка}
  
  Самый простой способ увеличить число примеров миноритарного класса~--- 
случайным образом выбрать наблюдения из него и~добавить их в~общий набор 
данных, пока не будет достигнут баланс между классами большинства 
и~меньшинства. В~зависимости от того, какое соотношение классов 
необходимо, выбирается число случайных записей для дублирования. Одна из 
проблем со случайной наивной выборкой заключается в~том, что она просто 
дублирует уже существующие данные. К~достоинствам такого подхода 
относятся его простота, легкость реализации и~предоставляемая им 
возможность изменить баланс в~любую нужную сторону. Про недостатки 
нужно говорить отдельно в~соответствии с~тем, какая стратегия семплинга 
используется: несмотря на то что обе из них изменяют общий размер данных 
с~целью поиска баланса, их применение влечет разные последствия. 

В~случае 
undersampling удаление данных может привести к~потере классом важной 
информации и,~как следствие, к~понижению показателя его презентативности. 
В~свою очередь, применение oversampling может привести 
к~переобучению~\cite{3-sev}. 

Такой подход к~восстановлению баланса не 
всегда эффективен, поэтому был предложен специальный метод увеличения 
числа примеров миноритарного класса~--- алгоритм SMOTE~\cite{5-sev}.
  
  Алгоритм SMOTE основан на идее генерации некоторого числа 
искусственных примеров, которые были бы <<похожи>> на имеющиеся 
в~миноритарном классе, но при этом не дублировали их. Для создания новой 
записи находят разность  $d\hm= X_b\hm-X_a$, где~$X_a$ и~$X_b$~--- векторы 
признаков <<соседних>> примеров~$a$ и~$b$ из миноритарного класса. Их 
находят, используя алгоритм ближайших соседей (KNN, k-nearest neighbors). В~данном случае 
необходимо и~достаточно для примера~$b$ получить набор из~$k$~соседей, из 
которого в~дальнейшем будет выбрана запись~$b$. Остальные шаги алгоритма 
KNN не требуются. Далее из~$d$ путем умножения каждого его элемента на 
случайное число в~интервале $(0, 1)$ получают~$\tilde{d}$. Вектор признаков 
нового примера вычисляется путем сложения~$X_a$ и~$\tilde{d}$. Алгоритм 
SMOTE позволяет задавать число записей, которое необходимо искусственно 
сгенерировать. Степень сходства примеров~$a$ и~$b$ можно регулировать 
путем изменения значения~$k$ (числа ближайших соседей). 
  
 Алгоритм SMOTE решает многие проблемы, которые присущи методу случайной 
выборки, и~действительно увеличивает изначальный набор данных таким 
образом, что модель обучается гораздо эффективнее~\cite{9-sev}. Тем не менее 
данный алгоритм имеет и~свои недостатки, главный из которых~--- 
игнорирование мажоритарного класса. Это может проявиться в~том, что при 
сильно разреженном распределении объектов миноритарного класса 
относительно мажоритарного наборы данных <<смешаются>>, т.\,е.\ 
расположатся в~таком виде, что отделить объекты одного класса от другого 
будет очень трудно. Примером данного явления может служить случай, когда
между объектом и~его соседом, на основе которых генерируется новый 
экземпляр, находится объект другого класса. В~результате синтетически 
созданный объект будет находиться ближе к~противоположному классу, чем 
к~классу своих родителей. Кроме того, число сгенерированных с~помощью 
SMOTE экземпляров задается заранее; следовательно, уменьшается 
возможность изменения баланса и~гибкость метода. 

Важно отметить 
существенное ограничение SMOTE. Поскольку он работает путем 
интерполяции между редкими примерами, то может генерировать примеры 
только внутри тела доступных примеров~--- никогда снаружи. Формально 
SMOTE может только заполнить выпуклую оболочку существующих примеров 
меньшинства, но не создавать для них новые внешние области. 

Основное 
преимущество SMOTE по сравнению с~традиционной случайной наивной 
чрезмерной выборкой заключается в~том, что при создании синтетических 
наблюдений вместо повторного использования существующих наблюдений 
данный классификатор с~меньшей вероятностью будет переобучен. В~то же 
время всегда необходимо убедиться, что наблюдения, созданные SMOTE, 
реалистичны.
  
  
 % \vspace*{-6pt}
  
  \subsection{Адаптивный синтетический семплинг и~его обобщения}
  
  В основе данного метода лежат алгоритмы синтетического семплинга, 
основные из которых~--- Borderline-SMOTE и~ADASYN~\cite{6-sev, 7-sev}.
 Borderline-SMOTE накладывает ограничения на 
выбор \mbox{объектов} миноритарного класса, на основе которых генерируются новые 
экземпляры. Происходит это следующим образом: для каждого объекта 
миноритарного класса определяется набор~$k$~ближайших соседей, затем 
производится подсчет, сколько экземпляров из этого набора принадлежат 
к~мажоритарному классу (это число принимается за~$m$). После этого 
отбираются те объекты миноритарного класса, для которых верно неравенство  
  $k/2\hm\leq m\hm<k$. Полученный набор представляет собой экземпляры 
миноритарного класса, находящиеся на границе распределения, и~именно у них 
вероятность оказаться некорректно классифицированными выше, чем у прочих. 
Следует отметить, почему неравенство, определяющее отбор объектов, 
исключает случаи, при которых все~$k$~соседей принадлежат 
к~мажоритарному классу: это связано с~тем, что подобные экземпляры 
расположены в~зоне <<смешивания>> двух классов и~на их основе могут быть 
сгенерированы лишь искажающие процесс обучения модели объекты. В~связи 
с~этим они объявляются шумом (\textit{англ}.\ noise) и~игнорируются алгоритмом. 
  
  Алгоритм ADASYN же, в~свою очередь, основывается на систематическом 
методе, позволяющем адаптивно генерировать разные объемы данных 
в~соответствии с~их распределениями~\cite{6-sev}. Входные данные для 
алгоритма~--- обучающий набор данных: $D_r$ с~$m$~выборками с~$\{x_i, 
y_i\}$, $i\hm=\overline{1,m}$, где~$x_i$~---\linebreak $n$-мер\-ный вектор 
в~пространстве признаков, а~$y_i$~--- соответствующий класс. 

Пусть~$m_r$ 
и~$m_x$~--- число образцов классов меньшинства и~большинства 
соответственно, такие что $m_r\ll m_x$ и~$m_r\hm+m_x\hm=m$. Псевдокод 
алгоритма имеет следующий вид.
  \begin{enumerate}[1.]
\item Вычислить пропорцию классов $d \hm= m_r/m_x$.
\item Если $d < d_x$ (где $d_x$~--- заданный порог для максимально 
допустимого дисбаланса классов), то:
\begin{itemize}
\item[(a)] найти число синтетически создаваемых образцов минорного класса 
$G\hm= (m_x\hm- m_r)\beta$,  где $\beta$~---
параметр, используемый для определения 
желаемого уровня баланса ($\beta\hm=1$  означает полный баланс классов); 
\item[(б)] для каждого $x_i\hm\in \mbox{minority\ class}$ найти~$K$~ближайших 
соседей, используя евклидово расстояние, и~вычислить $r_i\hm= \Delta_i/K$;
\item[(в)] нормализовать $r_x\hm= r_i/\sum\nolimits_i r_i$ так, чтобы~$r_x$ стал 
плотностью распределения;
\item[(г)] вычислить $g_i\hm= r_x  G$ синтетической выборки, сформированной 
для каждого образа из класса меньшинства, где~$G$~--- общее число примеров 
синтетических данных; 
\item[(д)] для каждого примера данных из класса меньшинства~$x_i$ создать 
примеры синтетических данных~$g_i$ в~соответствии со сле\-ду\-ющи\-ми шагами: 
\begin{itemize}

\pagebreak

\item[--]  \textit{в цикле от~$1$ до~$i$}:
  \begin{itemize}
  \item[(i)] случайным образом выбрать один пример данных меньшинства, 
$x_u$ из~$K$~ближайших соседей для данных~$x_i$;
  \item[(ii)] создать пример синтетических данных: 
  $$
  g_i= x_i+ (x_u-  x_i)\lambda,
  $$
   где $(x_u\hm- x_i)$~--- $n$-мер\-ный вектор евклидова 
пространства; $\lambda$~---  случайное число: $\lambda\hm\in [0,1]$.
  \end{itemize}
  \end{itemize}
  \end{itemize}
\end{enumerate}
  
  Основное различие между SMOTE и~ADASYN заключается в~способах 
создания синтетических выборочных образцов для класса меньшинства. 
В~ADASYN используется функция плотности~$r_x$, определяющая число 
синтетических образцов, которые будут созданы для конкретной точки, тогда 
как в~SMOTE существует единый вес для всех точек меньшинства.
  
\section{Исследуемые данные: описание и~характеристики}

  В настоящей работе для тестирования и~сравнительного анализа описанных 
выше методов устранения дисбаланса классов был использован набор данных 
о~заболеваниях кожи. Диагностика эри\-те\-ма\-тоз\-но-плос\-ко\-кле\-точ\-ных 
заболеваний~--- серь\-ез\-ная проб\-ле\-ма в~дерматологии, а современные принципы 
диагностики и~лечения опираются на наиболее раннее обнаружение заболевания. 
Все они имеют общие клинические особенности с~очень небольшими 
различиями. Еще одна трудность для диагностики заключается в~том, что 
заболевание может проявлять признаки другого заболевания на начальной 
стадии и~иметь характерные признаки на последующих стадиях. 
  
  Исследуемые данные были созданы компанией Nielsen в~1998~г.\ и~содержат 
366~наблюдений, формирующих 6~классов, которые могут быть 
охарактеризованы 34~признаками~\cite{8-sev}. Классами являются: 
\begin{itemize}
\item псориаз 
(класс~1)~--- 112~случаев; 
\item себорейный дерматит (класс~2)~--- 72~случая; 
\item плоский лишай (класс~3)~--- 61~случай; 
\item розовый лишай (класс~4)~--- 49~случаев; 
\item хронический дерматит (класс~5)~--- 52~случая; 
\item красный волосяной 
лишай (класс~6)~--- 20~случаев.
\end{itemize}
 Полное описание данных приведено 
в~\cite{11-sev}.
  
\section{Компьютерные эксперименты}

  Исследования данных проводились по сле\-ду\-юще\-му алгоритму.
  \begin{enumerate}[1.]
\item Предварительная обработка данных: заполнение пропусков в~данных 
и~использование кодирования признаков. 
\item Балансирование классов с~помощью описанных выше алгоритмов 
семплинга.
\item Отбор признаков по их важности. 
\item Классификация с~использованием логистической регрессии и~метода 
опорных векторов (SVM~--- Support Vector Machine).
\item Оценка качества классификации.
\end{enumerate}

  В настоящей работе отбор признаков по их важности и~информативности 
был осуществлен следующими методами:
\begin{itemize}
\item[(а)]~рекурсивное исключение 
признаков RFE~\cite{9-sev}; 
\item[(б)] деревья решений RF~\cite{10-sev}; 
\item[(в)] Boruta~\cite{4-sev}.
\end{itemize}
  
  Алгоритм Random Forest представляет собой ансамбль многочисленных 
алгоритмов классификации (деревьев решений). Каждый из этих 
классификаторов строится на случайном подмножестве объектов 
и~случайном подмножестве признаков. Пусть обучающая выборка состоит 
из~$N$~примеров, размерность пространства признаков равна~$M$ 
и~задан дополнительный параметр~$m$. Все деревья строятся независимо 
друг от друга по следующей процедуре.
  \begin{enumerate}[1.]
  \item Сгенерируем случайную подвыборку с~повторением размером~$n$ из 
обучающей выборки.
  \item Построим решающее дерево, классифицирующее примеры данной 
подвыборки, причем в~ходе создания очередного узла дерева будем выбирать 
признак, на основе которого производится разбиение, не из 
всех~$M$~признаков, а~лишь из~$m$~случайно выбранных.
  \item Дерево строится до полного исчерпания подвыборки и~не 
подвергается процедуре отсечения ветвей.
  \end{enumerate}
  
     \begin{table*}[b]\small %tabl1
  \begin{center}
  \Caption{Результаты классификации методом опорных векторов}
  \vspace*{2ex}
  
  \begin{tabular}{|l|l|c|c|c|c|}
  \hline
\multicolumn{1}{|c|}{\tabcolsep=0pt\begin{tabular}{c}Семплинг\\ для 
несбалансированных\\ классов\end{tabular}}&
\multicolumn{1}{c|}{\tabcolsep=0pt\begin{tabular}{c}Методы\\ выбора\\ 
признаков\end{tabular}}&
\tabcolsep=0pt\begin{tabular}{c}Число\\ выбранных\\ 
признаков\end{tabular}&Точность&F1-мера&Полнота\\
\hline
\raisebox{-18pt}[0pt][0pt]{Несбалансированные данные}&Все признаки&630&0,9324&0,9337&0,9324\\
&RFE&\hphantom{9}65&0,9595&0,9598&0,9595\\
&Случайный лес&\hphantom{9}32&0,9595&0,9590&0,9595\\
&Boruta&207&0,9324&0,9330&0,9324\\
\hline
\raisebox{-18pt}[0pt][0pt]{Случайная выборка}&Все признаки&630&0,9324&0,9337&0,9324\\
&RFE&\hphantom{9}44&0,9359&0,9468&0,9459\\
&Случайный лес&\hphantom{9}44&0,9465&0,9466&0,9465\\
&Boruta&284&0,9595&0,9598&0,9595\\
\hline
\raisebox{-18pt}[0pt][0pt]{SMOTE}&Все признаки&630&0,9324&0,9337&0,9324\\
&RFE&\hphantom{9}68&0,9595&0,9730&0,9730\\
&Случайный лес&\hphantom{9}42&0,9595&0,9072&0,9054\\
&Boruta&257&0,9459&0,9337&0,9324\\
\hline
\raisebox{-18pt}[0pt][0pt]{ADASYN}&Все признаки&630&0,9324&0,9337&0,9324\\
&RFE&\hphantom{9}44&0,9459&0,9459&0,9459\\
&Случайный лес&\hphantom{9}40&\textbf{0,9845}&0,9330&0,9324\\
&Boruta&276&0,9459&0,9602&0,9595\\
\hline
\end{tabular}
\end{center}
\end{table*}
  
  Классификация объектов проводится путем голо\-со\-ва\-ния: каждое дерево 
ансамбля относит классифицируемый объект к~одному из классов, 
и~побеждает класс, за который проголосовало наибольшее число деревьев. 
Для применения RF в~задаче оценки важности признаков необходимо обучить 
алгоритм на выборке и~для каждого примера обучающей выборки посчитать 
out-of-bag-ошиб\-ку~\cite{10-sev}. 

Пусть~$X_n$~--- бутстрэпированная 
выборка дере\-ва~$b_n$. Бутстрэппинг представляет собой 
выбор~$l$~объектов из выборки с~возвращением, в~результате чего 
некоторые объекты выбираются\linebreak несколь\-ко раз, а~некоторые~--- ни разу. 
Помещение нескольких копий одного объекта в~бутстрэпированную выборку 
соответствует выставлению веса при данном объекте, соответствующее ему 
слагаемое несколько раз \mbox{войдет} в~функционал, и~поэтому штраф за 
ошибку на нем будет больше. Пусть~$L(y, z)$~--- функция потерь,  
$y_i$~--- ответ на $i$-м объекте обучающей выборки, тогда  
out-of-bag-ошиб\-ка вычисляется по следующей формуле:
  $$
  \mathrm{OOB}=\sum\limits^l_{i=1} L\left( y_i, \fr{\sum\nolimits^N_{n=1} [x_i\not\in 
X^l_n] b_n(x_i)} {\sum\nolimits^N_{n=1} [x_i\not\in X^l_n]}\right)\,.
  $$
  
  Затем для каждого объекта такая ошибка усредняется по всему 
случайному лесу. Чтобы оценить важность признака, его значения 
перемешиваются для всех объектов обучающей выборки  
и~out-of-bag-ошиб\-ка считается снова. Важность признака оценивается 
путем усреднения по всем деревьям разности показателей  
out-of-bag-оши\-бок до и~после перемешивания значений. При этом значения 
таких ошибок нормализуются на стандартное отклонение. 
  
  Boruta~--- эвристический алгоритм отбора значимых признаков, 
основанный на использовании Random Forest~\cite{4-sev}. На каждой 
итерации удаляются признаки, у~которых Z-ме\-ра меньше максимальной  
Z-ме\-ры среди добавленных признаков. Чтобы получить Z-ме\-ру 
признака, необходимо посчитать важность признака, полученную  
с~по\-мощью встроенного алгоритма в~Random Forest, и~поделить ее на 
стандартное отклонение важности признака. Добавленные признаки 
получаются следующим образом: копируются признаки, имеющиеся 
в~выборке, а затем каждый новый признак заполняется путем перетасовки 
его значений. В~целях получения статистически значимых результатов эта 
процедура повторяется несколько раз, переменные генерируются независимо 
на каждой итерации. Запишем пошагово алгоритм Boruta.
  \begin{enumerate}[1.]
  \item Добавить в~данные копии всех признаков. В~дальнейшем копии 
будем называть скрытыми признаками.
  \item Случайным образом перемешать каждый скрытый признак. 
   \item Запустить Random Forest и~получить Z-ме\-ру всех признаков.
  \item Найти максимальную Z-ме\-ру из всех Z-мер для скрытых 
признаков.
  \item Удалить признаки, у которых Z-ме\-ра меньше, чем найденная на 
предыдущем шаге. 
  \item Повторять все шаги до тех пор, пока Z-ме\-ра всех признаков не 
станет больше, чем максимальная Z-ме\-ра скрытых признаков.
  \end{enumerate}
  
   \begin{table*}\small %tabl2
  \begin{center}
  \Caption{Результаты классификации с~использованием логистической регрессии}
  \vspace*{2ex}
  
  \begin{tabular}{|l|l|c|c|c|c|}
  \hline
\multicolumn{1}{|c|}{\tabcolsep=0pt\begin{tabular}{c}Семплинг\\ для 
несбалансированных\\ классов\end{tabular}}&
\multicolumn{1}{c|}{\tabcolsep=0pt\begin{tabular}{c}Методы\\ выбора\\ 
признаков\end{tabular}}&
\tabcolsep=0pt\begin{tabular}{c}Число\\ выбранных\\ 
признаков\end{tabular}&Точность&F1-мера&Полнота\\
\hline
\raisebox{-18pt}[0pt][0pt]{Несбалансированные данные}&Все признаки&630&0,9330&0,9234&0,9231\\
&RFE&\hphantom{9}19&0,9459&0,9459&0,9459\\
&Случайный лес&\hphantom{9}32&0,9595&0,9590&0,9595\\
&Boruta&200&0,9595&0,9590&0,9595\\
\hline
\raisebox{-18pt}[0pt][0pt]{Случайная выборка}&Все признаки&630&0,9630&0,9634&0,9630\\
&RFE&\hphantom{9}48&0,9665&0,9866&0,9865\\
&Случайный лес&\hphantom{9}44&0,9730&0,9730&0,9730\\
&Boruta&290&0,9730&0,9765&0,9730\\
\hline
\raisebox{-18pt}[0pt][0pt]{SMOTE}&Все признаки&630&0,9730&0,9734&0,9730\\
&RFE&\hphantom{9}20&0,9459&0,9459&0,9459\\
&Случайный лес&\hphantom{9}41&0,9324&0,9330&0,9324\\
&Boruta&264&0,9595&0,9590&0,9595\\
\hline
\raisebox{-18pt}[0pt][0pt]{ADASYN}&Все признаки&630&0,9530&0,9534&0,9530\\
&RFE&\hphantom{9}67&0,9595&0,9602&0,9595\\
&Случайный лес&\hphantom{9}42&\textbf{0,9895}&\textbf{0,9859}&\textbf{0,9893}\\
&Boruta&245&0,9595&0,9590&0,9595\\
\hline
\end{tabular}
\end{center}
\end{table*}
  
\section{Результаты исследования и~их~обсуждение}
 
  Для решения задачи многоклассовой классификации на несбалансированных 
данных были вы\-бра\-ны алгоритмы машинного обучения: ло\-ги\-стическая 
регрессия и~метод опорных векторов\linebreak с~линейным ядром (Linear SVM). Все 
вычисления были программно реализованы на языке PYTHON, их результаты, 
данные, а также коды программ размещены в~репозитории авторов данной 
статьи~\cite{11-sev}. Для сравнения результатов классификации были 
использованы три метрики: точность (accuracy), полнота (precision) и~F1-ме\-ра. 
Результаты проведенных исследований представлены в~табл.~1.
  

  
  В ее первом столбце перечислены применявшиеся методы семплирования. 
Во втором столбце приведены применявшиеся методы отбора признаков, 
в~третьей~--- число отобранных при этом признаков. В~остальных столбцах 
приведены значения метрик качества, полученные в~результате применения 
к~преобразованным данным алгоритма опорных векторов (SVM). 

Аналогично 
построена табл.~2, содержащая результаты классификации с~применением 
логистической регрессии.
  
 
  
  Из анализа полученных результатов, приведенных в~табл.~1 и~2, можно 
видеть, что во всех случа\-ях применение методов семплирования позволило\linebreak 
получить более высокую точность классификации, чем на несбалансированных 
данных. В~рамках применения описанной в~работе схемы наилучшая точность 
классификации была достигнута в~результате применения алгоритма 
балансирования классов ADASYN и~затем отбора признаков алгоритмом 
случайного леса. Для сравнения, в~работах других исследователей, 
проводивших подобные исследования, например~\cite{9-sev, 10-sev}, точность 
классификации достигала лишь~93\%.
  
\section{Заключение}

  В настоящей работе предложена схема повышения точности классификации 
на несбалансированных данных с~использованием алгоритмов балансирования 
классов и~отбора признаков, таких как RFE, Boruta, Random Forest и~др. 
Результаты проведенных в~работе вычислительных экспериментов показали 
эффективность ее применения для решения поставленной задачи. В частности, 
алгоритм ADASYN, по сравнению с~другими алгоритмами, повысил точность 
классификации до~98\%. В~заключение стоит отметить, что рассмотренная 
в~работе проблема по-прежнему актуальна, а существующие методы могут 
быть улучшены.
  
{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{1-sev}
\Au{Паттерсон Дж., Гибсон~А.} Глубокое обучение с~точки зрения практика~/ Пер. с~англ. 
А.\,А.~Слинкина.~--- М.: ДМК Пресс, 2018. 417~с. (\Au{Patterson~J., Gibson~A.} Deep 
learning: A~practitioner's approach.~--- O'Reilly Media, 2017. 532~p.)

\bibitem{3-sev} %2
\Au{Japkowicz N., Stephen~S.} 
The class imbalance problem: A~systematic study~// Intell. 
Data Anal., 2002. Vol.~6. Iss.~5. P.~429--449. doi: 10.3233/IDA-2002-6504.

\bibitem{2-sev} %3
\Au{He H., Garcia~A.} Learning from imbalanced data~// IEEE~T. Knowl. Data 
En., 2009. Vol.~21. Iss.~9. P.~1263--1284. doi: 10.1109/TKDE.2008.239.


\bibitem{5-sev} %4
\Au{Chawla N.\,V., Bowyer~K.\,W., Hall~L.\,O., Kegelmeyer~W.\,P.} SMOTE: Synthetic minority 
over-sampling technique~// J.~Artif. Intell. Res., 2002. Vol.~16. P.~321--357. doi: 
10.1613/jair.953.

\bibitem{9-sev} %5
\Au{Lin X., Yang~F., Zhou~L.} A~support vector machine recursive feature elimination feature 
selection method based on artificial contrast variables and mutual information~// 
J.~Chromatogr.~B, 2012. 
Vol.~10. P.~149--155. doi: 10.1016/j.jchromb.2012.05.020.

\bibitem{7-sev} %6
\Au{Han H., Wen-Yuan~W., Bing-Huan~M.} Borderline-SMOTE: 
A~new over-sampling method in 
imbalanced data sets learning~// Advances in
intelligent computing~/
Eds. De-Shuang Huang, Xiao-Ping Zhang, Guang-Bin Huang.~--- 
Lecture notes in computer science book ser.~--- Springer, 2005. Vol.~3644. P.~878--887. 
doi: 10.1007/11538059\_91.

\bibitem{6-sev} %7
\Au{He H., Bai~Ya., Garcia~A., Li~Sh.} ADASYN: Adaptive synthetic sampling approach for 
imbalanced learning~// IEEE  Joint Conference (International) on Neural Networks (IEEE World 
Congress on Computational Intelligence).~--- IEEE, 2008. P.~1322--1328.

\bibitem{8-sev} %8
\Au{Murphy P.\,M., Aha~D.\,W.} UCI repository of machine learning databases.~--- 
Irvine, CA, USA: 
University of California, Department of Information and Computer Science, 
1998. {\sf 
https://www.ics.uci.edu/mlearn/\linebreak MLRepository.html}.


\bibitem{11-sev} %9
Dermatology-article. {\sf https://github.com/riviera2015/\linebreak Dermatology-article}.
\bibitem{10-sev} %10
\Au{Tuv E., Borisov~A., Runger~G., Torkkola~K.} Feature selection with ensembles, artificial 
variables, and redundancy elimination~// J.~Mach. Learn. Res., 2009. Vol.~10.  
P.~1341--1366.

\bibitem{4-sev} %11
\Au{Kursa M., Rudnicki~W.} Feature selection with the Boruta package~// J.~Stat. 
Softw.,  2010. Vol.~36. Iss.~11. P.~1--13. doi: 10.18637/jss.v036.i11.

 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-9pt}

\hfill{\small\textit{Поступила в~редакцию 29.11.19}}

\vspace*{6pt}

%\pagebreak

%\newpage

%\vspace*{-28pt}

\hrule

\vspace*{2pt}

\hrule

\vspace*{-4pt}

\def\tit{ON METHODS FOR~IMPROVING THE~ACCURACY OF~MULTICLASS 
CLASSIFICATION ON~IMBALANCED DATA\\[-5pt]}


\def\titkol{On methods for~improving the~accuracy of~multiclass 
classification on~imbalanced data}

\def\aut{L.\,A.~Sevastianov$^1$ and~E.\,Yu.~Shchetinin$^2$}

\def\autkol{L.\,A.~Sevastianov and~E.\,Yu.~Shchetinin}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-16pt}


\noindent
$^1$Peoples' Friendship University of Russia (RUDN University), 6~Miklukho-Maklaya Str., 
Moscow 117198, Russian\linebreak
$\hphantom{^1}$Federation

\noindent
$^2$Financial University under the Government of the Russian Federation, 49~Leningradsky 
Prospekt, Moscow\linebreak
$\hphantom{^1}$125993, Russian Federation

\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2020\ \ \ volume~14\ \ \ issue\ 1}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2020\ \ \ volume~14\ \ \ issue\ 1
\hfill \textbf{\thepage}}}

\vspace*{1pt} 
  
  
  
\Abste{This paper studies methods to overcome the imbalance of classes in order to improve the 
quality of classification with accuracy higher than the direct use of classification algorithms to 
unbalanced data. The scheme to improve the accuracy of classification is proposed, consisting in the 
use of a~combination of classification algorithms and methods of selection of features such as RFE
(Recursive Feature Elimination), 
Random Forest, and Boruta with the preliminary use of balancing classes by random sampling 
methods, SMOTE (Synthetic Minority
Oversamplimg TEchnique)
and ADASYN (ADAptive SYNthetic sampling). 
By the example of data on skin diseases, computer experiments 
were conducted which showed that the use of sampling algorithms to eliminate the imbalance of 
classes as well as the selection of the most informative features significantly increases the accuracy 
of the classification results. The most effective classification accuracy was the Random Forest 
algorithm for sampling data using the ADASYN algorithm.}
  
\KWE{imbalanced data; classification; sampling; random forest; ADASYN; SMOTE}
  

  
\DOI{10.14357/19922264200109} 

\vspace*{-24pt}

\Ack

\vspace*{-3pt}

\noindent
The paper was prepared with the support of the Russian Foundation for Basic 
Research (project 18-07-00567).

%\vspace*{6pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{1-sev-1} %1
\Aue{Patterson, J., and А.~Gibson.} 2017. \textit{Deep learning: A~practitioner's approach}. 
O'Reilly Media. 532~p.

\bibitem{3-sev-1} %2
\Aue{Japkowicz, N., and S.~Stephen.} 2002. The class imbalance problem: A~systematic study. 
\textit{Intell. Data Anal.} 6(5):429--449. doi: 10.3233/IDA-2002-6504.

\bibitem{2-sev-1} %3
\Aue{He, H., and A.~Garcia.} 2009. Learning from imbalanced data. \textit{IEEE~T. 
Knowl. Data En.} 21(9):1263--1284. doi: 10.1109/TKDE.2008.239.


\bibitem{5-sev-1} %4
\Aue{Chawla, N.\,V., K.\,W.~Bowyer, L.~O.Hall, and W.\,P.~Kegelmeyer.} 2002. SMOTE: 
Synthetic minority over-sampling technique. \textit{J.~Artif. Intell. Res.} 
16:321--357.

\bibitem{9-sev-1} %5
\Aue{Lin, X., F.~Yang, and L.~Zhou.} 2012. A support vector machine recursive feature 
elimination feature selection method based on artificial contrast variables and mutual information. 
\textit{J.~Chromatogr.~B} 
10:149--155. doi: 10.1016/ j.jchromb.2012.05.020.

\bibitem{7-sev-1} %6
\Aue{Han, H., W.~Wen-Yuan, and M.~Bing-Huan.} 2005. Borderline-SMOTE: A~new  
over-sampling method in imbalanced data sets learning. \textit{Advances  
in intelligent computing}. 
Eds. De-Shuang Huang, Xiao-Ping Zhang, and Guang-Bin Huang. 
Lecture notes in computer science book ser. Springer.
3644:878--887. http://dx.doi.org/ 10.1007/11538059\_91.

\bibitem{6-sev-1} %7
\Aue{He, H., Ya.~Bai, A.~Garcia, and Sh.~Li.} 2008. ADASYN: Adaptive synthetic sampling 
approach for imbalanced learning. \textit{IEEE Joint Conference (International) on Neural 
Networks (IEEE World Congress on Computational Intelligence)}. China. 1322--1328.

\bibitem{8-sev-1}
\Aue{Murphy, P.\,M., and D.\,W.~Aha.} 1998. UCI repository
 of machine learning databases. 
Irvine, CA: University of California-Irvine, Department of Information 
and Computer Science. Available at: 
{\sf https://www.ics. uci.edu/mlearn/MLRepository.html} (accessed December~27, 2019).


\bibitem{11-sev-1} %9
Dermatology-article. Available at: {\sf  
https://github.com/ riviera2015/Dermatology-article} (accessed December~27, 2019).
\bibitem{10-sev-1}
\Aue{Tuv, E., A.~Borisov, G.~Runger, and K.~Torkkola.} 2009. Feature selection with ensembles, 
artificial variables, and redundancy elimination. \textit{J.~Mach. Learn. Res.}
10:1341--1366.


\bibitem{4-sev-1} %11
\Aue{Kursa, M., and W.~Rudnicki.} 2010. Feature selection with the Boruta package. 
\textit{J.~Stat. Softw.} 36(11):1--13. doi: 10.18637/jss.v036.i11.

\end{thebibliography}

 }
 }

\end{multicols}

%\vspace*{-7pt}

\hfill{\small\textit{Received November 29, 2019}}

%\pagebreak

%\vspace*{-22pt}
  
\Contr

\noindent
\textbf{Sevastianov Leonid A.} (b.\ 1949)~--- Doctor of Science in physics and mathematics, 
professor, Peoples' Friendship University of Russia (RUDN University), 6~Miklukho-Maklaya Str., 
Moscow 117198, Russian Federation; \mbox{leonid.sevast@gmail.com}

\vspace*{3pt}

\noindent
\textbf{Shchetinin Eugene Yu.} (b.\ 1962)~--- Doctor of Science in physics and mathematics, 
professor, Department of Data Analysis, Decision-Making and Financial Technology, Financial 
University under the Government of the Russian Federation, 49~Leningradsky Prosp., Moscow 
125993, Russian Federation; \mbox{riviera-molto@mail.ru} 
  

  
\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература} 
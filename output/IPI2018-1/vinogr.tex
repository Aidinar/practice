\def\stat{vinogradov}

\def\tit{УЧЕТ ПРЕДВАРИТЕЛЬНЫХ ОЦЕНОК СКОРОСТИ ПОРОЖДЕНИЯ СХОДСТВ 
СПАРИВАЮЩЕЙ ЦЕПЬЮ МАРКОВА}

\def\titkol{Учет предварительных оценок скорости порождения сходств 
спаривающей цепью Маркова}

\def\aut{Д.\,В.~Виноградов$^1$}

\def\autkol{Д.\,В.~Виноградов}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Виноградов Д.\,В.}
\index{Vinogradov D.\,V.}




%{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
%{Работа выполнена при финансовой поддержке РФФИ (проект 17-01-00816).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Институт проблем информатики Федерального исследовательского центра <<Информатика 
и~управление>> Российской академии наук, \mbox{krrguest@yandex.ru}}

\vspace*{-6pt}
    
    

\Abst{В~современном интеллектуальном анализе данных возрастает доля методов 
статистического машинного обучения. Для подхода, основанного на бинарной операции 
сходства, таковым является ве\-ро\-ят\-ност\-но-ком\-би\-на\-тор\-ный формальный метод  
(ВКФ-ме\-тод). Его основной алгоритм~--- спаривающая цепь Маркова. 
В~статье предложен механизм учета длин траекторий (до склеивания) 
с~формированием верхней границы, по которой следует останавливать излишне длинные 
траектории в~дальнейшем. Теоретический результат, доказанный в~статье, 
утверждает, что при учете достаточно большого числа предварительных траекторий 
вероятности изменяются экспоненциально малым образом в~метрике тотальной вариации. 
Это предложение особенно полезно, когда имеется малая доля длинных траекторий 
относительно остальных, так как в~этом случае обеспечивается баланс между величиной 
границы и~изменением вероятностей.}

\KW{сходство; спаривающая цепь Маркова; метрика тотальной вариации}

  \DOI{10.14357/19922264180106} 
  
%\vspace*{-6pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}

\section{Введение}

    Одним из современных методов интеллектуального анализа данных 
является ДСМ-ме\-тод~[1] автоматического порождения гипотез. Этот метод 
возник как синтез нескольких когнитивных процедур в~трудах проф.\ 
В.\,К.~Финна, осмыслившего идеи Д.\,С.~Милля, Ч.\,С.~Пирса 
и~К.~Поппера.
    
    Одна из процедур ДСМ-ме\-то\-да~--- индуктивное обобщение 
обучающих примеров~--- была исследована примерно в~это же самое время 
группой зарубежных ученых под руководством проф.\ Рудольфа Вилле под 
названием теории формальных понятий~[2]. Однако ДСМ-ме\-тод ввел 
в~рас\-смот\-ре\-ние понятие контрпримера, чего нет в~теории формальных 
понятий.
    
    Следует признать, что имеются некоторые сложности в~применении 
ДСМ-ме\-то\-да к~анализу данных.
    
    Во-первых, множество порождаемых ДСМ-ги\-по\-тез может оказаться 
экспоненциально велико по сравнению с~размером обучающей выборки.
    
    Во-вторых, С.\,О.~Кузнецовым~[3], М.\,И.~Забежайло и~др.\ были 
доказаны пессимистические оценки сложности для многих ДСМ-про\-це\-дур 
(так называемые NP-пол\-но\-та и~\#P-пол\-но\-та).
    
    В-третьих, попытки присоединить де\-дук-\linebreak тивный вывод оказались 
неудачными: Д.\,П.~Сквор\-цовым~[4] установлена неарифметичность 
стандартного подхода через кванторы по конечным \mbox{множествам}, 
а~автором~[5]~--- невыразимость этой теории средствами логики предикатов 
первого порядка.
    
    Наконец, автор~[6] сумел обнаружить еще одну трудность: 
существование так называемых случайных ДСМ-ги\-по\-тез. Эти случайные 
гипотезы возникают тогда, когда вычисляется сходство двух (или более) 
обучающих примеров, каждый из которых имеет свой механизм порождения 
целевого свойства. Это сходство оказывается фрагментом (набором общих 
признаков), случайно имеющимся в~каждом из этих объектов. Возникновение 
таких сходств следует рассматривать как аналог <<пе\-ре\-обуче\-ния>> для 
многих методов машинного обуче\-ния, когда максимальный учет информации 
из обуча\-ющей выборки приводит к~модели, де\-мон\-ст\-ри\-ру\-ющей плохую 
предсказательную способность.
    
    Для преодоления указанных трудностей автором~[7] был предложен  
ве\-ро\-ят\-ност\-но-ком\-би\-на\-тор\-ный подход. Так как некоторые 
ингредиенты заимствованы из теории формальных понятий, автор назвал его 
ВКФ-ме\-то\-дом.
    
    Ключевой процедурой ВКФ-ме\-то\-да является вероятностный 
алгоритм нахождения случайного ВКФ-кан\-ди\-да\-та с~помощью 
спаривающей цепи Маркова. Автором показано, что это действительно цепь 
Маркова, которая останавливается с~вероятностью единица. Однако хорошей 
оценки на длины траекторий получить пока не удалось. 
    
    Может так случиться, что малое число траекторий имеют длину 
(например, экспоненциальную), значительно превосходящую (например, 
полиномиальную) длину остальных траекторий. Тогда можно предложить 
использовать~$r$~предварительных прогонов цепи Маркова для получения 
верхней границы ожидания спаривания (как суммы длин этих 
предварительно вычисленных траекторий). Если текущая траектория не 
завершается до этой границы, то текущие вычисления прекращаются, а цепь 
Маркова запускается заново. 
    
    Ясно, что при такой модификации изменяются вероятности появления 
результатов (эргодических состояний). В~настоящей работе будет получена 
хорошая оценка, показывающая, что при выборе достаточно большого числа 
$r$ предварительных прогонов изменение вероятностей может быть сделано 
сколь угодно малым.

\section{Вспомогательные определения и~факты}

    Сходство является бинарной операцией на множестве~$X$, 
объемлющем множество объектов, т.\,е.\ представляет собой отображение 
$\bigcap: X\times X\hm\to X$. Элементы множества~$X$ будем называть 
\textit{фрагментами}. 
    
    Для независимости результата нахождения сходства нескольких 
объектов операция сходства должна удовлетворять аксиомам \textit{нижней 
полурешетки}.
    
    Для выражения тривиальности сходства до\-бав\-ля\-ет\-ся специальный 
\textit{пустой фрагмент}~$\emptyset$ со свойством наименьшего элемента.
    
    Важнейшим примером будет нижняя полурешетка, состоящая из 
битовых строк фиксированной длины с~побитовым умножением в~качестве 
операции сходства. Пустым фрагментом будет служить строка, состоящая из 
одних нулей. Каждый бит может быть отождествлен с~бинарным признаком. 
Тогда битовая строка соответствует множеству признаков, в~которых 
встречаются единицы. При этом операция сходства соответствует 
пересечению множеств признаков, а пустой фрагмент~--- пустому множеству 
признаков.
    
    Ясно, что в~этом примере строка из одних единиц будет соответствовать 
наибольшему элементу.
    
    Важность этого примера объясняется тем, что операция побитового 
умножения допускает эффективную реализацию на современных ЭВМ. 
Существуют специальные классы объектов (например, {dynamic\_bitset} 
в~C++), реализующие удобное оперирование битовыми строками.
    
    С другой стороны, теорема Рудольфа Вилле~\cite{2-vin} утверждает, что 
эта конструкция позволяет построить любую конечную решетку из нижней 
полурешетки битовых строк с~операцией побитового умножения, если к~ней 
добавить наибольший элемент.
    
    Собирая вместе битовые строки, представляющие объекты, получаем 
прямоугольную таблицу~$I$, которую будем называть \textit{формальным 
контекстом}~\cite{2-vin}. 

Формальный контекст можно воспринимать как 
бинарное отношение между элементами множества~$O$, которые назовем 
\textit{именами объектов}, и~элементами множества~$F$, которые назовем 
\textit{признаками}. Если в~строке, соответствующей объекту $o\hm\in O$, 
и~столбце, соответствующем фрагменту $f\hm\in F$, стоит единица, то 
говорим, что \textit{объект~$o$ обладает признаком}~$f$, и~обозначаем это 
через~$oIf$. В~противном случае говорим, что \textit{объект~$o$ не имеет 
признака}~$f$.
    
    Для подмножества $A\hm\subseteq O$ объектов его \textit{сходством} 
называется подмножество $A^\prime\hm= \{f \in F:\ \forall\ o\in 
A[oIf]\}\hm\subseteq F$. Полагаем $\emptyset^\prime\hm=F$.
    
    На самом деле это определение совпадает с~последовательным 
вычислением побитового умножения строк, соответствующих отобранным 
во множество~$A$ объектам. 
    
    Для подмножества $B\hm\subseteq F$ признаков его \textit{сходством} 
называется подмножество $B^\prime\hm= \{o\in O:\ \forall\ f\hm\in B[oIf]\} 
\hm\subseteq O$. Полагаем $\emptyset^\prime\hm=O$.
    
    Легко проверить стандартные свойства операции сходства.
    
    \smallskip
    
    \noindent
    \textbf{Определение~1.}\ Пару $\langle A, B\rangle$  назовем 
    ВКФ-кан\-ди\-да\-том, если $A\hm= B^\prime \hm\subseteq O$ 
и~$B\hm= A^\prime \hm\subseteq F$.
    
    \smallskip
    
    Обычно такие пары называют \textit{формальными понятиями}, но 
предпочтительнее сменить название, так как оригинальное название 
подвергается обосно\-ван\-ной критике со стороны специалистов по философии и~искусственному интеллекту.
    
    Легко проверить, что \textbf{множество всех ВКФ-кан\-ди\-да\-тов} 
(для фиксированного формального контекста) \textbf{образует решетку}.
    
    Теорема Р.~Вилле~\cite{2-vin, 8-vin} гласит: \textbf{Любая конечная 
решетка изоморфна решетке всех ВКФ-кан\-ди\-да\-тов для подходяще 
выбранного формального контекста.}
    
    Основываясь на операциях сходства, зададим ключевое понятие 
предлагаемого подхода:
    
    \smallskip
    
    \noindent
     \textbf{Определение~2.}\ Операция \textit{за\-мы\-кай-по-од\-но\-му-вниз} на  
ВКФ-кан\-ди\-да\-те $\langle A, B\rangle$ и~объекте $o\hm\in O$ по\-рож\-да\-ет пару 
$\mathrm{CbODown}\,(\langle A, B\rangle, o)\hm= \langle (A\cup \{o\})^{\prime\prime}, 
(A\hm\cup 
\{o\})^\prime\rangle$. Операция \textit{за\-мы\-кай-по-од\-но\-му-вверх} на 
     ВКФ-кан\-ди\-да\-те $\langle A, B\rangle$ и~признаке $f\hm\in F$ порождает пару 
$\mathrm{CbOUp}\,(\langle A,B\rangle,f)\hm= \langle  (B\cup \{f\})^\prime, (B\cup 
\{f\})^{\prime\prime}\rangle$.
    
    Операция $\mathrm{CbODown}$ соответствует шагу алгоритма  
<<За\-мы\-кай-по-од\-но\-му>>, который был предложен 
С.\,О.~Кузнецовым~\cite{9-vin} для вычисления всех ВКФ-кан\-ди\-да\-тов 
перебором сверху вниз.
    
    
 \begin{figure*}[b]
    {\small
    \hrule
    \vspace*{4pt}
    
    \textbf{Алгоритм 1:} {\small Спаривающая цепь Маркова}
    
        \vspace*{4pt}
    
    \hrule
    
        \vspace*{4pt}
        
    \textbf{Data:} множество обучающих ($+$)-примеров; внешние функции 
$\mathrm{CbOUp}\,(,)$и~$\mathrm{CbODown}\,(,)$ операций <<за\-мы\-кай-по-од\-но\-му>>
    
    \textbf{Result:} ВКФ-кандидат $\langle A, B\rangle$
    
    $O$\;:=\;($+$)-примеры, $F$\;:=\;признаки; $I\subseteq O\times F$~--- 
формальный контекст для ($+$)-примеров;
    
    $R$\;:=\;$O\cup F$\,;\ $\mathrm{Min}$\;:=\;$\langle O,O^\prime\rangle$;\ 
$\mathrm{Max}$\;:=\;$\langle F^\prime, F\rangle$;
    
    \textbf{while} $(\mathrm{Min}\not= \mathrm{Max})$ \textbf{do}
    
    \hspace*{5mm}Выбираем случайный элемент $r\in R$;
    
    \hspace*{5mm}\textbf{if} $(r\in O)$ \textbf{then}
    
    \hspace*{10mm}$\mathrm{Min}$\;:=\;$\mathrm{CbODown}\,(\mathrm{Min},r)$;\ 
$\mathrm{Max}$\;:=\;$\mathrm{CbODown}\,(\mathrm{Max},r)$;
    
    \hspace*{5mm}\textbf{end}
    
    \hspace*{5mm}\textbf{else}
    
    \hspace*{10mm}$\mathrm{Min}$\;:=\;$\mathrm{CbOUp}\,(\mathrm{Min},r)$; 
$\mathrm{Max}$\;:=\;$\mathrm{CbOUp}\,(\mathrm{Max},r)$;
    
    \hspace*{5mm}\textbf{end}
    
    \textbf{end}
    
        \vspace*{4pt}
    
    \hrule
    }
    \end{figure*}
    
    \noindent
    \textbf{Лемма~1.}\ \textit{Для любого ВКФ-кандидата $\langle A, 
B\rangle$ и~любого объекта $o\hm\in O$ пара $\mathrm{CbODown}\,(\langle A,B\rangle, 
o)$ является ВКФ-кан\-ди\-да\-том}.
    
    \textit{Аналогично для любого ВКФ-кан\-ди\-да\-та $\langle A, B\rangle$ 
и~любого признака $f\hm\in F$ пара $\mathrm{CbOUp}\,(\langle A,B\rangle, f)$ является 
ВКФ-кан\-ди\-да\-том.}
    
    \smallskip
    
    \noindent
     \textbf{Определение~3.}\ \textbf{Порядок} на ВКФ-кан\-ди\-да\-тах: $\langle A_1, 
B_1\rangle \hm\leq \langle A_2, B_2\rangle$, если $B_1\hm\subseteq B_2$.
     
     \smallskip
     
    Это двойственное (с~точки зрения теории формальных понятий) 
определение приводится в~настоящем виде для согласованности с~традицией 
отечественной школы.
    
    \smallskip
    
    \noindent
   \textbf{Лемма~2.} \textit{Для всякой упорядоченной пары  
ВКФ-кан\-ди\-да\-тов $\langle A_1, B_1\rangle \hm\leq \langle A_2, B_2\rangle$ 
и~любого объекта $o\hm\in O$ имеем} $\mathrm{CbODown}\,(\langle A_1, B_1\rangle, 
o)\hm\leq \mathrm{CbODown}\,(\langle A_2, B_2\rangle, o)$.
    
    \textit{Для всякой упорядоченной пары ВКФ-кан\-ди\-да\-тов $\langle 
A_1, B_1\rangle \hm\leq \langle A_2, B_2\rangle$ и~любого признака $f\hm\in 
F$ имеем} $\mathrm{CbOUp}\,(\langle A_1, B_1\rangle, o)\hm\leq \mathrm{CbOUp}\,(\langle A_2, 
B_2\rangle, o)$. 
    
   
    
    Заметим, что состоянием изменяемых переменных в~цикле (состоянием 
спаривающей цепи Маркова) является упорядоченная пара ВКФ-объ\-ек\-тов 
$\langle A_1, B_1\rangle \hm\leq \langle A_2, B_2\rangle$. 
    
    Первоначально меньший ВКФ-кан\-ди\-дат совпадает с~наименьшим  
ВКФ-кан\-ди\-да\-том $\mathrm{Min}$\;:=\;$\langle O, O^\prime\rangle$, а~больший~--- 
с~наибольшим $\mathrm{Max}$\;:=\;$\langle F^\prime, F\rangle$.
    
    В цикле к~обоим ВКФ-кан\-ди\-да\-там применяется одна и~та же 
операция $\mathrm{CbODown}$ с~выбранным объектом или $\mathrm{CbOUp}$ c выбранным 
признаком.
    
    Процесс останавливается, когда меньший ВКФ-кан\-ди\-дат совпадет 
с~большим. Тогда этот общий ВКФ-кан\-ди\-дат и~выдается алгоритмом~1.
    
    Следующая теорема из статьи~\cite{7-vin} объясняет, откуда здесь 
возникает цепь Маркова.
    
    \smallskip
    
    \noindent
    \textbf{Теорема~1.}\ \textit{Алгоритм~$1$ соответствует цепи 
Мар\-кова.}
    
    \smallskip
    
    Теперь переходим к~вопросу о спаривании.
    
    \smallskip
    
    \noindent
    \textbf{Определение~4.}\ Состояние вида $\langle A, B\rangle \hm= 
\langle A, B\rangle $ спаривающей цепи Маркова для совпадающей пары  
ВКФ-кан\-ди\-да\-тов называется \textit{эргодическим}. Состояние вида 
$\langle A_1, B_1\rangle\hm< \langle A_2, B_2\rangle$ называется 
\textit{невозвратным}.
    
    \smallskip
    
    Теперь имеем классическую теорему о схо\-ди\-мости с~вероятностью 
единица.
    
    \smallskip
    
    \noindent
    \textbf{Теорема~2.}\ \textit{Вероятность того, что состояние  
    $\langle A_1, B_1\rangle\hm \leq \langle A_2, B_2\rangle$
спаривающей цепи Маркова окажется невозвратным, стремится к~нулю, 
когда} $t\hm\to\infty$.
    
    \smallskip
    
    В~статье~\cite{10-vin} приводится доказательство этого результата, но 
это классический результат в~теории цепей Маркова~\cite{11-vin}, поэтому 
здесь не будем приводить доказательство.

\section{Основной результат}

    Так как спаривающая цепь Маркова может иметь траектории 
существенно разной длины, то возможно применение следующей техники 
остановки длинной траектории и~запуска цепи заново.
    
    \smallskip
    
    \noindent
    \textbf{Определение~5.}\ Если $T_1,\ldots ,T_r$~--- независимые 
целочисленные случайные величины, имеющие распределение времени~$T$ 
склеивания, то \textit{верхняя граница склеивания} 
по~$r$~предварительным прогонам определяется как $\hat{T}\hm= 
T_1+\cdots +T_r$.
    
    \smallskip
    
    На практике предлагается сделать~$r$ прогонов спаривающей цепи 
Маркова c соответствующими временами склеивания $t_1,\ldots , t_r$ и~взять 
оценку $t_1+\cdots +t_r$ верхней границы склеивания.
    
    Оценим, как изменяются вероятности попадания в~эргодические 
состояния при остановке спаривающей цепи Маркова по~$r$~прогонам.
    
    \smallskip
    
    \noindent
    \textbf{Определение~6.} \ Для целочисленной случайной 
величины~$\hat{T}$, независимой от целочисленной случайной 
величины~$T$, \textit{условное распределение} состояний относительно 
события $B\hm = \left\{ T\leq \hat{T}\right\}$ есть распределение 
    $$
    \mu\left( \hat{T}\right)_t= \fr{{\sf P}\left[ X_T=i, T\leq \hat{T}\right]} 
    {{\sf P}\left[ 
T\leq \hat{T}\right]}
    $$
для любого эргодического состояния~$i$.

\smallskip

\noindent
\textbf{Определение~7.}\ Расстояние \textit{тотального изменения} между 
распределениями вероятностей $\mu\hm= (\mu_i)_{i\in U}$ и~$\nu\hm= 
(\nu_i)_{i\in U}$ на конечном пространстве~$U$ определяется правилом: 
$$
\parallel \mu-\nu\parallel_{TV} =\fr{1}{2}\sum\limits_{i\in U} \vert \mu_i -
\nu_i\vert\,.
$$

    Это расстояние является половиной метрики~$l_1$, следовательно, само 
является метрикой (в~част\-ности, симметрично).
    
    \smallskip
    
    Известна классическая и~доказываемая прямо из определения~7 
    
    \smallskip
    
    \noindent
     \textbf{Лемма~3.}\ $\parallel \mu - \nu\parallel_{TV}\hm=\max\nolimits_{R\subseteq U} 
\vert \mu(R)\hm- \nu(R)\vert$.
     
     \smallskip
      
    В~лемме~3 подмножество~$R$, на котором достигается максимум, 
определяется так: $$
R= \{ i\in U\vert \mu_i>\nu_i\}\,.
$$
    
    Следующая лемма является технической.
    
    \smallskip
    
    \noindent
    \textbf{Лемма~4.}\ $\parallel\! \mu - \mu(\hat{T})\!\parallel_{TV}\hm\leq 
{\sf P}\left[T>\hat{T}\right]\Big/\left(\vphantom{\hat{T}}1\;-\right.\linebreak$ 
$\left.-\;{\sf P}\left[T>\hat{T}\right]\right)$, \textit{где $\mu(\hat{T})$~--- распределение 
останов\-лен\-ной на верхней границе~$\hat{T}$ склеивания по $r\hm>1$ 
испытаниям, а~$\mu$~--- распределение выдачи не\-останов\-лен\-ной цепи.}
    \smallskip
    
    \noindent
    Д\,о\,к\,а\,з\,а\,т\,е\,л\,ь\,с\,т\,в\,о\,.\ \  По определению~6 
$\mu(\hat{T})_i\hm= {\sf P}\left[X_T=i, T\leq \hat{T}\right]\Big/{\sf P}\left[T\leq \hat{T}\right]$. Тогда
    \begin{multline*}
    {\sf P}\left[ T\leq \hat{T}\right]\left( \mu\left( \hat{T}\right)_i-
    \mu_i\right) ={}\\
    {}=
    {\sf P}\left[ X_T=i, T\leq \hat{T}\right] -{\sf P}
    \left[ T\leq \hat{T}\right] \mu_i={}\\
    {}=
{\sf  P}\left[ T>\hat{T}\right] \mu_i - {\sf P}\left[ X_T=i,\ T>\hat{T}\right] \leq{}\\
    {}\leq {\sf P}\left[ 
T>\hat{T}\right] \mu_i\,.
    \end{multline*}
        Суммируя по множеству $R\hm= \{i\in U \vert \mu_i> \mu(\hat{T}_i\}$, 
получим: 
    $$
    {\sf P}\left[ T\leq \hat{T}\right] \parallel\! \mu-\mu(\hat{T})\!\parallel_{TV} \leq 
{\sf P}\left[ T>\hat{T}\right]\,,
    $$
    что и~приводит к~утверждению леммы.
    
    \smallskip
    
    Теперь докажем основную лемму.
    
    \smallskip
    
    \noindent
     \textbf{Лемма~5.}\  $\parallel\! \mu-\mu(\hat{T})\!\parallel_{TV}\hm\leq 1/(2^r-1)$, \textit{где 
$\mu(\hat{T})$~--- распределение остановленной на верхней границе~$\hat{T}$ склеивания по 
$r\hm>1$ испытаниям, а~$\mu$~--- распределение выдачи неостановленной цепи.}
     
     \smallskip
     
    \noindent
    Д\,о\,к\,а\,з\,а\,т\,е\,л\,ь\,с\,т\,в\,о\,.\ \ Из-за леммы~4 достаточно 
доказать, что 
    $$
    {\sf P}\left[ T>\hat{T}\right] \leq 2^{-r}\,.
    $$
    
    Из определения~$T, T_1, \ldots , T_r$ как независимых одинаково 
распределенных случайных величин следует, что ${\sf P}[T>T_j]\hm\leq 1/2$ для 
всех $1\hm\leq j\hm\leq r$.
    
    Докажем субмультипликативность: 
    $$
    {\sf P}\left[ T> \sum\limits^k_{j=1} T_j\right] \leq {\sf P}\left[ T> 
\sum\limits_{j=1}^{k-1} T_j\right]\cdot {\sf P}\left[ T>T_k\right]
    $$
    для всех $1\hm < k\hm\leq r$.
    
    Но это следует из формулы условной вероят\-ности, так как если 
    $T\hm> \sum\nolimits_{j=1}^{k-1} T_j,$
    то 
   \begin{multline*}
    \left\langle A_1 \left( \sum\limits_{j=1}^{k-1} T_j\right), B_1\left( 
\sum\limits_{j=1}^{k-1} T_j\right) \right\rangle<{}\\
{}<
    \left\langle A_2\left( \sum\limits_{j=1}^{k-1} T_j\right), B_2\left( 
\sum\limits_{j=1}^{k-1} T_j\right)\right\rangle\,.
    \end{multline*}
        Поэтому, применяя ко всем четырем ВКФ-кан\-ди\-да\-там 
    \begin{multline*}
    \mathrm{Min} \leq \left\langle A_1\left( 0+\sum\limits_{j=1}^{k-1} T_j\right), 
B_1\left(0+\sum\limits_{j=1}^{k-1} T_j\right)\right\rangle<{}\\
    {}< \left\langle A_2\left( 0+\sum\limits_{j=1}^{k-1} T_j\right),
    B_2\left( 0+\sum\limits_{j=1}^{k-1} T_j\right)\right\rangle\leq \mathrm{Max}
    \end{multline*}
одинаковые операции $\mathrm{CbODown}$ и~$\mathrm{CbOUp}$, имеем, что если 
\begin{multline*}
\left\langle A_1\left( t+\sum\limits_{j=1}^{k-1} T_j\right), 
B_1\left(t+\sum\limits_{j=1}^{k-1} T_j\right)\right\rangle<{}\\
    {}< \left\langle A_2\left( t+\sum\limits_{j=1}^{k-1} T_j\right),
B_2\left( t+\sum\limits_{j=1}^{k-1} T_j\right)\right\rangle
\end{multline*}
спаривается позднее момента $T_k\hm+ \sum\nolimits_{j=1}^{k-1} T_j\hm= 
\sum\nolimits^k_{j=1} T_j$, то и~спаривание $\mathrm{Min} \hm< \mathrm{Max}$ совершается 
позднее момента~$T_k$, т.\,е.
$$
{\sf P}\left( T> \sum\limits^k_{j=1} T_j \vert T> \sum\limits_{j=1}^{k-1} T_j\right) 
\leq {\sf P}\left[ T>T_k\right]\,.
$$ 
    
    Теперь результат леммы следует по индукции.
    
    Соединяя результаты лемм~3 и~5, получим
    
    \smallskip
    
    \noindent
    \textbf{Теорема~3.}\ \textit{Для любого~$R$ c~$\mu(R)\hm=\rho$ 
и~$r\hm> \log_2(1-1/\rho)$ имеем $\mu(\hat{T})(R)\hm\geq \rho\hm- 1/(2^r-1)$ 
на верхней границе~$\hat{T}$ склеивания по $r\hm>1$ испытаниям.}
    
    \smallskip
    
    \noindent
    Д\,о\,к\,а\,з\,а\,т\,е\,л\,ь\,с\,т\,в\,о\,. 
    \begin{multline*}
    \rho-\fr{1}{2^r-1}\leq \mu(R)-\parallel\! \mu-\mu(\hat{T})\!\parallel_{TV}={}\\
    {}= \mu(R)-\max\limits_{Q\subseteq U}\left\vert \mu(Q) -
\mu(\hat{T})(Q)\right\vert \leq{}\\
    {}\leq \mu(R) -\left\vert \mu(R) -\mu(\hat{T}(R)\right\vert \leq 
\mu(\hat{T})(R)\,.
    \end{multline*}

\section{Заключение}

    В настоящей работе получена оценка изменения вероятностей появления 
ВКФ-кандидатов, если спаривающую цепь Маркова из ВКФ-метода 
остановить на основании~$r$~предварительных прогонов. Эта задача 
естественно возникает тогда, когда малое число траекторий имеют длину 
(например, экспоненциальную), значительно превосходящую (например, 
полиномиальную) длину остальных траекторий. В~такой ситуации путем 
подбора большого числа предварительных прогонов можно получить 
достаточно малую верхнюю оценку на время спаривания так, чтобы 
вероятности появления ВКФ-кан\-ди\-да\-тов изменились как угодно мало.
    
    \bigskip
    
    Автор благодарит проф.\ В.\,К.~Финна за внимание к~работе, проф.\ 
Е.\,М.~Бениаминова за полезные обсуждения и~своих коллег по 
лаборатории~35 ФИЦ ИУ РАН за поддержку и~полезные дискуссии.
    
{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{1-vin}
\Au{Финн В.\,К.} Об интеллектуальном анализе данных~// 
Новости искусственного интеллекта, 2004. №\,3. C.~3--18.
\bibitem{2-vin}
\Au{Ganter B., Wille~R.} Formal concept analysis.~--- 
Berlin: Springer-Verlag, 1999. 284~p.
\bibitem{3-vin}
\Au{Кузнецов С.\,О.} 
Быстрый алгоритм построения всех пересечений объектов из нижней полурешетки~// 
Научная и~техническая информация. Сер.~2, 1993. №\,1. C.~17--20.
\bibitem{4-vin}
\Au{Скворцов Д.\,П.} 
О~некоторых способах построения логических языков с~кванторами по кортежам~// 
Се\-мио\-ти\-ка и~информатика, 1983. Вып.~20. C.~102--126.
\bibitem{5-vin}
\Au{Виноградов Д.\,В.} Формализация правдоподобных рассуждений в~логике предикатов~// 
Научная и~техническая информация. Сер.~2, 2000. №\,11. C.~17--20.
\bibitem{6-vin}
\Au{Виноградов Д.\,В.} Вероятность порождения случайного ДСМ-сход\-ст\-ва 
при наличии контр-при\-ме\-ров~// Научная и~техническая информация. Сер.~2, 2015.
 №\,3. C.~1--5.
\bibitem{7-vin}
\Au{Vinogradov D.\,V.} VKF-method of hypotheses generation~// 
Comm. Com. Inf. Sc., 2014. Vol.~436. P.~237--248.
\bibitem{8-vin}
\Au{Davey B.\,A., Priestley~H.\,A.}
Introduction to lattices and order.~--- 2nd ed.~--- 
Cambridge: Cambridge University Press, 2002. 298~p.
\bibitem{9-vin}
\Au{Кузнецов С.\,О.} 
Интерпретация на графах и~сложностные характеристики задач поиска 
закономерностей определенного вида~// Научная и~техническая информация. Сер.~2, 1989. 
№\,1. C.~23--28.
\bibitem{10-vin}
\Au{Виноградов Д.\,В.}
Вероятностное порождение гипотез в ДСМ-ме\-то\-де с~по\-мощью простейших цепей Маркова~// 
Научная и~техническая информация. Сер.~2, 2012. №\,9. C.~20--27.
\bibitem{11-vin}
\Au{Кемени Дж., Снелл~Дж., Кнэпп~А.}
Счетные цепи Маркова~/ Пер. с англ.~--- М.: Наука, 1987. 416~c.
(\Au{Kemeny~J.\,G., Snell~J.\,L., Knapp~A.\,W.}
{Denumerable Markov chains}.~--- Princeton, NJ, USA: Van Nostrand,  1966. 439~p.)

 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 24.04.17}}

\vspace*{8pt}

%\newpage

%\vspace*{-24pt}

\hrule

\vspace*{2pt}

\hrule

%\vspace*{8pt}


\def\tit{INFLUENCE OF PRELIMINARY ESTIMATES ON~THE~SPEED OF~SEARCH 
OF~SIMILARITIES BY~THE~COUPLING MARKOV CHAIN}

\def\titkol{Influence of preliminary estimates on~the~speed of~search 
of~similarities by~the~coupling Markov chain}

\def\aut{D.\,V.~Vinogradov}

\def\autkol{D.\,V.~Vinogradov}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-9pt}


\noindent
Institute of Informatics Problems, Federal Research Center 
``Computer Science and Control'' of the 
Russian Academy of Sciences, 44-2~Vavilov Str., 
Moscow 119333, Russian Federation



\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2018\ \ \ volume~12\ \ \ issue\ 1}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2018\ \ \ volume~12\ \ \ issue\ 1
\hfill \textbf{\thepage}}}

\vspace*{3pt}




\Abste{At present, Data Mining expands usage of statistical machine learning methods. The 
similarity-based approach uses the probabilistic combinatorial formal method (VKF
(variational Kalman filter) method). The main algorithm is 
based on a~coupling Markov chain. The authors propose a mechanism to convert lengths 
of preliminary
trajectories (before coalescence) to an upper bound on which it is 
necessary to stop excessively long 
successive\linebreak\vspace*{-12pt}}

\Abstend{runs. The main result claims that the change of probabilities is exponentially small with 
respect to total variation distance, if the chain uses sufficient number of preliminary runs. This proposal 
may be useful when there exists a small fraction of long trajectories with respect to the rest, because it 
provides a~balance between the size of the bound and changes of probabilities.}

\KWE{similarity; Markov chain; VKF candidate; total variation; coupling}

  \DOI{10.14357/19922264180106} 

%\vspace*{-12pt}

%\Ack
%\noindent




%\vspace*{3pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99} 
\bibitem{1-vin-1}
\Aue{Finn, V.\,K.} 2004. Ob intellektual'nom analize dannykh 
[On intelligent data analysis]. 
\textit{Novosti iskusstvennogo intellekta} [Artificial Intelligence News] 3:3--18.
\bibitem{2-vin-1}
\Aue{Ganter, B.,  and R.~Wille}. 1999. 
\textit{Formal concept analysis}. Berlin: Springer-Verlag. 284~p.
\bibitem{3-vin-1}
\Aue{Kuznetsov, S.\,O.} 1993. Bystryy algoritm postroeniya vsekh peresecheniy 
ob''ektov iz nizhney polureshetki 
[Fast algorithm for computation of all intersections of objects from 
a~low-semilattice]. 
\textit{Nauchnaya i~tekhnicheskaya informatsiya. Ser.~2} 
[Scientific and technical information. 
Ser.~2] 1:17--20.
\bibitem{4-vin-1}
\Aue{Skvortsov, D.\,P.} 1983. O~nekotorykh sposobakh po\-stro\-eniya logicheskikh 
yazykov s~kvantorami po kor\-te\-zham [On some ways to construct logical 
languages with tuples quantifiers]. 
\textit{Semiotika i~informatika} [Semiotics and Informatics] 20:102--126.
\bibitem{5-vin-1}
\Aue{Vinogradov, D.\,V.} 2000. Formalizing plausible arguments in predicate logic. 
\textit{Automatic Documentation Math. Linguistics} 34(6):6--10.
\bibitem{6-vin-1}
\Aue{Vinogradov, D.\,V.} 2015. The probability of encountering an accidental 
DSM similarity in the presence of counter examples. 
\textit{Automatic Documentation Math. Linguistics} 49(2):43--46.
\bibitem{7-vin-1}
\Aue{Vinogradov, D.\,V.} 2014. VKF-method of hypotheses generation. 
\textit{Comm. Comp. Inf. Sc.} 436:237--248.
\bibitem{8-vin-1}
\Aue{Davey, B.\,A., and H.\,A.~Priestley.} 2002. 
\textit{Introduction to lattices and order.} 2nd ed. 
Cambridge: Cambridge University Press. 298~p.
\bibitem{9-vin-1}
\Aue{Kuznetsov, S.\,O.} 1989. Interpretatsiya na grafakh 
i~slozhnostnye kharakteristiki zadach poiska zakonomernostey opredelennogo vida 
[Graphs interpretation and complexity characteristics of tasks of search for 
patterns of special form]. 
\textit{Nauchnaya i~tekhnicheskaya informatsiya. Ser.~2} 
[Scientific and technical information. Ser.~2]  1:23--28. 
\bibitem{10-vin-1}
\Aue{Vinogradov, D.\,V.} 2012. Random generation of hypotheses in the JSM method 
using simple Markov chains. 
\textit{Automatic Documentation Math. Linguistics} 46(5):221--228.
\bibitem{11-vin-1}
\Aue{Kemeny, J.\,G., J.\,L.~Snell, and A.\,W.~Knapp.}
 1966. \textit{Denumerable Markov chains}. Princeton, NJ: Van Nostrand. 439~p.

\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Received April 24, 2017}}

%\vspace*{-10pt}

\Contrl

\noindent
\textbf{Vinogradov Dmitry V.} (b.\ 1964)~--- Candidate of Science (PhD) in physics and mathematics, 
senior scientist, Institute of Informatics Problems, Federal Research Center ``Computer Science and 
Control'' of the Russian Academy of Sciences, 44/2~Vavilova Str., Moscow 119333, Russian Federation; 
\mbox{krrguest@yandex.ru} 
\label{end\stat}


\renewcommand{\bibname}{\protect\rm Литература} 
\def\stat{bazilevski}

\def\tit{МНОГОФАКТОРНЫЕ МОДЕЛИ ПОЛНОСВЯЗНОЙ 
ЛИНЕЙНОЙ РЕГРЕССИИ БЕЗ~ОГРАНИЧЕНИЙ 
НА~СООТНОШЕНИЯ ДИСПЕРСИЙ ОШИБОК ПЕРЕМЕННЫХ}

\def\titkol{Многофакторные модели полносвязной линейной 
регрессии без~ограничений на~соотношения дисперсий ошибок} 
%переменных}

     \def\aut{М.\,П.~Базилевский$^1$}

     \def\autkol{М.\,П.~Базилевский}

\titel{\tit}{\aut}{\autkol}{\titkol}

     \index{Базилевский М.\,П.}
\index{Bazilevskiy M.\,P.}
 

%{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
%{Работа выполнена при финансовой поддержке РФФИ (проект 19-29-03051~мк).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Иркутский государственный университет путей сообщения, кафедра 
математики, \mbox{mik2178@yandex.ru}}

\vspace*{6pt}

     \Abst{Статья посвящена проблеме построения регрессионных моделей с~ошибками 
в~объясняющих переменных. В настоящее время такие модели широкого практического 
применения почти не находят, потому что они не пригодны для прогнозирования 
и~интерпретации, их сложно оценивать, и~при этом неизвестны дисперсии ошибок 
переменных. Для устранения перечисленных недостатков ранее автором были разработаны 
и~исследованы двухфакторные модели полносвязной линейной регрессии. Такие модели 
легко оцениваются, их можно использовать для прогнозирования, и~они лишены эффекта 
мультиколлинеарности. В~данной работе впервые рассмотрены многофакторные модели 
полносвязной линейной регрессии. Доказано, что в~случае снятия ограничений 
с~соотношения дисперсий ошибок переменных существуют единственные оценки 
полносвязной регрессии, при которых аппроксимационные качества ее вторичного 
уравнения и~классической модели множественной линейной регрессии, оцененной 
с~помощью метода наименьших квадратов, совпадают.}
     
     \KW{EIV-модель; полносвязная регрессия; регрессия Деминга; метод наименьших 
квадратов}
     
\DOI{10.14357/19922264200213} 
 
\vspace*{1pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}
     
     
\section{Введение}

    Одним из методов исследования влияния одной или нескольких 
объясняющих переменных на объясняемую переменную служит регрессионный 
анализ~[1, 2], применение которого приводит к~построению математических 
моделей статистического типа~--- регрессионных моделей. За\-час\-тую такие\linebreak 
модели оцениваются с~помощью метода наименьших квадратов (МНК) 
в~предположении, что объясняющие переменные не содержат случайных 
ошибок. Однако на практике даже при использовании современных 
технических средств значения таких переменных часто оказываются не вполне 
правильно измеренными. Поэтому возникает необходимость в~построении 
моделей с~ошибками в~объясняющих переменных.
    
    Регрессионные модели с~ошибками в~объясняющих переменных, известные 
в~настоящее время в~зарубежных источниках как <<Errors-In-Variables models>>  
(EIV-модели) и~<<measurement error models>>, имея внушительный 
математический аппарат для своего оценивания~[3--5], к~сожалению, 
практического применения почти не находят. Исключение составляет регрессия 
Деминга~[6], которая служит прекрасным инструментом в~клинических 
лабораториях для численного сопоставления новых и~существующих 
измерительных методов~[7--9]. 

К~недостаткам EIV-моделей относится то, что 
они не пригодны для прогнозирования, возникают проблемы с~их 
интерпретацией, априори нужно знать дисперсии ошибок переменных, а~также 
они сложны в~оценивании. 

Для устранения перечисленных недостатков 
в~работах~[10, 11] была предложена и~исследована двухфакторная модель 
полносвязной линейной регрессии. Если классическая множественная 
регрессия строится по принципу <<объясняющие переменные влияют на 
объясняемую>>, то принцип полносвязной регрессии~--- <<все переменные 
влияют друг на друга>>. Уста\-нов\-ле\-но, что двухфакторная полносвязная 
регрессия достаточно просто оценивается, лишена эффекта 
мультиколлинеарности, имеет гораздо более разнообразную интерпретацию, 
чем множественная регрессия, и~пригодна для прогнозирования. 

Целью данной 
работы ставится обобщение полносвязной регрессии на случай многих 
переменных.


    
    Стоит отметить, что работа выполнена в~рамках ло\-ги\-ко-ал\-геб\-раи\-че\-ско\-го 
подхода к~обработке статистических данных, при котором предполагается, что 
никаких априорных сведений об их вероятностной природе нет, поэтому не 
изучаются традиционные свойства оценок параметров: несмещенность, 
состоятельность и~эффективность.

\section{Многофакторная модель полносвязной линейной 
регрессии}

    Пусть $x_{ij}$, $i\hm=\overline{1,n}$, $j\hm=\overline{1,m}$,~--- 
наблюдаемые значения~$m$~объясняющих (входных) переменных $x_1, 
x_2,\ldots , x_m$. Предположим, что существуют их <<истинные>> значения 
$x^*_{i1}, x_{i2}, \ldots , x^*_{im}$, $i\hm= \overline{1,n}$, которые 
отличаются от наблюдаемых на случайные отклонения:
    \begin{equation}
    x_{ij}= x_{ij}^*+\varepsilon_i^{(x_j)}\,,\enskip i=\overline{1,n},\ 
j=\overline{1,m}\,.
    \label{e1-baz}
    \end{equation}
    
    Предположим, что между <<истинными>> переменными $x_1^*,x_2^*, 
\ldots , x_m^*$ имеют место функциональные зависимости
    \begin{equation}
    x_j^* =a_j+b_j x_m^*\,,\enskip j=\overline{1,m-1}\,,
    \label{e2-baz}
    \end{equation}
где $a_j$ и~$b_j$, $j\hm= \overline{1,m-1}$,~--- неизвестные параметры.
    
    Уравнения~(\ref{e2-baz}) означают, что все <<истинные>> переменные 
связаны между собой через связующую переменную~$x_m^*$. Тогда 
совокупность уравнений~(1) и~(2) будем называть многофакторной 
($m$-фак\-тор\-ной) моделью полносвязной линейной регрессии без выходной 
переменной. Для ее оценивания будем использовать метод наименьших полных 
квад\-ра\-тов (МНПК), состоящий в~минимизации функционала
\begin{multline}
S=\lambda_1\sum\limits^n_{i=1} \left( x_{i1}-a_1-b_1x^*_{im}\right)^2+{}\\
{}+
\lambda_2\sum\limits^n_{i=1}\left( x_{i2}-a_2-b_2x^*_{im}\right)^2+\cdots\\
\cdots+ \lambda_{m-1} \sum\limits^n_{i=1} \left( x_{i,m-1}-a_{m-1}-b_{m-1} 
x^*_{im}\right)^2+{}\\
{}+
\sum\limits^n_{i=1} \left( x_m-x^*_{im}\right)^2 \to \min\,,
\label{e3-baz}
\end{multline}
где $\lambda_1, \lambda_2, \ldots, \lambda_{m-1}$~---
соотношения дисперсий ошибок переменных:
$$
\lambda_1=\fr{\sigma^2_{\varepsilon^{(x_m)}}}
{\sigma^2_{\varepsilon^{(x_1)}}}; \enskip
\lambda_2\hm=\fr{\sigma^2_{\varepsilon^{(x_m)}}}
{\sigma^2_{\varepsilon^{(x_2)}}}; \enskip\ldots;  \enskip 
\lambda_{m-1}\hm= \fr{\sigma^2_{\varepsilon^{(x_m)}}}
{\sigma^2_{\varepsilon^{(x_{m-1})}}}.
$$

    
    Если соотношения дисперсий ошибок $\lambda_1, \lambda_2, \ldots , 
\lambda_{m-1}$ известны, то, вычислив частные производные функции~(3) по 
переменным~$x^*_{im}$, $i\hm=\overline{1,n}$, и~приравняв их к~нулю, 
получим:
    \begin{multline}
    x^*_{im}=\fr{-\sum\nolimits_{j=1}^{m-1} \lambda_j a_j b_j 
+\sum\nolimits_{j=1}^{m-1} \lambda_j b_j x_{ij}+x_{im}} 
{1+\sum\nolimits_{j=1}^{m-1} \lambda_j b_j^2}\,,\\\enskip i=\overline{1,n}\,.
    \label{e4-baz}
    \end{multline}
    
    Выразив из уравнений $\partial S/\partial a_j\hm=0$, 
$j\hm= \overline{1,m-1}$, коэффициенты~$a_j$ и~подставив их 
в~формулы~(\ref{e4-baz}), получим равенство:
    \begin{equation}
    \overline{x_m^*}=\overline{x_m}\,.
    \label{e5-baz}
    \end{equation}
    %
    Тогда, применяя равенство~(\ref{e5-baz}), запишем уравнения $\partial 
S/\partial a_j\hm=0$, $j\hm= \overline{1,m-1}$, в~видеL
    \begin{equation}
    a_j=\overline{x_j}-b_j \overline{x_m}\,,\enskip j=\overline{1,m-1}\,.
    \label{e6-baz}
    \end{equation}
    
    Используя формулы~(\ref{e4-baz}) и~(\ref{e6-baz}), представим уравнения 
$\partial S/\partial b_j\hm=0$, $j\hm= \overline{1,m-1}$, в~виде нелинейной 
системы:
    \begin{multline}
b_p\left( D_{x_m} +\sum\limits_{j=1}^{m-1} \lambda_j^2 b_j^2 D_{x_j}+\right.{}\\
{}+2 \sum\limits^{m-2}_{j_1=1} \sum\limits^{m-1}_{j_2=j_1+1} 
\lambda_{j_1}\lambda_{j_2}b_{j_1} b_{j_2} 
K_{x_{j_1}x_{j_2}}+{}\\
\left.{}+2\sum\limits_{j=1}^{m-1} \lambda_j b_j K_{x_j 
x_m}\right)={}\\
{}=\left( 1+\sum\limits_{j=1}^{m-1} \lambda_j b_j^2\right)
    \left( \sum\limits^{m-1}_{j=1} \lambda_j b_j K_{x_j x_p}+K_{x_m 
x_p}\right)\,,\\ \enskip
p=\overline{1,m-1}\,.
    \label{e7-baz}
    \end{multline} 
    
    Дополним набор входных переменных $x_1, x_2, \ldots , x_m$ выходной 
переменной~$y$ и~свяжем <<истинные>> значения, например 
переменной~$x_m$, со значениями переменной~$y$ моделью парной линейной 
регрессии:
    \begin{equation}
    y_i=c_0 +c_1 x^*_{im}+\varepsilon_i\,,\enskip i=\overline{1,n}\,,
    \label{e8-baz}
    \end{equation}
где $c_0$ и~$c_1$~--- неизвестные параметры, которые находятся с~помощью 
обычного МНК. Уравнение~(\ref{e8-baz}) будем называть вторичным 
уравнением полносвязной регрессии.
    
    Тогда совокупность уравнений~ (1), (2) и~(\ref{e8-baz}) представляет 
собой многофакторную модель полносвязной линейной регрессии с~выходной 
переменной~$y$.
    
    Если известны соотношения дисперсий ошибок $\lambda_1, \lambda_2, 
\ldots , \lambda_{m-1}$, то оценки полносвязной регрессии~(1), (2) 
и~(\ref{e8-baz}) находятся в~два этапа.
    \begin{enumerate}[1.]
\item С помощью МНПК оценивается полносвязная регрессия без 
выходной переменной. Для этого численно решается система~(\ref{e7-baz}) 
и~находятся оценки $b_1^*, b_2^*, \ldots , b^*_{m-1}$, затем по 
формулам~(\ref{e6-baz})~--- коэффициенты $a_1^*, a_2^*, \ldots , a^*_{m-1}$ 
и,~наконец, по формулам~(\ref{e4-baz})~--- <<истинные>> значения 
переменной~$x_m^*$.
\item С помощью МНК оценивается модель парной линейной 
регрессии~(\ref{e8-baz}) по формулам:
\begin{equation*}
c_1^* = \fr{\overline{yx_m^*}-\overline{y}\overline{x_m^*}} 
{\left(x_m^*\right)^2-\left(\overline{x_m^*}\right)^2}\,,\enskip 
c_0^*=\overline{y}-c_1^* \overline{x_m^*}\,.
%\label{e9-baz} 
\end{equation*}
    \end{enumerate}
    
Тогда оцененная модель (1), (2) и~(\ref{e8-baz}) имеет вид:
\begin{align} 
y^* &= c_0^* +c_1^* x_m^*\,;\notag%\label{e10-baz}
\\
x_j^* &=a_j^*+b_j^* x_m^*\,,\enskip j=\overline{1,m-1}\,;\notag%\label{e11-baz}
\\
x_m^* &= A_0+\sum\limits^m_{j=1} A_j x_j\,, \label{e12-baz}
\end{align}
где 
\begin{align*}
A_0&= \fr{-\sum\nolimits_{j=1}^{m-1} \lambda_j a_j^* b_j^*}  
{1+\sum\nolimits_{j=1}^{m-1}\lambda_j(b_j^*)^2}\,;\\
A_j&= \fr{\lambda_j b_j^*}{1+\sum\nolimits^{m-1}_{j=1}\lambda_j 
(b_j^*)^2}\,,\enskip j=\overline{1,m-1}\,;\\
A_m&= \fr{1}{1+\sum\nolimits^{m-1}_{j=1}\lambda_j (b_j^*)^2}\,.
\end{align*}

    Взаимосвязи между переменными для многофакторной модели 
полносвязной линейной регрессии представлены на рисунке.
    

     
    Таким образом, в~результате оценивания полносвязной регрессии 
формируются <<истинные>> (скрытые, латентные) переменные $x_1^*, x_2^*, 
\ldots , x_m^*$,\linebreak каждая пара которых связана линейной функциональной 
зависимостью. При этом каждая из латентных переменных $x_1^*, x_2^*, \ldots 
, x_m^*$ одинаково коррелирует с~переменной~$y$, а~также представляет\linebreak 
собой линейную комбинацию наблюдаемых переменных $x_1, x_2, \ldots, 
x_m$, поэтому значения переменных $x_1^*, x_2^*, \ldots , x_m^*$ легко 
идентифицируются.

{ \begin{center}  %fig1
 \vspace*{12pt}
   \mbox{%
 \epsfxsize=79mm 
 \epsfbox{baz-1.eps}
 }

\end{center}

\vspace*{3pt}

\noindent
 {\small Спецификация многофакторной полносвязной регрессии}
 }

%\vspace*{6pt}

    
    Стоит отметить, что на основании предпосылки~(2) использовать 
полносвязные модели стоит в~случае сильной корреляции между переменными 
$x_1, x_2, \ldots , x_m$. В~этом случае, согласно~(1), латентные переменные 
$x_1^*, x_2^*, \ldots , x_m^*$ практически не будут отличаться от переменных 
$x_1, x_2, \ldots , x_m$. Следовательно, оценки полносвязной регрессии $b_1^*, 
b_2^*, \ldots , b^*_{m-1}$ будут близки к~оценкам угловых коэффициентов 
соответствующих моделей парной регрессии, знаки которых всегда совпадают 
со знаками соответствующих коэффициентов корреляции. А~это означает, что 
при сильной корреляции переменных $x_1, x_2, \ldots , x_m$ знаки оценок  
$b_1^*, b_2^*, \ldots , b^*_{m-1}$ также будут совпадать со знаками 
коэффициентов корреляции. То же самое справедливо для оценки~$c_1^*$ при 
сильной корреляции переменной~$y$ с~каждой из переменных $x_1, x_2, \ldots 
, x_m$.

\section{Связь между моделями множественной 
и~полносвязной линейной~регрессии}

    В дальнейшем будем полагать, что параметры $\lambda_1, \lambda_2, \ldots 
, \lambda_{m-1}$~~--- это не соотношения дисперсий ошибок переменных, 
а~просто некоторые заданные числа. В~соответствии с~этим не будем 
ограничивать их условиями положительности.
    
Используя~(\ref{e12-baz}), перепишем вторичную регрессию~(\ref{e8-baz}) 
в~виде:
\begin{equation}
y_i=\theta_0 +\sum\limits_{j=1}^m \theta_j x_{ij} +\varepsilon_i\,,\enskip 
i=\overline{1,n}\,, 
    \label{e13-baz}
    \end{equation}
где $\theta_0=c_0\hm+ A_0c_1$; $\theta_j\hm= A_j c_1$, $j\hm= \overline{1,m}$.
    
    Уравнение~(\ref{e13-baz}) есть классическая модель множественной 
линейной регрессии. На ее основании можно сделать очевидный вывод: для 
любых $\lambda_1, \lambda_2, \ldots , \lambda_{m-1}$ сумма квадратов остатков 
вторичной  
регрессии~(\ref{e8-baz}) не может быть меньше суммы квадратов остатков 
оцененной с~помощью МНК множественной регрессии~$y$ от $x_1, x_2, \ldots 
, x_m$. Возникает вопрос: существуют ли параметры $\lambda_1, \lambda_2, 
\ldots , \lambda_{m-1}$, при которых оценки вторичного 
уравнения~(\ref{e13-baz}) полносвязной регрессии совпадают  
с~МНК-оцен\-ка\-ми модели множественной линейной регрессии. Ответ на 
него дает следующая теорема.
    
    \smallskip
    
    \noindent
    \textbf{Теорема.}\ \textit{Пусть $\alpha_0^*, \alpha_1^*,\ldots , 
\alpha_m^*$~--- 
 МНК-оцен\-ки модели множественной линейной регрессии
 $$
 y_i= 
\alpha_0+ \sum\limits^m_{j=1} \alpha_j x_{ij} + \varepsilon_i,\enskip
i=  \overline{1,n}\,.
$$ 
Тогда всегда существуют единственные значения параметров 
$\lambda_1, \lambda_2, \ldots ,  
\lambda_{m-1}$ полносвязной регрессии} (1), (2) и~(\ref{e8-baz}), \textit{при 
которых оценки вторичного уравнения}~(\ref{e13-baz}) \textit{равны 
соответствующим МНК-оцен\-ка\-ми множественной модели, т.\,е.}\ 
$\theta_j\hm= \alpha_j^*$, $j\hm=\overline{0,m}$.
    
    \smallskip
    
    \noindent
    Д\,о\,к\,а\,з\,а\,т\,е\,л\,ь\,с\,т\,в\,о\,.\ \ Приравняем со\-от\-вет\-ст\-ву\-ющие 
оценки вторичной регрессии~(\ref{e13-baz}) к~оценкам множественной регрессии 
$\theta_j\hm= \alpha_j^*$, $j\hm= \overline{0,m}$. В~результате получим 
систему:
    \begin{equation}
    \left.
    \begin{array}{rl}
   \hspace*{-3mm}c_0+\displaystyle \fr{-\sum\nolimits_{j=1}^{m-1} \lambda_j a_j b_j} 
{1+\sum\nolimits_{j=1}^{m-1} \lambda_j b_j^2}\,c_1&=\alpha_0^*\,;\\[6pt]
    \hspace*{-3mm}c_1\displaystyle \fr{\lambda_p b_p}{1+\sum\nolimits_{j=1}^{m-1} \lambda_j 
b_j^2}&=\alpha_p^*\,,\ p=\overline{1,m-1}\,;\\[6pt]
    \hspace*{-3mm}c_1\displaystyle \fr{1}{1+\sum\nolimits_{j=1}^{m-1} \lambda_j 
b_j^2}&=\alpha_m^*\,.
    \end{array}
\!    \right\}\!
    \label{e14-baz}
    \end{equation}
    
    Поделив все уравнения системы~(\ref{e14-baz}), кроме первого, на 
последнее, получим:
    \begin{equation}
    \lambda_p b_p=q_p\,,\enskip p=\overline{1,m-1}\,,
    \label{e15-baz}
    \end{equation}
где $q_p=\alpha_p^*/\alpha_m^*$, $p\hm= \overline{1,m-1}$.
    
    С учетом~(\ref{e15-baz}), система~(\ref{e7-baz}) примет вид:
    \begin{equation}
    b_p B = \left( 1+\sum\limits_{j=1}^{m-1} q_j b_j\right) A_p\,,\enskip  
p=\overline{1,m-1}\,,
    \label{e16-baz}
    \end{equation}
где 
\begin{align*}
B&=D_{x_m}+\sum\limits_{j=1}^{m-1}q_j^2 D_{x_j}+ {}\\
&+2\sum\limits^{m-2}_{j_1=1} 
\sum\limits^{m-1}_{j_2=j_1+1} q_{j_1} q_{j_2} K_{x_{j_1}x_{j_2}} +  
2\sum\limits^{m-1}_{j=1} q_j K_{x_j x_m}\,;\\
A_p&= \sum\limits_{j=1}^{m-1} q_j K_{x_j x_p}+K_{x_m x_p}\,,\enskip 
p=\overline{1,m-1}\,.
\end{align*} 

Выражение~(\ref{e16-baz}) представляет собой систему линейных 
алгебраических уравнений относительно переменных $b_1, b_2, \ldots , b_{m-1}$:
    \begin{multline}
    \begin{pmatrix}
A_1 q_1-B & A_1q_2&\cdots& A_1q_{m-1}\\
A_2q_1 & A_2q_2-B &\cdots & A_2 q_{m-1}\\
\vdots &\vdots& \vdots& \vdots\\
A_{m-1} q_1 & A_{m-1}q_2&\cdots & A_{m-1} q_{m-1}-B\end{pmatrix}\times{}\\
{}\times \begin{pmatrix}
b_1\\ b_2\\ \vdots \\ b_{m-1}\end{pmatrix}=
-\begin{pmatrix}
A_1 \\ A_2\\ \vdots\\ A_{m-1}
\end{pmatrix}\,.
\label{e17-baz}
\end{multline}
    %
    Определитель~$\Delta$ основной матрицы системы~(\ref{e17-baz}):
    \begin{equation}
    \Delta =(-1)^{m-1} B^{m-1}\left( 1+\sum\limits_{j=1}^{m-1} 
\fr{A_j}{B}\,q_j\right)\,.
    \label{e18-baz}
    \end{equation}
    
Определитель~(\ref{e18-baz}) обращается в~нуль либо при $B^{m-1}\hm=0$, 
либо при
$1\hm- \sum\nolimits_{j=1}^{m-1}(A_j/B)q_j\hm=0$. Нетрудно показать, что  
$B^{m-1}\not= 0$. Так, величина дисперсии связующей переменной~$x_m^*$ 
составляет $B/(1\hm+ \sum\nolimits^{m-1}_{j=1} q_j b_j)^2$. Из этого следует, 
что для любых~$q_j$ справедливо неравенство $B\hm>0$, а~значит, 
и~неравенство $B^{m-1}\hm>0$.
    
    Докажем от противного, что 
    $$
    1- \sum\limits_{j=1}^{m-1}\fr{A_j}{B}\,q_j 
\not= 0\,.
$$

 Пусть
    \begin{equation}
1-\sum\limits_{j=1}^{m-1}\fr{A_j}{B}\,q_j=0\,.
\label{e19-baz}
\end{equation}
    %
    Из системы~(\ref{e16-baz}) следует, что
    \begin{equation}
    \fr{A_p}{B}= \fr{b_p}{1+\sum\nolimits_{j=1}^{m-1} q_j b_j}\,,\enskip p= 
\overline{1,m-1}\,.
    \label{e20-baz}
    \end{equation}
    %
    Подставляя~(\ref{e20-baz}) в~(\ref{e19-baz}), получим равенство 
    $$
    \fr{1}{1+ \sum\nolimits_{j=1}^{m-1} q_j b_j}=0\,,
    $$
     которое никогда не выполняется. 
Следовательно, 
$$
1- \sum\limits_{j=1}^{m-1}\fr{A_j}{B}\,q_j \not= 0\,.
$$
    
    Таким образом, определитель~(\ref{e18-baz}) никогда не обращается 
в~нуль, поэтому система~(\ref{e17-baz}) всегда совместна и~имеет 
единственное решение. Теорема доказана.
    
    \smallskip
    
    Решение линейной системы~(\ref{e17-baz}) дает оценки $b_1^*, b_2^*, 
\ldots ,  
b^*_{m-1}$, с~помощью которых, как следует из~(\ref{e15-baz}), определяются 
параметры $\lambda_1, \lambda_2, \ldots , \lambda_{m-1}$ по формулам 
$\lambda_p\hm= q_p/b_p^*$, $p\hm= \overline{1,m-1}$. Зная эти параметры, 
нетрудно вычислить оставшиеся оценки полносвязной регрессии 
в~соответствии с~описанной выше последовательностью.
    
    Предложенный в~рамках данной работы метод построения полносвязных 
регрессий предназначен для следующего.
    \begin{enumerate}[1.]
    \item Для множества наблюдаемых переменных $x_1, x_2, \ldots , x_m$ 
можно легко получить множество соответствующих латентных переменных 
$x_1^*, x_2^*, \ldots , x_m^*$, квадрат коэффициента корреляции для каждой 
из которых с~переменной~$y$\linebreak
 равен коэффициенту детерминации 
множественной модели~$y$ от $x_1, x_2, \ldots , x_m$. Иными словами, 
$m$~переменных можно эквивалентно заменить одной, т.\,е.\ провести 
снижение раз\-мер\-ности данных.
    \item При сильной корреляции между переменными $x_1, x_2, \ldots , x_m$ 
возникает эффект мультиколлинеарности, который делает невозможным 
интерпретацию коэффициентов множественной регрессии. Однако, как было 
отмечено выше, знаки оценок $b_1^*, b_2^*, \ldots , b^*_{m-1}$, $c_1^*$ 
полносвязной регрессии в~этом случае будут совпадать со знаками 
соответствующих коэффициентов корреляции, поэтому такие оценки можно 
интерпретировать по принципу: если переменная~$x_m^*$ изменится 
на~1~единицу, то переменная~$x_1^*$ изменится на~$b_1^*$, 
переменная~$x_2^*$ на~$b_2^*$ и~т.\,д.
    \end{enumerate}
    
{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
    
\bibitem{1-baz}
\Au{Носков С.\,И.} Технология моделирования объектов с~нестабильным 
функционированием и~неопределенностью в~данных.~--- Иркутск: Облинформпечать, 
1996. 321~с.
\bibitem{2-baz}
\Au{Носков С.\,И., Базилевский М.\,П.} Построение регрессионных моделей 
с~использованием аппарата линейно-бу\-ле\-во\-го программирования.~--- Иркутск: ИрГУПС, 
2018. 176~с.

\bibitem{4-baz} %3
\Au{Кендалл М., Стьюарт~А.} Статистические выводы и~связи~/
Пер. с~англ.~--- М.: Наука, 1973. 
899~с. (\Au{Kendall~M.\,G., Stuart~A.} The
advanced theory of statistics: Inference and relationship.~---
2nd ed.~--- Hafner Publishing Co., 1967. Vol.~2. 690~p.)
\bibitem{5-baz} %4
\Au{Golub G.\,H., Van Loan~C.\,F.} An analysis of the total least squares problem~// SIAM 
J.~Numer. Anal., 1980. Vol.~17. P.~883--893.

\bibitem{3-baz} %5
\Au{Демиденко Е.\,З.} Линейная и~нелинейная регрессия.~--- М.: Финансы и~статистика, 
1981. 304~с.

\bibitem{6-baz}
\Au{Deming W.\,E.} Statistical adjustment of data.~--- Wiley, 1943. 273~p.
\bibitem{7-baz}
\Au{Jensen A.\,L., Kjelgaard-Hansen~M.} Method comparison in the clinical laboratory~// 
Vet. Clin. Path., 2006. Vol.~35. Iss.~3. P.~276--286.

\bibitem{9-baz}
\Au{Dhanoa M.\,S., Sanderson~R., L$\acute{\mbox{o}}$pez~S., France~J.} Bivariate 
relationships incorporating method comparison: A~review of linear regression methods~// CAB 
Reviews, 2016. Vol.~11. Art. No.\,28. 15~p.

\bibitem{8-baz} %9
\Au{Wu C., Yu J.\,Z.} Evaluation of linear regression techniques for atmospheric applications: 
The importance of appropriate weighting~// Atmos. Meas. Tech., 2018. 
Vol.~11. P.~1233--1250.

\bibitem{10-baz}
\Au{Базилевский М.\,П.} Синтез модели парной линейной регрессии и~простейшей 
EIV-мо\-де\-ли~// Моделирование, оптимизация и~информационные технологии, 2019. 
Т.~7. №\,1. С.~170--182.
\bibitem{11-baz}
\Au{Базилевский М.\,П.} Исследование двухфакторной модели полносвязной линейной 
регрессии~// Моделирование, оптимизация и~информационные технологии, 2019. Т.~7. 
№\,2. С.~80--96.
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 07.09.19}}

\vspace*{8pt}

%\pagebreak

%\newpage

%\vspace*{-28pt}

\hrule

\vspace*{2pt}

\hrule

%\vspace*{-2pt}

\def\tit{MULTIFACTOR FULLY CONNECTED LINEAR REGRESSION MODELS 
WITHOUT~CONSTRAINTS TO~THE~RATIOS\\ OF~VARIABLES ERRORS VARIANCES}


\def\titkol{Multifactor fully connected linear regression models without 
constraints to the ratios of variables errors variances}

\def\aut{M.\,P.~Bazilevskiy}

\def\autkol{M.\,P.~Bazilevskiy}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-15pt}


\noindent
Irkutsk State Transport University, 15~Chernyshevskogo Str., Irkutsk 664074, Russian Federation


\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2020\ \ \ volume~14\ \ \ issue\ 2}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2020\ \ \ volume~14\ \ \ issue\ 2
\hfill \textbf{\thepage}}}

\vspace*{2pt} 


\Abste{The article is devoted to the problem of constructing errors-in-variables regression models. 
Currently, such models are not widely used because they are not suitable for forecasting and interpretation, 
they are difficult to estimate, and the variables errors variances are unknown. To eliminate these 
shortcomings, the author developed and investigated two-factor fully connected linear regression models. 
Such models are easily estimated, they can be used for forecasting, and they lack the effect of 
multicollinearity. In this paper, for the first time, multifactor fully connected linear regression models are 
considered. It is proved that in the case of removing the restrictions, on the ratio of variables errors variances, 
there are the one estimates of a~fully connected regression, in which the approximation qualities of its 
secondary equation and the classical multiple linear regression model, estimated using the ordinary least 
squares, coincide.}
\KWE{errors-in-variables models; fully connected regression; Deming regression; ordinary least squares}

\DOI{10.14357/19922264200213} 

%\vspace*{-20pt}

%\Ack
%\noindent
 

%\vspace*{6pt}

 \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}

\bibitem{1-baz-1}
\Aue{Noskov, S.\,I.} 1996. \textit{Tekhnologiya modelirovaniya ob"ektov s~nestabil'nym 
funktsionirovaniem i~neopredelennost'yu v~dannykh} [Technology for modeling objects with unstable 
functioning and data uncertainty]. Irkutsk: Oblinformpechat'. 321~p.
\bibitem{2-baz-1}
\Aue{Noskov, S.\,I., and M.\,P.~Bazilevskiy.} 2018. \textit{Postroenie regressionnykh modeley 
s~ispol'zovaniem apparata lineyno-bulevogo programmirovaniya} [Construction of regression models using 
linear Boolean programming]. Irkutsk: \mbox{IrGUPS}. 176~p.

\bibitem{4-baz-1} %3
\Aue{Kendall, M.\,G., and A.~Stuart.} 1967. \textit{The
advanced theory of statistics: Inference and relationship}.
2nd ed. Hafner Publishing Co. Vol.~2. 690~p.
\bibitem{5-baz-1} %4
\Aue{Golub, G.\,H., and C.\,F.~Van Loan.} 1980. An analysis of the total least squares problem. 
\textit{SIAM J.~Numer. Anal.} 17:883--893.

\bibitem{3-baz-1} %5
\Aue{Demidenko, E.\,Z.} 1981. \textit{Lineynaya i~nelineynaya regressiya} [Linear and nonlinear 
regression]. Moscow: Finansy i~statistika. 304~p.
\bibitem{6-baz-1}
\Aue{Deming, W.\,E.} 1943. \textit{Statistical adjustment of data}. Wiley. 273~p.
\bibitem{7-baz-1}
\Aue{Jensen, A.\,L., and M.~Kjelgaard-Hansen.} 2006. Method comparison in the clinical laboratory. 
\textit{Vet. Clin. Path.} 35(3):276--286.

\bibitem{9-baz-1}
\Aue{Dhanoa, M.\,S., R.Sanderson, S.~L$\acute{\mbox{o}}$pez, and J.~France.} 2016. Bivariate 
relationships incorporating method comparison: A~review of 
linear regression methods. \textit{CAB 
Reviews} 11:28. 15~p.
\bibitem{8-baz-1} %9
\Aue{Wu, C., and J.\,Z. Yu.} 2018. Evaluation of linear regression techniques for atmospheric applications: 
The importance of appropriate weighting. \textit{Atmos. Meas. Tech.} 11:1233--1250.

\bibitem{10-baz-1}
\Aue{Bazilevskiy, M.\,P.} 2019. Sintez modeli parnoy lineynoy regressii i prosteyshey EIV-modeli 
[Synthesis of linear regression model and EIV-model]. \textit{Modelirovanie, 
Optimizatsiya i Informatsionnye Tekhnologii} [Modeling, Optimization and Information Technology] 
7(1):170--182.
\bibitem{11-baz-1}
\Aue{Bazilevskiy, M.\,P.} 2019. Issledovanie dvukhfaktornoy modeli polnosvyaznoy lineynoy regressii 
[Investigation of a~two-factor fully connected linear regression model]. \textit{Modelirovanie, Optimizatsiya 
i~Informatsionnye Tekhnologii} [Modeling, Optimization and Information Technology] 7(2):80--96.

\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Received September 7, 2019}}

%\pagebreak

%\vspace*{-24pt}



\Contrl

\noindent
\textbf{Bazilevskiy Mikhail P.} (b.\ 1987)~--- Candidate of Science (PhD) in technology, associate 
professor, Irkutsk State Transport University, 15~Chernyshevkogo Str., Irkutsk 664074, Russian Federation; 
\mbox{mik2178@yandex.ru}
\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература} 
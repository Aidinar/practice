% \renewcommand{\figurename}{\protect\bf Figure}
%\renewcommand{\tablename}{\protect\bf Table}
\renewcommand{\bibname}{\protect\rmfamily References}

\def\stat{volodin}

\def\tit{COMPLETE CONVERGENCE FOR~ARRAYS OF~NEGATIVELY DEPENDENT RANDOM VARIABLES}

\def\titkol{Complete convergence for arrays of negatively dependent random variables}

\def\autkol{S.\,H.~Sung, K.~Budsaba, and~A.~Volodin}

\def\aut{S.\,H.~Sung$^1$, K.~Budsaba$^{2}$, and~A.~Volodin$^{3}$}

\titel{\tit}{\aut}{\autkol}{\titkol}

%{\renewcommand{\thefootnote}{\fnsymbol{footnote}}\footnotetext[1]
%{Received by the editors November, 2011. 1991 \textit{Mathematics Subject Classification}.
%Primary 62E20; Secondary 60F05.}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Department of Applied Mathematics, Pai Chai University, Taejon, South Korea, 
sungsh@pcu.ac.kr}
\footnotetext[2]{Center of Excellence in Mathematics, CHE, Bangkok, Thailand;
Department of Mathematics and Statistics, Thammasat University Rangsit Center, 
Pathumthani, Thailand, kamon@mathstat.sci.tu.ac.th}
\footnotetext[3]{School of Mathematics and Statistics, University of Western Australia, 
Crawley, Australia; University of Regina, Canada,\linebreak 
Andrei.Volodin@uregina.ca}

\Abste{A general result establishing complete convergence for the row sums 
of an array of rowwise negatively dependent random variables is presented. From this result, 
a number of complete convergence results have been obtained
for weighted sums of negatively dependent 
random variables.}


\KWE{complete convergence; negatively dependent; weighted sums; arrays}

\vskip 14pt plus 9pt minus 6pt

      \thispagestyle{headings}

      \begin{multicols}{2}

            \label{st\stat}


\section{Introduction}

\noindent
The concept of complete convergence of a sequence of random variables was 
introduced by Hsu and Robbins~[1]
as follows. A~sequence $\{U_n, n\ge 1\}$ of random variables  converges completely 
to the constant $\theta$ if
$$ 
\sum\limits_{n=1}^{\infty}{\sf P}(|U_n-\theta |>\varepsilon)< \infty \quad 
\mbox{ for all }\quad \varepsilon> 0\,. 
$$

In view of the Borel--Cantelli lemma, this implies that $U_n \rightarrow \theta$ almost surely. 
The converse is true if $\{U_n, n \geq 1\}$ are independent random variables. 
Hsu and Robbins~[1] and Katz~[2] ($p=1$ and
$1< p <2,$ respectively) proved that if $\{X_n, n\ge 1\}$ is a sequence of independent and identically
distributed random variables with mean zero and ${\sf E}|X_1|^{2p}<\infty,$  
then $\sum\limits_{i=1}^n X_i/n^{1/p}$ converges
completely to zero.

The paper~[1] initiated numerous explorations of 
the complete convergence of sums of
independent random variables. The research was continued by Erd$\ddot{\mbox{o}}$s~[3, 4], 
Spitzer~[5], Baum and
Katz~[6], and Gut~[7]. This subject is actively discussed in scientific press during the last few decades.
For example, Hu {\it et al.}~[8] extended the result of Hsu--Robbins--Katz to the case where $\{X_{ni}, 1\le
i\le n, n\ge 1\}$ is an array of rowwise independent random variables which are stochastically dominated 
by a
random variable $X$ satisfying ${\sf E}|X|^{2p}<\infty$ for some $1\le p<2$.

The papers~[9, 10] contain, up to the authors' knowledge, the most general theorems
that provide sufficient conditions for complete convergence for sums of arrays of rowwise independent random
variables.

In the following, let $\{k_n, n\ge 1\}$ be a sequence of positive integers. In general,
the case $k_n=\infty$ is
not precluded. When $k_n=\infty,$ it will be assumed that $\sum\limits_{i=1}^\infty X_{ni}$ converges almost surely. Recall
that an array $\{X_{ni}, 1\le i\le k_n, n\ge 1\}$ of random variables is said to be {\it stochastically
dominated} by a random variable $X$ if there exists a positive constant $C>0$ such that

\noindent
\begin{multline*}
{\sf P}\{|X_{ni}|>x\}\le C {\sf P}\{|X|>x\}\\ \mbox{ for all } x>0, 1\le i\le k_n, \mbox{ and } n\ge 1\,.
\end{multline*}

Recently, some complete convergence theorems for negatively dependent random variables have been obtained by
many authors (see, for example,~[11, 12] and references in
these papers). Taylor \textit{et al}.~[11] extended the result of Hu 
\textit{et al}.~[8] to the array of rowwise negatively
dependent random variables. Giuliano {\it et al.}~[12] considered so-called acceptable random variable, which
is more general notion than negative dependency.

The finite set of random variables $X_1, \cdots, X_n $ is said to be 
{\it negatively dependent} if

\noindent
\begin{multline*}
{\sf P}\{X_1\le x_1, \dots, X_n\le x_n\}\\
{} \le {\sf P}\{X_1\le x_1\} \cdots {\sf P}\{X_n\le x_n\}\,;
\end{multline*}

\vspace*{-12pt}

\noindent
\begin{multline*}
{\sf P}\{X_1> x_1, \dots, X_n> x_n\}\\ \le {\sf P}\{X_1> x_1\} \cdots
{\sf P}\{X_n> x_n\} 
\end{multline*}
for all real $x_1, \dots, x_n$. An infinite sequence $\{X_n, n\ge 1\}$ is said to be negatively dependent if every
finite subset of the sequence $\{X_1, \dots, X_n\}$ is negatively dependent.

In this paper, a general result establishing complete convergence for the row sums of an array of
rowwise negatively dependent random variables is presented. 
It also specifies the corresponding rate of convergence. From
this result, a number of complete convergence\linebreak\vspace*{-12pt}

\pagebreak

\noindent
 results for negatively dependent random variables
have been obtained. As a
corollary, the result of Taylor {\it et al.}~[11] is obtained.

Throughout this paper, $C$ denotes a positive constant which may be different in various places, and
it is convenient to define $\log x =\max\{1, \ln x\}$.

\section{Preliminary Lemmas}

\noindent
To prove the main result, the following lemmas are necessary. The first two lemmas are well known and can be found,
for example, in~[11].

\medskip
\noindent 
\textbf{Lemma~1.} \textit{Let $\{X_n, n\ge 1\}$ be a sequence of negatively dependent  random variables and
$\{f_n, n\ge 1\}$ be a sequence of Borel functions all of which are monotone increasing (or monotone
decreasing), then $\{f_n(X_n), n\ge 1\}$ is a sequence of negatively dependent random variables.}

The second lemma mainly states that negatively dependent random variables are 
negatively correlated.

\medskip

\noindent 
\textbf{Lemma 2.} {\it Let $X_1,\dots, X_n$ be nonnegative negatively dependent integrable random variables. Then}
$$ 
{\sf E} \prod\limits_{i=1}^n X_i \le \prod\limits_{i=1}^n {\sf E}X_i\,. 
$$

\medskip

The following lemma plays an essential role in the main result. Of course, this lemma is of interest only if
positive constants $d_i$, and, hence, second moments ${\sf E}X_i^2, 1\le i \le n$, are close to zero (at least less than
one). Otherwise, there is an alternative so-called subgaussian estimations 
(see, for example,~[12]).

\medskip

\noindent 
\textbf{Lemma~3.} \textit{Let $X_1,\dots, X_n$ be negatively dependent  mean zero random variables such
that}
$$
\left|X_i\right|\le d_i\,, \quad 1\le i\le n\,,
$$
for a sequence of positive constants $d_1,\cdots, d_n$. Then, for any $t>0,$
$$ 
{\sf E} \exp\left\{t\sum\limits_{i=1}^n X_i\right\} \le \exp\left\{\fr{t^2}{2} 
\sum\limits_{i=1}^n e^{td_i} {\sf E}X_i^2 \right\}\,.
$$

\noindent 

P\,r\,o\,o\,f\,.\ From the inequality 
$e^x\le 1+x+({x^2}/{2}) e^{|x|}$, which is true for all $x$, one
has
\begin{multline*}
{\sf E} e^{tX_i} \le 1+ t{\sf E}X_i + \fr{t^2}{2} {\sf E} \left(X_i^2 e^{t|X_i|}\right) \\
{}= 1+ \fr{t^2}{2}\, {\sf E}\left( X_i^2 e^{t|X_i|}\right) \mbox{ (since $X_i$ have mean zero)} \\
{}\le 1+ \fr{t^2}{2}\,e^{td_i}  {\sf E}X_i^2 \le \exp\left\{ \fr{t^2}{2}\,e^{td_i} {\sf E}X_i^2\right\}\,,
\end{multline*}
since $1+x \le e^x$ for all $x$. It follows from Lemmas~1 and~2 that

\columnbreak

\noindent
\begin{multline*}
{\sf E} \exp\left\{t\sum\limits_{i=1}^n X_i\right\} \le \prod\limits_{i=1}^n {\sf E} e^{tX_i} \\
\hspace*{-3pt}{}\le \prod\limits_{i=1}^n \exp\left\{\!\fr{t^2}{2}
\,e^{td_i} {\sf E}X_i^2 \!\right\}
=\exp\left\{\!\fr{t^2}{2} \sum\limits_{i=1}^n e^{td_i} {\sf E}X_i^2\!\right\}.~\Box
\end{multline*}

\section{Main Result}

\noindent
With the preliminary lemmas, the  main result may now be stated and proved.

\medskip

\noindent 
\textbf{Theorem.} \textit{Let $\{X_{ni}, 1\le i \le k_n, n\ge 1\}$ be an array of rowwise negatively
dependent random variables, $\{a_n, n\ge 1\}$ be a sequence of positive constants, and $\{b_n, n\ge 1\}$ be a
sequence of positive constants such that $\lim\limits_{n\to\infty} b_n =\infty$. Suppose that}
\begin{enumerate}[($i$)]
\item $ \sum\limits_{n=1}^\infty a_n \sum\limits_{i=1}^{k_n} 
{\sf P}\{|X_{ni}|>\varepsilon\}<\infty$\ \textit{\ for all}\
$\varepsilon>0;$
\item $\sum\limits_{n=1}^\infty a_n \left(\sum\limits_{i=1}^{k_n}{\sf P}
\{ |X_{ni}|>1/b_n\} \right)^{N_1}\!<\infty$\
\textit{\ for some}\ $N_1>0;$
\item $b_n \sum\limits_{i=1}^{k_n} {\sf E} X_{ni}^2 I\{|X_{ni}|\le 1/b_n\} \to 0$ 
\textit{as}\ $n\to \infty;$ \textit{and}\
\item $\sum\limits_{n=1}^\infty a_n \exp\{-N_2 b_n\}<\infty$\ \textit{\ for some} $N_2>0.$
\end{enumerate}


\noindent
\textit{Then} 
\begin{multline*}
\sum\limits_{n=1}^\infty a_n {\sf P}\left\{\left| 
\sum\limits_{i=1}^{k_n} X_{ni}- {\sf E}X_{ni}I\left\{|X_{ni}|\le \fr{1}{b_n}
\right\}\right|> \varepsilon\right\}\\
{} <\infty
\end{multline*}
\textit{for all} $\varepsilon>0.$

\medskip 

\noindent 
P\,r\,o\,o\,f\,.\ The set of all natural numbers is partitioned into two subsets:
$$ 
A'=\left\{n : \sum\limits_{i=1}^{k_n} {\sf P}\left\{|X_{ni}|>\fr{1}{b_n}\right\}\le 1\right\} \,;
$$
$$
A''=\left\{n : \sum\limits_{i=1}^{k_n} {\sf P} \left\{|X_{ni}|>\fr{1}{b_n}\right\}> 1\right\}\,. 
$$
Applying~($ii$), one obtains
\begin{multline*}
\sum\limits_{n\in A''} \!a_n {\sf P}\left\{\left| \sum\limits_{i=1}^{k_n} \!X_{ni}- 
{\sf E}X_{ni}I\left\{|X_{ni}|\le \fr{1}{b_n}
\right\}\right|> \varepsilon\right\}
\\
\hspace*{-0.55399pt}{}\le \!\sum\limits_{n\in A''} \!a_n\le \!\sum\limits_{n\in A''}\! a_n \left( 
\sum\limits_{i=1}^{k_n} {\sf P}\left\{|X_{ni}|>\fr{1}{b_n}\right\} \right)^{N_1}\!\!\!\!<\infty. 
\end{multline*}
Hence, it is enough to show that
\begin{multline*}
\sum\limits_{n \in A'} a_n {\sf P}\left\{\left|
\sum\limits_{i=1}^{k_n} X_{ni}- {\sf E}X_{ni}I\left\{|X_{ni}|\le \fr{1}{b_n}\right\}\right|>\varepsilon
\right\}\\
<\infty \mbox{ for all } \varepsilon>0\,. 
\end{multline*}
For $1\le i\le k_n$ and $n\ge 1$, define
\begin{align*}
Y_{ni}&= X_{ni}I\left\{|X_{ni}|
\le \fr{1}{b_n}\right\}+ \fr{1}{b_n}\,I\left\{X_{ni}> \fr{1}{b_n}\right\}\\
&\hspace*{38mm}-\fr{1}{b_n}\,I\left\{X_{ni}< -\fr{1}{b_n}\right\}\,;\\
U_{ni}&= \fr{1}{b_n}\left(I\left\{X_{ni}<-\fr{1}{b_n}\right\}-{\sf P}
\left\{X_{ni}<-\fr{1}{b_n}\right\}\right)\,;\\
V_{ni}&= -\fr{1}{b_n}\left(I\left\{X_{ni}>\fr{1}{b_n}\right\}-{\sf P}\left\{X_{ni}> \fr{1}{b_n}\right\}\right)\,;\\
Z_{ni}&= X_{ni} I\left\{\fr{1}{b_n} <|X_{ni}|\le \fr{\varepsilon}{4[N_1+1]}\right\}\,.
\end{align*}
Then, $\{Y_{ni}-{\sf E}Y_{ni}$, $1\le i\le k_n$, $n\ge 1 \}$, $\{U_{ni}$, 
$1\le i\le k_n$, $n\ge 1 \}$, and $\{V_{ni}$, $1\le i\le
k_n$, $n\ge 1 \}$ are the arrays of rowwise negatively dependent  random variables by Lemma~1.

Note that if one defines
$$ 
W_{ni}= \fr{1}{b_n}\left(I\left\{|X_{ni}|>\fr{1}{b_n}\right\}-{\sf P}
\left\{|X_{ni}|>\fr{1}{b_n}\right\}\right)\,,
$$
then it cannot be stated that $\{W_{ni}$, $1\le i\le k_n$, $n\ge 1 \}$ is an array of negatively dependent random
variables. This is a sort of the main disadvantage when one is 
dealing with negatively dependent random variables.

Since $\lim\limits_{n\to\infty}b_n=\infty$, there exists a positive integer $M$ such that
$$
\fr{\varepsilon}{4[N_1+1]}>\fr{1}{b_n}
$$
for all $n>M$. For $n>M$,  one can write that
\begin{multline*} 
\sum\limits_{i=1}^{k_n} X_{ni}- {\sf E}X_{ni}I\left\{|X_{ni}|\le \fr{1}{b_n}\right\}\\
{} = \sum\limits_{i=1}^{k_n}(Y_{ni}-{\sf E}Y_{ni})+ \sum\limits_{i=1}^{k_n} U_{ni} + 
\sum\limits_{i=1}^{k_n} V_{ni}+ \sum\limits_{i=1}^{k_n} Z_{ni}\\
{}+\sum\limits_{i=1}^{k_n} X_{ni} I\left\{|X_{ni}|>\fr{\varepsilon}{4[N_1+1]}\right\}\,. 
\end{multline*}
It follows that

\noindent
\begin{multline*}
\sum\limits_{\substack{{n>M,}\\ {n \in A'}}}  a_n {\sf P}\left\{
\sum\limits_{i=1}^{k_n} X_{ni}- {\sf E}X_{ni}I\left\{|X_{ni}|\le \fr{1}{b_n}\right\}>\varepsilon\right\}\\
{}\le \sum\limits_{n>M, \ n \in A'} a_n {\sf P}\left\{\sum\limits_{i=1}^{k_n} 
Y_{ni}- {\sf E}Y_{ni}>\fr{\varepsilon}{4} \right\}\\
{}+ \sum\limits_{n>M, \ n \in A'} a_n {\sf P}\left\{\sum\limits_{i=1}^{k_n} U_{ni}>
\fr{\varepsilon}{4}\right\}\\
{}+ \sum\limits_{n>M, \ n \in A'} a_n {\sf P}\left\{\sum\limits_{i=1}^{k_n} V_{ni}>
\fr{\varepsilon}{4 }\right\} \\
{}+ \sum\limits_{n>M, \ n \in A'} a_n {\sf P}\left\{\sum\limits_{i=1}^{k_n} Z_{ni}>
\fr{\varepsilon}{4}\right\}\\
{}+ \sum\limits_{\substack{{n>M,}\\ {n \in A'}}} a_n 
{\sf P}\left\{ |X_{ni}|>\fr{\varepsilon}{4[N_1+1]} 
\mbox{ for some }1\le i\le k_n \right\}\\
{}=:I_1+I_2+I_3+I_4+I_5\,.
\end{multline*}

Now, let estimate each sum separately.

For $I_1,$ note that $|Y_{ni}|\le 1/b_n$ and
$$
Y_{ni}^2=X_{ni}^2I\left\{|X_{ni}|\le \fr{1}{b_n} \right\}+\left(\fr{1}{b_n}\right)^2 
I\left\{|X_{ni}|>\fr{1}{b_n}\right\}\,.
$$
Moreover, one has that
$$ 
\fr{1}{b_n} \sum\limits_{i=1}^{k_n} {\sf P}\left\{|X_{ni}|>\fr{1}{b_n}\right \}=o(1) \mbox{ for } n\in A'\,. 
$$
By Lemma~3 with $t=4(N_2+1)b_n/\varepsilon$,  one obtains that for $n\in A'$,
\begin{multline*}
{\sf P}\left\{\sum\limits_{i=1}^{k_n} (Y_{ni}- {\sf E}Y_{ni})>\fr{\varepsilon}{4}\right\}\\
{}\le \exp\left\{-\fr{t\varepsilon}{4}\right\} {\sf E} \exp\left\{t\sum\limits_{i=1}^{k_n} Y_{ni}-{\sf E}Y_{ni}\right\} \\
{}\le \exp\left\{-\fr{t\varepsilon}{4}\right\} \exp\left\{\fr{t^2}{2}\,e^{2t/b_n}
\sum\limits_{i=1}^{k_n} {\sf E}(Y_{ni}-{\sf E}Y_{ni})^2 \right\}\\
{}\le \exp\left\{\!-\fr{t\varepsilon}{4}\!\right\} 
\exp\left\{\fr{t^2}{2}\, e^{2t/b_n}\sum\limits_{i=1}^{k_n} {\sf E}Y_{ni}^2 \right\}
=  \exp\left\{\!
-\fr{t\varepsilon}{4}\!\right\} \\
{}\times \exp\left\{\fr{t^2}{2}\, e^{2t/b_n}\sum\limits_{i=1}^{k_n}
{\sf E}X_{ni}^2I\left\{|X_{ni}|\le \fr{1}{b_n} \right\}\right.\\
\left.{} + \fr{1}{b_n^2} \,{\sf P}\left\{
\left|X_{ni}\right|>\fr{1}{b_n}\right\} \right\}
\le 
\exp\left\{-\vphantom{8\left(N_2 +1\right)^2 e^{8(N_2+1)/\varepsilon}}
\left(N_2+1\right)b_n\right.\\
\left.{}+ {8\left(N_2 +1\right)^2 e^{8(N_2+1)/\varepsilon}}{\varepsilon^{-2}}
o(1)b_n\right\} \mbox{ (by~($iii$))}\\
{}=\exp\left\{-\left(N_2+1 - o(1)\right)b_n \right\} 
\le \exp\{-N_2 b_n\}
\end{multline*}
for all large $n$. Thus, $I_1<\infty$ by~($iv$).

For $I_2,$ it can be observed that $|U_{ni}|\le 1/b_n$ and  ${\sf E}U_{ni}^2\le 
{\sf P}(|X_{ni}|>1/b_n)/b^2_n$. Hence,
\begin{multline*}
\sum\limits_{i=1}^{k_n} {\sf E} U_{ni}^2\le \fr{1}{b^2_n} 
\sum\limits_{i=1}^{k_n} {\sf P}\left\{|X_{ni}|>\fr{1}{b_n}\right\}=\fr{1}{b_n}\, o(1)\\
 \mbox{for } n\in  A'\,. 
\end{multline*}

By Lemma~3 with $t=4(N_2+1)b_n/\varepsilon$,  one obtains that for $n\in A',$
\begin{multline*}
{\sf P}\left\{\sum\limits_{i=1}^{k_n} U_{ni}>\fr{\varepsilon}{4}\right\} 
\le \exp\left\{-\fr{t\varepsilon}{4}\right\}
{\sf E}\exp\left\{t\sum\limits_{i=1}^{k_n}U_{ni}\right\}\\
{}\le\exp\left(-\fr{t\varepsilon}{4}\right) \exp\left\{\fr{t^2}{2}\,e^{t/b_n} 
\sum\limits_{i=1}^{k_n}\sf{E} U_{ni}^2 \right\} \\
\le\exp\left\{-
\vphantom{(N_2 +1)^2 e^{4(N_2+1)/\varepsilon}\varepsilon^{-2}}
(N_2+1)b_n\right.\\
\hspace*{-2.61754pt}\left.{} + 8(N_2 +1)^2 e^{4(N_2+1)/\varepsilon}\varepsilon^{-2}o(1) b_n \right\} \le \exp\{-N_2 b_n\}
\end{multline*}
for all large $n$. Thus, $I_2<\infty$ by~($iv$).

Similarly to $I_2,$ one gets $I_3<\infty.$

For $I_4,$ note that
\begin{equation*}
{\sf P}\left\{\sum\limits_{i=1}^{k_n} Z_{ni}>\fr{\varepsilon}{4}\right\} \le 
{\sf P}\{ \mbox{ at least } 
[N_1 +1] \mbox{ of } Z_{ni}\not= 0\} 
\end{equation*}
because
\begin{multline*}
  Z_{ni}<\fr{\varepsilon}{4[N_1 + 1]}\\
= {\sf P}\left\{\vphantom{\fr{1}{b_n}}\mbox{at least } 
[N_1 +1] \mbox{ of } X_{ni} \mbox{ have the property}\right.\\
\left.\fr{1}{b_n}<|X_{ni}|
\le \fr{\varepsilon}{(4[N_1+1])} \right\} \\
{}\le \!\!\sum\limits_{j_1<\cdots <j_{[N_1+1]}}\!\!\!\! {\sf P}\left\{X_{n,j_1}>\fr{1}{b_n} , \dots, 
X_{n,j_{[N_1+1]}}>\fr{1}{b_n}\right\} \\
\mbox{(where the summation is taken for all }\ [N_1+1]\\
{}- \mbox{tuple } (j_1, \cdots, j_{[N_1+1]}) \\
\mbox{ with } j_1<\cdots <j_{[N_1+1]} \mbox{ and } j_i=1,\dots,k_n \mbox{ for each } i)\\
{}\le\!\! \!\!\!\sum\limits_{j_1<\cdots <j_{[N_1+1]}}\!\!\!\!\!\!\!\! {\sf P}\left\{X_{n,j_1}>\fr{1}{b_n}\right\} 
\cdots {\sf P}\left\{X_{n,j_{[N_1+1]}}>\fr{1}{b_n} \right\}\\
\mbox{(by negative dependence)}\\
{}= \sum\limits_{j_1<\cdots <j_{[N_1+1]}} \prod\limits_{k=1}^{[N_1+1]}  {\sf P}\left\{X_{n,j_k}>\fr{1}{b_n}\right\} \\
{}\le \sum\limits_{j_1, \cdots, j_{[N_1+1]}} \prod\limits_{i=1}^{[N_1+1]}{\sf  P}\left\{X_{n,j_i}>\fr{1}{b_n}\right\} \\
\end{multline*}

\noindent
\begin{multline*}
\mbox{(where the summation is taken for all possible}\\
[N_1+1] -
\mbox{tuple } (j_1, \dots, j_{[N_1+1]})
\\
\mbox{ and}\ j_i=1,\dots,k_n \mbox{ for each } i)\\
{}=\left(\sum\limits_{i=1}^{k_n} {\sf P}\left\{|X_{ni}|>\fr{1}{b_n}\right\} \right)^{[N_1+1]}\,.
\end{multline*}
Thus, $I_4<\infty$ by~($ii$).

Obviously, $I_5<\infty$ by~($i$).

Therefore, one has that
\begin{multline*}
\hspace*{-6.64308pt}\sum\limits_{\substack{{n>M,}\\ {n \in A'}}}\!\!\!  a_n {\sf P}\left(\sum_{i=1}^{k_n} \left(X_{ni}- 
{\sf E}X_{ni}I\left(|X_{ni}|\le \fr{1}{b_n}\right)\right)\right.
\left.{}>\varepsilon\vphantom{\sum_{i=1}^{k_n}}\right)\\ <\infty\,. 
\end{multline*}
Since $\{-X_{ni}\}$ is also an array of rowwise  negatively dependent  random variables, one can replace $X_{ni}$
by $-X_{ni}$ in the above statement. That is,
\begin{multline*}
\hspace*{-3.68776pt}\sum\limits_{\substack{{n>M,}\\ {n \in A'}}} \!\!\! a_n {\sf P}\!
\left(\sum\limits_{i=1}^{k_n}\! \left(X_{ni}- {\sf E}X_{ni}I\!\left(\left|X_{ni}\right|
\le \fr{1}{b_n}\right)\!\right)
<-\varepsilon
\vphantom{\sum\limits_{i=1}^{k_n}}\right)\\
~<\infty\,.~~\square 
\end{multline*}


\noindent 
{\bf Remark 1.} In view of assumption~($iii$), it is interesting to consider sequences $\{b_n, n\ge 1\}$
that increase to infinity as slow as possible for~($iv$) still be true. If the sequence $\{a_n, n\ge 1\}$ has a
polynomial growth or a constant (that is, $a_n=n^t$, $t\ge 0$), then the good choice is $b_n=\log n$, $n\ge 1$,
which has been explored in~[10] for the case of rowwise independent arrays. 
But the present
theorem can be
applied for sequences $\{a_n$, $n\ge 1\}$ with a different than polynomial behavior. 
The main idea is that it is possible to
link sequences $\{a_n, n\ge 1\}$ and $\{b_n, n\ge 1\}$ according to assumption~($iv$).

\section{Corollaries}

\noindent
The theorem presented and proved in the previous section can be applied in different situations for various choices
of weights and moment conditions.

\smallskip

\noindent 
\textbf{Corollary 1.} \textit{Let $\{X_{ni}, 1\le i \le n, n\ge 1\}$ be an array of rowwise negatively
dependent mean zero random variables which are stochastically dominated by a random variable~$X$ with
${\sf E}|X|^{2p}<\infty$ for some $p\ge 1$. Let $\{a_{ni}$, $1\le i\le n$, $n\ge 1\}$ be an array of real numbers and
$\{b_n$, $n\ge 1\}$ be a sequence of positive constants such that}
\begin{itemize}
\item[($a$)] $\lim\limits_{n\to\infty} b_n=\infty$;
\item[($b$)] $b_n= O(n^q)$\ for some $0<q <1/(2p)$;
\item[($c$)] $\sum\limits_{n=1}^\infty \exp\{-N_2 b_n\}<\infty$\ for some $N_2>0;$
\item[($d$)] $b_n \sum\limits_{i=1}^n a_{ni}^2=o(1)$ as $n\to\infty$; and 
\item[($e$)] $\max\limits_{1\le i\le n} |a_{ni}|=O(1/n^{1/p})$.
\end{itemize}
\textit{Then, $ \sum\limits_{i=1}^n a_{ni}X_{ni} \to 0$ completely.}

\medskip

\noindent 
P\,r\,o\,o\,f\,. 
Without loss of generality, one may assume that $a_{ni}\ge 0$  for $1\le i\le n$ and $n\ge
1$. Otherwise, let prove the result separately for two arrays of constants $\{a^+_{ni}$, $1\le i\le n$, 
$n\ge 1\}$ and
$\{a^-_{ni}$, $1\le i\le n$, $n\ge 1\}$, where  the notations $a^+=\max\{a, 0\}$ and $a^-=\max\{-a, 0\}$
are used. Then,
$\{a_{ni} X_{ni}$, $1\le i\le n$, $n\ge 1\}$ is an array of rowwise negatively 
dependent random variables by 
Lemma~1. It can  be also assumed that $\max\limits_{1\le i\le n} a_{ni}\le 1/n^{1/p}$.

Let apply the theorem with $a_n=1$, $n\ge 1$, and $X_{ni}$ replaced by $a_{ni}X_{ni}$, 
$1\le i\le n$, $n\ge 1$.

In order to check condition~($i$) of the theorem, note that by the stochastic domination hypothesis,
\begin{multline*}
\hspace*{-6.44308pt}\sum\limits_{n=1}^\infty  \sum\limits_{i=1}^n {\sf P}
\left\{\left\vert a_{ni}X_{ni}\right\vert>\varepsilon\right\}\le\sum\limits_{n=1}^\infty
\sum\limits_{i=1}^n
{\sf P}\{|X_{ni}|>\varepsilon n^{1/p}\} \\
{}\le C \sum\limits_{n=1}^\infty  n {\sf P}\{|X|>\varepsilon n^{1/p}\}\,.
\end{multline*}
The sum $\sum\limits_{n=1}^\infty  n {\sf P}\{|X|^p>n\}<\infty$ if and only if 
${\sf E}|X|^{2p}<\infty$.
Thus, condition~($i$) of the theorem holds.

For condition~($ii$), taking $N_1>1/(1-2pq)$, one has by Markov's inequality and the stochastic domination
hypothesis that
\begin{multline*}
\sum\limits_{n=1}^\infty \left(\sum\limits_{i=1}^n {\sf P}\left\{|a_{ni}X_{ni}|>\fr{1}{b_n}\right\} \right)^{N_1}\\
{}\le \sum\limits_{n=1}^\infty \left( b_n^{2p} \sum\limits_{i=1}^n |a_{ni}|^{2p} 
{\sf E}|X_{ni}|^{2p} \right)^{N_1} \\
{}\le \sum\limits_{n=1}^\infty \left(C {\sf E}|X|^{2p} \fr{b_n^{2p}}{n} \right)^{N_1} 
\Bigg( \mbox{by assumption~($e$))} \\
{}< \infty \left(
\vphantom{\fr{1}{pq}}\mbox{by assumption~($b$) and the fact}\right.\\
\left.\mbox{that } N_1>\fr{1}{1-2pq}\right)\,.
\end{multline*}
Thus, condition~($ii$) holds.

For condition~($iii$),
\begin{multline*}
 b_n \sum\limits_{i=1}^n {\sf E}(a_{ni}X_{ni})^2I\left(\left|a_{ni}X_{ni}\right|\le \fr{1}{b_n}\right)\\
{}  \le b_n\sum\limits_{i=1}^n a_{ni}^2 {\sf E} X_{ni}^2 
\le C {\sf E} X^2 b_n \sum\limits_{i=1}^n a_{ni}^2\to 0 \mbox{ (by ($d$))}
\end{multline*}
Thus, condition~($iii$) holds.

Condition~($iv$) holds by the assumption~($c$).

By the theorem, one obtains that
\begin{multline*}
\hspace*{-1.55515pt}\sum\limits_{n=1}^\infty {\sf P}\left\{
\left\vert \sum\limits_{i=1}^n a_{ni}\left(X_{ni}- 
{\sf E}X_{ni}I\left\{\left\vert a_{ni}X_{ni}\right\vert \le \fr{1}{b_n}\right\}\right)
\right\vert\right.\\ 
\left.{}>\varepsilon 
\vphantom{\sum\limits_{i=1}^n}\right\}
<\infty
\end{multline*}
for all $\varepsilon>0$.
It remains to show that
$$ 
\sum\limits_{i=1}^n a_{ni}{\sf E}X_{ni}I\left\{|a_{ni}X_{ni}|\le \fr{1}{b_n}\right\}\to 0\,. 
$$
Since ${\sf E}X_{ni}=0$,
$$
{\sf E}X_{ni}I\left\{\!|a_{ni}X_{ni}|\le \fr{1}{b_n}\!\right\}=
-{\sf E}X_{ni}I\left\{\!|a_{ni}X_{ni}|>\fr{1}{b_n}\!\right \}.
$$
It follows that
\begin{multline*}
\left\vert\sum\limits_{i=1}^n a_{ni}{\sf E}X_{ni}I
\left\{\left\vert a_{ni}X_{ni}\right\vert \le \fr{1}{b_n}\right\}\right\vert\\
{}\le\sum\limits_{i=1}^n \left\vert a_{ni}\right\vert {\sf E} \left\vert X_{ni}\right\vert
I\left\{\left\vert a_{ni}X_{ni}\right\vert > \fr{1}{b_n}\right\}\\
{}\le \fr{1}{n^{1/p}} \sum\limits_{i=1}^n {\sf E}\left\vert X_{ni}\right\vert
I\left\{\left\vert X_{ni}\right\vert>\fr{n^{1/p}}{b_n}\right\}\\
\mbox{ (by assumption ($e$))} \\
{}\le C n^{1 - \ 1/p} {\sf E}|X|I\left\{|X|>\fr{n^{1/p}}{b_n}\right\}\\
{}\le C n^{1 - \ 1/p} {\sf E}|X|^{2p} |X|^{1-2p} I\left\{|X|>\fr{n^{1/p}}{b_n}\right\} \\
{}\le C {\sf E}|X|^{2p} n^{1 - \ 1/p}\left(\fr{b_n}{n^{1/p}}\right)^{2p-1} \le C n^{-1/(2p)}\to 0
\end{multline*}
since $b_n<Cn^{1/(2p)}$ for $n$ large enough. Thus, the proof is completed.\hfill $\Box$

\medskip
As a special case of Corollary~1, one gets the following corollary which was proved by 
Taylor \textit{et al}.~[11].

\medskip

\noindent 
\textbf{Corollary 2.} \textit{Let $\{X_{ni}, 1\le i \le n, n\ge 1\}$ be an array of rowwise negatively dependent
mean zero random variables which are stochastically dominated by a random variable~$X$ with 
${\sf E}|X|^{2p}<\infty$
for some $1\le p<2.$ Then,
$\sum\limits_{i=1}^n {X_{ni}}/{n^{1/p}} \to 0$ completely.} 


\noindent 
P\,r\,o\,o\,f\,.\ Let $a_{ni}=1/n^{1/p}$ for $1\le i\le n$ and $n\ge 1.$ 
Then, conditions of Corollary~1 are
trivially satisfied with $b_n= n^q$ for some $0<q <\min\left\{{1}/({2p}), {2}/{p}-1\right\}.\hfill~\Box$

\medskip

\noindent 
\textbf{Corollary 3.} \textit{Let $t>-1, p>0$, and $\beta \in {\mathbb R}$. Denote $\Delta=p(t+\beta+1)$ and
assume that $\Delta\ge 1$. Let $\{X_{ni}, i\ge 1, n\ge 1\}$ be an array of rowwise negatively dependent  mean
zero random variables which are stochastically dominated by a random variable~$X$ with 
${\sf E}|X|^{\Delta}<\infty$.
Let $\{a_{ni}, i\ge 1, n\ge 1\}$ be a bounded array of real numbers such that}
\begin{enumerate}[(1)]
\item  $ \sum\limits_{i=1}^\infty |a_{ni}|^q =O(n^\beta)$\ \textit{for some} $q< \Delta$; \textit{and}
\item  If $\Delta \ge 2$, \textit{then} $\sum\limits_{i=1}^\infty a_{ni}^2=O(n^\gamma )$
\textit{for some} $\gamma<2/p$.
\end{enumerate}
\textit{Then,}
$$ 
\sum\limits_{n=1}^\infty n^t {\sf P}\left\{ 
\fr{\left|\sum\nolimits_{i=1}^\infty a_{ni}X_{ni}\right|}{n^{1/p}}>\varepsilon \right\}
<\infty \mbox{ for all } \varepsilon>0\,.
$$

\noindent 
P\,r\,o\,o\,f\,.\ The same as in the proof of Corollary~1, without loss of generality, 
one may assume that
$a_{ni}\ge 0$ for $i\ge 1, n\ge 1.$ Then, $\{a_{ni}X_{ni}/n^{1/p}$, $i\ge 1$, $n\ge 1\}$ 
is an array of rowwise
negatively dependent random variables by Lemma~1. 
Let apply the theorem with $a_n=n^t$, $n\ge 1$, and $X_{ni}$
replaced by $a_{ni}X_{ni}/n^{1/p}$, $i\ge 1$, $n\ge 1$.

Consider the sequence $b_n=n^{\alpha}, n\ge 1$, 
where $0<\alpha <(t+1)/\Delta$. For the case $\Delta\ge 2$,
let require additionally that $0<\alpha< 2/p - \gamma$.

The fact that
\begin{multline*}
\sum\limits_{n=1}^\infty n^t \sum\limits_{i=1}^\infty 
{\sf P}\left( \left\vert a_{ni}n^{-1/p} X_{ni}\right\vert>
\varepsilon\right)\\
\le C{\sf E}|X|^{p(t+\beta+1)}
< \infty 
\end{multline*}
was established in many papers (see, for example,~[13]) 
(beginning of the proof of Theorem~3.1),
[14] (beginning of the proof of Theorem~3.1), and~[10] 
(beginning of the proof of Theorem~2 and Lemma~3). 
Note also  that the proof presented in~[13] is rather complicated
once it uses the Stieltjes integration technique, summation by parts lemma, and so on. 
The proof presented in~[14] is much more elegant. Also, Hu {\it et al.}~[13] and 
Ahmed {\it et al.}~[14] 
are dealing with an array of constants $\{ a_{ni}X_{ni}$, $i\ge 1$, $n\ge 1\}$ rather 
than the array $\{a_{ni}X_{ni}/n^{1/p}$, $i\ge 1$, $n\ge 1\}$ which is considered in~[10] 
and this paper.

According to the inequality presented above, condition~($i$) of the theorem holds.

For~($ii$), taking $N_1>(t+1)/(t+1 -\alpha\Delta)>0$, one has by Markov's inequality, 
$|a_{ni}|=O(1),$ and~(1)
that
\begin{multline*}
\sum\limits_{n=1}^\infty n^t \left(
\sum\limits_{i=1}^\infty {\sf P}\left\{\left|a_{ni}n^{-1/p}X_{ni}\right|>\fr{1}{b_n}\right\} \right)^{N_1}
\end{multline*}

\noindent
\begin{multline*}
{}\le \sum\limits_{n=1}^\infty n^t \left( b_n^{\Delta} n^{-(t+\beta+1)} 
\sum\limits_{i=1}^\infty \left|a_{ni}\right|^{\Delta}
{\sf E}|X_{ni}|^{\Delta} \right)^{N_1}\\
{}\le C\sum\limits_{n=1}^\infty n^t \left( b_n^{\Delta} n^{-(t+\beta + 1)} 
\sum\limits_{i=1}^\infty \left|a_{ni}\right|^{q} \left|a_{ni}\right|^{\Delta-q} \right)^{N_1} \\
{}\le C \sum\limits_{n=1}^\infty n^{t + \alpha \Delta N_1 -(t+1)N_1} <\infty\,,
\end{multline*}
since  $t +\alpha \Delta N_1 -(t+1)N_1< -1$. Thus, condition~($ii$) of the theorem holds.

For condition~($iii$), let consider two cases. If $1\le \Delta<2$, by~(1), one obtains

\noindent
\begin{multline*}
b_n \sum\limits_{i=1}^\infty {\sf E}\left(a_{ni}n^{-1/p} X_{ni}\right)^2 
I\left\{\left|a_{ni}n^{-1/p} X_{ni}\right|\le \fr{1}{b_n}\right\}\\
{}=b_n \sum\limits_{i=1}^\infty {\sf E}\left|a_{ni}n^{-1/p} X_{ni}\right|^{\Delta} 
\left|a_{ni}n^{-1/p} X_{ni}\right|^{2-\Delta}\\
{}\times I\left\{\left|a_{ni}n^{-1/p} X_{ni}\right|\le \fr{1}{b_n}\right\}\\
\hspace*{-0.36795pt}{}\le b_n^{\Delta-1}\!\sum\limits_{i=1}^\infty {\sf E}\left|a_{ni}n^{-1/p} X_{ni}\right|^{\Delta} 
\!I\!\left\{\!\left|a_{ni}n^{-1/p}X_{ni}\right|\le \fr{1}{b_n}\!\right\} \\
{}\le b_n^{\Delta-1}\sum\limits_{i=1}^\infty {\sf E}\left|a_{ni}n^{-1/p} X_{ni}\right|^{\Delta} \\
{}\le C b_n^{\Delta-1}{\sf E}|X|^{\Delta} \sum\limits_{i=1}^\infty \left|a_{ni}n^{-1/p}\right|^{\Delta} \\
{}\le C n^{\alpha\Delta -\alpha -t -1} < Cn^{-\alpha} \to 0 \mbox{ as } n\to \infty
\end{multline*}
by the choice of~$\alpha$.

If $ \Delta\ge 2$, then by~(2)

\noindent
\begin{multline*}
b_n \sum\limits_{i=1}^\infty {\sf E}\left(a_{ni}n^{-1/p} X_{ni}\right)^2 I
\left\{\left|a_{ni}n^{-1/p} X_{ni}\right|\le\fr{1}{b_n}\right\} \\
{}\le  C b_n {\sf E} X^2 \sum\limits_{i=1}^\infty \fr{a_{ni}^2}{n^{2/p}} 
\le C {\sf E} X^2 n^{\alpha +\gamma- \ 2/p} \to 0
\end{multline*}
as $n\to \infty$
by the choice of~$\alpha$. Thus, condition~($iii$) of the theorem holds.

Condition~($iv$) holds trivially.

Hence, one gets by the theorem that

\noindent
\begin{multline*}
\sum\limits_{n=1}^\infty n^t {\sf P}\left\{ \left|\sum\limits_{i=1}^\infty a_{ni}n^{-1/p} 
\left(X_{ni}\right.\right.\right.\\
\left.\left.\left.{}- {\sf E}X_{ni}I\left\{\left|a_{ni}X_{ni}\right|\le
\fr{n^{1/p}}{b_n}\right\}\right)\right|> \varepsilon \right\}<\infty
\end{multline*}
for all $\varepsilon>0$.
It remains to show that

\noindent
$$ 
\sum\limits_{i=1}^\infty a_{ni}n^{-1/p}  {\sf E}X_{ni}I
\left\{\left|a_{ni}X_{ni}\right|\le \fr{n^{1/p}}{b_n}\right\}\to 0\,. 
$$
Since ${\sf E}X_{ni}=0$,
\begin{multline*}
 {\sf E}X_{ni}I\left(|a_{ni}X_{ni}|\le \fr{n^{1/p}}{b_n}\right)\\=
-{\sf E}X_{ni}I\left\{|a_{ni}X_{ni}|\right.
\left.
>\fr{n^{1/p}}{b_n}\right\}\,.
\end{multline*}
It follows that
\begin{multline*}
\left|\sum\limits_{i=1}^\infty a_{ni}n^{-1/p} {\sf E}
X_{ni}I\left\{\left|a_{ni}X_{ni}\right|\le \fr{n^{1/p}}{b_n}\right\}\right|\\
{}\le n^{-1/p} \sum\limits_{i=1}^\infty   {\sf E}\left|a_{ni}X_{ni}\right| I
\left\{\left|a_{ni}X_{ni}\right|> \fr{n^{1/p}}{b_n}\right\}\\
{}\le n^{-1/p} \left(\fr{b_n}{n^{1/p}}\right)^{\Delta-1}\\
{}\times \sum\limits_{i=1}^\infty  
{\sf E}\left|a_{ni}X_{ni}\right|^{\Delta} I\left\{\left|a_{ni}X_{ni}\right|>
\fr{n^{1/p}}{b_n}\right\}\\
{}\le \fr{C  (b_n)^{\Delta-1} {\sf E}|X|^{\Delta}}{n^{t+1}} \le C n^{\alpha(\Delta-1)-t-1} \to 0
\end{multline*}
by the choice of~$\alpha$.

Thus, the proof is completed.\hfill~$\Box$

\medskip
\noindent 
\textbf{Remark~2.} If $t<-1,$ then the conclusion of Corollary~3 holds trivially. When 
$t\ge -1$, Sung~[10] proved Corollary~3 under the stronger condition that $\{X_{ni}$, $i\ge 1$, $n\ge 1\}$ is an 
array  of rowwise
independent random variables. However, the relatively important case $t=-1$ in 
Corollary~3 cannot be proved by
using the theorem. The present authors left as an open problem whether Corollary~3 holds for $t=-1$.

\medskip
As a special case of Corollary 3, let get the following corollary.

\medskip

\noindent 
\textbf{Corollary~4.} \textit{Let $t>-1$ and $1\le p<2.$ 
Let $\{X_{ni}$, $1\le i \le n$, $n\ge 1\}$ be an array
of rowwise negatively dependent  mean zero random variables which are stochastically dominated by a random
variable~$X$ with ${\sf E}|X|^{p(t+2)}<\infty.$ Then,}
$$
\sum\limits_{n=1}^\infty n^t {\sf P}\left\{ \fr{\left|\sum\nolimits_{i=1}^n X_{ni}\right|}{n^{1/p}}>
\varepsilon\right\}<\infty
\mbox{ for all } \varepsilon>0\,.
$$

\noindent 
P\,r\,o\,o\,f\,.\ Let $a_{ni}=1$ for $1\le i\le n$ and $a_{ni}=0$ for $i>n.$ 
Then, for $q<p(t+2),$
$\sum\limits_{i=1}^\infty |a_{ni}|^q=n.$ Thus, assumption~(1) of Corollary~3 holds for 
$\beta=1$. Since $1\le p<2$,
assumption~(2) holds for $\gamma=1$. Thus, the result follows from Corollary~3.\hfill~$\Box$

\medskip

\noindent 
\textbf{Remark~3.} When $t=0,$  Corollary~4 is the same as Corollary~2.

\section*{Acknowledgments} 

\noindent
This research is partially supported by the 
Center of Excellence in Mathematics, the Commission on Higher Education, Thailand.

  {\small\frenchspacing
{%\baselineskip=10.8pt
\addcontentsline{toc}{section}{Литература}
\begin{thebibliography}{99}

\bibitem{7-vol} %1
\Au{Hsu P.\,L., Robbins~H.}  
Complete convergence and the law of large numbers~// Proc. Nat. Acad. Sci. USA, 1947.
Vol.~ 33. P.~25--31.

\bibitem{10-vol} %2
\Au{Katz M.}  The probability in the tail of a distribution~//
Ann. Math. Stat., 1963. Vol.~34. P.~312--318.

\bibitem{3-vol} %3
\Au{Erd$\ddot{\mbox{o}}$s~P.} 
On a theorem of Hsu and Robbins~// Ann. Math. Statist., 1949. Vol.~20. P.~286--291.

\bibitem{4-vol} %4
\Au{Erd$\ddot{\mbox{o}}$s~P.} 
Remark on my paper ``On a theorem of Hsu and Robbins''~// Ann. Math. Statist., 1950.
Vol.~21. P.~138.

\bibitem{12-vol} %5
\Au{Spitzer F.\,L.}
 A~combinatorial lemma and its applications~//  Trans. Amer. Math. Soc., 1956. Vol.~82.
P.~323--339.


\bibitem{2-vol} %6
\Au{Baum K.\,B., Katz M.} 
Convergence rates in the law of large numbers~// Trans. Amer. Math. Soc., 1965.
Vol.~120. P.~108--123.

\bibitem{6-vol} %7
\Au{Gut A.} Complete convergence for arrays~// 
Periodica Math. Hungarica, 1992. Vol.~25. P.~51--75.


\bibitem{9-vol} %8
\Au{Hu~T.-C., M$\acute{\mbox{o}}$ricz~F., Taylor~R.\,L.}
 Strong laws of large numbers for arrays of rowwise independent
random variables~//  Acta Math. Hung.,  1989. Vol.~54. P.~153--162.

\bibitem{11-vol} %9
\Au{Kruglov V.\,M., Volodin A.\,I., Hu~T.-C.}
 On complete convergence for arrays~//  Stat. Prob. Lett.,  2006.
Vol.~76. P.~1631--1640.

\bibitem{13-vol} %10
\Au{Sung S.\,H.}  Complete convergence for weighted sums of random variables~//
Stat. Prob. Lett., 2007. Vol.~77. P.~303--311.

\bibitem{14-vol} %11
\Au{Taylor R.\,L., Patterson~R.\,F., Bozorgnia~A.} 
A~strong law of large numbers for arrays of rowwise
negatively dependent random variables~// Stochastic Anal. Appl., 2002. Vol.~20. P.~643--656.

\bibitem{5-vol} %12
\Au{Giuliano Antonini~R., Kozachenko~V., Volodin~A.}  Convergence of series of dependent
$\varphi$-subgaussian random variables~// J.~Math. Anal. Appl., 2008.
Vol.~338. P.~1188--1203.

\bibitem{8-vol} %13
\Au{Hu T.-C., Li~D., Rosalsky~A., Volodin~A.}  On the rate of complete convergence for weighted sums of
Banach space valued random elements~//  Theor. Prob. Appl., 2002. Vol.~47. P.~5455--5468.

\bibitem{1-vol} %14
\Au{Ahmed S.\,E., Giuliano Antonini~R., Volodin~A.}
On the rate of complete convergence for weighted sums of
arrays of Banach space valued random elements with application to moving average process~// 
Statist. Prob. Lett.,  2002. Vol.~58. P.~185--194.



\end{thebibliography}
}
}


\end{multicols}

%\vspace*{6pt}

%\hrule

%\vspace*{6pt}


\def\tit{ПОЛНАЯ СХОДИМОСТЬ СУММ В~СХЕМЕ СЕРИЙ ОТРИЦАТЕЛЬНО ЗАВИСИМЫХ СЛУЧАЙНЫХ ВЕЛИЧИН}

\def\aut{С.\,Х.~Санг$^1$, К.~Будсаба$^2$, А.~Володин$^3$}

\titelr{\tit}{\aut}

\vspace*{6pt}


\noindent
$^1$Университет Пай Чай, Республика Корея, sungsh@pcu.ac.kr\\[1pt]
\noindent
$^2$Университет Таммасат, Таиланд, kamon@mathstat.sci.tu.ac.th\\[1pt]
\noindent
$^3$Университет Реджайны, Канада,  Andrei.Volodin@uregina.ca

\vspace*{6pt}

\Abst{Приводится  результат о полной сходимости для сумм в схеме 
серий для отрицательно зависимых случайных величин в весьма общей форме.
Из этого результата следуют многие факты о полной сходимости взвешенных 
сумм отрицательно зависимых случайных величин.}

\label{end\stat}


\KW{полная сходимость; отрицательная зависимость; взвешенные суммы; схема серий}

%\renewcommand{\figurename}{\protect\bf Рис.}
%\renewcommand{\tablename}{\protect\bf Таблица}
\renewcommand{\bibname}{\protect\rmfamily Литература}
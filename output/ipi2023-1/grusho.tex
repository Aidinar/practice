
\def\stat{grusho}

\def\tit{ПРИЧИННО-СЛЕДСТВЕННЫЕ СВЯЗИ В~ЗАДАЧАХ~КЛАССИФИКАЦИИ}

\def\titkol{Причинно-следственные связи в~задачах классификации}

\def\aut{А.\,А.~Грушо$^1$, Н.\,А.~Грушо$^2$, М.\,И.~Забежайло$^3$, 
В.\,В.~Кульченков$^4$, 
Е.\,Е.~Тимонина$^5$,\\ С.\,Я.~Шоргин$^6$}

\def\autkol{А.\,А.~Грушо, Н.\,А.~Грушо, М.\,И.~Забежайло и~др.}
%$^3$,  В.\,В.~Кульченков$^4$,  Е.\,Е.~Тимонина$^5$, С.\,Я.~Шоргин$^6$}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Грушо А.\,А.}
\index{Грушо Н.\,А.}
\index{Забежайло М.\,И.}
\index{Кульченков В.\,В.}
\index{Тимонина Е.\,Е.}
\index{Шоргин С.\,Я.}
\index{Grusho A.\,A.}
\index{Grusho N.\,A.}
\index{Zabezhailo M.\,I.}
\index{Kulchenkov V.\,V.} 
\index{Timonina E.\,E.}
\index{Shorgin S.\,Ya.}


%{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
%{Работа выполнена при поддержке Министерства науки и~высшего образования
%Российской федерации, грант №\,075-15-2020-799.}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Федеральный исследовательский центр <<Информатика и~управление>> Российской академии наук, 
\mbox{grusho@yandex.ru}}
\footnotetext[2]{Федеральный исследовательский центр <<Информатика и~управление>> Российской академии наук, info@itake.ru}
\footnotetext[3]{Федеральный исследовательский центр <<Информатика и~управление>> Российской академии наук, 
\mbox{m.zabezhailo@yandex.ru}}
\footnotetext[4]{Банк ВТБ (ПАО), vlad.kulchenkov@gmail.com}
\footnotetext[5]{Федеральный исследовательский центр <<Информатика и~управление>> Российской академии наук, 
\mbox{eltimon@yandex.ru}}
\footnotetext[6]{Федеральный исследовательский центр <<Информатика и~управление>> Российской академии наук, 
\mbox{sshorgin@ipiran.ru}}

\vspace*{-6pt}

 

  
  \Abst{В данной работе объект классификации рассматривается как носитель причины 
появления одного или нескольких следствий и~любой алгоритм классификации принимает 
решение о классе, наблюдая следствия из анализируемой причины.
  Рассматриваются следствия причины в~задаче бинарной классификации как источники 
дополнительной информации, подтверждающие или отвергающие гипотезу о~причине 
в~классифицируемом объекте. При рассмотрении гипотезы о наличии или отсутствии 
определенной причины в~классифицируемом по этому свойству объекте на основании 
нескольких следствий автоматически строится язык представления знаний. Тогда легко 
использовать доступную информацию из разных информационных пространств в~задаче 
классификации объекта.
  Для использования при\-чин\-но-след\-ст\-вен\-ных связей в~задаче классификации 
необходимо использовать машинное обучение. В~условиях обучения с~учителем имеем 
множество прецедентов, когда известно наличие интересующей нас причины. Тогда можно 
статистически выделить события, которые стали следствиями этой причины. 
Детерминированные отношения причины и~следствия порождают ошибки только за счет шума. 
В тех прецедентах, где нет причины, позитивная классификация появляется только за счет 
шума независимо от прецедента к~прецеденту. Таким образом, даже слабое отклонение от 
равновероятного шума позволяет построить состоятельный критерий, выделяющий следствия 
от случайного шума. 
  Выделение следствий можно проводить независимо друг от друга. Это следует из 
детерминированности отношения при\-чи\-на--след\-ст\-вие и~независимости шума.}
  
  \KW{задача конечной классификации; причинно-следственные связи; машинное обучение} 
  
\DOI{10.14357/19922264230106} 
  
%\vspace*{-4pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}
  
  \section{Введение }
  
  Вопросы причинно-следственных связей в~построении решающих правил 
принятия решений исследовались в~ряде научных публикаций. Обзор методов 
выявления причин в~графических моделях можно найти в~статье~[1]. 
Большинство публикаций базируется на результатах исследований~[2--4]. 
Особенность этих работ заключается в~прогнозировании следствия в~условиях 
зашумленной причины. Этот подход оказался достаточно успешным. Например, 
такой подход повысил устойчивость нейронных сетей~[5]. 
  
  Отметим, что рассматриваемый класс задач тесно связан с~необходимостью 
распознавания следствий при различных модификациях причин. Чтобы сократить 
множество изменений причин, \mbox{в~работе}~[6] использовались специальные методы 
преобразования причин для повышения устойчивости работы классификаторов. 
  
  В данной статье предлагается иной подход к~использованию  
при\-чин\-но-след\-ст\-вен\-ных связей. В~работе~[7] решается задача 
восстановления знаний из фрагментов информации, причем эти фрагменты не 
всегда бывают точными. Следуя этой же идее, задачу конечной классификации 
можно рассматривать как выявление одной из $k$ причин по следствиям, 
наблюдаемым в~объектах, содержащих по одной из этих причин. Эти следствия 
можно рассматривать как источники дополнительной информации, доступные 
для уточнения искомой причины. 
  
  Таким образом, рассматривается задача классификации элементов 
множества~$D$ на конечное чис\-ло непересекающихся классов, т.\,е.\ 
существует множество~$D$ и~функция $F: D\hm\to K\hm= \{1,2,\ldots , k\}$, где 
$k$~--- чис\-ло классов. Рас\-смот\-рим несколько примеров постановки задачи 
классификации.
  \begin{enumerate}[1.]
\item \textbf{Искусственные нейронные сети и~машинное обуче\-ние 
с~учителем.} Пусть дана конечная учебная выборка~$S$, состоящая из $m$ 
элементов, и~$S\hm\subseteq D$. Обучение с~учителем означает, что известна 
частичная функция $F_S: S\hm\to K$, совпадающая с~$F$ на всех элементах 
из~$S$. Искусственная нейронная сеть (ИНС) помнит значения функции~$F_S$, но если $x\hm\not\in S$, то 
пользователи ИНС надеются, что $F_S$ может правильно вычислить значение 
этой функции для данного~$x$. Здесь возникают два вопроса:
\begin{itemize}
\item[(а)] почему $F_S$ должна уметь вычислять что-то вне своей об\-ласти 
определения?
\item[(б)] если ИНС выдаст некоторый ответ при подаче на вход~$x$, то почему 
этот ответ будет правильным, т.\,е.\ совпадет со значением $F(x)$?
\end{itemize}
  
  Без ответов на эти вопросы возникает противоречие с~определениями 
традиционной математики, а~корректные ответы на них предполагают 
использование дополнительной \mbox{информации} о~свойствах ИНС, функции~$F$ 
и~свойствах множества~$D$. Если дополнительной информации нет, то оба 
вопроса некорректны и~любые ответы на них не имеют смысла.
  
  \item \textbf{Риски в~кредитных учреждениях.} Рассматривается поток заявок на 
кредит в~фиксированном банке. Кредит и~условия кредита определяются 
финансовым состоянием обратившегося за кредитом, который рассказывает 
о~своих доходах и~других аспектах своего финансового состояния. Описание 
состояния, данное клиентом, дополнительно оценивается по более чем 
50~характеристикам, собранным из различных, чаще всего внешних для банка, 
источников. По собранным значениям проводится классификация обратившегося 
за кредитом с~точки зрения возможности дефолта при выдаче кредита. Здесь, как 
и~в~предыдущем примере, обучение на опыте работы по выдаче кредитов 
формирует допустимые значения указанных характеристик. В~работе~[8] 
перечень источников допустимых дополнительных данных называется 
метаданными. Тогда заявленная обратившимся за кредитом клиентом и~собранная 
дополнительная информация по клиенту определяют допустимость выдачи ему 
кредита, и~причиной этого решения становится проведенная оценка его 
финансового состояния.
  
  \item \textbf{Поиск причин сбоев в~компьютерной системе.} Рассматривается 
задача удаленного администрирования информационной системы~[9] в~случае 
возникновения сбоя. Необходимо выявить первоначальную причину сбоя, с~тем 
чтобы провести ремонт в~кратчайшее время и~восстановить рабочие процессы 
в~информационной системе. Первопричина сбоя порождает информацию, 
которую администратор может\linebreak запросить и~получить удаленно. Такую 
информацию о~первопричине опытный администратор умеет собирать 
и~анализировать, чтобы локализовать место исходного сбоя. \mbox{Достаточно} часто такая 
дополнительная информация хранится в~базе данных администратора и~позволяет 
использовать накопленный опыт~[10].\linebreak Заменив блок, содержащий причину, 
администратор быст\-ро восстанавливает работоспособность всей информационной 
системы. Как правило, этот алгоритм работает, когда первопричина единственна, 
хотя производных сбоев может быть несколько. В~этом примере, как 
и~в~предыдущих, локализация происходит за счет дополнительной информации, 
по\-сту\-па\-ющей из различных источников, которые могут иметь отношение 
к~причине сбоя. Еще один интересный момент, связанный с~этим примером, 
состоит в~том, что не всегда надо искать собственно первопричину, а достаточно 
покрыть ее достаточно малой частью системы, которую несложно заменить~[11]. 
В этом состоит идея приближенного причинно-следственного анализа.
  
  \item \textbf{В задачах медицинской диагностики~[12, 13] необходимо 
выявить причину исходного заболевания.} Врач обращается к~различным 
источникам дополнительной информации для подтверждения диагноза. 
Существенной стороной этого примера является то, что врач несет повышенную 
ответственность за решение задачи о поставленном диагнозе.
  \end{enumerate}
  
  В рассмотренных задачах важно различать дополнительную информацию 
о~классифицируемом объекте, которая помогает правильной классификации, 
и~информацию о предположениях, которые позволяют построить модель поиска 
и~в~рамках этой модели оценивать качество принимаемого данным методом 
классификации решения.
  %
  Например, для ИНС это предположение о независимости и~одинаковой 
распределенности выбора точек в~множество~$S$, а~также метрические 
соотношения между элементами в~множестве~$D$. 
  
   В данной работе объект классификации рас\-смат\-ри\-ва\-ет\-ся с~точки зрения 
присутствия в~нем причины появления одного или нескольких следствий и~любой 
алгоритм классификации принимает решение о~классе, наблюдая следствия из 
анализируемой причины. В~примере с~ИНС предполагаемая близость~$x$ 
к~одному из элементов~$x^*$ множества~$S$ позволяет допустить, что класс 
элемента~$x$ совпадает с~классом $F_S(x^*)$. Существует множество работ, 
подтверждающих или опровергающих это утверждение в~различных моделях, но 
все эти работы основаны на предположениях, а~не на доста\-точ\-ности информации 
для правильной классификации причины в~данной задаче. 
  
  С другой стороны, объект классификации, отож\-дест\-в\-ля\-емый 
с~классифицируемой причиной, может породить множество следствий, которые 
несут информацию о~принадлежности причины к~заданному классу и~могут 
служить подтверждением этой причины в~данном классе или отрицанием этой 
причины в~полученных новых данных. Дополнительная информация может 
ослабить предположения о модели классификатора.
  
  В работе рассматриваются следствия причины\linebreak в~бинарной классификации как 
источники дополнительной информации, подтверждающие или\linebreak отвергающие 
гипотезу о~\mbox{причине} в~клас\-си\-фи\-ци\-ру\-емом объекте. При рассмотрении гипотезы 
о~причине в~классифицируемом по этому свойству \mbox{объекте} на основании 
нескольких следствий автоматически строится язык представления знаний при 
описании использования доступной информации из разных информационных 
пространств~[14] в~задаче классификации объекта.
  
  \section{Математическая модель причинно-следственных связей 
в~задачах классификации}
  
  Сделаем два предположения о причинах и~следствиях. Для простоты полагаем, 
что в~задаче распознавания объект распознавания отождествляется только с~одной 
причиной. Если есть причина, то каждое следствие возникает детерминированно 
(будем также говорить с~вероятностью~1). Невозможно усовершенствовать одну 
процедуру (алгоритм) классификации, чтобы повысить качество и~устойчивость 
распознавания, защищенность от атак. Для решения этих задач необходима 
дополнительная информация, которая содержится только в~следствиях причины. 
Таким образом, описание процедуры распознавания одним алгоритмом 
описывается двумерным булевым вектором $(y_1,y_2)$), где $y_1$ соответствует 
наличию или отсутствию причины в~распознаваемом объекте, т.\,е.\  при 
$y_1\hm=1$ гипотеза о распознаваемой причине считается правильной, а при 
$y_1\hm=0$ неверной. Рассматриваемый алгоритм распознавания при $y_2\hm=1$ 
определяет распознаваемую причину, а при $y_2\hm=0$ не распознает причину. 
Поскольку распознавание идет по следствию причины, то объясним 
происхождение возможных ошибок распознавания. Предположим, что возможен 
шум в~определении наличия следствия причины, т.\,е.\ алгоритм не 
идентифицирует появление следствия, если шум позволяет изменить значение 
переменной~$y_2$ на~0 или в~отсутствие причины изменить значение~$y_2$ на~1. 
Для простоты будем оценивать вероятность влияния шума одной 
константой $\varepsilon \hm>0$. Тогда условные вероятности результатов 
классификации наличия причины данным алгоритмом можно оценить 
следующим образом:
  \begin{align*}
  {\sf P}\left( y_2=1\vert y_1=1\right) &\geq 1-\varepsilon\,;\\
{\sf  P}\left( y_2=0\vert y_1=1\right) &\leq \varepsilon\,;\\
{\sf  P}\left( y_2=1\vert y_1=0\right) &\leq \varepsilon\,;\\
{\sf  P}\left( y_2=0\vert y_1=0\right) &\geq 1-\varepsilon\,.
  \end{align*}
  
  В предположении, что шум появляется независимо, но, возможно, действует  
по-преж\-не\-му одна причина, то независимо для любого другого возможного 
следствия и~алгоритма классификации этого следствия верны такие же формулы. 
  
  Рассмотрим важное понятие устойчивости распознавания одним алгоритмом 
для бинарного случая~[15]. Численная оценка устойчивости в~этой работе имеет 
следующий вид:
  $$
  R= \fr{{\sf P}(y_2=1\vert y_1=1)}{{\sf P}\left(y_2=1\vert y_1=1\right)+{\sf P}\left(y_2=1\vert y_1=0\right)}\,.
  $$
  %
  Путем несложных преобразований при небольших $\varepsilon$ получим 
  $$
  R\geq 1-\fr{\varepsilon}{1-\varepsilon}\,.
  $$
  
  Предположим, что помимо основного алгоритма распознавания используется 
еще одно следствие для подтверждения вывода, т.\,е.\ рассматривается вектор 
$\left( y_2^{(1)}, y_2^{(2)}\right)$. Опираясь на то, что следствия при наличии 
причины возникают с~вероятностью~1, а шумы независимы, получаем:
  \begin{align*}
 {\sf P}\left( y_2^{(1)}=1,\ y_2^{(2)}=1\vert y_1=1\right) &\geq (1-\varepsilon)^2\,;\\
   {\sf P}\left( y_2^{(1)}=1,\ y_2^{(2)}=0\vert y_1=1\right)& ={}\\
  &\hspace*{-22mm}{}=P\left( y_2^{(1)}=0,\  y_2^{(2)}=1\vert y_1=1\right)\leq \varepsilon\,;\\
   {\sf P}\left( y_2^{(1)}=1,\ y_2^{(2)}=1\vert y_1=0\right)& \leq \varepsilon^2\,;\\
   {\sf P}\left( y_2^{(1)} =1,\ y_2^{(2)}=0\vert  y_1=0\right)&={}\\ 
  &\hspace*{-22mm}{}= {\sf P}\left( y_2^{(1)}=0,\ 
y_2^{(2)}=1\vert y_1=0\right) \leq \varepsilon\,;\\
   {\sf P}\left( y_2^{(1)}=0,\ y_2^{(2)}=0\vert y_1=0\right)&\geq (1-\varepsilon)^2.
  \end{align*}
  
  Вероятность, что произойдет хотя бы одна ошибка классификации, когда нет 
причины, равна
  $$
  1- {\sf P}\left( y_2^{(1)}=0,\ y_2^{(2)}=0\vert y_1=0\right) \leq 1-(1-\varepsilon)^2.
  $$
  
  Если рассматривать $k$ следствий в~качестве источников дополнительной 
информации, то обобщение со случая $k\hm= 2$ проводится следующим образом. 
Необходимо работать с~двоичным вектором $\left( y_2^{(1)}, y_2^{(2)}, \ldots , 
y_2^{(k)}\right)$ длины~$k$, и~вероятность хоть одной случайной позитивной 
классификации, когда нет причины, равна

\vspace*{-8pt}

\noindent
 \begin{multline*}
 1- {\sf P}\left( y_2^{(1)} =0\,,\
  y_2^{(2)}=0,\ldots , y_2^{(k)} =0\vert y_1=0\right) \leq{}\\
  {}\leq  1-\left( 1- \varepsilon\right)^k.
\end{multline*}

\vspace*{-4pt}
  
  Эта формула может использоваться в~оценке устойчивости, когда 
имеются~$k$~источников взаимно дополняющей информации.
  
  Параметр $k$ может трактоваться как дискретное время, тогда можно 
рассматривать задачу об оптимальной остановке случайного процесса 
с~дискретным временем~[16, 17].

\vspace*{-3pt}
  
\section{Подготовительная работа для~использования 
причинно-следственных связей в~задаче конечной классификации }

\vspace*{-3pt}

   Для использования причинно-следственных\linebreak связей в~задаче классификации 
необходимо использовать машинное обуче\-ние. В~условиях обуче\-ния с~учителем 
имеем множество прецедентов,\linebreak когда известно наличие причины. Тогда можно 
статистически выделить события, которые стали следствиями причины. 
Детерминированные отношения причины и~следствия порождают ошибки \mbox{только} 
за счет шума. В~тех прецедентах, где нет причины, позитивная классификация 
появляется только за счет шума независимо от прецедента к~прецеденту. Таким 
образом, даже слабое отклонение~$\varepsilon$ от~$1/2$ позволяет построить 
состоятельный критерий, выделяющий следствия среди случайного шума. 
  
  Выделение следствий можно проводить независимо друг от друга. Это следует 
из детерминированности отношения при\-чи\-на--след\-ст\-вие и~независимости 
шума. Однако нельзя исключать \mbox{возможность} зависимости найденных следствий 
между собой. В~силу детерминированности отношения при\-чи\-на--след\-ст\-вие, 
если утверждение~$B$ является следствием утверждения~$A$, но 
утверждение~$C$ является следствием утверждения~$B$, то утверждение~$C$ 
является следствием утверждения~$A$. При этом использовать утверждение~$C$ 
как источник дополнительной информации нельзя, так как вся информация об 
утверждении~$A$ уже использована в~утверждении~$B$. 
  
  Конечно, возможна ситуация, когда утверждение~$B$ является сильно 
зашумленным и~по\-рож\-да\-ет ошибку классификации, а~утверж\-де\-ние~$C$ дает 
правильный ответ, но для такого рассмотрения нужна другая дополнительная 
информация. Поэтому необходимо на множестве следствий 
\mbox{изучить} при\-чин\-но-след\-ст\-вен\-ные связи утверждений без привязки к~основной первопричине 
утверждения~$A$.
  
  Эту задачу также можно решать с~по\-мощью обуче\-ния. Если утверждение~$C$ 
является следствием причины~$B$, то на множестве прецедентов, содержащих 
утверждение~$B$ и~не содержащих утверждение~$A$, построенный выше 
алгоритм определяет, является ли утверждение~$C$ следствием 
утверждения~$B$.

  
  Рассмотрим ситуацию приближенных при\-чин\-но-след\-ст\-вен\-ных связей, когда 
утверждение~$A$ содержит (покрывает) настоящую причину~$A^*$, т.\,е.\ $A^*
\hm\subset A$, а~утверждение~$B$ является следствием утверждения~$A^*$. 
Тогда утверждение~$B$ является также следствием утверждения~$A$, т.\,е.\ при 
появлении утверждения~$A$ следствие~$B$ появляется детерминированно. 
Поэтому если в~классификации используется утверждение~$A$, но не 
первопричина~$A^*$, то дополнительная информация в~классификации причины, 
содержащейся в~утверждении~$A$, также содержится в~утверждении~$B$. 
  
  Проблема может возникнуть, когда в~утверждении~$A$ содержится искомая 
причина~$A^*$ и~содержится утверждение~$B$. Утверждение~$B$ не является 
следствием утверждения~$A^*$, но у~утверждения~$B$ есть следствие~$C$, 
которое не является следствием~$A^*$. В~этом случае нельзя углубляться в~поиск 
первопричины~$A^*$ и~можно рассматривать утверждение~$C$ как источник 
дополнительной информации\linebreak для классификации утверждения~$A$. Пусть 
явля-\linebreak ется истинным, что утверждение~$A$ покрывает причину~$A^*$ 
и~утверждение~$A$ правильно отнесено к~нужному классу за счет 
дополнительной для утверждения~$A$ информации в~утверждении~$C$. Тогда 
дополнительная информация в~утверждении~$C$ может быть использована для 
классификации утверж\-де\-ния~$A$. 
{ %\looseness=1

}
  
  Ситуация становится хуже, когда утверждение~$A$ не содержит 
причину~$A^*$, но утверждение~$C$ является следствием~$A$. Однако при 
обуче\-нии с~учителем использовались только те прецеденты, которые содержат 
искомую причину~$A^*$. Поэтому рассматриваемый случай следствия~$C$ из 
утверждения~$A$ не попадает в~обуча\-ющую выборку. 
  
  Проведенные рассуждения показывают, что в~задаче классификации 
с~использованием при\-чин\-но-след\-ст\-вен\-ных связей можно пользоваться 
приближенным каузальным анализом.
  
  Рассмотренный подход позволяет использовать различные множества данных, 
содержащих дополнительную для классификации информацию. Поиск 
дополнительной информации проводится в~различных информационных 
пространствах, поэтому языки описания утверждений (событий) в~разных 
информационных пространствах могут кардинально отличаться. Вместе с~тем для 
использования дополнительной информации в~методе  
при\-чин\-но-след\-ст\-вен\-ных связей особенности языков в~разных 
информационных пространствах\linebreak не являются существенными, и~для 
использования в~описанном методе достаточно только идентификации следствия, 
привязанной к~причине.\linebreak Таким образом, построен простой язык представления 
знаний, объединяющий информацию, представленную в~разных информационных 
пространствах для целей повышения достоверности решения задачи 
классификации.

\vspace*{-9pt}
  
  \section{Заключение }
  
  \vspace*{-2pt}
  
  
  В работе построен метод использования при\-чин\-но-след\-ст\-вен\-ных связей для 
повышения до\-сто\-вер\-ности решения задачи конечной классификации. Основная 
идея работы со\-ст\-оит в~том, что любой алгоритм решения задачи конечной 
классификации не может быть совершенным, так как информация для его 
применения ограничена множеством допустимых входных данных. В~то же 
время качество классификации можно многократно повышать с~помощью 
привлечения дополнительной уточ\-ня\-ющей информации. Если представить задачу 
конечной классификации в~виде идентификации одной из конечного множества 
причин и~решать эту задачу по наблюдениям за следствиями этих причин, то 
множество следствий и~есть дополнительная, уточ\-ня\-ющая правильное решение 
задачи классификации, информация.
  
  Для восстановления структуры при\-чин\-но-след\-ст\-вен\-ных связей можно 
использовать машинное обуче\-ние. При этом различные следствия одной причины 
можно независимо друг от друга выявлять, используя одну обуча\-ющую выборку. 
Это позволяет строить эффективные алгоритмы решения таких задач.
  
  Предложенный метод позволяет строить прос\-тые языки пред\-став\-ле\-ния знаний. 
Такие языки позволяют использовать информацию из различных 
информационных пространств и~возможности статистического анализа в~этих 
пространствах. Объединяющим элементом многих информационных пространств 
являются при\-чин\-но-след\-ст\-вен\-ные связи, порождаемые классифицируемой 
первопричиной.

\vspace*{-9pt}
  
{\small\frenchspacing
 {%\baselineskip=10.8pt
 %\addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
 
 \vspace*{-2pt}
    
\bibitem{1-gru}
\Au{Glymour C., Zhang~K., Spirtes~P.} Review of causal discovery methods based on graphical 
models~// Frontiers Genetics, 2019. Vol.~10. Art. 524. 15~p. doi: 10.3389/ fgene.2019.00524.
\bibitem{2-gru}
\Au{Halpern J.\,Y., Pearl J.}  Causes and explanations: A~structural-model approach. 
Part~I: Causes.~// Brit. J.~Philos. Sci., 2005. Vol.~56. No.\,4. P.~843--887. 
\bibitem{3-gru}
\Au{Pearl J.} Causal inference~// Workshop on Causality Proceedings: Objectives and Assessment at 
NIPS~/ Eds. I.~Guyon, D.~Janzing, B.~Scholkopf.~--- Proceedings of machine learning research 
ser.~--- Whistler, Canada, 2010. Vol.~6. P.~39--58. 
\bibitem{4-gru}
\Au{Pearl J.} The mathematics of causal inference~// Joint Statistical Meetings Proceedings.~--- ASA, 
2013. P.~2515--2529.
\bibitem{5-gru}
\Au{Zhang C., Zhang K., Li~Y.} A~causal view on robustness of neural networks~// Working Papers 
by Cornell University, 2020. arXiv:2005.01095v3 [cs.LG]. 21~p.
\bibitem{6-gru}
\Au{Wang B., Lyle~C., Kwiatkowska~M.} Provable guarantees on the robustness of decision rules to 
causal interventions~// 30th Joint Conference (International) on Artificial Intelligence 
Proceedings, 2021. P.~4258--4265. {\sf 
https://www.ijcai.org/proceedings/2021/0585.pdf}.
\bibitem{7-gru}
\Au{Valiant L.\,G.} Knowledge infusion: In pursuit of robustness in artificial intelligence~// IARCS 
Annual Conference on Foundations of Software Technology and Theoretical Computer Science~/ 
Eds.\ R.~Hariharan, M.~Mukund, V.~Vinay.~---
Leibniz international iroceedings in informatics ser.~--- Wadern, Germany: Schloss 
Dagstuhl~~--- Leibniz-Zentrum fuer Informatik, 2008.  Vol.~2. P.~415--422. doi: 
10.4230/LIPIcs.FSTTCS.2008.1770.
\bibitem{8-gru}
\Au{Grusho A., Grusho~N., Zabezhailo~M., Timonina~E., Senchilo~V.} Metadata for root cause 
analysis~// Communications ECMS, 2021. Vol.~35. Iss.~1. P.~267--271. doi: 10.7148/ 2021-0267.
\bibitem{9-gru}
\Au{Грушо А.\,А., Грушо~Н.\,А., Забежайло~М.\,И., Тимонина~Е.\,Е.} Удаленный мониторинг 
рабочих процессов~// Информатика и~её применения, 2021. Т.~15. Вып.~3. С.~2--8.
\bibitem{10-gru}
\Au{Грушо А.\,А., Грушо~Н.\,А., Забежайло~М.\,И., Тимонина~Е.\,Е.}
 Локализация исходной причины аномалии~// 
Проблемы информационной безопас\-ности. Компьютерные сис\-те\-мы, 2020.
№\,4. С.~9--16. EDN: JXHQTY.
 

\bibitem{11-gru}
\Au{Грушо Н.\,А., Грушо~А.\,А., Забежайло~М.\,И., Тимонина~Е.\,Е.} Методы нахождения 
причин сбоев в~информационных технологиях с~помощью метаданных~// Информатика и~её 
применения, 2020. Т.~14. Вып.~2. С.~33--39. doi: 10.14357/19922264200205.

\bibitem{13-gru} %12
\Au{Richens J.\,G., Lee~C.\,M., Johri~S.} Improving the accuracy of medical diagnosis with causal 
machine learning~// Nat. Commun., 2020. Vol.~11. Art. 3923. 9~p.  
doi: 10.1038/s41467-020-17419-7.

\bibitem{12-gru} %13
\Au{Забежайло М.\,И., Грушо~А.\,А., Грушо~Н.\,А., Тимонина~Е.\,Е.} Поддержка решения задач 
диагностического типа~// Системы и~средства информатики, 2021. Т.~31. №\,1. С.~69--81. 

\bibitem{14-gru}
\Au{Grusho~A., Grusho~N., Timonina~E.} Method of several information spaces for identification of 
anomalies~// Intelligent distributed computing XIII~/ 
Eds. I.~Kotenko, 
C.~Badica, V.~Desnitsky, D.~El~Baz, M.~Ivanovic.~---
Studies in  computational intelligence ser.~--- Cham: Springer, 2020. Vol.~868. P.~515--520. doi: 10.1007/978-3-030-32258-8\_60. 


\bibitem{15-gru}
\Au{Xiong P., Buffett~S., Iqbal~S., Lamontagne~P., Mamun~M., Molyneaux~H.} Towards a robust 
and trustworthy machine learning system development: An engineering perspective~// J.~Information 
Security Applications, 2022. Vol.~65. Art. 103121. 58~p. doi: 10.1016/j.jisa.2022.103121.
\bibitem{16-gru}
\Au{Вальд А.} Последовательный анализ~/ Пер. с~англ.~--- М.: Физматлит, 1960. 328~с. 
(\Au{Wald~A.} {Sequential analysis}.~--- J.~Wiley \& Sons, 1959. 212~p.)
\bibitem{17-gru}
\Au{Ширяев А.\,Н.} Вероятность: в~2 кн.~--- 3-е изд.~--- М.: МЦНМО, 2004. 521~c.

  \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 12.01.23}}

\vspace*{8pt}

%\pagebreak

%\newpage

%\vspace*{-28pt}

\hrule

\vspace*{2pt}

\hrule

%\vspace*{-2pt}

\def\tit{CAUSAL RELATIONSHIPS IN~CLASSIFICATION PROBLEMS}


\def\titkol{Causal relationships in~classification problems}


\def\aut{A.\,A.~Grusho$^1$, N.\,A.~Grusho$^1$, M.\,I.~Zabezhailo$^1$, V.\,V.~Kulchenkov$^2$, 
E.\,E.~Timonina$^1$,\\ and~S.\,Ya.~Shorgin$^1$}

\def\autkol{A.\,A.~Grusho, N.\,A.~Grusho, M.\,I.~Zabezhailo, et al.}
% V.\,V.~Kulchenkov$^2$,  E.\,E.~Timonina$^1$, and~S.\,Ya.~Shorgin$^1$}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-8pt}


\noindent
$^1$Federal Research Center ``Computer Science and Control'' of the Russian Academy of Sciences,  
44-2~Vavilov\linebreak
$\hphantom{^1}$Str., Moscow 119133, Russian Federation

\noindent
$^2$VTB Bank, 43-1~Vorontsovskaya Str., Moscow 109147, Russian Federation


\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2023\ \ \ volume~17\ \ \ issue\ 1}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2023\ \ \ volume~17\ \ \ issue\ 1
\hfill \textbf{\thepage}}}

\vspace*{3pt} 




\Abste{In the present paper, a classification object is considered as the cause for the appearance 
of one or more consequences and any classification algorithm decides on the class 
observing the consequences from the analyzed cause. The paper considers the 
consequences of the cause in the binary classification problem as sources of additional 
information confirming or rejecting the hypothesis of the cause in the classified object. 
When considering a~hypothesis about the presence or absence of a~certain cause in an 
object classified by this property, the knowledge presentation language is automatically 
built based on several consequences. Then, it is easy to use the available information 
from different information spaces in an object classification task. To use cause-and-effect relationships in a~classification task, machine learning
should be used. In 
conditions of teaching with a~teacher, there are many precedents when the presence of
 a~cause is known. Then one can statistically single out events that are the consequences of the 
cause. Deterministic cause-and-effect relationships generate errors only at the expense 
of noise. In those precedents where there is no cause, positive classification appears 
only at the expense of noise regardless of precedent to precedent. Thus, even a~weak 
deviation from equally probable noise allows one to build a~consistent criterion that 
distinguishes consequences from random noise. Sequelae can be isolated independently 
of each other. This follows from the determinism of the cause-and-effect relationship 
and the independence of noise.}

\KWE{finite classification task; cause-and-effect relationships; machine learning}

\DOI{10.14357/19922264230106} 

%\vspace*{-16pt}

%\Ack
%\noindent

  

\vspace*{-3pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{1-gru-1}
\Aue{Glymour, C., K.~Zhang, and P.~Spirtes.} 2019. Review of causal discovery methods based on 
graphical models. \textit{Frontiers Genetics} 10:524. 15~p. doi: 10.3389/fgene. 2019.00524. 
\bibitem{2-gru-1}
\Aue{Halpern, J.\,Y., and J.~Pearl.} 2005. Causes and explanations: A~structural-model approach. 
Part~I: Causes. \textit{Brit. J. Philos. Sci.} 56(4):843--887. 
\bibitem{3-gru-1}
\Aue{Pearl, J.} 2010. Causal inference. \textit{Workshop on Causality Proceedings: Objectives and 
Assessment at NIPS}. Eds. I.~Guyon, D.~Janzing, and B.~Scholkopf. Proceedings of machine learning 
research ser. 6:39--58. 
\bibitem{4-gru-1}
\Aue{Pearl, J.} 2013. The mathematics of causal inference. \textit{Joint Statistical Meetings 
Proceedings}. ASA. 2515--2529.
\bibitem{5-gru-1}
\Aue{Zhang, C., K.~Zhang, and Y.~Li.} 2021. A~causal view on robustness of neural networks. 
\textit{arXiv.org}. 21~p. Available at: {\sf https://arxiv.org/abs/2005.01095} (accessed January~20, 
2023).
\bibitem{6-gru-1}
\Aue{Wang, B., C.~Lyle, and M.~Kwiatkowska.} 2021. Provable guarantees on the robustness of 
decision rules to causal interventions. \textit{30th Joint Conference (International) on Artificial 
Intelligence Proceedings}. 4258--4265. Available at: {\sf 
https://www.ijcai.org/proceedings/2021/0585.pdf} (accessed March~13, 2023).
\bibitem{7-gru-1}
\Aue{Valiant, L.} G. 2008. Knowledge infusion: In pursuit of robustness in artificial intelligence. 
\textit{IARCS  Annual Conference on Foundations of Software Technology and Theoretical Computer Science 
Proceedings}. 
Eds.\ R.~Hariharan, M.~Mukund, and V.~Vinay.
Leibniz international iroceedings in informatics ser. Wadern, Germany: Schloss 
Dagstuhl~~--- Leibniz-Zentrum fuer Informatik. 2:415--422. doi: 10.4230/ LIPIcs.FSTTCS.2008.1770.

\bibitem{8-gru-1}
\Aue{Grusho, A., N.~Grusho, M.~Zabezhailo, E.~Timonina, and V.~Senchilo.} 2021. Metadata for 
root cause analysis. \textit{Communications ECMS} 35(1):267--271. doi: 10.7148/ 2021-0267.
\bibitem{9-gru-1}
\Aue{Grusho, A., N.~Grusho, M.~Zabezhailo, and E.~Timonina.} 2021. Udalennyy monitoring 
rabochikh protsessov [Remote monitoring of workflows]. \textit{Informatika i~ee Primeneniya~--- 
Inform. Appl.} 15(3):2--8.
\bibitem{10-gru-1}
\Aue{Grusho, A., N.~Grusho, M.~Zabezhailo, and E.~Timonina.} 2021. Localization of the root cause 
of the anomaly. \textit{Autom. Control Comp.~S.} 55(8):978--983.
\bibitem{11-gru-1}
\Aue{Grusho, N.\,A., A.\,A.~Grusho, M.\,I.~Zabezhailo, and E.\,E.~Ti\-mo\-ni\-na.} 2020. Metody 
nakhozhdeniya prichin sboev v informatsionnykh tekhnologiyakh s~pomoshch'yu metadannykh 
[Methods of finding the causes of information technology failures by means of meta data]. 
\textit{Informatika i~ee Primeneniya~--- Inform. Appl}. 14(2):33--39. doi: 10.14357/19922264200205.
 
\bibitem{13-gru-1} %12
\Aue{Richens, J.\,G., C.\,M.~Lee, and S.~Johri}. 2020. Improving the accuracy of medical diagnosis 
with causal machine learning. \textit{Nat. Commun.} 11(1):3923. 12~p. doi:  
10.1038/s41467-020-17419-7.

\bibitem{12-gru-1} %13
\Aue{Zabezhailo, M.\,I., A.\,A.~Grusho, N.\,A.~Grusho, and E.\,E.~Ti\-mo\-ni\-na.} 2021. Podderzhka 
resheniya zadach diagnosticheskogo tipa [Support for solving diagnostic type problems]. 
\textit{Sistemy i~Sredstva Informatiki~--- Systems and Means of Informatics} 31(1):69--81.

\bibitem{14-gru-1}
\Aue{Grusho, A., N.~Grusho, and E.~Timonina.} 2020. Method of several information spaces for 
identification of anomalies. \textit{Intelligent distributed computing XIII}. Eds. I.~Kotenko, 
C.~Badica, V.~Desnitsky, D.~El~Baz, and M.~Ivanovic. Studies in computational intelligence ser. 
Cham: Springer. 868:515--520. doi: 10.1007/978-3-030-32258-8\_60. 
\bibitem{15-gru-1}
\Aue{Xiong, P., S.~Buffett, S.~Iqbal, P.~Lamontagne, M.~Mamun, and H.~Molyneaux.} 2022. 
Towards a robust and trustworthy machine learning system development: An engineering perspective. 
\textit{J.~Information Security Applications} 65:103121. 58~p. doi: 10.1016/j.jisa.2022.103121.
\bibitem{16-gru-1}
\Aue{Wald, A.} 1959. \textit{Sequential analysis}. J.~Wiley \& Sons. 212~p. 
\bibitem{17-gru-1}
\Aue{Shiryaev, A.\,N.} 2004. \textit{Veroyatnost'} [Probability]. 3rd ed. Moscow: MTsNMO. 2~vols. 521~p.
 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Received January 12, 2023}}

\Contr

\noindent
\textbf{Grusho Alexander A.} (b.\ 1946)~--- Doctor of Science in physics and mathematics, professor, 
principal scientist, Institute of Informatics Problems, Federal Research Center ``Computer Science and 
Control'' of the Russian Academy of Sciences, 44-2~Vavilov Str., Moscow 119333, Russian 
Federation; \mbox{grusho@yandex.ru}

\vspace*{3pt}

\noindent
\textbf{Grusho Nikolai A.} (b.\ 1982)~--- Candidate of Science (PhD) in physics and mathematics, 
senior scientist, Institute of Informatics Problems, Federal Research Center ``Computer Science and 
Control'' of the Russian Academy of Sciences, 44-2~Vavilov Str., Moscow 119133, Russian 
Federation; \mbox{info@itake.ru}

\vspace*{3pt}

\noindent
\textbf{Zabezhailo Michael I.} (b.\ 1956)~--- Doctor of Science in physics and mathematics, principal 
scientist, A.\,A.~Dorodnicyn Computing Center, Federal Research Center ``Computer Science and 
Control'' of the Russian Academy of Sciences, 40~Vavilov Str., Moscow 119333, Russian Federation; 
\mbox{m.zabezhailo@yandex.ru}

\vspace*{3pt}

\noindent
\textbf{Kulchenkov Vladislav V.} (b.\ 1989)~--- head of risk monitoring department, VTB Bank,  
43-1~Vorontsovskaya Str., Moscow 109147, Russian Federation; 
\mbox{vlad.kulchenkov@gmail.com}

\vspace*{3pt}

\noindent
\textbf{Timonina Elena E.} (b.\ 1952)~--- Doctor of Science in technology, professor, leading scientist, 
Institute of Informatics Problems, Federal Research Center ``Computer Science and Control'' of the 
Russian Academy of Sciences, 44-2~Vavilov Str., Moscow 119133, Russian Federation; 
\mbox{eltimon@yandex.ru}

\vspace*{3pt}

\noindent
\textbf{Shorgin Sergey Ya.} (b.\ 1952)~--- Doctor of Science in physics and mathematics, professor, 
principal scientist, Institute of Informatics Problems, Federal Research Center ``Computer Science and 
Control'' of the Russian Academy of Sciences, 44-2~Vavilov Str., Moscow 119133, Russian 
Federation; \mbox{sshorgin@ipiran.ru}

   
\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература} 
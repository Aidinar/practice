\newcommand{\bomega}{\boldsymbol{\omega}}

\def\stat{rudoy}

\def\tit{МОДИФИКАЦИЯ ФУНКЦИОНАЛА КАЧЕСТВА\\ В~ЗАДАЧАХ НЕЛИНЕЙНОЙ РЕГРЕССИИ\\ 
ДЛЯ~УЧЕТА ГЕТЕРОСКЕДАСТИЧНЫХ ПОГРЕШНОСТЕЙ\\ ИЗМЕРЯЕМЫХ ДАННЫХ}

\def\titkol{Модификация функционала качества в~задачах нелинейной регрессии 
для~учета гетероскедастичных погрешностей} % измеряемых данных}

\def\aut{Г.\,И.~Рудой$^1$}

\def\autkol{Г.\,И.~Рудой}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Рудой Г.\,И.}
\index{Rudoy G.\,I.}


%{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
%{Работа выполнена при финансовой поддержке РФФИ (проекты 16-07-00677 
%и~15-37-20611-мол\_а\_вед).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Московский физико-технический институт, 
\mbox{0xd34df00d@gmail.com}}

\vspace*{-10pt}



\Abst{Рассматривается задача восстановления нелинейной
  регрессионной зависимости по данным, имеющим погрешности определения как 
зависимых,   так и~независимых переменных, при этом распределения погрешностей
  разных измерений могут иметь разную дисперсию.
  Предлагается модифицированный функционал качества,
  учитывающий по\-греш\-но\-сти определения независимых переменных и~разные 
рас\-пре\-де\-ле\-ния   погрешностей в~разных точ\-ках.
  Приводятся результаты численного моделирования на данных, полученных в~ходе
  эксперимента по измерению зависимости мощ\-ности лазера от прозрачности
  резонатора. Рассматривается сходимость вектора па\-ра\-мет\-ров, минимизирующего
  предлагаемый функционал качества, к~оптимальному для классического
  функционала сред\-не\-квад\-ра\-тич\-ной ошиб\-ки.
  Сравнивается сходимость па\-ра\-мет\-ров, оптимальных для предлагаемого
  и~классического функционалов, к~некоторым <<истинным>> параметрам
  модели на данных, сгенерированных со\-глас\-но этим <<истинным>> параметрам 
  и~зашумленным   со\-глас\-но предположениям о~погрешностях измерений, 
  в~зависимости от па\-ра\-мет\-ров  этих   погрешностей.}

\KW{гетероскедастичные ошибки;
  ошибки измерения независимых переменных; символьная регрессия; нелинейная 
регрессия}

%\vspace*{-6pt}

\DOI{10.14357/19922264170209} 


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}


\section{Введение}

\vspace*{-2pt}

В ряде приложений (см., например,~\cite{Gladun2004Labs,Rudoy15MonteCarlo})
возникает задача нахождения оптимальных
коэффициентов~$\bomega$ некоторой регрессионной модели~$f$, заданной
в~виде аналитической формулы, по набору экспериментальных данных. Для этого
в~предположении о~нормальном распределении регрессионных остатков
строится функционал $\sum\nolimits_i (y_i \hm- f(x_i, \bomega))^2$,
представляющий сумму квад\-ра\-тов отклонений экспериментальных точек~$y_i$ от
значения регрессионной кривой $f(x, \bomega)$ в~точке~$x_i$,
и~находится вектор параметров~$\bomega$, его минимизирующий.

Однако данный функционал корректен только для точно измеренных независимых
переменных и~гомоскедастичных ошибок измерения зависимой переменной:
в~част\-ности, для линейных моделей соответствующая оценка параметров является
несмещенной, со\-сто\-ятель\-ной и~наиболее эффективной только при выполнении этих
условий. В~случае\linebreak нелинейных моделей вывод функционала среднеквадратичной
ошибки согласно методу наибольшего прав\-до\-по\-до\-бия также опирается на
эти предположения (с~обобщением в~виде взвешенного метода\linebreak наименьших
квад\-ра\-тов (МНК) в~случае разных стандартных отклонений зависимой
переменной).
Иными словами, предполагается существование лишь ошибок измерения зависимой
переменной, распределение которых принимается одинаковым.

На практике, как правило, это предположение не выполняется,
особенно при измерениях в~достаточно широких диапазонах.
Например, в~задаче на\-хож\-де\-ния за\-ви\-си\-мости коэффициента преломления~$n$ 
прозрачного
полимера от длины волны~$\lambda$ по\-греш\-ности измерения каждого физического
па\-ра\-мет\-ра в~разных точках, вообще говоря, различны~\cite{Rudoy15MonteCarlo}.
Так, если для измерения длины волны~$\lambda$ используется дифракционная
решетка, то постоянной является относительная погрешность определения длины 
волны
${\sigma_{\lambda_i}}/{\lambda_i} \hm\approx {const}$ и,~следовательно,
погрешность определения длины волны зависит от самой длины волны. Подобная 
ситуация
фиксированной относительной (а~не абсолютной) ошибки является типичной для
физических экспериментов.

Таким образом, возникает задача поиска оптимальных коэффициентов регрессионной
формулы с~учетом различающихся погрешностей измерения в~разных экспериментальных 
точках. Для некоторых частных случаев эта задача решена.

Так, детальный обзор методов решения этой задачи
в~случае линейной регрессии приведен в~\cite{gillard2006historical}.
В~частности, для линейных моделей рассматривается даже более общая задача,
когда распределение ошибок не является точно известным.
Однако, по\linebreak крайней мере для ряда методов, априорная инфор\-мация все
равно необходима, как то: значение\linebreak отношения стандартных отклонений
зависимой и~независимой переменных
в~случае регрессии Деминга~\cite{Deming1943Statistical}
либо наличие инструментальных переменных
при использовании одноименного метода~\cite{Bowden1990Instrumental}.
Отметим, что дополнительная априорная информация необходима
для обеспечения воз\-мож\-ности однозначного определения параметров
модели, иначе модель становится неидентифицируемой. При этом
условие идентифицируемости модели для случая многомерной
линейной регрессии в~общем виде до сих пор неизвестно~\cite{Bekker1986Comment}.

Обзор методов решения аналогичной задачи для случая нелинейной
регрессии приведен в~\cite{Carrol06MeasurementErrors}.
Так, например, метод инструментальных переменных
обобщается на случай нелинейных моделей, при этом
опять же требуется наличие дополнительных наблюдаемых переменных,
пропорциональных регрессору с~точ\-ностью до некоторой аддитивной ошибки.
Заметим, что условия иден\-ти\-фи\-ци\-ру\-емости модели при этом неизвестны.

В ряде работ изучаются конкретные нелинейные
регрессионные модели, и~соответствующие ошибки измерений
предполагаются экспертно заданными.
Например, в~\cite{jukic2013nonlinear} рассматривается модель Басса,
описывающая динамику процесса распространения новых потребительских продуктов,
для которой вводится предположение о~неравной точ\-ности измерений в~разных
экспериментальных точках, что описывается разными весовыми коэффициентами при
соответствующих регрессионных остат\-ках. При этом весовые коэффициенты имеют 
достаточно общий вид и~вводятся произвольно в~виде экспертно указанных значений.

Другим примером является~\cite{jukic2010nonlinear}, где рассматривается задача 
оценки коэффициентов трех\-па\-ра\-мет\-ри\-че\-ско\-го распределения Вейбулла по неточно 
измеренным данным.
Для этого используется метод латентных переменных: 
к~независимым переменным~$t_i$ добавляются <<свободные>> переменные~$\delta_i$,
предоставляющие степень свободы в~про\-стран\-ст\-ве независимых переменных, 
и~минимизируется функционал вида
\begin{multline*}
  T(\alpha, \beta, \eta, \boldsymbol{\delta}) ={}\\
  {}= \sum\limits_{i = 0}^n w_i \left[f(t_i + 
\delta_i; \alpha, \beta, \eta) - y_i\right]^2 +\sum\limits_{i = 0}^n p_i \delta_i^2,
\end{multline*}
где $\alpha$, $\beta$ и~$\eta$~--- параметры распределения, а~$w_i$ и~$p_i$ являются
некоторыми экспертно заданными весами, соответствующими относительной точ\-ности
\mbox{$i$-го} измерения аналогично~\cite{jukic2013nonlinear}.

В настоящей работе рассмотрена более общая ситуация, в~которой не только 
зависимые, но и~независимые переменные определяются неточно и~каж\-дая переменная
имеет свою погрешность измерения, заданную экспертно.
Исследуется случай нелинейной регрессионной зависимости, в~отличие, 
например,  от~\cite{kiryati2000heteroscedastic}, где изучается линейная модель.
Предлагается модифицированный функционал качества, учитывающий погрешности как
зависимых, так и~независимых переменных в~виде, достаточном для большинства
практических приложений. Весовые коэффициенты при регрессионных остатках
в~настоящей работе выводятся из базовых предположений о~рас\-пре\-де\-ле\-нии
погрешностей измерения и~о~поведении регрессионной модели в~окрест\-ности каж\-дой
экспериментальной точки. В~част\-ности, оказывается, что весовые коэффициенты
зависят не только от самой по\-греш\-ности измерений в~данной точке, но и~от
производных регрессионной модели в~окрестности этой точки.

Предложенный функционал наиболее близок к~описанному в~\cite{Boggs1987Stable}.
Однако, кроме того, в~настоящей работе предлагается вероятностная интерпретация
 функционала для случая нормально распределенных ошибок.

В разд.~2 настоящей работы формально по\-став\-ле\-на задача нахождения
оптимальных па\-ра\-мет\-ров регрессионной модели с~учетом гетероскедастичных
по\-греш\-но\-стей определения как зависимых, так и~независимых переменных.
В~разд.~3 выводится предлагаемый функционал качества.
Затем, в~разд.~4, описывается метод использования имеющихся
алгоритмов оптимизации, при\-ме\-ня\-емых в~подобных задачах (как, например, алгоритм
Ле\-вен\-бер\-га--Марк\-вард\-та~\cite{Marquardt1963Algorithm}), для минимизации
предлагаемого функционала. В~разд.~5 приводятся результаты
вычислительного эксперимента, со\-сто\-ящие из трех частей: во-пер\-вых,
приводятся результаты анализа экспериментальных данных по измерению
параметров усиливающей среды газового лазера; затем сравнивается сходимость
оптимальных параметров для предложенного функционала качества к~параметрам,
минимизирующим классический функционал среднеквадратичной ошибки,
в~зависимости от па\-ра\-мет\-ров распределения ошибок; кроме того, фиксируется 
некоторый вектор па\-ра\-мет\-ров
модели, принимаемый <<истинным>>, согласно которому генерируется набор
зашумленных обучающих выборок, для которых анализируется сходимость па\-ра\-мет\-ров,
оптимальных для классического и~для предлагаемого функционалов качества,
к~<<истинным>> в~за\-ви\-си\-мости от параметров шума. Показано, что в~подавляющем 
большинстве рассмотренных случаев предложенный функционал дает лучшие приб\-ли\-жения.

\section{Постановка задачи}

Дана обучающая выборка~$D$:
\begin{equation}
  D = \left\{ \mathbf{x}_i, y_i\right \} | i \in \{ 1, \dots, \ell \}\,,\enskip
   \mathbf{x}_i \in  \mathbb{R}^m, y_i \in \mathbb{R}.
  \label{eq:d}
\end{equation}
Для каждой зависимой переменной~$y_i$ известно
стандартное отклонение ошиб\-ки ее измерения~$\sigma_{y_i}$, а для 
соответствующего вектора независимых переменных~$\mathbf{x}_i$ 
аналогично известны стандартные
отклонения его компонент $\sigma_{x_{ij}} | j \hm\in \{ 1, \dots, m \}$.
При этом допускается, что близ\-кие точки могут иметь сколь угодно различные  ошиб\-ки.
Кроме того, различные ошиб\-ки измерения независимы.

Для удобства введем вектор ошибок измерений зависимых переменных~$\sigma_{y_i}$:
$$
  \boldsymbol{\sigma}_y =\left \{ \sigma_{y_1}, \dots, \sigma_{y_{\ell}} \right\}\,.
$$

Аналогично введем матрицу ошибок измерений независимых переменных~$\sigma_{x_{ij}}$:
$$
  \Sigma_x = \| \sigma_{x_{ij}} \|\, | i \in \{ 1, \dots, \ell \}\,,\enskip j \in \{ 1, 
\dots, m \}\,.
$$
Отметим, что эта мат\-ри\-ца не является ковариационной мат\-ри\-цей ошибок
каждого конкретного объекта из обучающей выборки,
поэтому нельзя утверждать, что она является
диагональной (и,~более того, квадратной).

Пусть выбрана некоторая регрессионная модель
$y \hm= f (\mathbf{x}, \bomega)$, параметризованная вектором~$\bomega$.
Требуется построить функционал ошибки~$\breve{S}(\bomega)$ вектора 
па\-ра\-мет\-ров~$\bomega$ 
модели~$f$, учитывающий ошибки измерений~$\boldsymbol{\sigma}_y$ и~$\Sigma_x$:
\begin{equation}
  \breve{S}(\bomega) = \breve{S}\left(\bomega, \boldsymbol{\sigma}_y, \Sigma_x, D\right)\,,
  \label{eq:s_modified}
\end{equation}
и,~кроме того, найти вектор па\-ра\-мет\-ров~$\omega$, минимизирующий 
функционал~\eqref{eq:s_modified}:
\begin{equation*}
  \hat{\bomega} = \mathop{\arg \min}\limits_{\bomega} \breve{S}(\bomega)\,.
\end{equation*}

\section{Модифицированный функционал качества}

Воспользуемся следующим качественным соображением:
чем больше погрешность определения переменных (зависимых или независимых)
для некоторой экспериментальной точки, тем в~меньшей степени соответствующий
регрессионный оста\-ток должен учитываться при оптимизации па\-ра\-мет\-ров\linebreak\vspace*{-12pt}

\columnbreak

 { \begin{center}  %fig1
 \vspace*{1pt}
 \mbox{%
\epsfxsize=77.346mm
\epsfbox{rud-1.eps}
}
\end{center}

\vspace*{-1pt}


\noindent
{{\figurename~1}\ \ \small{Различные способы определения расстояния от точки до прямой: 
$\tilde{\rho}$~---
    истинное расстояние как минимум расстояния от точки $(x_i, y_i)$ до ка\-кой-ли\-бо
    точки на прямой; $y_i \hm- f(x_i, \bomega)$~--- расстояние в~классическом
    функционале среднеквадратичной ошибки в~предположении об отсутствии ошибок 
измерения     независимой переменной~$x$; $\rho$~--- предлагаемое расстояние}}
}

\vspace*{12pt}

\addtocounter{figure}{1}



\noindent
 модели.
Кроме того, с~физической точки зрения\linebreak складывать
можно только величины, име\-ющие одинаковую раз\-мер\-ность, либо безразмерные
величины, поэтому необходима соответствующая нормировка невязок по каждой
из переменных.

Для упрощения изложения рассмотрим случай одной независимой переменной:
$x \hm\in \mathbb{R}$. С~учетом приведенных выше соображений введем
сле\-ду\-ющее определение расстояния~$\rho(x, i)$
от точки $(x_i, y_i)$ до некоторой точ\-ки
$(x, f(x, \bomega))$ на кривой, описываемой регрессионной моделью $y \hm= f(x, 
\bomega)$:
\begin{equation}
  \tilde{\rho}^2(x, i) = \fr{(x_i - x)^2}{\sigma_{x_i}^2} + \fr{(y_i - f(x, 
\bomega))^2}{\sigma_{y_i}^2}\,.
  \label{eq:dist0}
\end{equation}



Непосредственное точное определение расстояния от экспериментальной
точки до регрессионной кривой представляется отдельной
сложной вы\-чис\-ли\-тель\-ной задачей
оптимизации (решаемой, например, итерационно),
поэтому предлагается рассматривать
расстояние от точки не до самой кривой, а~до
линеаризованной кривой в~окрестности этой точки. На рис.~1
показаны различные варианты определения рас\-сто\-яния, при этом
в~иллюстративных целях раз\-мер\-ности и~по\-греш\-ности определения~$x$ и~$y$ приняты
одинаковыми.

Итак, линеаризуем~$f(x, \bomega)$ в~окрестности точки $(x_i, f(x_i, \bomega))$,
обозначив оператор линеаризации в~окрестности этой точки~$\mathbb{L}_i$:
\begin{multline}
  f(x, \bomega) \approx \mathbb{L}_{i}[f](x, \bomega) = {}\\
  {}=f(x_i, \bomega) + \left(x - 
x_i\right) \fr{\partial f}{\partial x}\left(x_i, \bomega\right)\,.
  \label{eq:f_linear}
\end{multline}
Расстояние~\eqref{eq:dist0} выражается для линеаризованной функции~\eqref{eq:f_linear} 
сле\-ду\-ющим образом:

\noindent
\begin{multline*}
  \rho^2(x, i) = \fr{(x_i - x)^2}{\sigma_{x_i}^2} +{}\\
  {}+ \fr{\left(y_i - f\left(x_i, 
\bomega\right) - ({\partial f}/{\partial x})\left(x_i, \bomega\right) \left(x - 
x_i\right)\right)^2}{\sigma_{y_i}^2}\,.
  %\label{eq:dist_linear}
\end{multline*}
Минимизируя это выражение по~$x$:

\noindent
$$
  \hat{x} = \mathop{\arg \min}\limits_x \rho^2(x, i)\,,
$$
находим расстояние от точки $(x_i, y_i)$ из обуча\-ющей выборки до
линеаризованной в~ее окрест\-ности регрессионной за\-ви\-си\-мости~$f$ при
данном векторе па\-ра\-мет\-ров~$\bomega$:

\noindent
\begin{multline}
  \rho^2(f, \bomega, i) = \rho^2\left(\hat{x}, i\right) = {}\\
  {}=\fr{(y_i - f(x_i, 
\bomega))^2}{\sigma^2_{y_i} + ({\partial f}/{\partial x})\left(x_i, \bomega\right)^2 
\sigma^2_{x_i}}\,.
  \label{eq:rho_univar}
\end{multline}

Отметим, что решение~\eqref{eq:rho_univar} корректируется при
последовательном изменении линеаризации в~связи с~изменением вектора
па\-ра\-мет\-ров~$\bomega$ согласно выбранному итерационному методу решения
этой задачи.

Аналогично можно получить выражение для расстояния в~случае, когда объекты 
в~обучающей выборке представлены~$m$ независимыми переменными ($\mathbf{x}
\hm \in \mathbb{R}^m$):

\noindent
$$
  \rho^2(f, \bomega, i) = \fr{(y_i - f(\mathbf{x}_i, 
\bomega))^2}{\sigma_{y_i}^2 + \sum\nolimits_{j = 1}^m (({\partial f}/{\partial 
x_j})(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}}}\,.
$$

Таким образом, предлагаемый функционал, ми\-ни\-ми\-зи\-ру\-ющий сумму введенных
со\-глас\-но~\eqref{eq:dist0} рас\-сто\-яний с~учетом их линеаризации,
для достаточно глад\-ких функций выглядит следующим образом:
\begin{equation}
  \breve{S}(\bomega) = \sum\limits_{i = 1}^\ell\! \fr{(y_i - f(\mathbf{x}_i, 
\bomega))^2}{\sigma_{y_i}^2 + \sum\nolimits_{j = 1}^m (({\partial f}/{\partial 
x_j})(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}}}.\!
  \label{eq:s}
\end{equation}

Отметим следующее:
\begin{itemize}
  \item функционал~\eqref{eq:s} соответствует классической сумме квад\-ра\-тов 
регрессионных     остат\-ков при условии нормировки 
квад\-ра\-та каждого остатка на сумму 
квад\-ра\-тов по\-греш\-ности
    определения зависимой величины $\sigma_{y_i}$ и~произведения част\-ной 
производной     регрессионной модели по \mbox{$j$-й} компоненте вектора независимых величин на 
по\-греш\-ность     определения соответствующей компоненты~$\sigma_{x_{ij}}$;

  \item при прочих равных условиях в~выражении для расстояния~\eqref{eq:rho_univar} 
  и,~соответственно, в~функционале~\eqref{eq:s} с~большим весом учитываются те 
точ\-ки, в~которых производная регрессионной модели 
${\partial f}/{\partial x_j}$ по  соответствующей
    компоненте~$x_j$ больше, что соответствует соображениям здравого смысла: 
чем меньше наклон регрессионной зависимости в~окрест\-ности данной точки, тем меньше влияние 
неточного измерения соответствующей независимой переменной на значение регрессионной 
зависимости в~этой точке;

  \item если все независимые переменные измерены точно, т.\,е.\
    $\forall i, j : \sigma_{x_{ij}} \hm= 0$, то предложенный функционал переходит 
в~рассмотренный в~\cite{jukic2013nonlinear}. Если же, кроме того, все зависимые переменные 
имеют одну и~ту  же по\-греш\-ность~$\sigma_y$,
    то предложенный функционал переходит в~известную сумму квадратов 
регрессионных остатков с~точ\-ностью до некоторого множителя 
(а~именно ${1}/{\sigma_y}$), не 
влияющего на положения минимумов функционала среднеквадратичной ошибки.
\end{itemize}

Следует отметить возможность вероятностной интерпретации предложенного
выражения для рассто\-яния~\eqref{eq:dist0}.
Для случая одной независимой переменной предположим, что
вероятность соответствия некоторой точки $(\tilde{x}_i, f(\tilde{x}_i,  \bomega))$
на регрессионной кривой $y \hm= f(x, \bomega)$
данной экспериментальной точке $(x_i, y_i)$
описывается двумерным нормальным распределением
с~центром в~этой экспериментальной точке $(x_i, y_i)$
и~диагональной ковариационной мат\-ри\-цей
$$
\Sigma_i = \begin{Vmatrix} \sigma_{x_i}^2 & 0 \\[2pt] 
0 & \sigma_{y_i}^2  \end{Vmatrix}
$$
(т.\,е.\ ошибки измерения каждой координаты независимы):
\begin{multline*}
  P\left(\tilde{x}_i, f(\tilde{x}_i, \bomega)\right) \sim
   \mathcal{N}\left(\left(x_i, y_i\right), \Sigma_i\right)
    = \fr{1}{2 \pi \sqrt{\mathrm{det}\,\Sigma_i}}\times{}\\
    {}\times
            \exp \left\{\! -\fr{1}{2}\,
                        \left\|\begin{matrix} x_i - \tilde{x}_i \\ 
y_i - f(\tilde{x}_i, \bomega) \end{matrix}\right\|^\mathrm{T}
                        \Sigma_i^{-1}
                        \left\|\begin{matrix} x_i - \tilde{x}_i \\ 
y_i - f(\tilde{x}_i, \bomega) \end{matrix}\right\|
                 \right\}.\hspace*{-1.14958pt}
\end{multline*}
Максимизация логарифма правдоподобия
с~аналогичной~\eqref{eq:f_linear} линеаризацией
позволяет получить те же выражения~\eqref{eq:dist0} и~\eqref{eq:s}.
Более подробное рас\-смот\-ре\-ние такого подхода
и~следствий из него станет предметом дальнейшей работы.

\section{Метод оптимизации предложенного функционала}

Для численной оптимизации функционала~\eqref{eq:s} представим его в~виде
суммы квадратов регрессионных остатков путем следующего переобозначе-\linebreak\vspace*{-12pt}

\pagebreak

\noindent
ния 
переменных. Вместо выборки~\eqref{eq:d}
рас\-смот\-рим выборку 
$$
  \tilde{D} = \left\{ \tilde{\mathbf{x}}_i, \tilde{y}_i \right\} | i \in 
  \{ 1, \dots, \ell 
\},\enskip
 \tilde{\mathbf{x}}_i \in \mathbb{R}^{m + 1}\,,\enskip
  \tilde{y}_i \in \mathbb{R}\,,
$$
где $\tilde{y}_i \equiv 0$, а~$\tilde{\mathbf{x}}_i \hm= \{ \mathbf{x}_i, y_i \}$~--- 
исходный вектор~$\mathbf{x}_i$
с~дополнительно приписанным к~нему значением~$y_i$. Кроме того, примем
$$
  \tilde{f}(\tilde{\mathbf{x}}_i, \bomega) = \fr{f(\mathbf{x}_i, \bomega) - 
y_i}{\sqrt{\sigma_{y_i}^2 + \sum\nolimits_{j = 1}^m (({\partial f}/{\partial 
x_j})(\mathbf{x}_i, \bomega))^2 \sigma^2_{x_{ij}}}}.
$$
Тогда минимизация функционала~\eqref{eq:s} возможна известными методами 
оптимизации, так как прямой подстановкой можно убедиться, что~\eqref{eq:s} 
в~этом случае эквивалентен
$$
  S(\bomega) = \sum\limits_{i = 1}^\ell \left(
  \tilde{y}_i - \tilde{f}\left(\tilde{\mathbf{x}}_i,  \bomega\right)\right)^2.
$$

\begin{comment}
Легко показать, что градиент~$\tilde{f}$ по па\-ра\-мет\-рам выглядит следующим 
образом:
\begin{footnotesize}
\[
  \frac{\partial\tilde{f}}{\partial \omega_k}(\mathbf{x}_i, \bomega) = \frac{
        \frac{\partial f}{\partial \omega_k}(\mathbf{x}_i, \bomega) 
\Big(\sigma_{y_i}^2 + \sum_{j = 1}^m \big(\frac{\partial f}{\partial x_j} 
(\mathbf{x}_i, \bomega)\big)^2 \sigma_{x_{ij}}^2 \Big)-
        \big(f(\mathbf{x}_i, \bomega) - y_i\big) \sum_{j = 1}^m 
\sigma_{x_{ij}}^2 \frac{\partial f}{\partial x_j}(\mathbf{x}_i, \bomega) 
\frac{\partial^2 f}{\partial x_j \partial \omega_k}(\mathbf{x}_i, \bomega)}
  {\Big( \sigma_{y_i}^2 + \sum_{j = 1}^m \big(\frac{\partial f}{\partial x_j} 
(\mathbf{x}_i, \bomega)\big)^2 \sigma_{x_{ij}}^2 \Big)^{\sfrac{3}{2}}}.
\]
\end{footnotesize}
\end{comment}

Для таким образом преобразованного функционала
в~качестве базового алгоритма оптимизации может
быть использован любой метод решения зада\-чи о~наименьших квад\-ра\-тах, как, 
например,
метод градиентного спуска или алгоритм Ле\-вен\-бер\-га--Марк\-вард\-та~\cite{dlib09}.
В~этом случае при
соответствующих условиях глад\-кости частных производных функции~$f$ (что 
практически всегда выполняется в~реальных физических приложениях) 
сохраняются все свойства
исходного алгоритма.

Отметим, что предложенная идея введения весовых коэффициентов, отвечающих разным
измерениям и~зависящих от точности этих измерений, вообще говоря, применима 
и~для прочих методов решения задачи вос\-ста\-нов\-ле\-ния регрессии, отличных от символьной 
регрессии. Подробное рас\-смот\-ре\-ние этих методов в~совокупности с~предлага-\linebreak емым подходом 
выходит за рамки статьи, однако укажем, что при невозможности выполнить аналитическое
дифференцирование функции~$f$ предлагается использовать следующий
итеративный алгоритм, предназначенный для использования с~уже имеющимися
реализациями соответствующих методов оптимизации. Предполагается, что реализация
<<принимает на вход>> массив значений~$y_i$,
функцию вычисления значения~$f$ в~точках~$\mathbf{x}_i$ с~вектором па\-ра\-мет\-ров~$\bomega$.

Алгоритм выглядит следующим образом.
\begin{enumerate}[1.]
  \item Выбирается некоторое начальное приближение вектора па\-ра\-мет\-ров~$\bomega$.
  \item Для каждой пары $(\mathbf{x}_i, y_i)$ из обуча\-ющей выборки численно или
    аналитически рассчитывается значение част\-ной производной
    ${\partial f}/{\partial x}$ в~точке~$(\mathbf{x}_i, \bomega)$.
  \item Каждое значение зависимой переменной $y_i$ и~значение функции 
$f(\mathbf{x}_i, \bomega)$
    нормируется на соответствующую величину
$$
      \sigma_{y_i}^2 + \sum\limits_{j = 1}^m \left(\fr{\partial f}{\partial 
x_j}\left(\mathbf{x}_i, \bomega\right)\right)^2 \sigma^2_{x_{ij}}\,.
$$
  \item Выполняется итерация классического алгоритма оптимизации для таким 
образом модифицированных значений функции~$f$ и~зависимых переменных~$y_i$, 
получая     новое значение вектора~$\bomega$.
  \item Если критерий останова не достигнут, алгоритм продолжает выполнение 
  с~п.~2.
\end{enumerate}

Отметим следующее:
\begin{itemize}
  \item критерием останова могут служить обычные критерии, такие как достижение 
некоторого     числа итераций, порог нормы изменения вектора~$\bomega$ и~т.\,п.;
  \item если известно, что производная ${\partial f}/{\partial x}$ является 
достаточно гладкой в~окрестности $(\mathbf{x}_i, \bomega) \mid i \hm\in 
\{ 1, \dots, \ell \}$, 
на шаге ~4 алгоритма представляется разумным выполнить сразу несколько итераций
    классического алгоритма во избежание потенциально ресурсоемкого пересчета 
производных и~перенормировки значений~$y_i$ и~$f$.
\end{itemize}

\section{Вычислительный эксперимент}

В вычислительном эксперименте рассматриваются данные, полученные в~ходе 
измерения зависимости интенсивности излучения~$I$ лазера от прозрачности его резонатора.
Изучался лазер высокого давления ($\approx 3$~атм He, $\approx 60$~Торр\ Ne, 
$\approx 20$~Торр\ Ar) на
$3p$--$3s$ переходах неона (основной переход~--- 585~нм), возбуждаемый электронным 
пучком~\cite{alexandrov1991kinetics}.

Пусть насыщающая переход интенсивность излуче\-ния~--- $I_s$, наблюдаемая 
интенсивность~--- $I_l$. В~таком случае для безразмерной величины 
$y \hm= {I_l}/{I_s}$ с~учетом  однородного
уширения линии усиле\-ния при высоком давлении газа и~хорошей однородности 
возбуждения, обеспечиваемой электронным пучком, можно получить нелинейное 
уравнение~\cite{champagne1982transient}:
\begin{multline}
  \alpha_0 L - \fr{1}{2} \ln R_0 = g_0 L \fr{1 + \sqrt{R_0}}{1 - \sqrt{R_0}} \,
\fr{1}{y} \times{}
\\
{}\times \ln \left( 1 + \fr{y ({1 - \sqrt{R_0}})/({1 + \sqrt{R_0}})}{1 + y 
({2 \sqrt{R_0}})/({1 - R_0})} \right)\,,
  \label{eq:y_exact}
\end{multline}
где $\alpha_0$~--- распределенные потери (например, на рассеяние света);
$g_0$~--- коэффициент усиления слабого сигнала; $R_0$~--- коэффициент отражения 
выходного зеркала лазера. Однородность накачки означает, что~$g_0$ 
и~$\alpha_0$ одинаковы  по всему объему с~хорошей точностью.

Значение $R_0$ является независимой переменной, изменяемой экспериментаторами, 
и~в~данном разделе также обозначается~$x$ сообразно остальной части работы.

Для достаточно больших~$R_0$, близких к~единице (фактически для 
$R_0 \hm\geq 0{,}6\ldots0{,}7$),
можно упрос\-тить~\eqref{eq:y_exact}, заменив $2 \sqrt{R_0} \hm\approx 1 \hm+ R_0$ 
и~получив хорошо известное выражение~\cite{champagne1982transient}:
\begin{equation}
  y(R_0) = \gamma \fr{1 - R_0}{1 + R_0} \left(\fr{g_0}{\alpha_0 - 
({1}/({2L})) \ln R_0} - 1\right)\,,
  \label{eq:y_approx}
\end{equation}
где $\gamma$~--- нормировочный коэффициент.

В рассматриваемом физическом эксперименте длина активной среды $L$~--- 150~см,
точность определения мощности лазера~$y$ имеет
относительную погрешность в~2\%, точ\-ность определения прозрачности~$R_0$ имеет
абсолютную погрешность и~со\-став\-ля\-ет~0,01 при $R_0 \hm\geq 0{,}6$ и~0,02 при $R_0  \hm< 0{,}6$. 

В ходе измерений получены значения~$y(R_0)$, приведенные в~табл.~1.

\vspace*{6pt}

 {\small \begin{center}  %tabl1
 \noindent
\parbox{32mm}{{{\tablename~1}\ \ \small{Экспериментальные значения $y(R_0)$}}
}

\vspace*{2ex}


    \tabcolsep=12pt
    \begin{tabular}{|c|c|}
    \hline
    $R_0$   &   $y$\\
    \hline
    0,48 & \hphantom{9,}3,25\\
    0,56  &10,2\\
    0,65  &16,5\\
    0,73  &20,5\\
    0,80  &22,5\\
    0,87 &23,2\\
    0,94 &18,2 \\ 
    \hline
  \end{tabular}
  \end{center}
  }
%\end{table*}

%\vspace*{12pt}

\vspace*{6pt}

\addtocounter{table}{1}

Таким образом, решается задача минимизации функционала~\eqref{eq:s} при
\begin{align*}
  \bomega &= \left(\omega_1, \omega_2, \omega_3\right) = \left(\gamma, \alpha_0, g_0\right)\,;
\\
  f(x, \bomega) &= y\left(R_0, \gamma, \alpha_0, g_0\right)\,;
\end{align*}
\begin{equation}
\left.
  \begin{array}{rl}
    \sigma_{y_i} &= 0{,}02 y_i;\\
    \sigma_{x_i} &= \begin{cases}
        0{,}01 & \mid x_i \geq 0{,}6\,; \\
        0{,}02 & \mid x_i < 0{,}6\,.
        \end{cases}
      \end{array}
    \right\}
  \label{eq:sigmas_definition}
\end{equation}

\subsection{Оптимальные параметры модели}

Кроме предложенного в~настоящей работе функционала~\eqref{eq:s} рассмотрен
также и~классический функционал среднеквадратичной ошибки:
\begin{equation}
  S = \sum\limits_{i = 1}^\ell \left(y_i - f\left(x_i, \bomega\right)\right)^2\,.
  \label{eq:s_classic}
\end{equation}

 {\small \begin{center}  %tabl2
 \noindent
{{\tablename~2}\ \ \small{Оптимальные значения параметров модели}}


\vspace*{2ex}


    \begin{tabular}{|c|c|c|c|}
    \hline
    &&&\\[-9pt]
              Параметры  &  $\bomega$    &$\bomega^0$   & $\fr{|\omega_i - \omega^0_i|}{\omega^0_i}$\\
              \hline
            &&&\\[-9pt]
$g_0$ &  $2{,}93\cdot 10^{-3}$ & $2{,}92\cdot 10^{-3}$ & 0,31\%\\
$\alpha_0$   &$2{,}07\cdot 10^{-4}$& $2{,}22\cdot 10^{-4}$& 6,59\%\\
 $\gamma$      & 98,6& 101,5& 2,9\%\hphantom{9}\\
        $\breve{S}$~\eqref{eq:s} &  0,542& 0,645& 16\%\hphantom{999}\\
        $S$~\eqref{eq:s_classic} & 0,328& 0,183& 80\%\hphantom{999}\\ 
        \hline
  \end{tabular}
  \end{center}
  }
%\end{table*}

%\vspace*{12pt}

\vspace*{3pt}

\addtocounter{table}{1}

{ \begin{center}  %fig2
 \vspace*{1pt}
 \mbox{%
\epsfxsize=77.694mm
\epsfbox{rud-2.eps}
}
\end{center}

\vspace*{-1pt}


\noindent
{{\figurename~2}\ \ \small{Графики~\eqref{eq:y_approx}, соответствующие параметрам,
    минимизирующим~\eqref{eq:s} и~\eqref{eq:s_classic}:
    \textit{1}~--- экпериментальные данные;
    \textit{2}~--- $\omega^0$; \textit{3}~--- $\omega$}}
}

\vspace*{12pt}

\addtocounter{figure}{1}



В табл.~2 приведены значения параметров~$\bomega$ и~$\bomega^0$
функции~\eqref{eq:y_approx},
минимизирующие~\eqref{eq:s} и~\eqref{eq:s_classic} соответственно, а также 
относительные разности их компонент. Кроме того, приведены значения функционалов~\eqref{eq:s} 
и~\eqref{eq:s_classic} для обоих векторов параметров.



Отдельно отметим, что сравнивать непосредственные значения функционалов~\eqref{eq:s} 
и~\eqref{eq:s_classic} не имеет смысла. Вместо этого необходимо сравнивать 
различные модели по каждому из этих функционалов в~отдельности. Так, результаты, 
приведенные
в~табл.~2, показывают вполне естественный результат: каждый
из двух векторов параметров ($\bomega$ и~$\bomega^0$) является оптимальным лишь
для того функционала, который он минимизирует.

Графики регрессионной модели~\eqref{eq:y_approx}, соответствующие~$\bomega$ 
и~$\bomega^0$, приведены на рис.~2.


\subsection{Сходимость оптимальных параметров к~классическим}

Численно исследована зависимость сходимости параметров~$\bomega$ к~параметрам~$\bomega^0$,
получаемым минимизацией функционалов~\eqref{eq:s} и~\eqref{eq:s_classic} 
соответственно, от
погрешности~$\mathbf{\sigma}_y$ измерения зависимой переменной~$y$.

\begin{figure*}[b] %fig3
\vspace*{1pt}
\begin{center}
\mbox{%
\epsfxsize=162.722mm
\epsfbox{rud-3.eps}
}
\end{center}
\vspace*{-9pt}
   \Caption{Зависимость оптимальных параметров от $k \in [1; 100]$:
  (\textit{а})~$\sigma_{y_i} \hm= 0{,}02ky_i$;
  (\textit{б})~$\sigma_{y_i} \hm= 0{,}02ky_{\max}$; \textit{1}~--- классическое значение;
  \textit{2}~--- $g_0$; \textit{3}~--- $\alpha_0$;
  \textit{4}~--- $\gamma$}
  \label{fig:conv_varY}
\end{figure*}

Следует ожидать, что при увеличении погрешности измерения величины~$y$ при
фиксированной погрешности измерения~$R_0$ оптимальный вектор~$\bomega$
будет приближаться к~$\bomega^0$, так как тем более незначителен
вклад ошибки измерения независимой переменной.

Рассматриваются два случая.
\begin{enumerate}
  \item Погрешность $i$-го измерения~$y_i$ задается как $\sigma_{y_i} \hm= 
0{,}02ky_i$, т.\,е.\     погрешность зависит от значения самого~$y_i$.
  \item Погрешность $i$-го измерения~$y_i$ задается как $\sigma_{y_i} \hm= 
0{,}02ky_{\max}$,
    т.\,е.\ погрешность от значения конкретного~$y_i$ не зависит. Заметим, что 
выбор\linebreak конкретного значения~$y$, определяющего %\linebreak 
погрешность, является в~данном 
случае  достаточ\-но произвольным и~соответствует умножению всех погрешностей на 
некоторую константу     (что нивелируется соответствующим изменением выбора диапазона~$k$).
\end{enumerate}

В первом случае ошибки измерения~$y$ распределены неодинаково; следовательно, 
применение стандартного МНК не обосно\-ва\-но. В~то же время во втором 
случае ошибки принадлежат одному и~тому же распределению и,~кроме того, независимы, 
поэтому в~данном случае МНК-оцен\-ка применима (с~точностью до ошибки измерения 
независимой переменной).

Для обоих случаев подробно рассматривалась область $k\hm \in [1; 100]$, значение~$k$
изменялось с~шагом~0,01. Отметим, что уже при $k \hm\approx 25$ характерная 
погрешность измерения величины~$y$ сопоставима с~самой величиной~$y$, а при $k \hm> 50$ 
превышает ее.

Результаты приведены на рис.~\ref{fig:conv_varY}.
На графиках отображены компоненты вектора~$\bomega$, нормированные на
соответствующие значения~$\bomega^0$, в~за\-ви\-си\-мости от значения~$k$.



В случае фиксированной погрешности~$\sigma_{y_i}$ значения~$\bomega$
действительно стремятся к~$\bomega^0$ для разумных значений~$k$, а~в~случае
гетероскедастичных ошибок такой зависимости не наблюдается, хотя значения~$\bomega$
и~оказываются достаточно близки к~$\bomega^0$. По мнению автора, такое поведение
вектора оптимальных параметров является вполне ожидаемым и~демонстрирует
несостоятельность классического функционала качества в~случае неодинаково
распределенных ошибок.

\subsection{Сходимость параметров к~истинным}

Численно исследована зависимость сходимости параметров $\bomega \hm= \arg \min 
\breve{S}$
и~$\bomega^0 \hm= \arg \min S$ к~некоторому <<истинному>> значению вектора 
па\-ра\-мет\-ров~$\hat{\bomega}$ от числа точек~$\ell$ в~обучающей выборке и~от погрешности 
определения независимой переменной.

Для этого вектор параметров~$\bomega$, полученный минимизацией обучающей выборки 
из табл.~1, принимается за некоторый <<истинный>> вектор 
параметров~$\hat{\bomega}$
и~на каждой $j$-й итерации генерируется обучающая выборка $D_j(\ell, k)$:
\begin{multline*}
  D_j(\ell, k) = {}\\
  {}=\left\{ (x_i + \xi^x_i, y(x_i, \hat{\bomega}) + \xi^y_i) \right\} \mid 
\xi^x_i \sim \mathcal{N}(0, k \sigma_{x_i}),\\
\xi^y_i \sim \mathcal{N}(0, 
\sigma_{y_i})\,,\enskip i \in \{ 1, \dots, \ell \},
\end{multline*}
где $y(x, \bomega)$ задано соотношением \eqref{eq:y_approx}, а $\sigma_{x_i}$ 
и~$\sigma_{y_i}$
определяются соотношениями~\eqref{eq:sigmas_definition}.

\begin{figure*}[b] %fig4
\vspace*{3pt}
\begin{center}
\mbox{%
\epsfxsize=162.35mm
\epsfbox{rud-4.eps}
}
\end{center}
\vspace*{-9pt}
  \Caption{Сходимость параметров $g_0$~(\textit{a}),
  $\alpha_0$~(\textit{б}) и~$\gamma$~(\textit{в})
  к~истинным при $k \hm= 0{,}2$ (левый столбец) и~0,65
  (правый столбец):
   \textit{1}~--- $\omega^0$;
  \textit{2}~--- $\omega$}
  \label{fig:comparison_0.2}
\end{figure*}

Иными словами, генерируется обучающая выборка согласно искомой модели 
с~известным и~фиксированным вектором параметров, которая затем зашумляется 
нормально распределенными случайными величинами. При этом стандартное отклонение
шума для зависимой величины совпадает с~экспертно предложенной погрешностью
измерений для реального эксперимента, а~стандартное отклонение независимой
величины отличается от экспертно предложенной погрешности для этой величины
в~$k$~раз.

После генерации выборки $D_j(\ell, k)$ по ней находятся значения $\bomega_j \hm= 
\arg \min \breve{S}(D_j)$
и $\bomega_j^0 \hm= \arg \min S(D_j)$, что повторяется~$N$~раз, и~$\forall\ i$ 
рассматриваются значения:

\noindent
\begin{align*}
  \overline{\delta \omega_i} &= \fr{\sum\nolimits_{j = 1}^N (\omega_{ji} - 
\hat{\omega}_i)}{N}\,;
\\
  \overline{\delta \omega^0_i} &= \fr{\sum\nolimits_{j = 1}^N (\omega^0_{ji} - 
\hat{\omega}_i)}{N}\,.
\end{align*}

\vspace*{-4pt}

Обозначим, кроме того, $\overline{\delta \bomega} \hm= \{ \overline{\delta 
\omega_1}, \overline{\delta \omega_2}, \overline{\delta \omega_3} \}$.

Таким образом, при варьировании~$k$ и~изуче\-нии поведения~$\overline{\delta 
\bomega}$ и~$\overline{\delta \bomega^0}$ исследуется влияние по\-греш\-ности определения
независимой переменной на раз\-ность между~$\hat{\bomega}$ и~оптимальными
па\-ра\-мет\-ра\-ми~$\bomega$ и~$\bomega^0$ согласно~\eqref{eq:s} и~\eqref{eq:s_classic}
соответст\-венно.
{\looseness=1

}

Так как при уменьшении~$k$ монотонно уменьшается погрешность определения 
независимой перемен\-ной, естественно ожидать, что различие между~$\bomega$ и~$\bomega^0$ 
будет уменьшаться. Однако вы\-чис\-ли\-тельный эксперимент демонстрирует, что это не так.

В проведенном эксперименте $N \hm= 1000$, $\ell \hm\in \{ 10; \dots; 5000 \}$,
$k \hm\in \{ 0{,}2; 0{,}25; \dots; 1 \}$.

Результаты представлены на рис.~4 и~5.






Результаты на рис.~4 (левый столбец) характерны для всех
значений $k \hm\in [0{,}2; 0{,}6]$. Однако при $k\hm \geq 0{,}65$ 
поведение оптимальных
параметров резко меняется. Так, на рис.~4 (правый столбец) приведены
графики сходимости для $k\hm = 0{,}65$. Видно, что поведение параметров,
оптимизированных согласно классическому функционалу качества~$S$, является
существенно более хаотическим, что может говорить о меньшей 
устойчивости~\cite{Rudoy16StabilityAnalysis} модели, оптимизированной согласно~$S$.

Более того, для оценок параметров~$g_0$ и~$\alpha_0$ соответствующее
приближение на несколько порядков хуже, чем полученное минимизацией~$\breve{S}$,
вплоть до того, что кривые, соответствующие минимизирующим~$\breve{S}$ 
параметрам,
практически не видны на графиках (см.\ рис.~4,\,\textit{а} 
и~4,\,\textit{б}, правый столбец), поскольку в~выбранном масштабе они
практически совпадают с~осью абсцисс. С~другой стороны, важно отметить, что 
оценка параметра~$\gamma$, полученная минимизацией~$S$, 
является несколько лучшей для
$k \hm= 0{,}65$ (для $k \hm= 0{,}7$ график выглядит аналогично), но 
минимизация~$\breve{S}$
дает все лучшие и~лучшие приближения с~ростом~$k$ (см.\ 
рис.~5).

Отметим следующее:
\begin{itemize}
  \item практически во всех случаях (кроме оценки~$\gamma$ для $k \hm= 0{,}8$)
    предложенный в~настоящей работе функционал~\eqref{eq:s}
    дает лучшее приближение, в~том числе при разумно малом объеме обуча\-ющей 
выборки. Кроме того, в~подав\-ля\-ющем большинстве случаев предпочтительность 
предложенного  функционала  сохраняется и~для большего числа экспериментальных точек;\\[-8pt]
  \item для малых $k \hm\leq 0{,}6$ ошибка оценки параметров при помощи классического
    функционала~\eqref{eq:s_classic} имеет ярко выраженный минимум 
    в~окрест\-ности 60--100 для~$\alpha_0$ и~$\gamma$ и~400 для~$g_0$ 
    экспериментальных точек  (см.\ рис.~4, левый столбец).
    Дальнейшее увеличение обучающей выборки ведет к~ухудшению приближения, 
получаемого минимизацией~\eqref{eq:s_classic};
 \end{itemize}
 
 %\columnbreak

 { \begin{center}  %fig5
 \vspace*{1pt}
 \mbox{%
\epsfxsize=70.982mm
\epsfbox{rud-6.eps}
}
\end{center}

\vspace*{-1pt}


\noindent
{{\figurename~5}\ \ \small{Сходимость параметра $\gamma$ при $k \hm= 0{,}8$~(\textit{а}),
    0,9~(\textit{б}) и~1,0~(\textit{в}): \textit{1}~--- $\omega^0$;
    \textit{2}~--- $\omega$}}
}

\vspace*{3pt}

\addtocounter{figure}{1}

\

\begin{itemize}
 \item для некоторых~$k$ ошибка приближения, получаемого минимизацией 
предложенного функционала~\eqref{eq:s}, имеет явную горизонтальную асимптоту 
(см.\ рис.~4,\,\textit{а}, 4,\,\textit{б} и~4,\,\textit{в} (левый столбец)).
\end{itemize}

Причины подобного поведения оптимальных параметров являются предметом
дальнейших исследований.

\section{Заключение}

Предложен модифицированный функционал среднеквадратичной ошибки для задач
регрессии, применимый в~случае наличия ошибок измерения независимых
переменных и~различных распределений, к~которым принадлежат ошибки, в~разных
точках обучающей выборки. Предложена вероятностная интерпретация этого
функционала для случая нормального распределения ошибок измерения.

Показана сходимость предложенного функционала к~классическому функционалу
сред\-не\-квад\-ра\-тич\-ной ошибки для случая гомоскедастичности погрешностей
зависимой переменной и~пренебрежимо малой погрешности измерения независимых
переменных.

Исследовано поведение оптимального вектора параметров для предлагаемого
функционала в~зависимости от параметров распределений ошибок независимых
переменных, в~том числе в~сравнении с~вектором параметров, минимизирующим
классический функционал качества.

Представляется разумным использовать предложенный в~настоящей работе функционал
при оптимизации параметров регрессионных моделей и~анализе их
устойчивости к~погрешностям как зависимых, так и~независимых
переменных~\cite{Rudoy15MonteCarlo,Rudoy16StabilityAnalysis}.


{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{Gladun2004Labs}
\Au{Гладун А.\,В.}  
Лабораторный практикум по общей физике.~---
М.: МФТИ, 2004. 316~с.

\bibitem {Rudoy15MonteCarlo}
\Au{Рудой Г.\,И.}  О~возможности применения методов Мон\-те-Кар\-ло в~анализе
нелинейных регрессионных моделей~//
Сиб. ж. вычисл. мат., 2015. Т.~4. С.~425---434.

\bibitem {gillard2006historical}
\Au{Gillard J.\,W.} 2006.  
An historical overview of linear regression with errors in both variables.
Cardiff University School of Mathematics. Technical Report.

\bibitem {Deming1943Statistical}
\Au{Deming W.\,E.}  {Statistical adjustment of data.}~---
New York, NY, USA: Wiley, 1943.  216~p.

\bibitem {Bowden1990Instrumental}
\Au{Bowden R.\,J., Turkington D.\,A.}
{Instrumental variables}.~---
Cambridge: Cambridge University Press, 1990. 236~p.

\bibitem {Bekker1986Comment}
\Au{Bekker~P.\,A.}  Comment on
  identification in the linear errors in variables model~//
{Econometrica}, 1986. Vol.~54. No.\,1. P.~215--217.

\bibitem {Carrol06MeasurementErrors}
\Au{Carroll~R.\,J., Ruppert~D, Stefanski~L.\,A., Crainiceanu~C.\,M.} 
{Measurement error in nonlinear models: A~modern perspective}.~---
London: Chapman and Hall/CRC, 2006. 484~p.

\bibitem {jukic2013nonlinear}
\Au{Juki$\acute{\mbox{c}}$~D.} 
On nonlinear weighted least squares
  estimation of bass diffusion model~//
{Appl. Math. Comput.}, 2013.  Vol.~219. No.\,14. P.~7891--7900.

\bibitem {jukic2010nonlinear}
\Au{Juki$\acute{\mbox{c}}$~D.,
Markovi$\acute{\mbox{c}}$~D.} 
On nonlinear weighted errors-in-variables
  parameter estimation problem in the three-parameter Weibull model~//
{Appl. Math. Comput.}, 2010. Vol.~215. No.\,10. P.~3599--3609.

\bibitem {kiryati2000heteroscedastic}
\Au{Kiryati N., Bruckstein~A.\,M.} 
Heteroscedastic hough transform (HtHT): An
  efficient method for robust line fitting in the `errors in the
  variables' problem~//
{Comput. Vis. Image Und.}, 2000. Vol.~78. No.\,1. P.~69--83.

\bibitem {Boggs1987Stable}
\Au{Boggs~P.\,T., Byrd~R.\,H., Schnabel~R.\,B.}
A~stable and efficient algorithm for nonlinear orthogonal distance regression~//
{SIAM J.~Sci. Stat.  Comp.},   1987. Vol.~8. No.\,6. P.~1052--1078.

\bibitem {Marquardt1963Algorithm}
\Au{Marquardt~D.\,W.}  An algorithm for
  least-squares estimation of non-linear parameters~//
{J.~Soc. Ind. Appl.   Math.}, 1963. Vol.~11. No.\,2. P.~431--441.

\bibitem {dlib09}
\Au{King D.\,E.}  Dlib-ml: A~machine
learning toolkit~// {J.~Mach. Learn. Res.}, 2009. Vol.~10. P.~1755--1758.

\bibitem {alexandrov1991kinetics}
\Au{Александров~А.\,Ю., Долгих~В.\,А.,
Рудой~И.\,Г., Сорока~А.\,М.} Кинетика возбуждаемого электронным пучком 
лазера высокого давления на <<желтой>> линии неона~//
Квантовая электроника, 1991. Т.~18. №\,9. С.~1029--1033.

\bibitem {champagne1982transient}
\Au{Champagne L.\,F.}  Transient optical absorption 
in the ultraviolet~// \textit{Applied atomic collision physics. Vol.~3: Gas lasers}~/
Eds. E.\,W.~McDaniel, W.\,L.~Nighan.~--- Amsterdam, Netherlands:
Elsevier, 1982.  349--386.

\bibitem {Rudoy16StabilityAnalysis}
\Au{Rudoy~G.\,I.}  Analysis of the
  stability of nonlinear regression models to errors in measured data~//
{Pattern Recognit. Image Anal.}, 2016. Vol.~26. No.\,3. P.~608--616.
 \end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-3pt}

\hfill{\small\textit{Поступила в~редакцию 15.09.16}}

%\vspace*{8pt}

\newpage

\vspace*{-28pt}

%\hrule

%\vspace*{2pt}

%\hrule

\vspace*{-2pt}


\def\tit{ON MODIFICATION OF THE MEAN SQUARED ERROR LOSS FUNCTION FOR~SOLVING NONLINEAR 
HETEROSCEDASTIC ERRORS-IN-VARIABLES PROBLEMS}

\def\titkol{On modification of the mean squared error loss function for~solving nonlinear 
heteroscedastic errors-in-variables problems}

\def\aut{G.\,I.~Rudoy}

\def\autkol{G.\,I.~Rudoy}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-9pt}


\noindent
Moscow Institute of Physics and Technology, 9~Institutskiy Per.,
Dolgoprudny, Moscow Region 141700, Russian Federation


\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2017\ \ \ volume~11\ \ \ issue\ 2}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2017\ \ \ volume~11\ \ \ issue\ 2
\hfill \textbf{\thepage}}}

\vspace*{3pt}


\Abste{The paper considers the problem of finding the optimal parameters of 
a~nonlinear regression model accounting for errors in both dependent and
  independent variables. The errors of different measurements are assumed to
  belong to different probability distributions with different variances.
  A~modified mean squared error-based loss function is derived and analyzed
  for this case.
  In the computational experiment, the measurements of the laser's radiation
  power as a~nonlinear function of the resonator's transparency are used to
  compare the parameters vectors minimizing the presented
  loss function and the classical mean squared error.
  The convergence of the parameters minimizing the presented loss function
  to the optimal parameters for the classical loss function is studied.
  In addition, some values of the parameters are considered to be ``true''
  ones and are used to generate synthetic data using the physical model and
  Gaussian noise, which is then used to study the convergence of the parameters
  minimizing the presented and the classical loss function, respectively, as
  the function of the noise parameters.}

\KWE{errors-in-variables models; heteroscedastic errors;
  symbolic regression; nonlinear regression}
  
\DOI{10.14357/19922264170209} 

%\vspace*{-18pt}

%\Ack
%\noindent



%\vspace*{3pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}

\bibitem{Gladun2004Labs-en}
\Aue{Gladun, A.\,V.} 2004. \textit{Laboratornyy praktikum po obshchey fizike} 
[Laboratory classes on general  physics].
Moscow: MFTI. 316~p.

\bibitem {Rudoy15MonteCarlo-en}
\Aue{Rudoy, G.\,I.} 2015. {Applying
    Monte Carlo methods to analysis of nonlinear regression
    models}.
\textit{Numer. Anal. \mbox{Appl.}} 4:344--350.

\bibitem {gillard2006historical-en}
\Aue{Gillard, J.\,W.} 2006.  
{An historical overview of linear regression with errors in both variables}.
Cardiff University School of Mathematics. Technical Report.

\bibitem {Deming1943Statistical-en}
\Aue{Deming, W.\,E.} 1943. \textit{Statistical adjustment of data.}
New York, NY: Wiley. 216~p.

\bibitem {Bowden1990Instrumental-en}
\Aue{Bowden, R.\,J., D.\,A.~Turkington}.
 1990. \textit{Instrumental variables}.
Cambridge: Cambridge University Press. 236~p.

\bibitem {Bekker1986Comment-en}
\Aue{Bekker, P.\,A.} 1986. Comment on
  identification in the linear errors in variables model.
\textit{Econometrica} 54(1):215--217.

\bibitem {Carrol06MeasurementErrors-en}
\Aue{Carroll, R.\,J., D.~Ruppert, L.\,A.~Stefanski, and C.\,M.~Crainiceanu.} 2006.
\textit{Measurement error in nonlinear models: A~modern perspective}.
London: Chapman and Hall/CRC. 484~p.

\bibitem {jukic2013nonlinear-en}
\Aue{Juki$\acute{\mbox{c}}$,~D.} 2013.
On nonlinear weighted least squares
  estimation of bass diffusion model.
\textit{Appl. Math. Comput.}  219(14):7891--7900.

\bibitem {jukic2010nonlinear-en}
\Aue{Juki$\acute{\mbox{c}}$, D.,
and D.~Markovi$\acute{\mbox{c}}$.} 2010.
On nonlinear weighted errors-in-variables
  parameter estimation problem in the three-parameter Weibull model.
\textit{Appl. Math. Comput.} 215(10):3599--3609.

\bibitem {kiryati2000heteroscedastic-en}
\Aue{Kiryati, N., and A.\,M.~Bruckstein.} 2000.
Heteroscedastic hough transform (HtHT): An
  efficient method for robust line fitting in the errors in the
  variables' problem.
\textit{Comput. Vis. Image Und.} 78(1):69--83.

\bibitem {Boggs1987Stable-en}
\Aue{Boggs, P.\,T., R.\,H.~Byrd, and R.\,B.~Schnabel.}
  1987. A~stable and efficient algorithm for nonlinear orthogonal distance regression.
\textit{SIAM J.~Sci. Stat.  Comp.} 8(6):1052--1078.

\bibitem {Marquardt1963Algorithm-en}
\Aue{Marquardt, D.\,W.} 1963. An algorithm for
  least-squares estimation of non-linear parameters.
\textit{J.~Soc. Ind. Appl.  Math.} 11(2):431--441.

\bibitem {dlib09-en}
\Aue{King, D.\,E.} 2009. Dlib-ml: A~machine
  learning toolkit.
\textit{J.~Mach. Learn. Res.} 10:1755--1758.

\bibitem {alexandrov1991kinetics-en}
\Aue{Aleksandrov, A.\,Yu., V.\,A.~Dolgikh,
I.\,G.~Rudoy, and A.\,M.~Soroka.} 1991.
Kinetics of a high-pressure electron-beam-excited laser emitting the ``yellow'' 
neon line. \textit{Sov. J.~Quantum Electronics} 21(9):933--937.

\bibitem {champagne1982transient-en}
\Aue{Champagne, L.\,F.} 1982. {Transient optical absorption 
in the ultraviolet}. \textit{Applied atomic collision physics. Vol.~3: Gas lasers}.
Eds. E.\,W.~McDaniel and W.\,L.~Nighan. Amsterdam, Netherlands: Elsevier.
349--386.

\bibitem {Rudoy16StabilityAnalysis-en}
\Aue{Rudoy, G.\,I.} 2016. Analysis of the
  stability of nonlinear regression models to errors in measured data.
\textit{Pattern Recognit. Image Anal.} 26(3):608--616.
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Received September 15, 2016}}

\vspace*{-18pt}

\Contrl

\noindent
\textbf{Rudoy Georg I.} (b.\ 1991)~--- PhD student,
Moscow Institute of Physics and Technology, 9~Institutskiy Per., Dolgoprudny,
Moscow Region 141700, Russian Federation;
\mbox{0xd34df00d@gmail.com}


\label{end\stat}


\renewcommand{\bibname}{\protect\rm Литература} 
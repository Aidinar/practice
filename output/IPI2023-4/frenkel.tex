\def\stat{frenkel}

\def\tit{МОДЕЛИ УЧЕТА ВЛИЯНИЯ СТАТИСТИЧЕСКИХ ХАРАКТЕРИСТИК ТРАФИКА 
ВЫЧИСЛИТЕЛЬНЫХ СЕТЕЙ НА~ЭФФЕКТИВНОСТЬ ПРОГНОЗИРОВАНИЯ 
СРЕДСТВАМИ МАШИННОГО ОБУЧЕНИЯ}

\def\titkol{Модели учета влияния статистических характеристик трафика ВС %вычислительных сетей 
на эффективность прогнозирования} %средствами МО} %машинного обучения

\def\aut{С.\,Л.~Френкель$^1$, В.\,Н.~Захаров$^2$}

\def\autkol{С.\,Л.~Френкель, В.\,Н.~Захаров}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Френкель С.\,Л.}
\index{Захаров В.\,Н.}
\index{Frenkel S.\,L.}
\index{Zakharov V.\,N.}


%{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
%{Работа выполнено с~использованием инфраструктуры Центра коллективного пользования <<Высокопроизводительные вы\-чис\-ле\-ния и~большие данные>> 
%(ЦКП <<Информатика>>) ФИЦ ИУ РАН (г.~Москва).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Федеральный исследовательский центр <<Информатика и~управление>> Российской академии наук, 
\mbox{fsergei51@gmail.com}}
\footnotetext[2]{Федеральный исследовательский центр <<Информатика и~управление>> Российской 
академии наук, \mbox{vzakharov@ipiran.ru}}

%\vspace*{-7pt}

  
  
  

      
  
  \Abst{Статья представляет собой попытку некоторого упорядочения и~категоризации 
огромного потока публикаций по современным методам, техникам и~моделям прогнозов 
данных различной природы с~точки зрения их применимости для прогнозирования 
трафика в~вычислительных сетях (ВС). Указанное упорядочение выполняется в~рамках 
предложенной концептуальной модели (КМ) алгоритмов прогнозирования.
  В~рамках этой КМ выделены характеристики как моделей трафика 
ВС, так и~методов управления трафиком, которые явно или неявно 
могут быть используемы в~современных программных инструментах предсказания. 
Показано, что анализ таких вероятностных аспектов описания данных, как наличие 
существенной нестационарности, некоторых нелинейных эффектов в~моделях данных, 
а~также специфики законов распределения данных, позволяет воздействовать на 
эффективность обучения предикторов.}
  
  \KW{сетевой трафик; вероятностные модели прогнозирования}
  
  \DOI{10.14357/19922264230410}{CQYNFJ}
  
\vspace*{-2pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}
  
  \section{Введение}

\vspace*{-2pt}
   
  Значительные успехи в~развитии систем прогнозирования, достигнутые 
в~рамках работ по машинному обучению (МО), прежде всего на базе 
нейронных сетей, не устраняют тем не менее трудности их использования. 
Среди многих причин этого можно указать также на то, что современные 
методы МО не учитывают некоторые математические свойства пред\-ска\-зы\-ва\-емых 
случайных данных. Это 
происходит вследствие того, что в~сис\-те\-мах МО формирование прогноза 
значений по обуча\-ющим выборкам данных час\-то выполняется как поиск 
внутренних закономерностей и~связей без использования ка\-ких-ли\-бо 
математических вероятностных моделей данных. Это относится и~к методам, 
использующим явные математические модели связи про\-гно\-зи\-ру\-емо\-го 
значения в~будущем, такие как методы линейной, полиномиальной или 
экспоненциальной регрессии~[1], но не учи\-ты\-ва\-ющим математические 
свойства вход\-ных данных, например автокорреляционные. Эти 
обстоятельства час\-то дополняются непрозрачностью преобразования 
входных данных в~пространство при\-зна\-ков (features), например 
в~алгоритмах глубокого обуче\-ния (deep learning) [1], что за\-труд\-ня\-ет 
априорную оценку влияния на результат тех или иных свойств входных 
данных. 
  
  Статья представляет собой анализ публикаций по современным методам 
прогнозирования поведения вычислительных сетей, основанных на 
использовании программных инструментов прогнозирования (ИП) МО, 
в~котором основное внимание уделяется способам учета влияния 
характеристик прогнозируемых данных на эффективность прогноза трафика 
с~учетом математических свойств моделей предсказания. 
  
  Для структуризации огромного массива пуб\-ли\-ка\-ций по современным 
методам, техникам и~моделям прогнозов, на котором выполнен данный 
анализ, предлагается классификация моделей \mbox{предсказания}, используемых 
в~современных про\-грам\-мных МО, с~использованием предложенного в~[2] 
обобщенного пред\-став\-ле\-ния \textit{концептуальной модели} задачи 
предсказания, широко ис\-поль\-зу\-емо\-го в~практике больших данных 
и~программной инженерии. {\bfseries\textit{Концептуальная модель}}~--- это набор <<точек зрения>> об элементах и~целях мо\-де\-ли\-ру\-емой 
сис\-те\-мы, а~так\-же отношений между ними, выраженных в~терминах той или 
иной тео\-рии~[3]. 
  
  В статье дается классификация подходов к~задаче предсказания~[2], 
выделяются основные ее элементы, дается общая характеристика задачи 
предсказания поведения ВС на основе 
инструментов МО, отмечается связь задачи пред\-ска\-за\-ния 
будущих значений временн$\acute{\mbox{о}}$го ряда с~общей задачей МО~[4] (разд.~2). 
Формулируются требования \textbf{к~математическим моделям 
предсказания} (разд.~3). Анализируется влияние вероятностных свойств 
случайных временн$\acute{\mbox{ы}}$х рядов на эффективность предсказания, в~частности 
возможность предсказания без точного знания вероятностных 
распределений предсказываемых данных (разд.~4). Показано, что ряд 
характеристик отражает нелинейную связь прошлых и~будущих значений 
временн$\acute{\mbox{ы}}$х рядов, моделирующих трафик, поэтому рассматриваем 
нелинейные алгоритмы предикторов (разд.~5). 

%\vspace*{-6pt}
  
\section{Постановка задачи предсказания }

%\vspace*{-3pt}

  Хотя вероятностные модели предсказания случайных 
последовательностей, рядов и~процессов имеют более чем 
восьмидесятилетнюю историю (Колмогоров, Винер~[5]), интенсивные 
исследования в~этой области продолжаются до сих пор. Это\linebreak связано 
с~огромным разнообразием задач, с~которыми сталкиваются при обработке 
больших массивов данных. Для этих задач могут требоваться различные 
критерии точности предсказания, \mbox{различные} модели данных, различные 
требования к~времени (оперативности) получения прогноза. Например, для 
многих задач предсказания значений финансовых индексов используется 
критерий минимума среднеквадратичной ошибки (MSE,\linebreak mean squared error) предсказанных данных 
относительно истинных значений (на которой основаны критерий  
Кол\-мо\-го\-ро\-ва--Ви\-не\-ра и~его многочисленные линейные 
модификации). Но этот \mbox{критерий} заведомо не подходит, когда требуется 
предсказать знак приращения. В~одних задачах вполне разумно 
использовать в~качестве критерия эффективности предсказания некоторые 
усредненные по большим прошлым выборкам значения, в~других нужны 
локальные оценки. Есть алгоритмы предсказания, которые используют 
числовые (измеряемые) данные без ка\-ких-ли\-бо предположений об их 
статистических и~вероятностных свойствах (например, нейронные сети~[1]), 
а~есть неявно рассматривающие данные как выборку из нормального 
стационарного процесса~[1] или марковской последовательности~[6]. В~[2] 
была обоснована классификация подходов к~задаче предсказания, которой 
будем пользоваться в~дальнейшем:
  \begin{itemize}
\item математическая задача оценки неизвестных условных распределений 
вероятности (тео\-ре\-ти\-ко-ве\-ро\-ят\-ност\-ный);
  \item математическая задача выбора оптимального решения (DM, decision 
making);
  \item алгоритмическая задача управления.
  \end{itemize}
  
  В \textit{теоретико-вероятностном} подходе~[7] задача предсказания 
состоит в~следующем: пусть в~момент времени~$t$ выполнена, по реализации 
процесса $x_1, x_2, \ldots, x_t$, рассматриваемого (здесь и~в~дальнейшем) 
как \textit{временной ряд}~[8], оценка условных вероятностей 
$\gamma(x_{t+1}\vert x_1, x_2, \ldots, x_t)$ того, что в~момент $t\hm+1$ увидим 
значение, которое предсказываем. Чем точнее (в~принятой метрике) эта 
оценка, тем лучше прогноз. Математическая модель оценки 
$\gamma(x_{t+1}\vert x_1, x_2, \ldots, x_t)$, как и~сама условная вероятность 
$\gamma(\,)$, называется в~литературе предиктором~]7].
  
  Предикторами наряду с~$\gamma(\,)$ также называют программные 
продукты (например, некоторые модули облачных MS AWS Azure ML, 
Google Cloud ML и~т.\,д.), которые используются для предсказания.
  
  В \textit{DM-постановке} формулируется сле\-ду\-ющая задача пред\-ска\-за\-ния 
с~учетом требования эф\-фек\-тив\-ности (точ\-ности) прогноза: для заданного 
\mbox{про\-стран\-ст\-ва} стратегий~$B$ возможных пред\-ска\-зы\-ва\-емых значений~$X$ 
и~функции потерь $l(b, x)$, $b \hm\in B$, $x \hm\in X$, выбрать как 
пред\-ска\-зы\-ва\-емое сле\-ду\-ющее значение $\hat{\,}b_t$ при знании прош\-лых 
со\-сто\-яний $x_1, \ldots, x_{t-1}$ так, чтобы общие потери от ошибки 
пред\-ска\-за\-ния (<<проигрыш>>) были бы асимптотически близ\-ки к~проигрышу, полученному наилучшей фиксированной стратегией, известной 
апостериорно после про\-смот\-ра всей по\-сле\-до\-ва\-тель\-ности $x \hm= x_1, \ldots, 
x_{t-1}$, т.\,е. предсказанное~$\hat{\,}b_t$ долж\-но минимизировать возможные 
апостериорные потери. \textit{Стратегия} пред\-став\-ля\-ет собой функцию, 
воз\-вра\-ща\-ющую выходной вектор~$y$~для каж\-дой по\-сле\-до\-ва\-тель\-ности~$x$ 
в~соответствии с~функцией услов\-но\-го распределения $P(y\vert )$ (или 
$\gamma (\,)$)~[7, 9].
  
  \textit{Третья} постановка задачи прогнозирования,\linebreak которую назовем 
\textit{алгоритмической моделью предсказания}, состоит в~использовании 
различных концептуальных моделей получения прогноза, не \mbox{содержащих} 
явно ка\-ких-ли\-бо основных элементов\linebreak математических моделей указанных 
выше двух\linebreak математических формулировок проб\-ле\-мы предсказания. 
Содержательно указанные концептуальные\linebreak модели лежат в~основе входного 
языка описания задачи прогнозирования, в~част\-ности па\-ра\-мет\-ров 
вызываемых функций из тех или иных биб\-лио\-тек машинного обуче\-ния~[1, 
10, 11]. Например, это может быть дерево поиска в~известном предикторе 
eXtreme Gradient Boosting Regressor (\mbox{XGBRegressor})~[11], если 
пользовательская концептуальная модель рас\-смат\-ри\-ва\-ет процесс по\-строения 
прогноза некоторого процесса как поиск\linebreak приемлемого (по выбранному 
критерию эффективности) предсказания среди набора возможных его 
значений в~фиксированной об\-ласти данных. XGB применяет к~обуча\-юще\-му 
набору различные способы предсказания будущих значений временн$\acute{\mbox{ы}}$х 
рядов. Это можно рас\-смат\-ри\-вать как последовательный поиск <<моделей 
предсказания>>, улуч\-ша\-ющих прошлые результаты. Эти модели могут быть 
разработаны на основе любых из указанных выше классов математических 
моделей, но во входном языке алгоритмической модели те или иные 
параметры и~переменные этих моделей никак не присутствуют.
  
  Поскольку предсказание выполняется по конечной выборке, естественной 
оценкой качества для всех трех типов моделей служит некоторая 
статистическая оценка, которая зависит от статистических характеристик 
данных, что должно быть отражено в~моделях предсказания.

%\vspace*{-6pt}

\section{Концептуальная модель задачи~предсказания}

%\vspace*{-3pt}

  В~]2] предложена следующая обобщенная форма представления 
концептуальной модели проблемы предсказания:
  \begin{equation}
  \mathrm{FM}=\{M^\theta_D(S), M_L, F_{\mathrm{LM}}\}.
  \label{e1-fr}
  \end{equation}
Здесь $M^\theta_D(S)$~--- модель предсказания, опре\-де\-ля\-емая типом данных 
$D$ (двоичные, действительные), c вероятностной мерой~$\theta$ на~$D$ 
и~структурой~$S$ на~$D$, под которой понимается способ структурирования 
данных типа~$D$ как входных переменных предиктора (например, 
разделение данных на обуча\-ющую и~тес\-то\-вую выборку, скалярные либо 
векторные данные и~т.\,д.); 
   $M_L$~--- модель потерь (<<штрафов>>) от принятого предсказания 
с~функцией потерь $L(X_S,Y_S)$, где $X_S, Y_S \hm\in D$~--- 
соответственно наблюдаемые и~предсказанные данные со структурой~$S$; 
  $F_{\mathrm{LM}}\hm =\{f_1, \ldots , f_m\}$~--- множество предикторов, основанных на 
моделях $M_D^\theta(S)$ и~$M_L$,  т.\,е.\ способов вычисления оценки 
будущего значения с~распределением условной вероятности предсказанного 
значения~$\gamma(\,)$. 
  
  Предполагается, что предиктор $f_p$, $p\hm=1, \ldots, m$, всегда 
соответствует \textit{допустимой} модели предсказания, т.\,е.\ все его 
входные данные и~параметры могут быть однозначно определены в~рамках 
рассматриваемой в~данный момент модели. 
  
  FM позволяет представлять классы моделей согласно типам 
предсказываемых данных, используемым вероятностным мерам, функциям 
потерь при принятии решений о~приемлемости полученного прогноза. При 
этом необходимость учета вероятностных характеристик данных следует из 
сделанного выше замечания о~статистическом характере оценки качества 
предсказания.

\smallskip
  
  \noindent
  \textbf{Пример~1.}
  
  Пусть $\theta = \mathrm{Prob}\,(x_i = 1)$~--- вероятностная мера Бернулли 
на $D\hm=\{0,1\}^n$~--- множестве последовательностей длиной~$n$. 
С~точки зрения~(1) $n$~--- это параметр структуры~$S$ для 
последовательности $X^t\hm =\{x_1, x_2, \ldots , x_i,\ldots, x_{t-1}, x_t\}\hm\in 
D$. 
  
  Рассмотрим последовательность нулей и~единиц как последовательность 
независимых испытаний Бернулли с~функцией потерь <<0/1loss>>~--- меры 
потерь пользователя предиктора от ошибок предсказания, равной доле 
неправильных предсказаний на последовательности длины~$n$~[6], т.\,е.
  \begin{equation}
  e_{\Theta} = {\sf E}_\Theta \left(\sum\limits_{i=1}^n \fr{I\left(b_t \not=x_t\right)}{n}\right),
  \label{e2-fr}
  \end{equation}
  
  \vspace*{-3pt}
  
  \noindent
где $\Theta=\mathrm{Prob}\,(x_t = 1)$~--- параметр распределения Бернулли; 
$I(\,)$~--- индикаторная функция; $b_t$ и~$x_t$~--- предсказанное и~истинное 
значение соответственно; ${\sf E}_\Theta$ означает математическое ожидание по 
распределению~$\Theta$.

  Заметим, что формула~(\ref{e2-fr}) универсальна в~том смысле, что она не 
зависит от распределения на~$D$. В~литературе критерии, подобные~(2), 
называют критериями \textit{минимизации эмпирического риска} 
(ERM, empirical risk minimization)~\cite{9-fr}.
  
  Можно рассмотреть следующий предиктор $b_t \hm= f (x_{t-1})$, где $f \hm\in 
F_{\mathrm{LM}}$ и~функция потерь определяется как <<оптимальное правило 
предсказания по критерию минимума потерь по мере~$e_n$>>~\cite{6-fr}:
  \begin{equation}
  b_t=\begin{cases}
  1, &\mbox{если}\ \mathrm{Prob}\,(1)>\fr{1}{2}\,;\\
  0, &\mbox{если}\  \mathrm{Prob}\,(1)<\fr{1}{2},
  \end{cases}
  \label{e3-fr}
  \end{equation}
  
    \vspace*{-3pt}
  
  \noindent
а~в~случае $\mathrm{Prob}\,(0) \hm= \mathrm{Prob}\,(1)\hm=1/2$ прогноз не 
выполняется.

  В этом случае, как легко видеть, средние потери равны вероятности 
ошибки $1\hm-\Theta$ при $\Theta \hm>1/2$, и~$\Theta$, если $\Theta\hm< 
1/2$, что можно выразить как 
  \begin{equation*}
  L_p=\min \left( \Theta, 1-\Theta\right).
%  \label{e4-fr}
  \end{equation*}
  
  Поскольку речь идет об использовании МО для предсказания, 
рассмотрим, существует ли необходимость трансформации 
сформулированной концептуальной модели предсказания для учета 
специфики МО. 

\smallskip
  
  \textbf{Концептуальная модель машинного обуче\-ния.} Одна из хорошо известных и~используемых формализаций МО~--- концепция PAC, Probably 
Approximately Correct, называемая так потому, что она оценивает 
\textit{ве\-ро\-ят\-ность} достижения заданной \textit{точности} классификации 
после обуче\-ния~[4]. 
  
  В PAC предсказание $b_t$ рассматривается как принятие гипотезы $h\hm\in 
H$, $H$~--- множество гипотез\linebreak о~предсказываемом значении случайной на 
мно\-жестве данных~$X$ величины с~неизвестным распределением, для 
которой целевая функция~$L$ вычисляет характеристики качества 
предсказанных \mbox{значений}~$Y$ (\textit{меток}~[4]), по которым должен быть 
выбран предиктор $h_S : X\hm\to Y$, где $S$~--- конечная выборка из~$X$, 
соответствующая структуре~$S$ (которая может, например, определять, 
какие элементы из~$X$ следует включать в~выборку~--- см.\ пример ниже). 
В~предыдущих обозначениях принятие гипотезы~$h$ означает выбор 
предиктора~$f$, реализующего предсказание некоторого значения из~$X$. 
Например, в~таком инструменте МО, как линейная регрессия, задача 
обучения может определяться на подмножестве действительных чисел~$R$ 
для некоторого чис\-ла переменных (коэффициентов регрессии)~$d$ при 
вычисленном предиктором $f(\,)$ наборе известных наблюдаемых 
значений~$Y$ (<<меток>> в~терминологии PAC) в~обуча\-ющей выборке. 
Гипотеза пред\-ска\-за\-ния соответствует в~этом случае поиску линейной 
функции~$h$, которая луч\-ше всего аппроксимирует отношения между 
переменными~$X$ и~$Y$ (например, объем трафика в~за\-ви\-си\-мости от 
времени и~ин\-тен\-сив\-ности атак).
  
  Итак, оценка качества модели обучения требует использования тех же 
математических инструментов (вероятностных мер, функций потерь) на 
множествах данных тех же типов, что и~задача предсказания. При этом 
структуризация~$S$ может разделять множество наблюдаемых данных на 
подмножества обучения предиктора и~его оценки (тестирования). Поэтому 
концептуальная модель~(1) покрывает также аспект обучения. 
  
  Ниже приведен более сложный пример задания и~анализа влияния 
структуры~$S$. 

\smallskip

\noindent
  \textbf{Пример~2.}
  
  Пусть в~двоичной последовательности Бернулли предсказание по 
правилу~(\ref{e3-fr}) делается с~учетом того, что~$k$ предшествующих 
предсказываемому моменту~$t$ значений единичные. Если 
параметр~$\Theta$ (т.\,е.\ вероятность $\mathrm{Prob}\,(1)$) неизвестен, то 
его оценивают как долю успехов (<<единиц>>) $\Theta_n$ на некоторой 
конечной подпоследовательности длиной~$n$ и~$\Theta$ представляет 
собой математическое ожидание этой оценки (на множестве таких  
$n$-под\-по\-сле\-до\-ва\-тель\-но\-стей). В~\cite{12-fr} показано, что 
условное математические ожидание оценки~$\Theta_n$ при условии, что 
оценка проводится по подпоследовательности, следующей за указанными 
$k$ единицами, меньше истинного значения~$\Theta$. Это значит, что 
в~рамках правила~(3) априорная оценка ошибки предсказания в~момент, 
следующий за указанными $k$ успехами, при использовании~$\Theta_n$ 
в~качестве Prob(1) будет выше, чем в~отсутствие фиксации указанных $k$ 
событий.
  
  Таким образом, свойства структуры данных~$S$ в~модели~(1) (в~данном 
примере~--- наличие $k$ выделенных наблюдений) влияют на 
эффективность прогноза. При этом незнание распределения данных~$D$ 
в~(1) означает невозможность априорной оценки точности предсказания при 
данном алгоритме предсказания. Поэтому необходима теоретическая 
модель, позволяющая опираться не на конкретные знания распределений, 
а~на знания о~тех или иных \textit{свойствах} распределений. 
  
%\vspace*{-8pt}

\section{Влияние вероятностных свойств случайных временных 
рядов на~эффективность предсказания}

%\vspace*{-3pt}

  Условное распределение предсказываемых данных $\gamma (\,)$ 
(предиктора) определяется статистической зависимостью данных в~прошлом 
и~будущем (линейные или нелинейные модели, корреляционные структуры) 
и~стационарностью данных в~том смыс\-ле, что некоторые вероятностные 
законы, управ\-ля\-ющие изменением данных во время наблюдения, остаются 
неизменными, по крайней мере на обуча\-ющей выборке, по который должен 
быть сделан прогноз. Например, для сильно коррелированных временн$\acute{\mbox{ы}}$х 
рядов линейные регрессионные модели (например, ARIMA~--- autoregressive integrated moving average~[1]) дают 
лучший прогноз, чем в~случае их слабой коррелированности. Иными 
словами, если при рассмотрении модельных свойств временн$\acute{\mbox{о}}$го ряда 
ограничиваться только корреляционными свойствами последовательности 
и~рассмотренная выше условная вероятность прогноза $\gamma(x_{t+1}\vert  
x_1, x_2, \ldots, x_t)$ явно связана с~коррелированностью (как это имеет место, 
например, для гауссовского распределения, когда коррелированность 
эквивалентна независимости), то по степени коррелированности можно 
ожидать то или иное качество предсказания линейным регрессионным 
пре\-дик\-тором.

  
  Рассмотрим, какие из измеряемых характеристик процессов можно 
использовать для оценки предсказуемости данных тем или иным 
предиктором. Согласно предложенной концептуальной модели задачи 
предсказания~(1), в~которой интегрируются вероятностные модели как 
данных, так и~принятия решения (через функцию потерь), рассмотрим 
следующие классы измеряемых характеристик выборок случайных 
временн$\acute{\mbox{ы}}$х рядов, связанных с~вероятностными законами поведения: 
  \begin{itemize}
\item свойства автокорреляционных функций удаленных по времени 
событий и~определяемые ими структурные свойства траекторий 
соответствующих процессов,
\item нелинейность связей прошлых и~будущих значений временн$\acute{\mbox{ы}}$х рядов, 
\item возможное влияние свойств тяжелых хвостов распределений,
\item оценки степени стационарности рас\-смат\-ри\-ва\-емых временн$\acute{\mbox{ы}}$х рядов.
\end{itemize}

  \textbf{Автокорреляционные свойства данных и~их влияние на 
предсказуемость.} Возможность предсказания по имеющимся данным, 
очевидно, зависит от степени корреляционной связи между их значениями, 
соответствующими удаленным событиям. Известной моделью такой 
зависимости, причем, как показали многолетние исследования, наблюдаемой 
в~трафиках для многих реальных сетей~[13, 14], является так называемое  
LRD-свойст\-во (Long-Range Dependence) случайного процесса. Временной 
ряд~$X_t$, $t\hm\in Z$, называется \textit{дальнозависимым} (LRD), если его 
ковариационная функция~[14--16] 

\vspace*{-6pt}

\noindent
 \begin{multline*}
  r(t) = {\sf E}\left((X_0 - EX_0)(X_t - EX_t)\right) \sim c_\gamma \vert t \vert^{2-2H} \\
\mbox{при }   t \to \infty\,.
  \end{multline*}
  
  \vspace*{-4pt}
  
  \noindent
 Здесь $c_\gamma\hm>0$~--- некоторая константа;   
$H$~--- показатель Херста,  который
вычисляется как математическое ожидание:
  $$
  H=\lim\limits_{n\to\infty} {\sf E}\left [\fr{R(n)}{S(n)}\right],
  $$
  
   \vspace*{-3pt}
  
  \noindent
где $R(n)$~--- диапазон (размах) накопленных отклонений первых значений 
от среднего значения ряда, $S(n)$~--- стандартное отклонение, $n$~--- 
величина промежутка времени (число точек на отрезке временн$\acute{\mbox{о}}$го ряда). 

  Показатель Херста $H$ характеризует время, отделяющее друг от друга 
коррелированные (например, сильнее, чем некоторое пороговое значение) 
отсчеты трафика.
  
  Наиболее известная модель процесса с~LRD~--- модель дробного 
броуновского движения fBM (fractal Brownian Movement)~[16], которая 
обладает указанной корреляционной функцией.
  
  Естественно предположить, что прогнозирование трафика со свойствами 
LRD может быть довольно эффективным (точным) с~точки зрения 
используемой функции потерь (см.\ разд.~3), поскольку информация о прошлом 
явно влияет на предсказываемое будущее. Существенно, что траектории 
случайных процессов с~такими свойствами обладают <<самоподобием>>, 
т.\,е.\ повторяемостью паттернов в~разных временн$\acute{\mbox{ы}}$х 
масштабах~\cite{10-fr, 17-fr}. Известно, что это свойство наиболее сильно 
проявляется при $H\hm\in  (1/2, 1)$~\cite{16-fr, 18-fr}. 
  
  При этом важно, что при нестационарности исходного дробного 
броуновского процесса (моделирующего данный трафик  
с~LRD-свой\-ст\-ва\-ми) его приращения положительно коррелированы, если 
$H\hm\in (1/2, 1)$, некоррелированы, если $H\hm = 1/2$, и~отрицательно 
коррелированы, если $H\hm\in (0, 1/2)$. Это свойство оказывается важным 
для предсказания знака приращений. 
  
   Если $H\not\in (1/2,1)$, то fBM утрачивает свойства самоподобия и~оно 
в~этом случае не может использоваться для предсказания. 
  
  Примером метода, зависящего от знака автокорреляции значений 
временн$\acute{\mbox{о}}$го ряда служит метод~\cite{19-fr}. В~\cite{19-fr, 20-fr} показана 
эффективность предсказания знака разности $y_{i+1}\hm- y_i$:

\vspace*{2pt}

\noindent
  \begin{equation}
  \mathrm{sign}\left(y_{i+1}- y_i\right)=- \mathrm{sign}\left(y_i\right),           
  \label{e5-fr}
  \end{equation}
  
   \vspace*{-4pt}
  
  \noindent
где $y_i = x_i \hm- ({\sf E}x_i)$, $i\hm= 1, \ldots, t$,~--- последовательность 
центрированных случайных величин с~нулевым средним.

  Признаком возможности использования~(\ref{e5-fr}) как предиктора 
служит некоррелированность (или\linebreak
 крайне слабая корреляция) 
последовательных разностей значений рассматриваемого временн$\acute{\mbox{о}}$го ряда 
(процесса) с~тенденцией к~отрицательной корреляции значений $y_{i+1}\hm- 
y_i$ и~$y_i$, так как \mbox{<<антикорреляция>>} двух случайных величин означает 
тенденцию к~изменению в~противоположную сторону~\cite{16-fr}.
  
  Напомним, что при $H \hm<1/2$ приращения имеют отрицательную 
корреляцию и~демонстрируют короткодействующую зависимость, для 
$H\hm>1/2$ автокореляция приращений положительна. Таким образом, два 
последовательных приращения, как правило, имеют одно и~то же 
направление.
  
  Примером полезности прогнозирования знака приращений трафика 
служит, например, ситуация, когда атаки на сеть осуществляются способом, 
который не может быть обнаружен антивирусным программным 
обеспечением, и~аналитикам приходится полагаться на анализ изменений 
объема трафика или направления изменений в~интенсивности записи в~файле 
журнала~\cite{14-fr}. 
  
\smallskip

  \textbf{Стационарность.} Большинство рас\-смат\-ри\-ва\-емых реальных 
процессов в~ВС формально не относятся к~стационарным, 
и~о~стационарности мож\-но говорить уверенно лишь до известной степени. 
Для оценки этой степени хорошо известны статистические тесты на 
стационарность~\cite{17-fr}, однако включить их результаты в~модель 
предсказания затруднительно, поскольку кроме интуитивных качественных 
рассуждений о~том, что стационарные процессы предсказывать лучше, чем 
нестационарные, ничего использовать в~модели нельзя. Однако для 
процессов с~LRD можно получить более обоснованные эвристические 
правила связи стационарности и~предсказуемости. В~\cite{15-fr} показано, 
что сравнительно короткие наблюдения можно рассматривать как 
стационарные, по крайней мере по дисперсии, поскольку при самоподобии 
дисперсия выборочного среднего уменьшается медленнее, чем величина, 
обратная размеру выборки, т.\,е.\
$$
\mathrm{var}\left(X^{(m)}\right) \sim a_2m^{-\beta},\ m \to \infty,\ 0<\beta < 1\,,
$$ 
где $a_2$~--- положительная константа.
  
  Существенно, что автокорреляции затухают как гиперболические 
функции, а~не экспоненциально. Соответственно, с~известной долей 
условности можно говорить и~о~стационарности по автокорреляционным 
функциям. 

\smallskip
  
  \textbf{Показатель Херста {\boldmath{$H$}} как показатель стационарности.} В~\cite{18-fr} показано, что в~качестве критерия 
  ста\-ци\-о\-нар\-ности для са\-мо\-по\-доб\-но\-го процесса может 
выступать Hurst exponent~$H$. Значения $H\hm>1$ указывают (с~учетом 
ста\-ти\-сти\-че\-ской труд\-ности его оцен\-ки по конечной выборке в~окрест\-ности 
$H\hm=1$) на невозможность рас\-смат\-ри\-вать пред\-ска\-зы\-ва\-емый процесс как 
стационарный. Для стационарного самоподобного процесса $H\hm\in (0{,}5, 
1)$ чем ближе значение па\-ра\-мет\-ра Херста к~1, тем медленнее затухает 
дисперсия по мере увеличения временн$\acute{\mbox{о}}$го мас\-шта\-ба, и~трафик становится 
более пуль\-си\-ру\-ющим и,~соответственно, нестационарным. 

\smallskip
  
  \textbf{Влияние свойств LRD и~стационарности на технику решения 
задач предсказания.} Если используется среднеквадратичный критерий 
точности прогноза и~ряд можно считать стационарным, то квадрат ошибки 
линейного предсказания для стационарного временн$\acute{\mbox{о}}$го ряда 
пропорционален $1\hm-\rho^2$~\cite{8-fr}, где 
$\rho\hm=\mathrm{ACF}(1)$, $\mathrm{ACF}(1)$ означает 
корреляционную функцию в~лаге~1. 
  
  С~учетом того, что для LRD-про\-цес\-са автокорреляционная функция 
экспоненциально зависит от~$H$~\cite{15-fr, 16-fr}, очевидна монотонная 
связь между среднеквадратичной ошибкой линейного предсказания 
и~экспонентой Херста в~интервале $(1/2, 1)$. 
  
  Этот анализ может быть полезным, например при использовании таких 
программных инструментов предсказания, как авторегрессионная модель 
ARMA (autoregressive moving average) со скользящим средним и~авторегрессионное интегрированное 
скользящее среднее \mbox{ARIMA}. 
{ %\looseness=1

}
  
  Для предсказания чистого случайного блуждания может быть полезным 
предсказание изменения знака~(\ref{e5-fr}), как говорилось выше. 
  
  Теория линейного предсказания с~критерием MSE, базирующаяся на 
модели Кол\-мо\-го\-ро\-ва--Ви\-не\-ра, исходит из того, что временной ряд 
имеет конечное математическое ожидание и~дисперсию. Однако для 
временн$\acute{\mbox{ы}}$х рядов со свойствами LRD это не всегда может быть выполнено, 
в~част\-ности если потоки данных имеют распределение Парето~\cite{21-fr}.

\smallskip
  
  \textbf{Влияние параметров вероятностных распределений процессов 
с~LRD на предсказание.} Мно\-го\-чис\-лен\-ные исследования LRD-рядов 
показали, что их распределения как распределения с~тяжелыми хвос\-та\-ми 
близки к~распределению Парето: 
$$
\mathrm{Pareto}\,(X)= \fr{ab^a}{x^{a+1}}\,,
$$ где 
$x\hm\geq a$. 
 % 
  При этом среднее и~дисперсия:
  
  \noindent
  \begin{align*}
  \mu\_\mathrm{Pareto}\,(x)&= \fr{ab}{a - 1}\,;\\
  \mathrm{Var}\_\mathrm{Pareto} &= \fr{ab^2}{(a - 1)^2 (a - 2)},
  \end{align*}
  
  \vspace*{-3pt}
  
  \noindent
где $x \geq a >0$, $b \hm> 0$.
  
  Очевидно, что $\mu\_\mathrm{Pareto}$ и~$\mathrm{Var}\_\mathrm{Pareto}$ не существуют при $a\hm = 1$ и~$2$ 
и~использование предикторов Ви\-не\-ра--Кол\-мо\-го\-ро\-ва для вывода 
о~пред\-ска\-зу\-емости ряда LRD относительно MSE неприемлемо. 

\smallskip
  
  \textbf{Учет нелинейных свойств траекторий трафика.} Рассмотренное 
выше самоподобие траектории трафика отражает нелинейную связь 
прошлого и~будущего, где степень нелинейности зависит  
от~$H$~\cite{14-fr, 16-fr, 2-fr}. 
  
  Как отмечалось, при $H\hm=1/2$ LRD-свой\-ст\-во 
 утрачивается~\cite{20-fr}. Значит, показатель~$H$ может указать, обладает 
ли прогнозируемый временной ряд свойством чистого случайного 
блуждания, для которого, очевидно, затруднительно эффективное 
использование МО, или же имеет некоторую корреляционную структуру, 
что означало бы потенциальную возможность эффективного прогноза. 
Напри-\linebreak мер, в~\cite{22-fr} для интернета вещей (IoT, Internet of Things) параметр~$H$ для 
сетевого трафика используется как <<априорное знание>> при обуче\-нии. 
\mbox{Поскольку} свойство самоподобия хорошо интерпретируемо в~терминах 
описания трафика IT-спе\-ци\-а\-ли\-ста\-ми, его использование повышает 
ин\-тер\-пре\-ти\-ру\-емость модели, а~именно: воз\-мож\-ность \mbox{понять}, как влияют на 
качество прогноза свойства данных, в~част\-ности самоподобие. 
  
  В ряде инструментов предсказания прогностическим признаком служит 
разрушение самоподобия~--- методы, основанные на том наблюдении, что 
наличие DDoS-ата\-ки (distributed denial of service) снижает степень самоподобия нормального трафика, 
поскольку инструменты DDoS не генерируют самоподобный 
трафик~\cite{14-fr}. 

%\vspace*{-6pt}

\section{Нелинейные модели и~нейронные алгоритмы}

%\vspace*{-3pt}

  Прежде всего заметим, что очевидным примером применения 
нелинейного алгоритма служит рассмотренный предиктор~(\ref{e5-fr}). 
Однако нелинейная модель прогнозирования в~современных ИП\linebreak 
реализуется, как правило, либо в~моделях нелинейной регрессии, либо 
в~искусственных нейронных сетях~\cite{1-fr, 23-fr, 24-fr}, например моделях 
глубокого обуче\-ния, управ\-ля\-емых данными, в~\mbox{дополнение} к~алгоритмам 
машинного обуче\-ния, таким как регрессия опорных векторов (SVR, Support 
Vector Regression)~\cite{10-fr, 24-fr}. 
  
   Нейронная сеть может иметь проблемы из-за \textit{переобучения, или 
оверфиттинга} (overfitting)~[24], когда модель хорошо работает 
\textbf{только} с~данными из обучающей выборки, адаптируясь  
к~обуча\-ющим примерам, вместо того чтобы учиться классифицировать 
данные, не участвовавшие в~обучении. Нетрудно видеть, что этому может 
способствовать такое нелинейное явление, как самоподобие. При этом такой 
распространенный способ снижения оверфиттинга, как dropout~--- 
выключение некоторых нейронов с~некоторой вероятностью на каком-то 
интервале данных из процесса обучения, может не работать из-за того, что 
обучающая выборка будет подобна предыдущей. Внезапные 
изменения~\cite{25-fr} в~трафике облачных сред нейронная сеть может 
легко спутать с~аномалиями трафика, что также ведет к~неэффективности 
обучения. Выраженность такой нестационарности можно оценить по 
значениям~$H$. 
  
  В~качестве нелинейного подхода к~прогнозированию трафика также 
применяется регрессия опорных векторов (SVR)~\cite{23-fr}, однако выбор 
подходящих функций ядра и~оптимальных параметров очень сложен. 
Примеры такого выбора кратко рас\-смат\-ри\-ва\-ют\-ся в~\cite{26-fr} (интересные 
результаты изучения влияния моделей данных на использование свойства 
самоподобия приведены в~\cite{27-fr}). 
  
  Из приведенного анализа зависимости эффективности использования 
моделей предсказания от конкретных свойств временн$\acute{\mbox{ы}}$х рядов следует, что 
естественным путем учета зависимости свойств предикторов от поведения 
данных (стационарность, нелинейность зависимости) должен быть их  
он\-лайн-вы\-бор в~процессе решения задачи управления сетью. Например, 
в~\cite{27-fr} для прогнозирования времени отклика и~пропускной 
способности облачных сервисов на разных этапах использования ресурсов 
рассматривались алгоритмы искусственных нейронных сетей (ANN, artificial neural networks) 
и~линейной регрессии. 

%\vspace*{-6pt}

\section{Заключение}

%\vspace*{-3pt}

  Как показывает анализ литературы, многие современные инструменты 
предсказания, основанные на принципах MO, работают недостаточно 
эффективно ввиду выраженной нелинейности \mbox{изменения} трафика, его 
нестационарности, необходимости большого объема предыду\-щих 
наблюдений. Данная статья представляет собой попытку некоторого 
упорядочения и~категоризации огромного потока публикаций по 
современным методам, техникам и~моделям прогнозов данных различной 
природы с~точки зрения возможности априорной оценки эффективности 
предсказания значений данных соответствующими ИП. Для этого выделены 
характеристики вероятностных моделей, явно или неявно используемых 
в~современных программных инструментах предсказания, позволяющие 
оценивать наличие существенной нестационарности в~потоках данных, 
некоторых нелинейных эффектов, специфики законов распределения, 
влияющих на эффективность обучения предикторов. 
  
  
{\small\frenchspacing
 {\baselineskip=10.5pt
 %\addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99}
\bibitem{1-fr}
\Au{Chen A., Law J., Aibin~M.} A~survey on traffic prediction techniques using artificial 
Intelligence for communication networks~// Telecom, 2021. Vol.~2. Iss.~4. P.~518--535.
doi: 10.3390/telecom2040029.
\bibitem{2-fr}
\Au{Frenkel S.} On impact of data models on predictability assessment of time series~// Data 
analysis and optimization~/ Eds. B.~Goldengorin, S.~Kuznetsov.~--- Optimization and its applications
ser.~--- Springer, 2023. Vol.~202.
P.~103--134.  doi:  10.1007/978-3-031-31654-8\_7.
\bibitem{3-fr}
\Au{Fettke P.} Conceptual modelling and artificial intelligence: Overview and research 
challenges from the perspective of predictive business process management~// CEUR Workshop 
Procee., 2020. Vol.~2542. P.~157--164. 
\bibitem{4-fr}
\Au{Shalev-Shwartz S.} Understanding machine learning: From theory to algorithms.~--- 
Cambridge University Press, 2014. 449~p.
\bibitem{5-fr}
\Au{Brovelli M., Sanso~F., Venuti~G.} A~discussion on the Wiener--Kolmogorov prediction 
principle with easy-to compute and robust variants~// J.~Geodesy, 2003. Vol.~76. P.~673--683.
doi: 10.1007/s00190-002-0292-3.
\bibitem{6-fr}
\Au{Merhav N., Feder~M.} Universal prediction~// IEEE T. Inform. Theory, 1998. Vol.~44. 
Iss.~6. P.~2124--2147. doi: 10.1109/18.720534.
\bibitem{7-fr}
\Au{Ryabko B.} Compression-based methods for nonparametric prediction and estimation of 
some characteristics of time series~// IEEE T. Inform. Theory, 2009. Vol.~55. Iss.~9.  
P.~4309--4315. doi: 10.1109/TIT.2009.2025546.
\bibitem{8-fr}
\Au{Box G.\,E.\,P., Jenkins~G.\,M., Reinsel~G.} Time series analysis: Forecasting and  
control.~--- New York, NY, USA: John Wiley \&~Sons, Inc, 2008. 709~p.
\bibitem{9-fr}
\Au{Aryan M.} Efficient methods for large-scale empirical risk minimization: D.Sc. 
Thesis.~--- Philadelphia, PA, USA: University of Pennsylvania, 2017. 303~p.
\bibitem{10-fr}
\Au{Sharma S.} Activation functions in neural networks, 2017. {\sf 
https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6}.
\bibitem{11-fr}
\Au{Weiwei J.} Cellular traffic prediction with machine learning: A~survey~// Expert Syst. Appl., 2022. Vol.~201. P.~117--163. 
doi: 10.1016/j.eswa.2022.117163.
\bibitem{12-fr}
\Au{Miller J., Sanjurjo~A.} Surprised by the hot hand fallacy? A~truth in the law of small 
numbers.~--- Cornell University, 2019. arXiv:1902.01265v1.  49~p.
\bibitem{13-fr}
\Au{Jiang M., Nikolic~M., Hardy~S., Trajkovic~L.} Impact of self-similarity on wireless data 
Network performance~//  Conference (International) on Communications Proceedings.~--- Piscataway, NJ, USA: IEEE, 2001. Vol.~2. 
P.~477--481. doi: 10.1109/ICC.2001.936986.
\bibitem{14-fr}
\Au{Brignoli D.} DDOS detection based on traffic self-similarity: Master Thesis.~--- Christchurch, 
New Zealand: University Canterbury, 2008. 100~p.
\bibitem{15-fr}
\Au{Lleland W.\,E., Taqqu M.\,S., Willinger~W., Wilson~D.\,V.} On the self-similar nature of 
Ethernet traffic (extended version)~// IEEE ACM T. Network., 
1994. Vol.~2. Iss.~1. P.~1--15. doi: 10.1109/90.282603.
\bibitem{16-fr}
\Au{Coskun B., Vardar-Acar~B., Demirtas~C.} A~generalized correlated random walk, 
converging to fractional Brownian motion.~--- Cornell University, 2019. arXiv:1903.05424v3. 21~p.
\bibitem{17-fr}
\Au{Riya J., Chetty~P.} What is a stationarity test and how to do it?~// PG, 2020. 
{\sf https://www.projectguru.in/what-is-a-stationarity-test-how-to-do-it}.
\bibitem{18-fr}
\Au{Park C., Hernandez~F., Long~L., Marron~J.} Long-range dependence analysis of internet 
traffic~// J.~Appl. Stat., 2011. Vol.~38. Iss.~7. P.~1407--1433. doi: 10.1080/ 02664763.2010.505949.
\bibitem{19-fr}
\Au{Sornette D., Andersen~J.} Increments of uncorrelated time series can be predicted with 
a~universal 75\% probability of success~// Int. J. Mod. Phys. C, 2000. Vol.~11. Iss.~4.  
P.~713--720. doi: 10.1142/S0129183100000626.
\bibitem{20-fr}
\Au{Frenkel S.} Predicting the direction of changes in the values of time series for relatively 
small training samples~// Cyber security, cryptography, and machine learning~/
Eds. S.~Dolev, J.~Katz, 
A.~Meisels.~--- Lecture notes in computer science  
ser.~--- Cham, Switzerland: Springer, 2022. Vol.~13301. P.~118--134.  
doi: 10.1007/978-3-031-07689-3\_9. 
\bibitem{21-fr}
\Au{Song W., Duan~S., Chen~D., Zio~E., Yan~W., Cai~F.} Finite iterative forecasting model 
based on fractional generalized Pareto motion~// Fractal, 2022. Vol.~6. Iss.~9. P.~471--490.
doi: 10.3390/fractalfract6090471.
\bibitem{22-fr}
\Au{Pan C., Wang~Y., Shi~H., Shi~J., Cai~R.} Network traffic prediction incorporating prior 
knowledge for an intelligent network~// Sensors, 2022. Vol.~22. Iss.~7. P.~2674--2690.
doi: 10.3390/s22072674.
\bibitem{23-fr}
\Au{Tong H., Li~C., He~J.} A~boosting-based framework for self-similar and non-linear internet 
traffic prediction~// Advances in neural networks~/ Eds. F.~Yin, J.~Wang, 
C.~Guo.~--- Lecture notes in computer science ser.~--- Berlin, Heidelberg: Springer, 
2004. Vol.~3174. P.~931--936. doi: 10.1007/978-3-540-28648-6\_148.
\bibitem{24-fr}
\Au{Zhang J., Tan~D., Zhu~H.} From machine learning to deep learning: Progress in machine 
intelligence for rational drug discovery~// Drug Discov. Today, 2017. Vol.~22. Iss.~11. 
P.~1680--1685. doi: 10.1016/j.drudis.2017.08.010.
\bibitem{25-fr}
\Au{Nikravesh A.\,Y., Ajila~S.\,A., Lung~C.-H.} An autonomic prediction suite for cloud 
resource provisioning~// J.~Cloud Computing, 2017. Vol.~6. Iss.~3. doi:  
10.1186/s13677-017-0073-4.
\bibitem{26-fr}
\Au{Dolev S., Frenkel~S., Zakharov~V.} Forecasting tools in practical applications: Selection 
and evaluation methodology~// Conference (International) on Engineering Technologies and 
Computer Science Proceedings.~--- Piscataway, NJ, USA: IEEE, 2021. P.~37--44. doi: 
10.1109/EnT52731.2021.00013. 
\bibitem{27-fr}
\Au{Lukashenko O., Morozov~E., Pagano~M.} A~Gaussian approximation of the distributed 
computing process~// Информатика и~её применения, 2019. Т.~13. Вып.~2. С.~109--116. 
doi: 10.14357/19922264190215. 
EDN: PKNUSZ.

\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 22.08.23}}

%\vspace*{6pt}

%\pagebreak

\newpage

\vspace*{-28pt}

%\hrule

%\vspace*{2pt}

%\hrule



\def\tit{MODELS FOR STUDY OF~THE~INFLUENCE OF~STATISTICAL 
CHARACTERISTICS OF~COMPUTER NETWORKS TRAFFIC 
ON~THE~EFFICIENCY OF~PREDICTION BY~MACHINE LEARNING 
TOOLS}


\def\titkol{Models for study of~the~influence of~statistical 
characteristics of~computer networks traffic 
on~the~efficiency of~prediction} % by~machine learning tools}


\def\aut{S.\,L.~Frenkel and~V.\,N.~Zakharov}

\def\autkol{S.\,L.~Frenkel and~V.\,N.~Zakharov}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-10pt}


\noindent
Federal Research Center ``Computer Science and Control'' of the Russian Academy 
of Sciences, 44-2~Vavilov Str., Moscow 119333, Russian Federation

\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2023\ \ \ volume~17\ \ \ issue\ 4}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2023\ \ \ volume~17\ \ \ issue\ 4
\hfill \textbf{\thepage}}}

\vspace*{3pt}







\Abste{The article is an attempt to streamline and categorize a huge stream of 
publications on modern methods, techniques, and models of data forecasting of 
various nature in terms of their applicability for traffic forecasting in computer 
networks. The specified ordering is performed within the framework of the 
proposed conceptual model of forecasting algorithms. Within the framework of 
this conceptual model, the characteristics of both computer network traffic models 
and traffic control methods that can be explicitly or implicitly used in modern 
prediction software tools are highlighted. It is shown that the analysis of such 
probabilistic aspects of data description as the presence of significant 
 nonstationarity, some nonlinear effects in data models, as well as the specifics of 
data distribution laws can influence the efficiency of learning predictors.}

\KWE{network traffic prediction; probabilistic models}

  \DOI{10.14357/19922264230410}{CQYNFJ}

%\vspace*{-16pt}

%\Ack
%\vspace*{-4pt}
%
%\noindent

  

%\vspace*{-5pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{99} 
\bibitem{1-fr-1}
\Aue{Chen, A., J.~Law, and M.~Aibin.} 2021. A~survey on traffic prediction 
techniques using artificial intelligence for communication networks. 
\textit{Telecom} 2(4):518--535. doi: 10.3390/telecom2040029.
\bibitem{2-fr-1}
\Aue{Frenkel, S.} 2023. On impact of data models on predictability assessment of 
time series. \textit{Data analysis and optimization.} Eds. B.~Goldengorin and 
S.~Kuznetsov.  Optimization and its applications ser. Springer. 202:103--134. doi: 
10.1007/978-3-031-31654-8\_7.
\bibitem{3-fr-1}
\Aue{Fettke, P.} 2020. Conceptual modelling and artificial intelligence: Overview 
and research challenges from the perspective of predictive business process 
management. \textit{CEUR 
Workshop Procee.} 2542:157--164. 
\bibitem{4-fr-1}
\Aue{Shalev-Shwartz, S.} 2014. \textit{Understanding machine learning: From 
theory to algorithms}. Cambridge University Press. 449~p.
\bibitem{5-fr-1}
\Aue{Brovelli, M., F.~Sanso, and G.~Venuti.} 2003. A discussion on the 
 Wiener--Kolmogorov prediction principle with easy-to compute and robust 
variants. \textit{J.~Geodesy} 76:673--683. doi: 10.1007/s00190-002-0292-3.
\bibitem{6-fr-1}
\Aue{Merhav, N., and M.~Feder.} 1998. Universal prediction. \textit{IEEE T. 
Inform. Theory} 44(6):2124--2147. doi: 10.1109/ 18.720534.
\bibitem{7-fr-1}
\Aue{Ryabko, B.} 2009. Compression-based methods for nonparametric prediction 
and estimation of some characteristics of time series. \textit{IEEE T. Inform. 
Theory} 55(9):4309--4315. doi: 10.1109/TIT.2009.2025546.
\bibitem{8-fr-1}
\Aue{Box, G.\,E.\,P., G.\,M.~Jenkins, and G.~Reinsel.} 2008. \textit{Time series 
analysis: Forecasting and control}. New York, NY: John Wiley \&~Sons, 
Inc. 709~p.
\bibitem{9-fr-1}
\Aue{Aryan, M.} 2017. Efficient methods for large-scale empirical risk 
minimization.  Philadelphia, PA: University of Pennsylvania. PhD Thesis. 303~p.
\bibitem{10-fr-1}
\Aue{Sharma, S.} 2017. Activation functions in neural networks. Available at: {\sf 
https://towardsdatascience.com/ activation-functions-neural-networks-1cbd9f8d91d6} 
(accessed December~4, 2023).

\bibitem{11-fr-1}
\Aue{Weiwei, J.} 2022. Cellular traffic prediction with machine learning: 
A~survey. \textit{Expert Syst. Appl.} 201:117--163. doi: 
10.1016/j.eswa.2022.117163.
\bibitem{12-fr-1}
\Aue{Miller, J., and A.~Sanjurjo.} 2019. Surprised by the hot hand fallacy? A~truth 
in the law of small numbers. \textit{arXiv.org}. 49~p. Available at: {\sf 
https://arxiv.org/abs/1902.01265v1} (accessed December~4, 2023).
\bibitem{13-fr-1}
\Aue{Jiang, M., M.~Nikolic, S.~Hardy, and L.~Trajkovic.} 2001. Impact of  
self-similarity on wireless data Network performance. \textit{Conference 
(International) on Communications Proceedings}. 
Piscataway, NJ: IEEE. 2:477--481. doi: 10.1109/ICC.2001.936986.
\bibitem{14-fr-1}
\Aue{Brignoli, D.} 2008. DDOS detection based on traffic self-similarity. 
 Christchurch, New Zealand: University Canterbury. Master Thesis. 100~p.
\bibitem{15-fr-1}
\Aue{Lleland, W.\,E., M.\,S.~Taqqu, W.~Willinger, and D.\,V.~Wilson.} 1994. On 
the self-similar nature of Ethernet traffic (extended version). \textit{IEEE ACM T. 
Network.} 2(1):1--15. doi: 10.1109/90.282603.
\bibitem{16-fr-1}
\Aue{Coskun, B., B.~Vardar-Acar, and C.~Demirtas.} 2019. A~generalized 
correlated random walk, converging to fractional Brownian motion. 
\textit{arXiv.org}. 21~p. Available at: {\sf https://arxiv.org/abs/1903.05424v3} 
(accessed December~4, 2023).
\bibitem{17-fr-1}
\Aue{Riya, J., and P.~Chetty.} 2020. What is a stationarity test and how to do it? 
\textit{PG}. Available at: {\sf  
https://www.\linebreak projectguru.in/what-is-a-stationarity-test-how-to-do-it} (accessed 
December~4, 2023).
\bibitem{18-fr-1}
\Aue{Park, C., F.~Hernandez, L.~Long, and J.~Marron.} 2011. Long-range 
dependence analysis of internet traffic. \textit{J.~Appl. Stat.} 38(7):1407--1433. 
doi: 10.1080/ 02664763.2010.505949.
\bibitem{19-fr-1}
\Aue{Sornette, D., and J.~Andersen.} 2000. Increments of uncorrelated time series 
can be predicted with a universal 75\% probability of success. \textit{Int. J. Mod. 
Phys.~C} 11(4):713--720. doi: 10.1142/S0129183100000626.
\bibitem{20-fr-1}
\Aue{Frenkel, S.} 2022. Predicting the direction of changes in the values of time 
series for relatively small training samples. \textit{Cyber security, cryptography, 
and machine learning}. Eds. S.~Dolev, J.~Katz, 
and A.~Meisels. Lecture notes in computer science ser. Cham, Switzerland: 
Springer. 13301:118--134. doi: 10.1007/978-3-031-07689-3\_9. 
\bibitem{21-fr-1}
\Aue{Song, W., S.~Duan, D.~Chen, E.~Zio, W.~Yan, and F.~Cai.} 2022. Finite 
iterative forecasting model based on fractional generalized Pareto motion. 
\textit{Fractal} 6(9):471--490. doi: 10.3390/fractalfract6090471.
\bibitem{22-fr-1}
\Aue{Pan, C., Y.~Wang, H.~Shi, J.~Shi, and R.~Cai.} 2022. Network traffic 
prediction incorporating prior knowledge for an intelligent network. 
\textit{Sensors} 22(7):2674--2690. doi: 10.3390/s22072674.
\bibitem{23-fr-1}
\Aue{Tong, H., C.~Li, and J.~He.} 2004. A boosting-based framework for  
self-similar and non-linear internet traffic prediction. \textit{Advances in neural 
networks}. Eds. F.~Yin, J.~Wang, and C.~Guo. Lecture notes in computer science 
ser. Berlin, Heidelberg: Springer. 3174:931--936. doi:  
10.1007/978-3-540-28648-6\_148.
\bibitem{24-fr-1}
\Aue{Zhang, J., D.~Tan, and H.~Zhu.} 2017. From machine learning to deep 
learning: progress in machine intelligence for rational drug discovery. \textit{Drug 
Discov. Today} 22(11):1680--1685. doi: 10.1016/j.drudis.2017.08.010.
\bibitem{25-fr-1}
\Aue{Nikravesh, A.\,Y., S.\,A.~Ajila, and C.-H.~Lung.} 2017. An autonomic 
prediction suite for cloud resource provisioning. \textit{J.~Cloud Computing}  
6(3). 20~p. doi: 10.1186/s13677-017-0073-4.
\bibitem{26-fr-1}
\Aue{Dolev, S., S.~Frenkel, and V.~Zakharov.} 2021. Forecasting tools in practical 
applications: Selection and evaluation methodology. \textit{Conference 
(International) on Engineering Technologies and Computer Science Proceedings}. 
Piscataway, NJ: IEEE. 37--44. doi: 10.1109/ EnT52731.2021.00013. 
\bibitem{27-fr-1}
\Aue{Lukashenko, O., E.~Morozov, and M.~Pagano.} 2019. A~Gaussian 
approximation of the distributed computing process. \textit{Informatika i~ee 
Primeneniya~--- Inform. \mbox{Appl}.} 13(2):109--116. doi: 10.14357/19922264190215. 
EDN: PKNUSZ.

\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-8pt}

\hfill{\small\textit{Received August 22, 2023}} 

\vspace*{-14pt}


\Contr

\vspace*{-4pt}

\noindent
\textbf{Frenkel Sergey L.} (b.\ 1951)~--- Candidate of Science (PhD) in 
technology, associate professor, senior scientist, Federal Research Center 
``Computer Sciences and Control'' of the Russian Academy of Sciences,  
44-2~Vavilov Str., Moscow 119333, Russian Federation; 
\mbox{fsergei51@gmail.com}

\vspace*{3pt}

\noindent
\textbf{Zakharov Victor N.} (b.\ 1948)~--- Doctor of Science in technology, 
associate professor; Scientific Secretary, Federal Research Center ``Computer 
Science and Control'' of the Russian Academy of Sciences, 44-2~Vavilov Str., 
Moscow 119333, Russian Federation; \mbox{vzakharov@ipiran.ru}



\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература} 
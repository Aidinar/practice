\def\stat{krivenko}

\def\tit{ВЫБОР МОДЕЛИ ПРИ ФАКТОРИЗАЦИИ МАТРИЦЫ ДАННЫХ С~ПРОПУСКАМИ}

\def\titkol{Выбор модели при факторизации матрицы данных с~пропусками}

\def\aut{М.\,П.~Кривенко$^1$}

\def\autkol{М.\,П.~Кривенко}

\titel{\tit}{\aut}{\autkol}{\titkol}

\index{Кривенко М.\,П.}
\index{Krivenko M.\,P.}


%{\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]
%{Работа выполнена при поддержке Министерства науки и~высшего образования Российской Федерации (проект 
%075-15-2020-799).}}


\renewcommand{\thefootnote}{\arabic{footnote}}
\footnotetext[1]{Федеральный исследовательский центр <<Информатика и~управление>> Российской академии наук, 
\mbox{mkrivenko@ipiran.ru}}


\vspace*{-6pt}






  \Abst{Работа посвящена задаче факторизации матрицы с~отсутствующими компонентами в~произведение двух матриц более низкого ранга. Исследовано влияние интенсивности 
пропусков на выбор модели факторизации. Рассматриваются два алгоритма оценивания 
параметров: альтернирующий наименьших квадратов (alternating least square, ALS) и~Вайберга (Wiberg)~--- для 
двух моделей факторизации: со средними и~без. Обосновывается, что в~использовании 
модели со средними нет необходимости: она является частным случаем другой модели, 
приводит в~отдельных случаях к~неоднозначным решениям. При проведении экспериментов 
предпочтение отдано более устойчивому ALS-ал\-го\-рит\-му. Продемонстрированы 
преимущества метода вставки перед случайным заполнением при начальных установках 
итерационных алгоритмов оценивания параметров модели. Выявлены причины негативных 
свойств существующей редакции алгоритма Вайберга. На основании экспериментов 
установлено: с~ростом вероятности пропуска точность представления имеющихся данных 
увеличивается, что приводит к~занижению истинного значения размерности модели.}
  
  \KW{понижающая ранг аппроксимация матрицы; сингулярное разложение; пропущенные 
данные; ALS-ал\-го\-ритм; алгоритм Вайберга}

\DOI{10.14357/19922264220307} 
  
\vspace*{-6pt}


\vskip 10pt plus 9pt minus 6pt

\thispagestyle{headings}

\begin{multicols}{2}

\label{st\stat}
  
\section{Введение}

     Факторизация матриц данных хорошо зарекомендовала себя как метод 
снижения размерности. Множество его приложений включает обработку 
изображений, сжатие передаваемой информации,\linebreak разведочный анализ данных, 
визуализацию, распознавание образов и~прогнозирование вре\-мен\-н$\acute{\mbox{ы}}$х рядов, 
а~популярность обусловлена, во-пер\-вых, оптимальностью (обычно с~точки 
зрения\linebreak среднеквад\-ра\-тич\-ной ошибки) линейной схемы сжатия совокупности  
вы\-со\-ко\-раз\-мер\-ных векторов в~набор векторов меньшей размерности 
с~возможностью последующей реконструкции, во-вто\-рых, тем, что сжатие 
и~восстановление~--- относительно простые операции, включающие лишь 
действия с~мат\-ри\-цами.
     
     Трудности при факторизации начинают проявляться при возникновении 
пропусков в~данных,\linebreak причем отбрасывание (игнорирование) или заполнение 
(вменение, приписывание) не\-до\-ста\-ющих элементов данных с~последующим 
применением обычных процедур анализа перестают эффективно работать при 
возрастании доли пропусков. Поэтому возрастает роль методов обработки 
наблюденных и~пропущенных данных в~со\-во\-куп\-ности, которые требуют 
разработки оригинальных процедур.
     %
     Среди них как самостоятельное сформировалось направление низкоранговой аппроксимации мат\-ри\-цы данных с~пропусками. С~общей 
характеристикой этой проб\-ле\-мы можно ознакомиться, например, в~\cite{1-kri}. 
     
     Можно выделить две базовые модели факторизации  
($m\times n$)-мат\-ри\-цы наблюдений~$\mathbf{Y}$:
     \begin{enumerate}[(1)]
\item представление ее при помощи $\tilde{\mathbf{Y}}\hm= \mathbf{UV}^{\mathrm{T}}$, 
где $\mathbf{U}$~--- ($m\times r$)-мат\-ри\-ца, $\mathbf{V}$~--- ($n\times r$)-мат\-рица;
\item представление ее с~учетом вектора средних при помощи 
$\tilde{\mathbf{Y}}_\mu\hm=\mathbf{UV}^{\mathrm{T}}\hm+\mathbf{1}_m\bm{\mu}^{\mathrm{T}}\hm\equiv \mathbf{UV}^{\mathrm{T}}\hm+\mathbf{M}$, где 
в~дополнение к~$\mathbf{U}$ и~$\mathbf{V}$ введен $n$-век\-тор~$\bm{\mu}$; 
$\mathbf{1}_m$~--- единичный $m$-век\-тор.
\end{enumerate}

Каждая модель принимает конкретный вид при одном из значений 
структурного параметра $r\hm= 1, \ldots , n$. Наличие пропусков в~данных 
будет отражаться в~($m\times n$)-мат\-ри\-це~$\mathbf{H}$:
$$
h_{ij}= \begin{cases}
1\,, & y_{ij}\ \mbox{присутствует};\\
0\,, & y_{ij}\ \mbox{пропущено}.
\end{cases}
$$
Для суммы ее элементов примем обозначение~$p$ (чис\-ло присутствующих 
элементов матрицы наблюдений~$\mathbf{Y}$).
     
     \textbf{Задача построения} $\tilde{\mathbf{Y}}$ формулируется как 
минимизация целевой функции:
     $$
     \varphi(\mathbf{U},\mathbf{V})= \left \| \mathbf{H}\odot \left( \mathbf{Y} -
\tilde{\mathbf{Y}}\right) \right \|_F^2\to \min\limits_{\mathbf{U},\mathbf{V}}.
     $$
Перепишем целевую функцию в~более удобном для аналитических 
преобразований виде, исключив~$\mathbf{H}$. Построчная запись матрицы 
$\mathbf{U}\hm= [\bm{u}_1, \ldots , \bm{u}_m]^{\mathrm{T}}$, где $\bm{u}_i$ суть 
$r$-век\-то\-ры, даст $mr$-век\-тор $\bm{u}\hm= [\bm{u}_1^{\mathrm{T}}, \ldots , 
\bm{u}^{\mathrm{T}}_m]^{\mathrm{T}}$. Также для матрицы~$\mathbf{V}$ определим  
$n(r\hm+1)$-век\-тор~$\bm{v}$. В~результате $\varphi(\mathbf{U}, 
\mathbf{V})$ можно переписать как
\begin{equation}
\varphi(\mathbf{U}, \mathbf{V}) \equiv \varphi (\bm{u}, \bm{v})=\left\vert 
\mathbf{F}\bm{u}-\bm{y}\right\vert^2 =\left\vert \mathbf{G}\bm{v}-
\bm{y}\right\vert^2\,,
\end{equation}
где $p$-вектор~$\bm{y}$ формируется из соответствующих 
элементов~$\mathbf{Y}$, а~($p\times mr$)-мат\-ри\-ца~$\mathbf{F}$~--- из 
векторов~$\bm{v}_i$ и~($p\times n(r\hm+1)$)-мат\-ри\-ца~$\mathbf{G}$~--- из 
векторов~$\bm{u}_i$ согласно только значениям $h_{ij}\hm=1$. Смысл~(1) 
в~том, что параметры модели (векторы~$\bm{u}$ и~$\bm{v}$) разделены 
в~двух эквивалентных представлениях целевой функции.
     
     Для нахождения минимума~(1) существует достаточно обширный набор 
алгоритмов: если в~\cite{1-kri} он включает 5~основных процедур, то 
 в~\cite{2-kri} их уже~16. Для целей данной статьи были выбраны 
и~реализованы два варианта: альтернирующий наименьших квадратов (ALS)~--- как наиболее очевидный алгоритм~--- 
и~канонический, но постоянно подвергающийся доработке алгоритм 
Вайберга~\cite{3-kri}. Они относятся к~итерационным и~подразумевают задание 
начального значения вектора~$\bm{v}$, что дает конкретную матрицу~$\mathbf{F}$. 
Далее согласно алгоритму ALS происходит пересчет по формулам:
     \begin{equation}
     \bm{u}=\left( \mathbf{F}^{\mathrm{T}}\mathbf{F}\right)^{-1} \mathbf{F}^{\mathrm{T}} \bm{y}\,,
     \end{equation}
     что дает новое значение~$\mathbf{G}$, и
     $$
     \bm{v}= \left( \mathbf{G}^{\mathrm{T}}\mathbf{G}\right)^{-1}\mathbf{G}^{\mathrm{T}} \bm{y}\,,
     $$ 
     что завершает очередной шаг итерации.
     
Для полноты описания последовательности действий достаточно добавить 
критерий завершенности итераций. 

     Последняя из опубликованных полная версия алгоритма 
Вайберга~\cite{4-kri} имеет следущий вид.
     \begin{enumerate}[1.]
\item Задать начальное значение~$\bm{v}$.
\item Сформировать $\mathbf{F}$ из~$\bm{v}$ и~найти вектор~$\bm{u}$, 
минимизирующий $\vert \mathbf{F}\bm{u}\hm- \bm{y}\vert^2$, по формуле~(2).
\item Завершить вычисления, если достигнута требуемая точность 
аппроксимации; в~противном случае продолжить и~выполнить шаг~4.
\item Сформировать $\mathbf{G}$ из~$\bm{u}$ и~найти вектор~$\Delta_{\bm 
v}$, минимизирующий $\vert \mathbf{QG}\Delta_{\bm v} \hm- 
\mathbf{Q}\bm{y}\vert^2$ при $\mathbf{Q}\hm= \mathbf{I}\hm- 
\mathbf{F}(\mathbf{F}^{\mathrm{T}}\mathbf{F})^{-1}\mathbf{F}^{\mathrm{T}}$, используя при этом 
формулу $\Delta_{\bm v}\hm= (\mathbf{QG})^+\mathbf{Q}\bm{y}$. Обновить 
вектор~$\bm{v}$ на $\bm{v}\hm+ \Delta_{\bm v}$ и~перейти к~шагу~2. 
\end{enumerate}
     
     Решение задачи минимизации на шаге~4 усложняется из-за того, что 
$\mathbf{QG}$ всегда является матрицей неполного ранга. Это дает толчок для 
уточнения общих идей~\cite{3-kri} и~рождения вариантов под общим 
наименованием алгоритма Вайберга. Согласно~\cite{4-kri}, перспективным 
вариантом оказывается использование псевдообращения Му\-ра--Пен\-роу\-за 
матрицы~$\mathbf{QG}$: если сингулярное разложение $\mathbf{QG}\hm= 
\mathbf{SDT}^{\mathrm{T}}$, то
     $$
     \left(\mathbf{QG}\right)^+ - \mathbf{T} \tilde{\mathbf{D}}^{-1} 
\mathbf{S}^{\mathrm{T}}\,,
     $$
где $\tilde{\mathbf{D}}^{-1}\hm= \mathrm{diag}\,(1/d_1, \ldots, 1/d_q, 0, \ldots)$ 
и~$d_1,\ldots, d_q$ суть ненулевые сингулярные числа матрицы~$\mathbf{QG}$. 
При этом надо выбирать $q\hm= (n\hm-r)r$.
     
     В заключение надо уточнить два момента: задание начального 
значения~$\bm{v}$ и~критерия завершенности итераций. 
Относительно~$\bm{v}$ обычно предлагается только лишь его случайное 
заполнение. Но не следует забывать о~возможностях более целенаправленных 
методов: если размерности задачи и~интенсивность пропусков позволяют, 
можно привлечь один из методов обработки неполных данных (например, 
использование подмножества данных без пропусков или заполнение пропусков 
средними значениями наблюденных значений) с~последующим нахождением 
сингулярного разложения теперь уже полной матрицы данных и~формирование 
начального значения~$\bm{v}$ из правых сингулярных векторов. Задание 
критерия завершенности в~работах по алгоритмам матричной факторизации 
умалчивается; в~данной работе контролировалось относительное изменение 
значений целевой функции на последовательных шагах итераций, которое 
должно стать меньше заданного значения~$\mathrm{Tol}$; при этом число шагов 
итерации не должно было превышать $t_{\max}$.
     
     \textbf{Задача построения} $\tilde{\mathbf{Y}}_\mu$ формулируется 
как минимизация по $\mathbf{U}$, $\mathbf{V}$ и~$\bm{\mu}$ целевой 
функции:
     $$
     \varphi(\mathbf{U}, \mathbf{V}, \bm{\mu}) =\left\| \mathbf{H}\odot 
\left(\mathbf{Y}-\tilde{\mathbf{Y}}_\mu \right)\right\|^2_F \to 
\min\limits_{\mathbf{U}, \mathbf{V}, \bm{\mu}}\,.
     $$
Шаги построения оценок параметров модели со средними подобны 
случаю~$\tilde{\mathbf{Y}}$. Построчная запись матрицы~$\mathbf{U}$ 
остается прежней. Введем матрицу $\hat{\mathbf{V}}\hm= 
[\mathbf{V},\bm{\mu}]$, для которой каждую строку обозначим 
как~$\hat{\bm{v}}_j$, и~определим $n(r\hm+1)$-век\-тор~$\hat{\bm{v}}$. 
В~результате для $\varphi(\mathbf{U}, \mathbf{V}, \bm{\mu})$ получим 
\begin{multline*}
\varphi (\mathbf{U}, \mathbf{V}, \bm{\mu})\equiv \varphi(\bm{u},\hat{\bm{v}}) =
\left\vert 
\mathbf{F}\bm{u}+(\bm{y}- \bm{\mu})\right\vert^2 ={}\\
{}=\left\vert \hat{\mathbf{G}} 
\hat{\bm{v}} -\bm{y}\right\vert^2\,,
\end{multline*}
где $p$-векторы~$\bm{y}$ и~$\bm{\mu}$ формируются из соответствующих 
элементов~$\mathbf{Y}$ и~$\mathbf{M}$, а  
($p\times mr$)-мат\-ри\-ца~$\mathbf{F}$~--- из векторов~$\bm{v}_i$ и~($p\times 
n(r\hm+1)$)-мат\-ри\-ца~$\hat{\mathbf{G}}$~--- из векторов $[\bm{u}_i, 
\mathbf{1}]$ согласно только значениям $h_{ij}\hm=1$. Алгоритмы оценивания 
параметров модели подобны случаю без средних. Соответственно при 
реализации 4-го шага алгоритма Вайберга рекомендовано для матрицы 
$\mathbf{Q}\hat{\mathbf{G}}$ выбирать $q\hm= (n\hm- r) (r\hm+ 1)$.

     
     Далее под алгоритмом Вайберга, согласно сложившимся традициям, 
будем понимать именно кратко описанный вариант с~рекомендациями 
относительно рангов.


     
\section{Эксперименты по~сравнительному анализу~моделей}

     Литература по матричной факторизации при наличии пропусков не 
содержит даже ориентировочных рекомендаций по выбору под\-хо\-дя\-щей модели 
данных (со средними или без, какова сниженная раз\-мер\-ность~$r$), 
ограничиваясь лишь демонстрацией ра\-бо\-то\-спо\-соб\-ности пред\-ла\-га\-емых 
решений. В~данной работе с~по\-мощью экспериментов делается попытка хотя 
бы час\-тич\-но прояснить си\-ту\-ацию. 
{\looseness=1

}
     
     Сравнение моделей осуществлялось с~помощью нормированной формы 
целевой функции $\varphi_N(\cdot) \hm=\varphi(\cdot)/p$, что позволяло 
сопоставлять результаты оценивания параметров модели для различных 
значений интенсивности пропусков. Основу выводов о свойствах моделей 
составляли результаты статистических испытаний, что давало возможность 
проследить за качеством восстановления пропущенных наблюдений 
с~помощью функции $\psi_N(\cdot)\hm= \psi(\cdot)/(mn\hm-p)$, где функция 
$\psi(\cdot)$ аналогична~$\varphi(\cdot)$, но вместо~$h_{ij}$ использует 
множители $(h_{ij}\hm+1) \mathrm{mod}\,2$.
     
     Эксперименты с~выбором конкретного варианта алгоритма проводились 
по следующей схеме: задание объемов анализируемых данных~$m$ и~$n$, 
назначение сниженной размерности~$r_M$, генерирование случайных данных с~фиксированными средним и~дисперсией, получение из них матрицы 
данных~$\mathbf{Y}$ в~подпространстве сниженной размерности, применение того или 
иного метода оценивания параметров модели для проведения сравнительного 
анализа результатов. Необходимость проделывать это многократно и~при 
различных комбинациях условий привела к~вынужденному ограничению 
значений основных параметров: $m\hm\leq 100$, $n\hm\leq 20$, $r_M\hm\leq 
10$. Для определенности рассматривалось ограничение $n\hm<m$. 
Относительно итерационных алгоритмов были приняты значения $\mathrm{Tol}\hm= 
0{,}01$ и~$t_{\max}\hm=100$. 
     
     \textbf{Выбор модели факторизации в~зависимости от учета вектора 
средних.} Если при определенном значения~$r$ и~использовании факторизации 
со средними получено представление $\tilde{\mathbf{Y}}_\mu\hm= 
\mathbf{U}\mathbf{V}^{\mathrm{T}}\hm+\mathbf{1}_m \bm{\mu}^{\mathrm{T}}$, то при $r\hm< n$ 
имеет место следующая комбинация:

\noindent
\begin{align*}
\mathrm{rank}\,(\mathbf{U}\mathbf{V})^{\mathrm{T}}&=r\,;\\
\mathrm{rank}\,(\mathbf{1}_m\bm{\mu}^{\mathrm{T}})&=1\,;\\ 
\mathrm{rank}\,(\mathbf{U}\mathbf{V}^{\mathrm{T}}+\mathbf{1}_m\bm{\mu}^{\mathrm{T}})&= r+1\,. 
\end{align*}
Тогда, использовав сингулярное разложение мат\-рицы
$$
\mathbf{W}= 
(\mathbf{U}\mathbf{V}^{\mathrm{T}}\hm+ \mathbf{1}_m\bm{\mu}^{\mathrm{T}})\hm= 
\mathbf{U}_{\mathbf{W}} \bm{\Sigma}_{\mathbf{W}} 
\mathbf{V}^{\mathrm{T}}_{\mathbf{W}},
$$
 получаем факторизацию 
 $$
 \tilde{\mathbf{Y}} = \left(\mathbf{U}_{\mathbf{W}} 
\bm{\Sigma}_{\mathbf{W}}\right)\mathbf{V}^{\mathrm{T}}_{\mathbf{W}}
$$ 
без явного учета 
средних, но уже порядка $(r\hm+1)$. При $r\hm=n$ картина меняется: если 
$\mathrm{rank}\,(\mathbf{U}\mathbf{V}^{\mathrm{T}})\hm=r$ и~$\mathrm{rank}\,(\mathbf{1}_m\bm{\mu}^{\mathrm{T}})\hm=1$ для любого вектора~$\bm{\mu}$, то 
$\mathrm{rank}\,(\mathbf{U}\mathbf{V}^{\mathrm{T}}\hm+ \mathbf{1}_m\bm{\mu}^{\mathrm{T}})\hm=r$. Тогда 
для любого отличного от~$\bm{\mu}$ вектора~$\tilde{\bm{\mu}}$ того же 
размера, что и~$\bm{\mu}$, верно
     \begin{multline*}
     \!\!\!\mathbf{U}\mathbf{V}^{\mathrm{T}} +\mathbf{1}_m\bm{\mu}^{\mathrm{T}}\! = \!
\left(\mathbf{U}\mathbf{V}^{\mathrm{T}} +\mathbf{1}_m\bm{\mu}^{\mathrm{T}} -
\mathbf{1}_m\tilde{\bm{\mu}}^{\mathrm{T}}\right) +\mathbf{1}_m\tilde{\bm{\mu}}^{\mathrm{T}} 
={}\hspace*{-0.67305pt}\\
{}=\mathbf{U}_{\tilde{\mathbf{W}}} \bm{\Sigma}_{\tilde{\mathbf{W}}}
\mathbf{V}^{\mathrm{T}}_{\tilde{\mathbf{W}}} +\mathbf{1}_m \tilde{\bm{\mu}}^{\mathrm{T}}\,,
   \end{multline*}
где $\mathbf{U}_{\tilde{\mathbf{W}}} \bm{\Sigma}_{\tilde{\mathbf{W}}} \mathbf{V}^{\mathrm{T}}_{\tilde{\mathbf{W}}}$~--- сингулярное разложение 
$\tilde{\mathbf{W}} \hm= \mathbf{U}\mathbf{V}^{\mathrm{T}}\hm+\mathbf{1}_m 
(\bm{\mu}^{\mathrm{T}}-\tilde{\bm{\mu}}^{\mathrm{T}})\bm{\mu}^{\mathrm{T}}$. Таким образом получена 
эквивалентная исходной факторизация, но с~произвольными средними. 
Следовательно, целесообразно не учитывать в~модели средние и~ограничиться 
лишь аппроксимацией~$\tilde{\mathbf{Y}}$, тем более что в~этом случае 
алгоритм оценивания параметров несколько проще.
     
     \textbf{Выбор способов реализации итерационного процесса} 
проводился на основании ряда экспериментов. Сравнение для обоих 
алгоритмов случайного задания начального значения~$\bm{v}$ и~заполнение 
пропусков доступными средними с~последующей оценкой~$\bm{v}$ позволили 
отдать предпочтение процедуре вставки из-за несколько более низких значений 
и~более стабильного поведения целевых функций. Негативные свойства 
случайного заполнения вектора~$\bm{v}$ более отчетливо выявлялись 
с~помощью~$\psi_N$ при $r\not= r_M$. При этом неустойчивость (увеличение 
значения $\varphi_N$ или $\psi_N$ для некоторых значений~$r$ могло 
достигать нескольких порядков) в~большей степени была характерна алгоритму 
Вайберга. 

 \begin{figure*}[b] %fig1
\vspace*{1pt}
\begin{minipage}[t]{80mm}
  \begin{center}  
    \mbox{%
\epsfxsize=77.742mm
\epsfbox{kri-1.eps}
}
\end{center}
\vspace*{-6pt}
\Caption{Зависимости точности аппроксимации присутствующих данных 
$\varphi_N$ от~$r$ при разных вероятностях пропуска: \textit{1}~--- $g\hm=0{,}0$; 
\textit{2}~--- $0{,}2$; \textit{3}~--- $0{,}4$; \textit{4}~--- $0{,}6$; 
\textit{5}~--- $g\hm=0{,}8$}
\end{minipage}
%\end{figure*}
\hfill
 %    \begin{figure*} %fig2
\vspace*{1pt}
     \begin{minipage}[t]{80mm}
       \begin{center}  
    \mbox{%
\epsfxsize=78mm
\epsfbox{kri-2.eps}
}
\end{center}
\vspace*{-6pt}
     \Caption{Зависимости точности восстановления пропущенных 
данных~$\psi_N$ от~$r$ при разных вероятностях пропуска: \textit{1}~--- $g\hm=0{,}2$; 
\textit{2}~---$0{,}4$; \textit{3}~--- $0{,}6$; \textit{4}~--- $g\hm=0{,}8$}
\end{minipage}
\end{figure*}

     
     Преимущество ALS-про\-це\-ду\-ры перед принятой реализацией 
алгоритма Вайберга подтвердилось в~ходе дополнительных более тщательных 
испытаний, так как чаще проявлялся спонтанный рост $\varphi_N$ или $\psi_N$ 
для отдельных значений~$r$ при практически одинаковых значениях целевых 
функций в~типичных ситуациях. Кроме этого, реализация ALS-ша\-гов проще и~поэтому более доступна для алгоритмической оптимизации.

\begin{center}  %tabl1
\vspace*{-3pt}
\noindent
\parbox{79mm}{{\tablename~1}\ \ \small{
Значения отклонений эмпирических рангов от теоретических для 
матрицы~$\mathbf{F}$ 
}}

\vspace*{6pt}

%\begin{table*}
{\small %tabl1
\tabcolsep=4.9pt
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\raisebox{-6pt}[0pt][0pt]{$g$}&\multicolumn{10}{c|}{$r$}\\
\cline{2-11}
&1&2&3&4&5&6&7&8&9&10\hphantom{9}\\
\hline
0,2&0&0&0&0&0&4&5&26\hphantom{9}&6&0\\
0,4&0&0&0&9&16\hphantom{9}&22\hphantom{9}&11&5&0&0\\
0,6&0&1&8&38\hphantom{9}&12\hphantom{9}&6&1&0&0&0\\
0,8&0&11\hphantom{9}&8&2&0&0&0&0&0&0\\
\hline
\end{tabular}
}
\end{center}



%\end{table*}
%\begin{table*}\small %tabl2
{\begin{center}



\noindent
\parbox{79mm}{{\tablename~2}\ \ \small{Значения отклонений эмпирических рангов от теоретических для 
матрицы~$\mathbf{G}$}}

\vspace*{6pt}

{\small \tabcolsep=4.9pt
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\raisebox{-6pt}[0pt][0pt]{$g$}&\multicolumn{10}{c|}{$r$}\\
\cline{2-11}
&1&2&3&4&5&6&7&8&9&10\hphantom{9}\\
\hline
0,2; 0,4; 0,6&0&0&0&0&0&0&0&0&0&0\\
0,8&0&11&8&2&0&1&2&1&16\hphantom{9}&11\hphantom{9}\\
\hline
\end{tabular}
}
\end{center}
}
%\end{table*}

\vspace*{6pt}
     
     Выяснение причин нестабильного поведение\linebreak алгоритма Вайберга 
привело к~необходимости дополнительных исследований поведения ран\-гов 
мат\-риц, участ\-ву\-ющих в~оцен\-ке па\-ра\-мет\-ров фак\-то\-ри\-за\-ции. 
     Согласно~\cite{4-kri}, ключевым является \mbox{предположение} о~том, что 
мат\-ри\-цы $\mathbf{U}$, $\mathbf{V}$, $\mathbf{F}$ и~$\mathbf{G}$~--- полного 
ранга. В~случае модели без средних речь идет о~сле\-ду\-ющих величинах: 
$\mathrm{rank}\,\mathbf{U}\hm= \min (r,m)$; $\mathrm{rank}\,\mathbf{V} \hm= \min(r,n)$; 
$\mathrm{rank}\,\mathbf{F}\hm= \min(p,mr)$; $\mathrm{rank}\,\mathbf{G}\hm= \min(p,nr)$, будем 
называть их теоретическими. Проведенные эксперименты показали, что это не 
совсем так:
     \begin{itemize}
\item для матриц $\mathbf{U}$ и~$\mathbf{V}$ отклонений не выявлено;
\item для матриц $\mathbf{F}$ и~$\mathbf{G}$ экспериментальные значения 
рангов подчас не совпадают с~теоретическими значения, причем реже 
для~$\mathbf{G}$ и~чаще для~$\mathbf{F}$.
\end{itemize}
     
     В табл.~1 и~2 для случая $m\hm=50$, $n\hm=10$, $r_M\hm=5$, а~также 
различных значений~$r$ и~вероятности пропуска~$g$ отражены результаты 
оценивания параметров факторизации. Для матрицы~$\mathbf{F}$ значения 
отклонений эмпирических рангов от теоретических приведены в~табл.~1.
          %
     Для сравнения: в~случае матрицы~$\mathbf{G}$ несовпадения 
экспериментальных и~теоретических значений обозначились лишь при 
большом числе пропусков (см.\ табл.~2).



     
     
     Важно обратить внимание, что во всех проведенных экспериментах 
отклонения в~значениях рангов зависят от реализации случайных пропусков, но 
неизменны в~ходе итераций.
     
     Выявленные контрпримеры поведения рангов основополагающих матриц 
объясняют неустойчивое поведение привычных алгоритмов оценивания 
параметров модели и~приводят к~необходимости их коррекции. Если кратко, то 
она свелась к~полному контролю рангов обрабатываемых матриц 
и~использованию псевдообращения Му\-ра--Пен\-роуза.
     
     С точки зрения выбора размерности модели интерес представляют 
исследования зависимости точности аппроксимации данных от~$r$, результаты 
которых приведены на рис.~1. Из них, в~частности, следует:
     \begin{itemize}
\item при умеренных значениях $g\hm< 0{,}5$ уверенно можно говорить о том, 
что увеличение~$r$ сверх значения $t_M\hm=5$ существенного улучшения 
качества аппроксимации не дает;
\item с~увеличением~$g$ точность представления присутствующих элементов 
данных растет, незначительно изменяясь при небольшом смещении~$g$; 
\item увеличение точности аппроксимации приводит к~занижению истинного 
значения~$r_M$. 
\end{itemize}

     
     Точность восстановления пропущенных данных (рис.~2), 
характеризуемая с~помощью~$\psi_N$, падает с~ростом~$g$. Проявляется это 
ярче при оценке параметров факторизации для б$\acute{\mbox{о}}$льших 
значений~$r$.
     
\section{Эксперименты по обработке реальных данных}

     Для апробации разработанных методов факторизации на реальных 
данных использовалась мат\-ри\-ца объект--при\-знак, фигурирующая 
в~исследованиях возможности прогнозировать химический состав мочевых 
камней у пациентов с~уролитиазом по метаболическим показателям мочи 
и~сыворотки крови (см.\ ссылку на проводимые работы и~краткое описание 
данных в~\cite{5-kri}). При этом объекты~--- это пациенты, а признаки 
включали, в~частности, группу метаболических и~антропологических признаков 
пациента.
     
     Общая характеристика данных в~обозначениях этой статьи: $m\hm= 471$, 
$n\hm=17$, количество пропусков~--- 475, тогда $p\hm=7532$ и~оценка $g\hm= 
0{,}06$. Далее понадобится размер так называемой комплектной (полной) 
матрицы данных$\mathbf{Y}_C$, состоящей из строк исходной матрицы без 
пропусков: $m_C\hm= 304$.
     
     Относительно небольшое число строк с~пропусками (35,5\%) позволило 
выделить комплектные данные и~использовать их для следующих целей: 
формирование начального приближения для вектора~$\bm{v}$ при обработке 
всей совокупности данных, обработка в~качестве самостоятельного набора 
данных для построения статистических выводов относительно объекта 
исследований. 
     
     В этой связи важно упомянуть теорему Эк\-кар\-та--Ян\-га  
(Eckart--Young, см.~\cite{6-kri}), касающуюся приближения некоторой матрицы 
$\mathbf{A}$ ранга~$r$ другой матрицей~$\mathbf{A}_s$ меньшего ранга~$s$. 
Если потребовать, чтобы такое приближение было наилучшим в~смыс\-ле нормы 
Фробениуса разности~$\mathbf{A}$ и~$\mathbf{A}_r$ (в~более общем случае 
любой унитарно инвариантной нормы), то оказывается, что~$\mathbf{A}_s$ 
получается из сингулярного разложения $\mathbf{A}\hm= 
\mathbf{P}\bm{\Sigma}\mathbf{Q}^{\mathrm{T}}$ по формуле $\mathbf{A}_s\hm= 
\mathbf{P}\bm{\Sigma}_s\mathbf{Q}^{\mathrm{T}}$, где $\bm{\Sigma}_s$~--- результат 
замены в~$\bm{\Sigma}$ нулями всех ее $(s\hm-r)$ наименьших диагональных 
элементов $\sigma^2_{s+1}, \ldots , \sigma^2_r$, при этом 
     $$
     \left\| \mathbf{A} -\mathbf{A}_s\right\|^2_F =\sigma^2_{s+1} + \cdots + 
\sigma^2_r\,.
     $$
     
     Размеры матрицы $m_C\hm= 304$ позволяют легко найти сингулярное 
разложения и~тем самым сформировать начальные приближения для 
различных~$r$ за счет усечения до первых~$r$ столб\-цов  
мат\-ри\-цы~$\mathbf{Q}$. Таким образом комплектную матрицу можно 
эффективно использовать при оценивании качества аппроксимации 
представленных данных с~пропусками. 

{ \begin{center}  %fig3
 \vspace*{-6pt}
    \mbox{%
\epsfxsize=79.335mm
\epsfbox{kri-3.eps}
}

\end{center}

\vspace*{-3pt}

\noindent
{{\figurename~3}\ \ \small{
Зависимость качества аппроксимации от размерности модели~$r$ в~виде~$\varphi_N$ для комплектных данных~(\textit{1})
и~для данных 
с~пропусками~(\textit{2}), а~также чис\-ла $N_A$ аномальных восстановленных элементов мат\-ри\-цы 
данных~(\textit{3}) 
}}}

\vspace*{9pt}


     
     В принципе, можно попробовать хотя бы грубо оценивать качество 
восстановления и~тех данных, которых нет. В~рассматриваемом случае 
анализируемая матрица данных содержит результаты измерений, 
принимающие неотрицательные значения, поэтому появление отрицательных 
элементов в~$\tilde{\mathbf{Y}}$ свидетельствует об их аномальности и~может 
стать мерой качества восстановления пропущенных данных. Число подобных 
ситуаций обозначим как~$N_A$. 
     


     
     Результаты экспериментов по оцениванию параметров факторизации 
     и~качества аппроксимации данных отражены на рис.~3. Из-за высокой 
временн$\acute{\mbox{о}}$й сложности соответствующих алгоритмов при 
нахождении~$\varphi_N$ для данных с~пропусками пришлось ограничиться 
диапазоном значений $r\hm=1, \ldots , 12$. Наряду с~этими значениями 
приведены числа~$N_A$, для $r\hm= 1,2,3$ они нулевые. Анализ полученных 
результатов позволяет сделать следующие выводы:
     \begin{itemize}
\item значения $\varphi_N$ для~$\tilde{\mathbf{Y}}$ несколько лучше, чем 
$\tilde{\mathbf{Y}}_C$; это говорит о том, что дополнение $\mathbf{Y}_C$ 
данными, пусть и~с~пропусками, улучшает аппроксимацию присутствующих 
наблюдений; 
\item значения $\varphi_N$ для $\tilde{\mathbf{Y}}$ и~$\tilde{\mathbf{Y}}_C$ 
практически не отличаются, но вычислять $\varphi_N$ при больших~$r$ 
становится затруднительным; поэтому в~рассматриваемом случае можно 
обойтись сингулярным разложением комплектной матрицы и~ее производными 
величинами, тем более что оно не существенно хуже;
\item как указано в~начальной части статьи, восстановление пропущенных 
данных ухудшается с~ростом принятых значений~$r$; в~частности, если 
появится необходимость использовать~$\tilde{\mathbf{Y}}$ в~полной мере, то 
следует ожидать появления аномальных данных.
\end{itemize}
     
     Обратим внимание на следующее. Если принять $g\hm= 0{,}06$ 
в~качестве вероятности пропуска, то строка матрицы вообще не содержит 
пропусков с~вероятностью 0,941$^{17}$. Для случайных пропусков при $m\hm= 471$ число 
строк без пропусков распределено приближенно нормально со средним~168 
и~стандартным отклонением~10, что никак не согласуется с~наблюденным 
значением $m_C\hm= 304$. Это ставит под сомнение возможность применения 
для анализа данных специфических вероятностных методов и~широко 
используемого механизма случайных пропусков (Missing at Random, MAR) 
и~побуждает отдать предпочтение общим методам факторизации матрицы 
данных. 

\vspace*{-6pt}

\section{Заключение}

\vspace*{-2pt}

     В работе используемые методы факторизации матриц данных 
с~пропусками дополнены деталями, выявляющими и~исправляющими ошибки 
в~доступных публикациях. Они имеют важное прикладное значение 
и~углубляют понимание особенностей моделей и~методов аппроксимации 
данных. Эксперименты с~синтетическими и~реальными данными позволили 
обратить внимание на возможности использования комплектных подмножеств 
исходных данных, на необходимость учитывать негативные свойства 
восстановленных данных.
     
     Наличие достаточно обширного набора алгоритмов факторизации 
матрицы данных с~пропусками в~первую очередь говорит об отсутствии 
универсальной эффективной процедуры оценивания параметров модели. 
Поэтому так важно продолжать работу по исследованию существующих 
и~созданию новых алгоритмов, активному внедрению решений, учитывающих 
специфические особенности обрабатываемых матриц. 

\vspace*{-6pt}

{\small\frenchspacing
 {\baselineskip=12pt
 %\addcontentsline{toc}{section}{References}
 \begin{thebibliography}{9}
 
 \vspace*{-2pt}
 
\bibitem{1-kri}
\Au{Chen P.} Optimization algorithms on subspaces: Revisiting missing data 
problem in low-rank matrix~// Int. J.~Comput. Vision, 2008. Vol.~80. Iss.~1. 
P.~125--142.
\bibitem{2-kri}
\Au{Hong J.\,H., Fitzgibbon~A.} Secrets of matrix factorization: Approximations, 
numerics, manifold optimization and random restarts. {\sf  
http://www.cv-foundation.org/openaccess/\linebreak content\_iccv\_2015/papers/Hong\_Secrets\_of\_Matrix\_\linebreak ICCV\_2015\_paper.pdf}.
\bibitem{3-kri}
\Au{Wiberg T.} Computation of principal components when data are missing~// 2nd 
Symposium of Computational Statistics Proceedings.~--- Wien: Physica-Verlag, 
1976. P.~229--236.
\bibitem{4-kri}
\Au{Okatani T., Deguchi~K.} On the Wiberg algorithm for matrix factorization in the 
presence of missing components~// Int. J.~Comput. Vision, 2007. Vol.~72. Iss.~3. 
P.~329--337.
\bibitem{5-kri}
\Au{Кривенко М.\,П.} Критерии значимости отбора признаков 
классификации~// Информатика и~её применения, 2016. Т.~10. Вып.~3.  
С.~32--40.
\bibitem{6-kri}
\Au{Seber G.\,A.\,F.} A~matrix handbook for statisticians.~--- Hoboken, NJ, USA: 
Wiley, 2008. 560~p.
\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Поступила в~редакцию 15.04.22}}

\vspace*{8pt}

%\pagebreak

%\newpage

%\vspace*{-28pt}

\hrule

\vspace*{2pt}

\hrule

%\vspace*{-2pt}

\def\tit{MODEL SELECTION FOR MATRIX FACTORIZATION\\ WITH~MISSING COMPONENTS}


\def\titkol{Model selection for matrix factorization with~missing components}


\def\aut{M.\,P.~Krivenko}

\def\autkol{M.\,P.~Krivenko}

\titel{\tit}{\aut}{\autkol}{\titkol}

\vspace*{-8pt}


\noindent
Federal Research Center ``Computer Science and Control'' of the Russian Academy of Sciences, 
44-2~Vavilov Str., Moscow 119333, Russian Federation

\def\leftfootline{\small{\textbf{\thepage}
\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND
APPLICATIONS\ \ \ 2022\ \ \ volume~16\ \ \ issue\ 3}
}%
 \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---
INFORMATICS AND APPLICATIONS\ \ \ 2022\ \ \ volume~16\ \ \ issue\ 3
\hfill \textbf{\thepage}}}

\vspace*{3pt} 






\Abste{The work is dedicated to the problem of factorizing a matrix with missing components into 
a product of two lower-rank matrices. The influence of the intensity of missing on the choice of the 
factorization model is studied. Two algorithms for parameter estimation are considered: alternating 
least squares (ALS) and Wiberg~--- for two factorization models: with and without means. It 
is substantiated that there is no need to use a~model with averages: it is a special case of another 
model and, in some cases, leads to ambiguous solutions. During the experiments, the preference was 
given to a more stable ALS algorithm. The advantages of the insertion method over random filling 
in the initial settings of iterative algorithms for estimating model parameters are demonstrated. The 
reasons for the negative properties of the existing version of the Wiberg algorithm are revealed. 
Based on the experiments, it was found that with an increase in the probability of missing, the 
accuracy of the presentation of the available data increases which leads to an underestimation of 
the true value of the model dimension.}

\KWE{lower-rank matrix approximation; singular value decomposition; missing data; ALS 
algorithm; Wiberg algorithm}

\DOI{10.14357/19922264220307} 

%\vspace*{-16pt}

%\Ack
%\noindent




%\vspace*{4pt}

  \begin{multicols}{2}

\renewcommand{\bibname}{\protect\rmfamily References}
%\renewcommand{\bibname}{\large\protect\rm References}

{\small\frenchspacing
 {%\baselineskip=10.8pt
 \addcontentsline{toc}{section}{References}
 \begin{thebibliography}{9}
\bibitem{1-kri-1}
\Aue{Chen, P.} 2008. Optimization algorithms on subspaces: Revisiting missing data problem in 
low-rank matrix. \textit{Int. J.~Comput. Vision} 80(1):125--142.
\bibitem{2-kri-1}
\Aue{Hong, J.\,H., and A.~Fitzgibbon.} 2015. Secrets of matrix factorization: Approximations, 
numerics, manifold optimization and random restarts. Available at: {\sf  
http://www.cv-\linebreak foundation.org/openaccess/content\_iccv\_2015/papers/\linebreak Hong\_Secrets\_of\_Matrix\_ICCV\_2015\_paper.pdf} (accessed 
May~26, 2022).
\bibitem{3-kri-1}
\Aue{Wiberg, T.} 1976. Computation of principal components when data are missing. \textit{2nd 
Symposium of Computational Statistics Proceedings}. Wien: Physica-Verlag. 229--236.

\vspace*{-1pt}

\bibitem{4-kri-1}
\Aue{Okatani, T., and K.~Deguchi.} 2007. On the Wiberg algorithm for matrix factorization in the 
presence of missing components. \textit{Int. J.~Comput. Vision} 72(3):329--337.

\vspace*{-1pt}

\bibitem{5-kri-1}
\Aue{Krivenko, M.\,P.} 2016. Kriterii znachimosti otbora pri\-zna\-kov klassifikatsii [Significance 
tests of feature selection for classification]. \textit{Informatika i~ee Primeneniya~--- Inform. Appl.} 
10(3):32--40.

\vspace*{-1pt}

\bibitem{6-kri-1}
\Aue{Seber, G.\,A.\,F.} 2008. \textit{A~matrix handbook for statisticians}. Hoboken, NJ, USA: 
Wiley. 560~p.

\end{thebibliography}

 }
 }

\end{multicols}

\vspace*{-6pt}

\hfill{\small\textit{Received April 15, 2022}}

\Contrl

\noindent
\textbf{Krivenko Michail P.} (b.\ 1946)~--- Doctor of Science in technology, professor, leading 
scientist, Institute of Informatics Problems, Federal Research Center ``Computer Science and 
Control'' of the Russian Academy of Sciences, 44-2~Vavilov Str., Moscow 119333, Russian 
Federation; \mbox{mkrivenko@ipiran.ru}




\label{end\stat}

\renewcommand{\bibname}{\protect\rm Литература} 
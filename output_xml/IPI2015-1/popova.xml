<?xml version="1.0" encoding="utf-8"?>
<papers>
  <paper>
    <metadata>
      <annotation value="\Abst{Решается проблема построения оптимальных устойчивых моделей в задаче классификации физической активности человека. Каждый тип физической активности конкретного человека описывается набором признаков, сгенерированных по временн${{ы}"/>
    </metadata>
    <fulltext>
      <literature/>
      <text value="popova"/>
      <text value="ВЫБОР ОПТИМАЛЬНОЙ МОДЕЛИ КЛАССИФИКАЦИИ ФИЗИЧЕСКОЙ АКТИВНОСТИ ПО ИЗМЕРЕНИЯМ АКСЕЛЕРОМЕТРА$^*$"/>
      <text value="Выбор оптимальной модели классификации физической активности по измерениям акселерометра"/>
      <text value="М."/>
      <text value="С. Попова$^1$, В."/>
      <text value="В. Стрижов$^2$"/>
      <text value="М."/>
      <text value="С. Попова, В."/>
      <text value="В. Стрижов"/>
      <command value="\titel">
        <braces>
          <braces>
            <text value="{}"/>
            <command value="\tit">
              <braces/>
              <braces>
                <braces>
                  <text value="{}"/>
                  <command value="\aut">
                    <braces/>
                    <braces>
                      <braces>
                        <text value="{}"/>
                        <command value="\autkol">
                          <braces/>
                          <braces>
                            <braces>
                              <text value="{}"/>
                              <command value="\titkol">
                                <braces/>
                                <braces>
                                  <braces>
                                    <text value="{}"/>
                                    <command value="\renewcommand">
                                      <braces>
                                        <braces>
                                          <text value="{}"/>
                                          <command value="\thefootnote">
                                            <braces/>
                                            <braces>
                                              <braces>
                                                <text value="{}"/>
                                                <command value="\fnsymbol">
                                                  <braces>
                                                    <braces>
                                                      <text value="{}"/>
                                                      <text value="footnote"/>
                                                    </braces>
                                                  </braces>
                                                  <braces/>
                                                  <braces/>
                                                </command>
                                              </braces>
                                            </braces>
                                          </command>
                                        </braces>
                                      </braces>
                                    </command>
                                  </braces>
                                </braces>
                              </command>
                            </braces>
                          </braces>
                        </command>
                      </braces>
                    </braces>
                  </command>
                </braces>
              </braces>
            </command>
          </braces>
        </braces>
      </command>
      <command value="\footnotetext">
        <braces>
          <text value="\footnotetext[1]{}"/>
        </braces>
      </command>
      <text value="Работа поддержана Skolkovo Institute of Science and Technology (Skoltech) в рамках SkolTech/MITInitiative."/>
      <command value="\renewcommand">
        <braces>
          <braces>
            <text value="{}"/>
            <command value="\thefootnote">
              <braces/>
              <braces>
                <braces>
                  <text value="{}"/>
                  <command value="\arabic">
                    <braces>
                      <braces>
                        <text value="{}"/>
                        <text value="footnote"/>
                      </braces>
                    </braces>
                    <braces/>
                    <braces/>
                  </command>
                </braces>
              </braces>
            </command>
          </braces>
        </braces>
      </command>
      <command value="\footnotetext">
        <braces>
          <text value="\footnotetext[1]{}"/>
        </braces>
      </command>
      <text value="Московский физико-технический институт, maria"/>
      <command value="\_popova">
        <braces>
          <text value="@phystech.edu"/>
        </braces>
        <braces/>
      </command>
      <command value="\footnotetext">
        <braces>
          <text value="\footnotetext[2]{}"/>
        </braces>
      </command>
      <text value="Вычислительный центр Российской академии наук им. А."/>
      <text value="А. Дородницына, strijov@ccas.com"/>
      <command value="\vspace*">
        <braces>
          <braces>
            <text value="{}"/>
            <text value="2pt"/>
          </braces>
        </braces>
        <braces/>
      </command>
      <formula id="id1" value="$м&#10;рядам с~акселерометра. В~условиях мультиколлинеарности признаков&#10;выбор устойчивых моделей классификации затруднен из-за необходимости&#10;оценки большого числа параметров этих моделей. Оценка оптимального&#10;значения параметров также затруднена в~связи с~тем, что функция&#10;ошибок имеет большое количество локальных минимумов в~пространстве&#10;параметров. В~работе исследуются модели, принадлежащие классу&#10;двуслойных нейронных сетей. Ставится задача нахождения&#10;Па\-ре\-то-опти\-маль\-но\-го фронта на множестве допустимых моделей.&#10;Предлагаются критерии оптимального, последовательного и~устойчивого&#10;прореживания нейронной сети, критерий наращивания сети, а~также&#10;строится стратегия пошаговой модификации модели с~использованием&#10;предложенных критериев. В~вычислительном эксперименте модели,&#10;порождаемые предложенной стратегией, сравниваются по трем критериям&#10;качества~--- сложности, точности и~устойчивости.}&#10;&#10;\vspace*{2pt}&#10;&#10;\KW{классификация; нейронные сети; сложность;&#10;устойчивость; оптимальность по Парето; критерии прореживания и~наращивания}&#10;&#10;\DOI{10.14357/19922264150107}&#10;&#10;\vspace*{6pt}&#10;&#10;&#10;\vskip 14pt plus 9pt minus 6pt&#10;&#10;\thispagestyle{headings}&#10;&#10;\begin{multicols}{2}&#10;&#10;\label{st\stat}&#10;&#10;&#10;&#10;\section{Введение}&#10;&#10;Для получения точного и~устойчивого прогноза физической активности&#10;человека необходимы\linebreak методы, позволяющие выбирать адекватные модели&#10;из некоторого множества допустимых моделей-пре\-тен\-ден\-тов. Проблема&#10;выбора моделей об\-суж\-да\-ет\-ся в~работах~[1--3]. Настройка параметров\linebreak&#10;универсальной модели является нетривиальной многоэкстремальной&#10;оптимизационной задачей. Предлагается упростить эту задачу,&#10;рассматривая наборы последовательно порождаемых устойчивых моделей&#10;заданной сложности. Модели порождаются путем модификации структуры&#10;искусственной нейронной сети. Решается задача последовательной&#10;модификации нейронной сети. Требуется получить нейронную сеть&#10;с~небольшим числом связей между нейронами, которая достаточно точно&#10;решала бы задачу классификации физической активности человека по&#10;показаниям акселерометра и~обладала бы устойчивостью к возмущениям&#10;данных. Ввиду этого возникает задача минимизации сложности модели&#10;без потери точности классификации~\cite{Myung2000Complexity}.&#10;&#10;Существует два базовых подхода к~решению задачи выбора сетей&#10;оптимальной структуры: \textit{наращивание структуры сети} (network&#10;growing)~\cite{MacLeod2001Grow} и~\textit{прореживание структуры сети}&#10;(network pruning)~[6--8].&#10;&#10;Согласно первому подходу в качестве начальной модели выбирается сеть&#10;недостаточной сложности, решающая поставленную задачу с~большим&#10;значением функции ошибки, после чего~в~сеть добавляются новые&#10;нейроны и~связи между ними. В~\cite{MacLeod2001Grow} описаны&#10;некоторые методы наращивания, приведен сравнительный анализ&#10;генетических алгоритмов с~алгоритмом байесовской оптимизации.&#10;В~алгоритмах метода прореживания модифицируется многослойная сеть&#10;с~избыточным числом нейронов и~связей между ними. Классическими&#10;алгоритмами прореживания нейронных сетей являются &amp;lt;&amp;lt;optimal brain&#10;damage&amp;gt;&amp;gt;~\cite{LeCun1990Optimal} и~&amp;lt;&amp;lt;optimal brain surgery&amp;gt;&amp;gt;~\cite{Hassibi93}, основанные на вычислении вторых производных&#10;функции ошибки. Также получили развитие \textit{гибридные алгоритмы},&#10;в~которых объединяются оба упомянутых выше подхода~[9--11].&#10;&#10;В данной работе предлагается стратегия пошаговой модификации&#10;нейронной сети, комбинирующая этапы добавления и~удаления&#10;па\-ра\-мет\-ров~\cite{Knerr1990Stepwise, Strijov2013Evidence-1}. Стратегия включает&#10;в~себя критерии прореживания и наращивания структуры сети, критерии&#10;останова этапов добавления и~удаления параметров, а~также критерий&#10;останова процедуры модификации. Согласно предложенной стратегии&#10;процедура модификации начинается с~нейронной сети избыточной&#10;сложности и чередует шаги удаления и добавления параметров до тех&#10;пор, пока этот процесс не стабилизируется согласно критерию останова&#10;процедуры модификации. Критерии прореживания и~наращивания позволяют&#10;на каж\-дом шаге процедуры модификации выбирать параметр, добавление&#10;или удаление которого улучшит качество нейронной сети. Качество сети&#10;оценивается по трем критериям: сложности, точ\-ности&#10;и~устойчивости~\cite{Tokmakova2012HyperPar, Leonteva2012Feature,&#10;Zaycev2012Evaluation}. Также предлагается рассматривать процедуру&#10;пошаговой модификации нейронной сети как путь в~многомерном кубе.&#10;&#10;В вычислительном эксперименте определяются значения критериев&#10;качества для нейронных сетей, порождаемых предложенной стратегией.&#10;В~качестве тестового примера рассматривается задача классификации&#10;физической активности человека по измерениям акселерометра~\cite{Kwapisz2010Activity}.&#10;&#10;\section{Постановка задачи}&#10;&#10;Дана выборка $\mathfrak{D} =\left\{ (\mathbf{x}_i,&#10;\mathbf{t}_i) \right\}, i\in\mathcal{I}\hm=\{1 \dots m\}$,&#10;состоящая из~$m$ объектов~$\mathbf{x}$, каждый~из~которых описывается~$n$&#10;признаками,~$\mathbf{x}_i\in\mathbb{R}^n$,&#10;и~принадлежит одному из~$z$ классов~$\mathbf{t}_i\hm\in\{0,1\}^z$.&#10;Также задано разбиение множества индексов выборки $\mathcal{I} \hm= \mathcal{L}\sqcup \mathcal{T}$ на обучающую&#10;${(\mathbf{x}_i,\mathbf{t}_i)}, i \hm\in \mathcal{L}$,&#10;и~контрольную ${(\mathbf{x}_i,\mathbf{t}_i)}, i \hm\in \mathcal{T}$.&#10;Требуется выбрать устойчивую модель классификации оптимальной сложности.&#10;&#10;&#10;\smallskip&#10;&#10;\noindent&#10;\textbf{Определение 1.}&#10;Моделью назовем отображение&#10;\begin{equation*}&#10;\mathbf{f} : (\mathop{\mathbf{w}}\limits_{k \times 1},&#10;\mathop{\mathbf{x}}\limits_{1 \times n}) \mapsto&#10;\mathop{\mathbf{y}}\limits_{1 \times z}\,,&#10;\end{equation*}&#10;где $\mathbf{w} = \left[w_1, \dots,w_j,\dots,w_k\right]^{\mathrm{T}}$, $j \hm\in \mathcal{J} = \{1,\dots, k\},$~---&#10;вектор параметров модели; $\mathbf{x} \hm\in \mathbb{R}^{n\times m}$~---&#10;матрица плана; $\mathbf{y} \in \{0,1\}^z$~--- зависимая переменная.&#10;&#10;\smallskip&#10;&#10;Предполагается, что переменная $\mathbf{y}$~--- мультиномиально&#10;распределенная случайная величина, а~переменная~$\mathbf{w}$ имеет нормальное&#10;распределение с~нулевым математическим ожиданием:&#10;\begin{equation}&#10;\label{eq:1}&#10;\mathbf{w} \sim \mathcal{N}\left(\boldsymbol{0}, \mathbf{A}^{-1}\right)\,,&#10;\end{equation}&#10;где $\mathbf{A}^{-1}$~--- ковариационная матрица параметров&#10;общего вида, по\-ло\-жи\-тель\-но-опре\-де\-лен\-ная:&#10;$\mathbf{w}^{\mathrm{T}}\mathbf{A}\mathbf{w} \hm&amp;gt; \boldsymbol{0}$ для любого&#10;$\mathbf{w} \in \mathbb{R}^k$.&#10;&#10;В данной работе  рассматриваются модели, принадлежащие классу&#10;двухслойных нейронных сетей c~функциями активации $\tanh$  и~softmax:&#10;\begin{align}&#10;\label{eq14}&#10;\mathbf{a(\mathbf{x})}&amp;amp; =&#10;\mathop{\mathbf{W}_2^{\mathrm{T}}}\limits_{N_h \times z}\mathbf{tanh}(\mathop{\mathbf{W}_1^{\mathrm{T}}}\limits_{n \times N_h}\mathbf{x})\,;&#10;\\&#10;\mathbf{f(\mathbf{x})} &amp;amp;= \fr{\exp(\mathbf{a(\mathbf{x})})}{\sum_{j=1}^n \exp(a_j(\mathbf{x}))}\,.\notag&#10;\end{align}&#10;Вектор~$\mathbf{f}$ интерпретируется как вектор&#10;вероятностей: $f_{\xi}$ есть вероятность того, что вектор~$\mathbf{x}$&#10;принадлежит классу с~номером~${\xi}$:&#10;\begin{equation*}&#10;\mathbf{f(\mathbf{x})} = \{f_\xi\}\,,\enskip&#10;0 \leq f_{\xi} \leq 1\,,\enskip \sum f_{\xi} = 1, \enskip {\xi} = 1\dots z\,.&#10;\end{equation*}&#10;Под вектором параметров двухслойной нейронной сети будем понимать&#10;$\mathbf{w}\hm=\mathbf{vec}\left(\mathbf{W}_1^{\mathrm{T}}|\mathbf{W}_2^{\mathrm{T}}\right)$,&#10;где $\mathbf{W}_1$ и~$\mathbf{W}_2$~--- матрицы весов первого и~второго слоя&#10;нейронной сети~(\ref{eq14}). Вектор $\mathbf{y}\hm=[y_1,\dots,y_{\xi},\dots,y_z]^{\mathrm{T}}$&#10;определим следующим образом:&#10;\begin{equation*}&#10;y_{\xi} = \begin{cases}&#10;1, &amp;amp; \mbox{если}~{\xi} = \mathop{\arg\max}\limits_{\xi \in \{1,\dots,z\}} (p_{\xi})\,;  \\&#10;0 &amp;amp; \mbox{иначе}\,.&#10;\end{cases}&#10;\end{equation*}&#10;Вектор $\mathbf{y}$~--- это вектор метки класса, полученный для&#10;объекта~$\mathbf{x}$ с~помощью построенной модели, в~то время как&#10;вектор~$\mathbf{t}$~-- это вектор метки класса объекта~$\mathbf{x}$ из&#10;выборки~$\mathfrak{D}$.&#10;&#10; Под \textit{структурным} параметром двухслойной нейронной сети&#10;будем понимать количество нейронов в~скрытом слое нейронной сети~--- $N_h$.&#10;Матрица весов первого слоя имеет размерность $n \times N_h$, мат\-ри\-ца весов&#10;второго слоя имеет размерность $N_h \times z$. Далее будем считать, что&#10;структурный параметр фиксирован и~одинаков для всех рассматриваемых моделей.&#10;&#10;\smallskip&#10;&#10;\noindent&#10;\textbf{Определение 2.}&#10;Параметр $w_j$ модели $\mathbf{f}$ назовем активным, если $w_j \hm\neq 0$.&#10;&#10;&#10;\smallskip&#10;&#10;\noindent&#10;\textbf{Определение 3.}&#10;Структурой $\mathcal{A}$ модели~$\mathbf{f}$ назовем множество&#10;индексов активных параметров этой модели $\mathcal{A} \hm= \{j: w_j \neq 0 \}&#10;\subseteq \mathcal{J}$.&#10;&#10;&#10;\smallskip&#10;&#10;Каждая структура $\mathcal{A} \subseteq \mathcal{J}$ задает некоторую модель&#10;\begin{equation*}&#10;\mathbf{f}_{\mathcal{A}} : \mathbf{\hat{w}}_{\mathcal{A}} \in \mathbb{R}^k\,,&#10;\end{equation*}&#10;где $\mathbf{f}_{\mathcal{A}}$~--- модель со структурой $\mathcal{A}$,&#10;а~$\mathbf{\hat{w}}_{\mathcal{A}}\hm\in \mathbb{R}^k$~--- оптимальный&#10;вектор параметров модели $\mathbf{f}_{\mathcal{A}}$, определение которому&#10;будет дано ниже. Объединение всех $\mathbf{f}_\mathcal{A}$ назовем множеством&#10;допустимых моделей&#10;\begin{equation}&#10;\label{eq:2}&#10;\mathfrak{F} = \bigcup\limits_{{\mathcal{A} \subseteq \mathcal{J}}}\{\mathbf{f}_\mathcal{A}\}\,.&#10;\end{equation}&#10;Оптимальную модель $\mathbf{\hat{f}}_{\mathcal{A}}$ будем выбирать из множества допустимых&#10;моделей~$\mathfrak{F}$.&#10;&#10;Согласно гипотезе~(\ref{eq:1}) о~распределении многомерных случайных&#10;величин~$\mathbf{y}$ и~$\mathbf{w}$ в~качестве функции ошибки выберем функцию&#10;\begin{equation*}&#10;%\label{eq:3}&#10;S(\mathbf{w}|\mathcal{K}) = - \sum\limits_{i \in \mathcal{K}}&#10;\sum\limits_{\xi = 1}^{z} t_{i{\xi}}\ln&#10;\left( f_{\xi}(\mathbf{x}_i, \mathbf{w}) \right)\,,&#10;\end{equation*}&#10;максимизирующую логарифм правдоподобия случайной&#10;величины~$\mathbf{y}$ и~заданную на разбиении выборки~$\mathfrak{D}$,&#10;определенном некоторым множеством индексов $\mathcal{K} \subseteq \mathcal{I}$,&#10;$\mathbf{t}_i \hm= [t_{i1},\dots,t_{i\xi},\dots,t_{iz}]^{\mathrm{T}}$.&#10;&#10;&#10;\smallskip&#10;&#10;\noindent&#10;\textbf{Определение 4.}&#10;Оптимальным вектором параметров модели~$\mathbf{f}_{\mathcal{A}}$  назовем&#10;такой вектор~$\mathbf{\hat{w}}_{\mathcal{A}}$, который является решением&#10;сле\-ду\-ющей задачи оптимизации:&#10;\begin{equation}&#10;\label{eq:4}&#10;\mathbf{\hat{w}}_{\mathcal{A}} = \mathop{\arg\min}\limits_{{\mathbf{w}_{\mathcal{A}} \in \mathbb{R}^k}}S(\mathbf{w}_\mathcal{A}|\mathcal{L})\,.&#10;\end{equation}&#10;&#10;&#10;\smallskip&#10;&#10;Для оценки качества моделей и~сравнения их друг с~другом введем три&#10;критерия качества~--- сложность, устойчивость и~точность.&#10;&#10;\smallskip&#10;&#10;\noindent&#10;\textbf{Определение 5.}&#10;Сложностью $C \hm= C(\mathbf{\hat{w}})$ модели~$\mathbf{f}$ c~вектором&#10;параметров $\mathbf{\hat{w}} \hm= [w_1, \dots, w_k]$ назовем мощность&#10;множества активных параметров этой модели&#10;\begin{equation*}&#10;C(\mathbf{w})= \sum\limits_{i=1}^k[w_i \neq 0] = |\mathcal{A}|\,.&#10;\end{equation*}&#10;&#10;&#10;\smallskip&#10;&#10;Чем больше мощность множества активных параметров, тем сложнее модель.&#10;Максимально возможная сложность модели равна размерности пространства&#10;параметров~$k$.&#10;&#10;\smallskip&#10;&#10;\noindent&#10;\textbf{Определение 6.}&#10;Устойчивостью $\eta \hm= \eta(\mathbf{\hat{w}})$ модели~$\mathbf{f}$&#10;c~вектором параметров $\mathbf{w}$ назовем число~$\eta$, равное числу&#10;обусловленности матрицы $\mathbf{A}$~($\ref{eq:1}$), т.\,е.&#10;\begin{equation*}&#10;\eta(\mathbf{\hat{w}})=\fr{\lambda_{\max}}{\lambda_{\min}}\,,&#10;\end{equation*}&#10;где $\lambda_{\max}$~--- максимальное, а~$\lambda_{\min}$~---&#10;минимальное собственное число матрицы $\mathbf{A}$.&#10;&#10;&#10;\smallskip&#10;&#10;Чем лучше обусловлена матрица $\mathbf{A}$, тем более устойчива модель.&#10;У~абсолютно устойчивой модели $\lambda_{\min} \hm= \lambda_{\max}$, $\eta \hm= 1$.&#10;&#10;&#10;\smallskip&#10;&#10;\noindent&#10;\textbf{Определение 7.}&#10;Под точностью $S$ модели~$\mathbf{f}$ с~вектором параметров~$\mathbf{\hat{w}}$&#10;будем понимать величину функции ошибки~(\ref{eq:2}) на контрольной выборке.&#10;&#10;\smallskip&#10;&#10;Чем больше значение функции ошибки, тем меньше точность модели.&#10;&#10;Введем на множестве допустимых моделей $\mathcal{F}$ отношение&#10;доминирования. Будем говорить, что \textit{модель~$"/>
      <command value="\mathbf">
        <braces>
          <braces>
            <text value="{}"/>
            <text value="f"/>
          </braces>
        </braces>
        <braces/>
        <braces>
          <text value="&amp;apos;"/>
        </braces>
      </command>
      <formula id="id2" value="$ доминирует&#10;над моделью~$"/>
      <command value="\mathbf">
        <braces>
          <braces>
            <text value="{}"/>
            <text value="f"/>
          </braces>
        </braces>
        <braces/>
      </command>
      <formula id="id3" value="$} и~обозначать $\mathbf{f}&amp;apos; \succ \mathbf{f}$, если&#10;\begin{equation*}&#10;C^\prime \leq C\,; \quad \eta^\prime \leq \eta\,;\quad S^\prime \leq S_b\,,&#10;\end{equation*}&#10;где $C$, $\eta$, $S$ и~$C^\prime$, $\eta^\prime$, $S^\prime$ --- сложность,&#10;устойчивость и~точность&#10;моделей~$\mathbf{f}$ и~$\mathbf{f}&amp;apos;$.&#10;&#10;\smallskip&#10;&#10;\noindent&#10;\textbf{Определение 8.}&#10;Модель $\mathbf{f} \hm\in \mathcal{F}$ назовем оптимальной по Парето, если&#10;не существует $\mathbf{f^\prime}\hm\in \mathcal{F}$ такой,&#10;что $\mathbf{f^\prime} \succ \mathbf{f}$.&#10;&#10;\smallskip&#10;&#10;\noindent&#10;\textbf{Определение 9.}&#10;Множество оптимальных по Парето моделей назовем Па\-ре\-то-опти\-маль\-ным&#10;фронтом {POF}$_{\mathfrak{F}}$ множества допустимых моделей~$\mathfrak{F}$.&#10;&#10;&#10;\smallskip&#10;&#10;Задача выбора оптимальной модели состоит в~том, чтобы найти&#10;Па\-ре\-то-опти\-маль\-ный фронт {POF}$_{\mathfrak{F}}$ множества&#10;допустимых моделей~$\mathfrak{F}$.&#10;&#10;\section{Стратегия пошаговой модификации модели}&#10;&#10;\noindent&#10;\textbf{Определение 10.}&#10;Стратегией пошаговой модификации модели называется процедура&#10;последовательного изменения модели, в~которой на каж\-дом шаге решается&#10;оптимизационная задача вида&#10;\begin{equation*}&#10;\hat{j} = \mathop{\arg \text{opt}}\limits_{{j \in \mathcal{A}}}Q&#10;(\mathbf{\hat{w}}_\mathcal{A})\,,&#10;\end{equation*}&#10;где $Q$~--- один из вышеприведенных критериев качества или их&#10;Па\-ре\-то-опти\-маль\-ный набор.&#10;&#10;\smallskip&#10;&#10;Стратегия задается следующими математическими объектами:&#10;\begin{itemize}&#10;\item набором критериев оптимизации~--- слож\-ностью, точ\-ностью, устойчивостью $\{C, S, \eta\}$,&#10;\item набором ограничений на структуру и~па\-ра\-мет\-ры&#10;модели~$\mathcal{A} \subseteq \mathcal{J}$,&#10;$\mathbf{w} = \mathbf{\hat{w}}_{\mathcal{A}}$ из~($\ref{eq:4}$),&#10;\item критериями останова шагов удаления~(см.\ ($\ref{eq:11}$))&#10;и~добавления~(см.\ $(\ref{eq:12})$),&#10;\item критерием останова процедуры выбора модели~(см.\ $(\ref{eq:13})$).&#10;\end{itemize}&#10;Действуя согласно стратегии, будем изменять структуру модели, удаляя&#10;из нее элементы и~добавляя их согласно~(\ref{eq:13}).&#10;&#10;Для определения индекса параметра~$\hat{j}$, который должен быть&#10;удален из модели или добавлен в~нее, ниже предлагается несколько&#10;критериев оптимизации модели.&#10;&#10;\subsection{Критерий оптимального прореживания}&#10;&#10;Этот критерий позволяет выяснить индекс параметра, удаление которого&#10;приведет к~минимизации приращения функции ошибки~(\ref{eq:2}).&#10;Для функции ошибки используется локальная аппроксимация вблизи некоторого&#10;локального минимума вектора параметров~$\mathbf{w}_0$:&#10;\begin{multline*}&#10;S(\mathbf{w}_0 + \Delta \mathbf{w}) = S(\mathbf{w}_0) +&#10;\mathbf{g}^{\mathrm{T}}\mathbf{(w}_0)\Delta \mathbf{w} + {}\\&#10;{}+\fr{1}{2}\,\Delta\mathbf{w}^{\mathrm{T}}&#10;\mathbf{H}\Delta \mathbf{w} + O(\parallel\Delta \mathbf{w}\parallel^3)\,,&#10;\end{multline*}&#10;где~$\Delta \mathbf{w}$~---~возмущение вектора параметров~в~данной&#10;точке~$\mathbf{w}_0;\,\mathbf{g(w}_0)$~--- вектор градиента, вычисленный&#10;в~точке $\mathbf{w}_0$; $\mathbf{H} \hm= \mathbf{H(w}_0)$~--- матрица&#10;вторых производных функции ошибки.&#10;Предполагается, что мат\-ри\-ца вторых производных&#10;$\mathbf{H}\hm=\mathbf{H}(\mathbf{w})$~--- диагональная, а~функция ошибки&#10;в~окрестности глобального или локального минимума является квадратичной.&#10;На основании этих гипотез аппроксимация функции ошибки записывается&#10;в~следующем виде:&#10;\begin{equation*}&#10;\Delta S = S(\mathbf{w}_0 + \Delta \mathbf{w}) - S(\mathbf{w}_0) =&#10;\fr{1}{2}\Delta \mathbf{w}^{\mathrm{T}}\mathbf{H}\Delta \mathbf{w}\,.&#10;\end{equation*}&#10;&#10;Пусть~$w_j$~--- некоторый параметр. Удаление этого параметра&#10;(присваивание ему нулевого значения) эквивалентно выполнению условия&#10;\begin{equation*}&#10;\mathbf{e}_j^{\mathrm{T}} \Delta \mathbf{w} + w_j = 0\,,&#10;\end{equation*}&#10;где~$\mathbf{e}_j^{\mathrm{T}}$~--- вектор, все элементы&#10;которого равны нулю, за исключением~$j$-го,~который равен единице.&#10;Таким образом, получаем задачу условной минимизации&#10;\begin{equation*}&#10;\Delta S = \fr{1}{2}\,\Delta \mathbf{w}^{\mathrm{T}}\mathbf{H}\Delta\mathbf{w}&#10;\rightarrow \min\,;\enskip \mathbf{e}_j^{\mathrm{T}} \Delta \mathbf{w} + w_j = 0\,.&#10;\end{equation*}&#10;Для решения этой задачи строим лагранжиан&#10;\begin{equation*}&#10;L = \fr{1}{2}\,\Delta \mathbf{w}^{\mathrm{T}}\mathbf{H}\Delta \mathbf{w} -&#10;\lambda_i(\mathbf{e}_j^{\mathrm{T}} \Delta \mathbf{w} + w_j)\,.&#10;\end{equation*}&#10;Продифференцировав~$L$~по~$\Delta \mathbf{w}$, получаем значение&#10;лагранжиана~$L_j$ для элемента~$w_j$:&#10;\begin{equation*}&#10;L_j = \fr{w_j^2}{2[\mathbf{H}^{-1}]_{j,j}}\,,&#10;\end{equation*}&#10;где~$\mathbf{H}^{-1}$~--- матрица, обратная&#10;гессиану~$\mathbf{H}$; $[\mathbf{H}^{-1}]_{j,j}$~--- $j$-й диагональный элемент&#10;этой матрицы. Значение лагранжиана~$L_j$ называется выпуклостью~$w_j$.&#10;Выпуклость~$L_j$ описывает рост среднеквадратичной ошибки, вызываемый удалением&#10;параметра~$w_j$.&#10;&#10;Критерию оптимального прореживания отвечает параметр~$w_{\hat{j}}$,&#10;соответствующий минимальному значению выпуклости:&#10;\begin{equation}&#10;\label{eq:6}&#10;\hat{j} = \mathop{\arg\min}\limits_{{j \in \mathcal{A}}}L_j\,.&#10;\end{equation}&#10;&#10;\subsection{Критерий последовательного прореживания}&#10;&#10;В качестве второго критерия предлагается прос\-той критерий последовательного&#10;удаления па\-ра\-мет\-ров~$w_j$~--- компонент вектора~$\mathbf{w}$. Основной идеей&#10;этого критерия является принцип ло\-каль\-но-опти\-маль\-но\-го выбора~---&#10;критерию отвечает параметр~$w_j$, без которого функция ошибки~(\ref{eq:2})&#10;оказывается минимальной.&#10;&#10;Для нахождения параметра, отвечающего этому критерию, решается задача&#10;\begin{equation}&#10;\label{eq:7}&#10;\hat{j} = \mathop{\arg\min}\limits_{{j \in \mathcal{A}}}S(\mathbf{w}_{\mathcal{A}}&#10;\setminus {w_j}|\mathcal{T})\,.&#10;\end{equation}&#10;&#10;\subsection{Критерий устойчивого прореживания}&#10;&#10;Помимо вышеописанных критериев предлагается критерий устойчивого прореживания,&#10;основанный на модификации метода Белсли~\cite{Belsley2005, Sandulyanu2012Feature}.&#10;&#10;Пусть $\mathbf{W}$~--- матрица реализаций оптимального вектора&#10;параметров~$\mathbf{\hat{w}}$, определенного в~($\ref{eq:4}$)&#10;и~рассматриваемого согласно~($\ref{eq:2}$) как&#10;многомерная случайная величина. Пусть эта матрица имеет размерность $r \times k$.&#10;Выполним ее сингулярное разложение:&#10;\begin{equation}&#10;\label{eq:8}&#10;\mathbf{W} = \mathbf{U}\mathbf{S}\mathbf{V}^{\mathrm{T}},&#10;\end{equation}&#10;где $\mathbf{U}$ и $\mathbf{V}$~--- ортогональные матрицы размера&#10;$r \times r$ и $k \times k$, при этом $r$~-- количество оценок,&#10;а~$k$~-- размерность вектора параметров~$\mathbf{w}$; $\mathbf{\Lambda}$~---&#10;матрица, на диагонали которой стоят сингулярные числа матрицы~$\mathbf{W}$.&#10;&#10;По определению ковариационная матрица вектора параметров~$\mathbf{w}$&#10;вычисляется как&#10;\begin{multline*}&#10;\mathbf{A}^{-1} = \text{\bf{cov}}(\mathbf{W}) =&#10; \mathsf{E}(\mathbf{W}^{\mathrm{T}}\mathbf{W}) -&#10;\mathsf{E}(\mathbf{W})\mathsf{E}(\mathbf{W}^{\mathrm{T}}) ={}\\&#10;{}= \mathsf{E}(\mathbf{W}^{\mathrm{T}}\mathbf{W})\,.&#10;\end{multline*}&#10;Последнее равенство выполняется в~силу предположения о~том,&#10;что математическое ожидание вектора параметров равно нулю:&#10;$\mathsf{E}(\mathbf{w}) \hm= \boldsymbol{0}$. По матрице реализаций~$\mathbf{W}$&#10;многомерной случайной величины~$\mathbf{w}$ ковариационная матрица может быть&#10;оценена следующим образом:&#10;\begin{equation*}&#10;\mathbf{A}^{-1} = \fr{1}{r}\,\mathbf{W}\mathbf{W}^{\mathrm{T}}.&#10;\end{equation*}&#10;У ковариационной матрицы есть нулевые строки с~индексами из&#10;множества $\mathcal{J}\backslash\mathcal{A}$, где $\mathcal{J}$~---&#10;множество индексов всех параметров модели,&#10;а~$\mathcal{A}$~--- множество индексов активных параметров. Таким образом,&#10;ковариационная матрица является неполноранговой.&#10;&#10;Используя сингулярное разложение~(\ref{eq:8}) матрицы~$\mathbf{W}$,&#10;получим выражение для матрицы $\mathbf{A}^{-1}$:&#10;\begin{multline*}&#10;\mathbf{A}^{-1} = (\mathbf{W}\mathbf{W}^{\mathrm{T}}) =&#10;(\mathbf{U}\mathbf{\Lambda}\mathbf{V}^{\mathrm{T}}\mathbf{V}\mathbf{\Lambda}^{\mathrm{T}}&#10;\mathbf{U}^{\mathrm{T}}) = {}\\&#10;{}=(\mathbf{U}\mathbf{\Lambda}\mathbf{\Lambda}^{\mathrm{T}}\mathbf{U}^{\mathrm{T}}) =&#10;\mathbf{U}\mathbf{\Lambda}^{2}\mathbf{U}^{\mathrm{T}}.&#10;\end{multline*}&#10;&#10;Индексом обусловленности $\eta_{\zeta}$ назовем отношение&#10;максимального элемента~$\lambda_{\max}$ матрицы~$\mathbf{\Lambda}$&#10;к~${\zeta}$-му по величине элементу~$\lambda_{\zeta}$ этой матрицы:&#10;\begin{equation*}&#10;\eta_{\zeta} = \fr{\lambda_{\max}}{\lambda_{\zeta}}\,.&#10;\end{equation*}&#10;&#10;Так как ковариационная матрица $\mathbf{A}^{-1}$&#10;неполноранговая, то некоторые значения индексов обу\-слов\-лен\-ности не определены.&#10;Чтобы избежать этой проблемы, исключим из рассмотрения параметры с~дисперсией,&#10;меньшей некоторого порога~$\alpha$, и~добавим к~каж\-до\-му элементу, стоящему&#10;на диагонали ковариационной матрицы, небольшое число~$\tau$.&#10;&#10;Оценками дисперсии параметров будут диагональные элементы~$\mathbf{A}^{-1}$:&#10;\begin{equation*}&#10;\sigma(w_{\zeta}) = \mathbf{A}^{-1}_{\zeta \zeta}\,.&#10;\end{equation*}&#10;&#10;Долевой коэффицент $q_{{\zeta}j}$ определим как вклад $j$-го признака&#10;в~дисперсию ${\zeta}$-го элемента вектора параметров~$\mathbf{w}$:&#10;\begin{equation*}&#10;q_{{\zeta}j} = \fr{u_{{\zeta}j}^2\lambda_{jj}^2}{\sigma(w_{\zeta})}\,.&#10;\end{equation*}&#10;&#10;Находим индексы обусловленности и~долевые коэффициенты для набора&#10;активных параметров~$\mathcal{A}$. Большие значения индексов обусловленности&#10;указывают на зависимость между признаками. Поэтому для нахождения параметра,&#10;отвечающего этому критерию прореживания, находим максимальный индекс&#10;обусловленности&#10;\begin{equation*}&#10;\hat{{\zeta}} = \mathop{\text{argmax}}\limits_{{{\zeta} \in&#10;\mathcal{A}}}\eta_{\zeta}\,.&#10;\end{equation*}&#10;Затем находим максимальный долевой коэффициент, соответствующий&#10;найденному максимальному индексу обусловленности~$\eta_{\hat{{\zeta}}}$:&#10;\begin{equation}&#10;\label{eq:9}&#10;\hat{j} = \mathop{\text{argmax}}\limits_{{j \in \mathcal{A}}}q_{\hat{{\zeta}}j}\,.&#10;\end{equation}&#10;Параметр $w_{\hat{j}}$ и есть параметр, отвечающий критерию устойчивого&#10;прореживания.&#10;&#10;\subsection{Критерий последовательного наращивания}&#10;&#10;Критерий последовательного добавления параметров, как&#10;и~критерий~($\ref{eq:7}$), основан на принципе ло\-каль\-но-опти\-маль\-но\-го&#10;выбора~--- критерию отвечает параметр, при добавлении которого&#10;в~сеть функция ошибки~(\ref{eq:2}) минимальна.&#10;&#10;Для нахождения параметра, отвечающего этому критерию, решается задача&#10;\begin{equation*}&#10;%\label{eq:10}&#10;\hat{j} = \mathop{\text{argmin}}\limits_{{j \in \mathcal{J} \backslash \mathcal{A}}}S(\mathbf{w}_{\mathcal{A}} \cup {w_j}|\mathcal{T})\,.&#10;\end{equation*}&#10;&#10;\subsection{Описание базовой стратегии}&#10;&#10;Стратегия пошаговой модификации модели состоит из двух этапов~--- Del&#10;и~Add. Перед началом процедуры модификации все параметры модели активны.&#10;&#10;\textbf{Этап Del.} Ищем параметр с~индексом~$\hat{j}$, отвечающий одному&#10;из критериев прореживания~($\ref{eq:6}$), ($\ref{eq:7}$) или~($\ref{eq:8}$),&#10;и~удаляем его из множества активных параметров:&#10;\begin{equation*}&#10;\mathcal{A} = \mathcal{A} \backslash \hat{j}\,.&#10;\end{equation*}&#10;Этап Del повторяем до тех пор, пока ошибка&#10;$S(\mathbf{w}_{\mathcal{A}}|\mathcal{T})$ не превысит свое&#10;минимальное значение на данном этапе более чем на некоторое заданное&#10;значение~$\delta S_1$. Критерием останова шага Del является&#10;следующее условие:&#10;\begin{equation}&#10;\label{eq:11}&#10;S(\mathbf{\hat{w}}_{\mathcal{A}}|\mathcal{T}) \geq S_{\min} + \delta S_1\,,&#10;\end{equation}&#10;где $S_{\min}$~--- некоторое заданное значение.&#10;&#10;\textbf{Этап Add.} В модели ищем параметр $\hat{j}$,&#10;отвечающий критерию наращивания~($\ref{eq:9}$), и~добавляем найденный параметр&#10;во множество активных параметров:&#10;\begin{equation*}&#10;\mathcal{A} = \mathcal{A} \cup {\hat{j}}\,.&#10;\end{equation*}&#10;Критерием останова шага Add является выполнение условия&#10;\begin{equation}&#10;\label{eq:12}&#10;S(\mathbf{\hat{w}}_{\mathcal{A}}|\mathcal{T}) \geq S_{\min} + \delta S_2\,,&#10;\end{equation}&#10;где $S_{\min}$~--- некоторое заданное значение. На рис.~1&#10;приведен график, демонстрирующий изменение функции ошибки при удалении&#10;параметров из модели. Аналогичным образом ведет себя фукнция ошибки при&#10;добавлении параметров в~модель. Из графика видно, что эта зависимость имеет&#10;минимум, а~значит модели с~большим чис\-лом параметров&#10;не являются наиболее точными. На рис.~2 показано, как&#10;согласно критериям останова~($\ref{eq:11}$) и~($\ref{eq:12}$) сменяются&#10;шаги удаления и~добавления.&#10;&#10;&#10;&#10;Процедура модификации продолжается до тех пор, пока процесс не стабилизируется.&#10;В~качестве критерия стабилизации предлагается использовать энтропию изменения&#10;структуры модели:&#10;\begin{equation}&#10;\label{eq:13}&#10;H(\mathcal{A},\mathcal{A&amp;apos;}) =&#10;-\sum\limits_{j=1}^{k} {\rho(a_j, a_j&amp;apos;)\ln(\rho(a_j, a_j&amp;apos;))}&#10;\end{equation}&#10;&#10;\begin{center}  %fig1&#10;\vspace*{1pt}&#10;\mbox{%&#10; \epsfxsize=80mm %476mm&#10; \epsfbox{pop-1.eps}&#10; }&#10;\end{center}&#10;&#10;%\vspace*{-3pt}&#10;&#10;\noindent&#10;{{\figurename~1}\ \ \small{Изменение функции ошибки при удалении параметров из модели}}&#10;&#10;&#10;\vspace*{6pt}&#10;&#10;&#10;\addtocounter{figure}{1}&#10;&#10;\begin{center}  %fig2&#10;\vspace*{1pt}&#10;\mbox{%&#10; \epsfxsize=80mm %.475mm&#10; \epsfbox{pop-2.eps}&#10; }&#10;&#10;\vspace*{6pt}&#10;&#10;&#10;{{\figurename~2}\ \ \small{Смена шагов Del и Add}}&#10;\end{center}&#10;&#10;&#10;\vspace*{9pt}&#10;&#10;&#10;\addtocounter{figure}{1}&#10;&#10;&#10;&#10;&#10;\noindent&#10;множества попарных нормированных расстояний Хэмминга между&#10;элементами наборов $\mathcal{A} \hm= \{a_1,\dots,a_k\}$&#10;и~$\mathcal{A}&amp;apos; \hm=\{a_1&amp;apos;,\dots,a_k&amp;apos;\}$, полученных на двух&#10;последовательных итерациях алгоритма следующим образом:&#10;\begin{equation*}&#10;a_j = \begin{cases}&#10;1, &amp;amp; \mbox{если}~w_j \neq 0\,;  \\&#10;0, &amp;amp; \mbox{если}~w_j = 0\,.&#10;\end{cases}&#10;\end{equation*}&#10;Процесс считается стабильным, если энтропия $H(\mathcal{A},\mathcal{A&amp;apos;})$&#10;не превосходит заданного порога.&#10;&#10;\section{Путь в {\boldmath{$k$}}-мерном кубе}&#10;&#10;В данной задаче будем иметь дело с~вектором параметров размерности~$k$.&#10;Это означает, что существует~$2^k$~вариантов структуры модели.&#10;Из этих~$2^k$~возможных вариантов структуры выбираются оптимальные.&#10;Все варианты можно представить в~виде вершин~$\mathbf{v}$&#10;$k$-мер\-но\-го куба~$\mathfrak{V}$. И~тогда стратегия задает путь~$\mathbf{V}$&#10;по его вершинам. Этот путь заканчивается в некоторой&#10;вершине~$\mathbf{\hat{v}}$, к~которой сходится процедура&#10;модификации. Будем искать оптимальные модели в~некоторой окрестности&#10;вершины~$\mathbf{\hat{v}}$. Так как охватить все возможные варианты&#10;слишком трудоемко, то в~качестве окрестности $\mathbf{\hat{v}}$&#10;будем рассматривать ведущий к~$\mathbf{\hat{v}}$ путь по вершинам&#10;куба, полученный по описанной выше стратегии.&#10;&#10;\smallskip&#10;&#10;\noindent&#10;\textbf{Пример 1}. В этом примере использовалась&#10;выборка~$\{\mathbf{x}_i, y_i\}$, $i\hm\in\{1,\dots,177\}$. Каждый&#10;объект выборки описывался 6~признаками $\mathbf{\chi}_1, \dots,&#10;\mathbf{\chi}_6$ и~принадлежал одному из трех классов. Схематично&#10;взаимное расположение векторов $\mathbf{\chi}_1, \dots,&#10;\mathbf{\chi}_6$ изображено на рис.~3.&#10;&#10;&#10;Для классификации такой выборки модифицировалась двухслойная&#10;нейронная сеть с~одним нейроном в~скрытом слое. Совокупное число&#10;параметров такой сети равно девяти. Нейронная сеть модифицировалась&#10;за 11~итераций. На рис.~4 изоб\-ражен путь по вершинам&#10;девятимерного куба. По вертикали отложен номер параметра, по&#10;горизонтали~--- номер итерации. Черная клетка означает, что параметр&#10;с~индексом~$j$~--- активный, белая клетка~---&#10; параметр неактивный.&#10;Например, на пятой итера-\linebreak\vspace*{-12pt}&#10;\begin{center}  %fig3&#10;\vspace*{12pt}&#10;\mbox{%&#10; \epsfxsize=74.023mm&#10; \epsfbox{pop-3.eps}&#10; }&#10;&#10;\vspace*{6pt}&#10;&#10;&#10;{{\figurename~3}\ \ \small{Данные}}&#10;\end{center}&#10;&#10;&#10;\vspace*{6pt}&#10;&#10;&#10;\addtocounter{figure}{1}&#10;&#10;\begin{center}  %fig3&#10;\vspace*{1pt}&#10;\mbox{%&#10; \epsfxsize=78.629mm&#10; \epsfbox{pop-4.eps}&#10; }&#10;&#10;\vspace*{6pt}&#10;&#10;&#10;{{\figurename~4}\ \ \small{Путь в кубе}}&#10;\end{center}&#10;&#10;&#10;%\vspace*{9pt}&#10;&#10;&#10;\addtocounter{figure}{1}&#10;&#10;\noindent&#10;ции из сети был удален параметр~9, а~на&#10;одиннадцатой итерации этот параметр был снова добавлен в~сеть.&#10;&#10;\section{Вычислительный эксперимент}&#10;&#10;С целью получить значение критериев качества описанной стратегии был&#10;проведен вычислительный эксперимент. Использовались данные&#10;акселерометра мобильного телефона. Данные состояли из~5418~векторов&#10;признаков, которые были получены в~результате обработки&#10;соответствующих временн$\acute{\mbox{ы}}$х рядов. Было выделено 43~признака&#10;и~6~классов физической активности: ходьба, бег, сидение, стояние, подъем&#10;и~спуск. Временн$\acute{\mbox{ы}}$е ряды записывались акселерометром мобильного&#10;телефона, который находился в~кармане у~человека, выполняющего один&#10;из типов физической активности. Для выделения признаков временн$\acute{\mbox{ы}}$е&#10;ряды разделялись на десятисекундные сегменты. Из этих сегментов&#10;извлекались признаки, такие как проекции среднего ускорения на&#10;координатные оси, среднеквадратические отклонения от проекций&#10;среднего ускорения на каждую из трех координатных осей, время между&#10;пиками синусоидального сигнала в~миллисекундах и~др. С~более&#10;подробным описанием признаков и~процессом их генерации можно&#10;ознакомиться в~\cite{Kwapisz2010Activity}.&#10;&#10;&#10;&#10;В вычислительном эксперименте оптимизировалась двухслойная нейронная&#10;сеть с~пятью нейронами в~скрытом слое. Размерность вектора&#10;па\-ра\-метров такой модели $k\hm=245$. Нейронная сеть\linebreak оптимизировалась по&#10;стратегии, описанной в~разд.~3. Был получен набор из 771~модели.&#10;В~процедуре модификации использовался каждый из трех критериев&#10;прореживания~--- оптимального, последовательного и~устойчивого. Для&#10;всех моделей были вычислены значения критериев качества. Был\linebreak&#10;построен Па\-ре\-то-опти\-маль\-ный фронт трех кри\-териев. На&#10;рис.~5&#10;изображены все полученные модели. Пустыми значками обозначены модели,&#10;\mbox{которые} были получены по стратегии с~применением критерия&#10;устойчивого прореживания, серыми значками~--- критерия&#10;последовательного прореживания, черными&#10; значками~--- оптимального&#10;прореживания. Па\-ре\-то-опти\-маль\-ные \mbox{модели} обозначены черными крестиками.&#10; Из&#10;рис.~5,\,\textit{а} видно, что самые устойчивые модели получаются при&#10;использовании критерия устойчивого прореживания. В~таблице приведены&#10;значения критериев качества моделей, \mbox{которые} являются точками&#10;останова процедуры модификации для каждого их трех критериев&#10;прореживания.&#10;&#10;&#10;&#10;\begin{center}  %fig5&#10;\vspace*{1pt}&#10;\mbox{%&#10; \epsfxsize=77.734mm&#10; \epsfbox{pop-5.eps}&#10; }&#10;\end{center}&#10;&#10;\vspace*{-12pt}&#10;&#10;\noindent&#10;{{\figurename~5}\ \ \small{Множество моделей в координатах &amp;lt;&amp;lt;устой\-чивость--слож\-ность&amp;gt;&amp;gt;~(\textit{а}),&#10;&amp;lt;&amp;lt;точ\-ность--слож\-ность&amp;gt;&amp;gt;~(\textit{б}) и~&amp;lt;&amp;lt;точ\-ность--устой\-чивость&amp;gt;&amp;gt;~(\textit{в})}}&#10;&#10;&#10;\vspace*{6pt}&#10;&#10;&#10;\addtocounter{figure}{1}&#10;&#10;\begin{table*}\small&#10;\begin{center}&#10;%\vspace*{2ex}&#10;&#10;&#10;\begin{tabular}{|l|c|c|c|}&#10;\multicolumn{4}{c}{Сложность, точность и устойчивость моделей}\\&#10;\multicolumn{4}{c}{\ }\\[-5pt]&#10;        \hline&#10;        \multicolumn{1}{|c|}{Стратегия}&amp;amp; Cложность &amp;amp; Точность &amp;amp; Устойчивость \\&#10;        \hline&#10;&amp;amp;&amp;amp;&amp;amp;\\[-10pt]&#10;        Оптимальное прореживавние &amp;amp; 50 &amp;amp; 877 &amp;amp; $1{,}2\cdot 10^{6}$ \\&#10;%        \hline&#10;        Последовательное прореживавние &amp;amp; 36 &amp;amp; 870 &amp;amp; $2{,}0 \cdot 10^{6}$ \\&#10; %       \hline&#10;        Устойчивое прореживание &amp;amp; 50 &amp;amp; 866 &amp;amp; $6{,}\cdot10^{5}$ \\&#10;        \hline&#10;\end{tabular}&#10;\end{center}&#10;\vspace*{-3pt}&#10;\end{table*}&#10;&#10;&#10;На рис.~6 приведена интерпретация полученных&#10;результатов. В~верхней области графика Па\-ре\-то-оп\-ти\-маль\-ные модели не&#10;интересны для рассмотрения, так как в~этой области имеет место&#10;недо\-обуче\-ние~--- модели излишне сложны. Па\-ре\-то-опти\-маль\-ные&#10; модели&#10;с~незначительной слож\-ностью находятся в~нижней области графика.&#10;&#10;\begin{center}  %fig6&#10;\vspace*{1pt}&#10;\mbox{%&#10; \epsfxsize=77.724mm&#10; \epsfbox{pop-8.eps}&#10; }&#10;&#10;\vspace*{3pt}&#10;&#10;&#10;{{\figurename~6}\ \ \small{Интерпретация результатов}}&#10;\end{center}&#10;&#10;&#10;\vspace*{9pt}&#10;&#10;&#10;\addtocounter{figure}{1}&#10;&#10;&#10;Также была визуализирована процедура пошаговой модификации модели&#10;как путь в $k$-мерном кубе. На рис.~7, так же как и~в~примере~1,&#10;по вертикали отложен номер параметра, по горизонтали~--- номер&#10;итерации. Черная клетка означает, что параметр активный, белая&#10;клетка~--- параметр неактивный. На рис.~6, 7,\,\textit{а} и~7,\,\textit{б}  указана&#10;последовательность, в~которой параметры удалялись из модели&#10;и~до\-бав\-ля\-лись в~нее. Из рис.~7,\,\textit{б} и~7,\,\textit{в}&#10;видно, что&#10;стратегия с~критериями оптимального и~последовательного&#10;прореживания, которые выбирают для удаления параметр, минимизирующий&#10;функцию ошибки, остав\-ля\-ет в~моделях параметры с~номерами с~216 по~245.&#10;Это связано с~тем, что параметры с~такими номерами относятся ко&#10;второму слою нейронной сети, а~удаление большого числа параметров&#10;второго слоя приводит к~росту функции ошибки.&#10;&#10;&#10;&#10;\section{Заключение}&#10;&#10;В работе была предложена стратегия пошаговой модификации моделей&#10;классификации согласно трем критериям качества~--- сложности,&#10;точности и~устойчивости. В~рамках стратегии были предложены критерии&#10;добавления и~удаления параметров в~модель, критерии останова шагов&#10;добавления и~удаления, а~также критерий останова&#10; процедуры&#10;модификации. Процедура пошаговой модификации модели была рассмотрена&#10;и~визуали-\linebreak\vspace*{-12pt}&#10;&#10;\begin{center}  %fig7&#10;\vspace*{1pt}&#10;\mbox{%&#10; \epsfxsize=77.709mm&#10; \epsfbox{pop-9.eps}&#10; }&#10;\end{center}&#10;&#10;\vspace*{-9pt}&#10;&#10;&#10;\noindent&#10;{{\figurename~7}\ \ \small{Путь в кубе: устойчивое~(\textit{а}); последовательное~(\textit{б})&#10;и~оптимальное~(\textit{в})  прореживания}}&#10;&#10;&#10;&#10;%\vspace*{9pt}&#10;&#10;&#10;\addtocounter{figure}{1}&#10;&#10;&#10;&#10;\noindent&#10;зирована как путь в~многомерном кубе. Был проведен&#10;вычислительный эксперимент, в~ходе которого был получен набор&#10;моделей и~найден Па\-ре\-то-опти\-маль\-ный фронт критериев качества этого&#10;набора. Вычислительный эксперимент показал, что наилучшие по&#10;рассматриваемым критериям качества модели получаются при&#10;использовании критерия устойчивого прореживания. Это связано с~тем,&#10;что\linebreak критерий устойчивого прореживания позволяет получать более&#10;устойчивые модели, удаляя коррелирующие параметры и~тем самым&#10;повышая устойчивость и~обобщающую способность модели классифика\-ции.&#10;Программная реализация стратегии пошаговой модификации нейронной&#10;сети в~среде разработки MatLab находится в~свободном доступе~\cite{StrategyCode}.&#10;&#10;&#10;&#10;{\small\frenchspacing&#10; {%\baselineskip=10.8pt&#10; \addcontentsline{toc}{section}{References}&#10; \begin{thebibliography}{99}&#10;\bibitem{Vizilter2012learnong} %1&#10;\Au{Визильтер Ю.\,В., Горбацевич В.\,С., Каратеев~С.\,Л., Кост\-ро\-мов~Н.\,А.}&#10;Обучение алгоритмов выделения кожи на цветных изображениях лиц~// Информатика&#10;и~её применения, 2012. Т.~6. Вып.~1. С.~109--113.&#10;&#10;&#10;&#10;\bibitem{Tokmakova2012HyperPar} %3&#10;\Au{Токмакова~А.\,А., Стрижов~В.\,В.} Оценивание гиперпараметров линейных&#10;и~регрессионных моделей при отборе шумовых и~коррелирующих признаков~//&#10;Информатика и~её применения, 2012. Т.~6. Вып.~4. С.~66--75.&#10;&#10;\bibitem{Haplanov2013assimptothic} %2&#10;\Au{Хапланов~А.\,Ю.} Асимптотическая нормальность оценки параметров&#10;многомерной логистической регрессии~// Информатика и~её применения, 2013. Т.~7.&#10;Вып.~2. С.~69--74.&#10;&#10;\bibitem{Myung2000Complexity} %4&#10;\Au{Myung~I.\,J.} The importance of complexity in model selection~//&#10;J.~Math. Psychol., 2000. Vol.~44. No.\,1. P.~190--204.&#10;&#10;\bibitem{MacLeod2001Grow} %5&#10;\Au{MacLeod~C., Maxwell~M.} Incremental evolution in ANNs: Neural&#10;nets which grow~// Artif. Intell. Rev., 2001. Vol.~16. No.\,3. P.~201--224.&#10;&#10;\bibitem{Karnin1990Simple} %6&#10;\Au{Karnin~E.\,D.} A simple procedure for pruning back-propagation&#10;trained neural networks~// IEEE Trans. Neural Networks, 1990. Vol.~1. No.\,2. P.~239--242.&#10;&#10;\bibitem{LeCun1990Optimal} %7&#10;\Au{LeCun~Y., Denker~L.\,S., Solla~S.\,A.} Optimal brain damage~//&#10;Adv. Neur. Inform. Processing Syst., 1990. Vol.~2. No.\,2. P.~598--605.&#10;&#10;\bibitem{Hassibi93} %8&#10;\Au{Hassibi~B., Stork~D.\,G., Woff~G.\,J.} Optimal brain surgeon and&#10;general network pruning~// IEEE  Conference (International) on Neural Networks&#10;Proceedings, 1993. Vol.~1. P.~293--299.&#10;&#10;\bibitem{Han2011Water} %9&#10; \Au{Hong-Gui~H., Qi-li~C., Jun-Fei~Q.} An efficient self-organizing {RBF}&#10;neural network for water quality prediction~// Neural Networks, 2011.&#10;Vol.~24. No.\,7. P.~717--725.&#10;&#10;&#10;\bibitem{Yang2012PruningAlgorithm} %10&#10;\Au{Yang~S., Chen~Y.} An evolutionary constructive and pruning algorithm&#10;for artificial neural networks and its prediction applications~// Neurocomputing,&#10;2012. Vol.~86. P.~140--149.&#10;&#10;\bibitem{Pu2013PruningAlgorithm} %11&#10;\Au{Pu~X., Pengfei Sun~P.} A~new hybrid pruning neural network algorithm based on&#10;sensitivity analysis for stock market forcast~// J.~Inform.&#10;Comput. Sci., 2013. Vol.~3. P.~883--892.&#10;&#10;&#10;\bibitem{Knerr1990Stepwise} %12&#10;\Au{Knerr~S., Personnaz~L., Dreyfus~G.} Single-layer learning revisited:&#10;A~stepwise procedure for building and training a neural network~//&#10;Neurocomputing Algorithms Architectures Applications, 1990.&#10;Vol.~68. No.\,1. P.~41--50.&#10;&#10;\bibitem{Strijov2013Evidence} %13&#10;\Au{Strijov~V., Krymova~E., Weber~S.\,V.} Evidence optimization for consequently&#10;generated models~// Math.  Comput. Modell., 2010. Vol.~57. No.\,1--2. P.~50--56.&#10;&#10;\bibitem{Leonteva2012Feature} %14&#10;\Au{Леонтьева~Л.\,Н.} Последовательный выбор признаков при восстановлении&#10;регрессии~// Машинное обучение и~анализ данных, 2012. Т.~1. №\,3. С.~335--346.&#10;&#10;\bibitem{Zaycev2012Evaluation} %15&#10;\Au{Зайцев~А.\,А., Токмакова~А.\,А.} Оценка гиперпараметров&#10;линейных регрессионных моделей методом максимального правдоподобия&#10;при отборе шумовых и~коррелирующих признаков~// Машинное обучение&#10;и~анализ данных, 2012. Т.~1. №\,3. С.~347--353.&#10;&#10;\bibitem{Kwapisz2010Activity} %16&#10;\Au{Kwapisz~J.\,R., Weiss~G.\,M., Moore~S.} Activity recognition using cell&#10;phone accelerometers~// SIGKDD Explorations, 2010. Vol.~12. No\,2. P.~74--82.&#10;&#10;\bibitem{Belsley2005} %17&#10;\Au{Belsley~D.\,A., Kuh~E., Welsch~R.\,E.} Regression diagnostics:&#10;Identifying influential data and sources of collinearity.~--~New York:&#10;John Wiley and Sons, 2005. 302~p.&#10;&#10;\bibitem{Sandulyanu2012Feature} %18&#10;\Au{Сандуляну~Л.\,Н., Стрижов~В.\,В.} Выбор признаков&#10;в~авторегрессионных задачах прогнозирования~// Информационные технологии, 2012.&#10;Т.~7. С.~11--15.&#10;&#10;\bibitem{StrategyCode} %19&#10;\Au{Попова~М.\,С.} Реализация стратегии пошаговой модификации нейронной сети~//&#10;Algorithms Machine Learning, 2014.&#10;{\sf http://sourceforge.net/p/mlalgorithms/&#10;code/HEAD/tree/Group174/Popova2014OptimalMode lSelection/code/main.m}.&#10; \end{thebibliography}&#10;&#10; }&#10; }&#10;&#10;\end{multicols}&#10;&#10;\vspace*{-1pt}&#10;&#10;\hfill{\small\textit{Поступила в редакцию 10.08.14}}&#10;&#10;\newpage&#10;&#10;%\vspace*{12pt}&#10;&#10;%\hrule&#10;&#10;%\vspace*{2pt}&#10;&#10;%\hrule&#10;&#10;%\vspace*{12pt}&#10;&#10;\def\tit{SELECTION OF~OPTIMAL PHYSICAL ACTIVITY CLASSIFICATION MODEL USING MEASUREMENTS&#10;OF~ACCELEROMETER}&#10;&#10;\def\titkol{Selection of~optimal physical activity classification model using measurements&#10;of~accelerometer}&#10;&#10;\def\aut{M.~Popova$"/>
      <text value="^1"/>
      <formula id="id4" value="$ and V.~Strijov$"/>
      <text value="^2"/>
      <formula id="id5" value="$}&#10;&#10;\def\autkol{M.~Popova and V.~Strijov}&#10;&#10;\titel{\tit}{\aut}{\autkol}{\titkol}&#10;&#10;\vspace*{-9pt}&#10;&#10;\noindent&#10;$^1$Moscow Institute of Physics and Technology,&#10;9 Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian\linebreak&#10;$\hphantom{^1}$Federation&#10;&#10;\noindent&#10;$^2$Dorodnicyn Computing Center, Russian Academy of Sciences,&#10;40~Vavilov Str.,  Moscow 119333, Russian\linebreak&#10;$\hphantom{^1}$Federation&#10;&#10;&#10;\def\leftfootline{\small{\textbf{\thepage}&#10;\hfill INFORMATIKA I EE PRIMENENIYA~--- INFORMATICS AND&#10;APPLICATIONS\ \ \ 2015\ \ \ volume~9\ \ \ issue\ 1}&#10;}%&#10; \def\rightfootline{\small{INFORMATIKA I EE PRIMENENIYA~---&#10;INFORMATICS AND APPLICATIONS\ \ \ 2015\ \ \ volume~9\ \ \ issue\ 1&#10;\hfill \textbf{\thepage}}}&#10;&#10;\vspace*{3pt}&#10;&#10;&#10;\Abste{The paper solves the problem of selecting optimal stable models&#10;for classification of physical activity. Each type of physical activity of&#10;a~particular person is described by a~set of features generated from an&#10;accelerometer time series. In conditions of feature&amp;apos;s multicollinearity,&#10;selection of stable models is hampered by the need to evaluate a~large number&#10;of parameters of these models. Evaluation of optimal parameter values&#10;is also difficult due to the fact that the error function has a~large number&#10;of local minima in the parameter space. In the paper, the optimal models from&#10;the class of two-layer artificial neural networks are chosen. The problem&#10;of finding the Pareto optimal front of the set of models is solved.&#10;The paper presents a~stepwise strategy of building optimal stable models.&#10;The strategy includes steps of deleting and adding parameters, criteria of&#10; pruning and growing the model and criteria of breaking the process of building.&#10;The computational experiment compares the models generated by the proposed strategy&#10;on three quality criteria~--- complexity, accuracy, and stability.}&#10;&#10;\KWE{classification; artificial neural networks; complexity;&#10;accuracy; stability; Pareto efficiency; growing and pruning criteria}&#10;&#10;&#10;\DOI{10.14357/19922264150107}&#10;&#10;\Ack&#10;\noindent&#10;The research was supported by Skolkovo Institute of Science and Technology&#10;(Skoltech) in the frame of SkolTech/MITInitiative.&#10;&#10;&#10;%\vspace*{3pt}&#10;&#10;  \begin{multicols}{2}&#10;&#10;\renewcommand{\bibname}{\protect\rmfamily References}&#10;%\renewcommand{\bibname}{\large\protect\rm References}&#10;&#10;&#10;&#10;{\small\frenchspacing&#10; {%\baselineskip=10.8pt&#10; \addcontentsline{toc}{section}{References}&#10; \begin{thebibliography}{99}&#10;&#10;\bibitem{Vizilter2012learnong-1}&#10;\Aue{Vizilter,~Y., V.~Gorbatcevich, S.~Karateev, and N.~Kostromov.}&#10;2012.&#10;Obuchenie algoritmov vydeleniya kozhi na tsvetnykh izobrazheniyakh lits&#10;[Teaching of skin extraction algorithms for human face color images].&#10;\textit{Informatika i~ee Primeneniya}~--- \textit{Inform. Appl.} 6(1):109--113.&#10;&#10;&#10;&#10;\bibitem{Tokmakova2012HyperPar-1} %3&#10;\Aue{Tokmakova,~A.\,A., and V.\,V.~Strizhov}.&#10;2012. Otsenivanie giperparametrov lineynykh i regressionnykh modeley&#10;pri otbore shumovykh i korreliruyushchikh priznakov&#10;[Estimation of linear model hyperparameters for noise or correlated&#10;feature selection poblem].&#10;\textit{Informatika i~ee Primeneniya}~--- \textit{Inform. Appl.} 6(4):66--75.&#10;&#10;\bibitem{Haplanov2013assimptothic-1} %2&#10;  \Aue{Khaplanov, A.\,Yu.}&#10;2013. Asimptoticheskaya normal&amp;apos;nost&amp;apos; otsenki parametrov mnogomernoy&#10;logisticheskoy regressii&#10;[Asymptotic normality of the estimation of the multivariate logistic&#10;regression]. \textit{Informatika i~ee Primeneniya}~--- \textit{Inform. Appl.} 7(2):69--74.&#10;&#10;&#10;\bibitem{Myung2000Complexity-1} %4&#10;\Aue{Myung,~I.\,J.} 2000. The&#10;importance of complexity in model selection.&#10;\textit{J.~Math. Psychol.} 44(1):190--204.&#10;&#10;\bibitem{MacLeod2001Grow-1} %5&#10;\Aue{MacLeod,~C., and M.~Maxwell}. 2001.&#10;Incremental evolution in ANNs: Neural nets which grow.&#10;\textit{Artif. Intell. Rev.} 16(3):201--224.&#10;\bibitem{Karnin1990Simple-1} %6&#10;\Aue{Karnin,~E.\,D.} 1990. A~simple procedure for pruning back-propagation&#10;trained neural networks. \textit{IEEE Trans. Neural Networks} 1(2):239--242.&#10;\bibitem{LeCun1990Optimal-1} %7&#10;\Aue{LeCun,~Y., L.\,S.~Denker, and S.\,A.~Solla}.&#10;1990. Optimal brain damage. \textit{Adv. Neur. Inform. Processing&#10;Syst.} 2(2):598--605.&#10;&#10;\bibitem{Hassibi93-1} %8&#10;\Aue{Hassibi,~B., D.\,G.~Stork, and G.\,J.~Woff}.&#10;1993. Optimal brain surgeon and general network pruning.&#10;\textit{IEEE Conference (International) on Neural Networks Proceedings}. 293--299.&#10;&#10;\bibitem{Han2011Water-1} %9&#10;\Aue{Hong-Gui,~H., C.~Qi-li, and Q.~Jun-Fei}. 2011.&#10;An efficient self-organizing {RBF} neural network for water quality prediction.&#10;\textit{Neural Networks} 24(7):717--725.&#10;&#10;\bibitem{Yang2012PruningAlgorithm-1} %10&#10;\Aue{Yang,~S., and Y.~Chen}.&#10;2012. An evolutionary constructive and pruning algorithm for artificial&#10;neural networks and its prediction applications.&#10;\textit{Neurocomputing} 86(1):140--149.&#10;{\looseness=1&#10;&#10;}&#10;&#10;\bibitem{Pu2013PruningAlgorithm-1} %11&#10;\Aue{Pu,~X., and P.~Pengfei-Sun}.&#10;2013. A~new hybrid pruning neural network algorithm based on sensitivity analysis&#10;for stock market forcast. \textit{J.~Inform. Comput. Sci.} 3(1):883--892.&#10;{\looseness=1&#10;&#10;}&#10;&#10;\bibitem{Knerr1990Stepwise-1} %12&#10;\Aue{Knerr,~S., L.~Personnaz, and G.~Dreyfus}.&#10;1990. Single-layer learning revisited: A~stepwise procedure for building&#10; and training a neural network.&#10;\textit{Neurocomputing Algorithms Architectures Applications} 68(1):41--50.&#10;&#10;&#10;&#10;\bibitem{Strijov2013Evidence-1} %13&#10;\Aue{Strijov,~V., E.~Krymova, and S.~Weber}.&#10;2013. Evidence optimization for consequently generated models.&#10;\textit{Math. Comput. Modell.} 57(1-2):50--56.&#10;&#10;\bibitem{Leonteva2012Feature-1} %14&#10;\Aue{Leont&amp;apos;eva,~L.\,N.}&#10;2012. Posledovatel&amp;apos;nyy vybor priznakov pri vosstanovlenii regressii&#10;[Feature selection in autoregression forecasting].&#10;\textit{J.~Machine Learning Data Analysis} 1(3):335--346.&#10;&#10;\bibitem{Zaycev2012Evaluation-1} %15&#10;\Aue{Zaytsev,~A.\,A., and A.\,A.~Tokmakova}.&#10;2012. Otsenka giperparametrov lineynykh regressionnykh modeley&#10;metodom maksimal&amp;apos;nogo pravdopodobiya pri otbore shumovykh&#10;i~korreliruyushchikh priznakov&#10;[Estimation regression model hyperparameters using&#10;    maximum likelihood]. \textit{J.~Machine Learning Data Analysis} 1(3):347--353.&#10;&#10;\columnbreak&#10;&#10;\bibitem{Kwapisz2010Activity-1} %16&#10;\Aue{Kwapisz,~J.\,R., G.\,M.~Weiss, and S.~Moore}.&#10;2010. Activity recognition using cell phone accelerometers.&#10;\textit{SIGKDD Explorations} 12(2):74--82.&#10;\bibitem{Belsley2005-1} %17&#10;\Aue{Belsley,~D.\,A., E.~Kuh, R.\,E.~Welsch}.&#10;2005. \textit{Regression diagnostics: Identifying influential data and sources of&#10;collinearity}. New York: John Wiley and Sons. 302~p.&#10;\bibitem{Sandulyanu2012Feature-1} %18&#10;\Aue{Sanduljanu,~L.\,N., and V.\,V.~Strizhov}.&#10;2012. Vybor priznakov v avtoregressionnykh zadachakh prognozirovaniya&#10;[Feature selection in autoregression forecasting].&#10;\textit{Information Technologies} 7:11--15.&#10;\bibitem{StrategyCode-1} %19&#10;\Aue{Popova,~M.\,S.}&#10;2014. Realizatsiya strategii poshagovoy modifikatsii neyronnoy seti&#10;[Realization of a~stepwise strategy for neural network modification].&#10;Available at: {\sf&#10;http://sourceforge.net/p/mlalgorithms/code/HEAD/&#10;tree/Group174/Popova2014OptimalModelSelection/\linebreak code/main.m} (accessed February~10, 2015).&#10;\end{thebibliography}&#10;&#10; }&#10; }&#10;&#10;\end{multicols}&#10;&#10;\vspace*{-3pt}&#10;&#10;\hfill{\small\textit{Received August 10, 2014}}&#10;&#10;%\vspace*{-18pt}&#10;&#10;&#10;\Contr&#10;&#10;\noindent&#10;\textbf{Popova Maria S.} (b.\ 1994)~--- student, Moscow Institute of Physics and Technology,&#10;9 Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian Federation;&#10;maria\_popova@phystech.edu&#10;&#10;\vspace*{3pt}&#10;&#10;\noindent&#10;\textbf{Strijov Vadim V.} (b.\ 1967)~--- Candidate of science (PhD)&#10;in physics and mathematics; associate professor,&#10;Moscow Institute of Physics and Technology,&#10;9 Institutskiy Per., Dolgoprudny, Moscow Region 141700, Russian Federation;&#10;leading scientist, Dorodnicyn Computing Center, Russian Academy of Sciences,&#10;40~Vavilov Str.,  Moscow 119333, Russian Federation;  strijov@ccas.com&#10;&#10;\label{end\stat}&#10;&#10;\renewcommand{\bibname}{\protect\rm Литература}"/>
    </fulltext>
  </paper>
</papers>